[
  {
    "title": "Explicit Memory through Online 3D Gaussian Splatting Improves\n  Class-Agnostic Video Segmentation",
    "authors": [
      "Anthony Opipari",
      "Aravindhan K Krishnan",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Remembering where object segments were predicted in the past is useful for improving the accuracy and consistency of class-agnostic video segmentation algorithms. Existing video segmentation algorithms typically use either no object-level memory (e.g. FastSAM) or they use implicit memories in the form of recurrent neural network features (e.g. SAM2). In this paper, we augment both types of segmentation models using an explicit 3D memory and show that the resulting models have more accurate and consistent predictions. For this, we develop an online 3D Gaussian Splatting (3DGS) technique to store predicted object-level segments generated throughout the duration of a video. Based on this 3DGS representation, a set of fusion techniques are developed, named FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve their respective foundation models' predictions. Ablation experiments are used to validate the proposed techniques' design and hyperparameter settings. Results from both real-world and simulated benchmarking experiments show that models which use explicit 3D memories result in more accurate and consistent predictions than those which use no memory or only implicit neural network memories. Project Page: https://topipari.com/projects/FastSAM-Splat/",
    "arxiv_url": "http://arxiv.org/abs/2510.23521v1",
    "pdf_url": "http://arxiv.org/pdf/2510.23521v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic\n  Reconstruction",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Jiaxin Guo",
      "Ziyan Chen",
      "Sihang Zhao",
      "Zhuoxiao Li",
      "Zhe Tang",
      "Baoru Huang",
      "Limin Yu"
    ],
    "abstract": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from endoscopic video is vital for downstream tasks and improved outcomes. However, endoscopic scenarios present unique challenges, including photometric inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most 3DGS-based methods that rely solely on appearance constraints for optimizing 3DGS are often insufficient in this context, as these dynamic visual artifacts can mislead the optimization process and lead to inaccurate reconstructions. To address these limitations, we present EndoWave, a unified spatio-temporal Gaussian Splatting framework by incorporating an optical flow-based geometric constraint and a multi-resolution rational wavelet supervision. First, we adopt a unified spatio-temporal Gaussian representation that directly optimizes primitives in a 4D domain. Second, we propose a geometric constraint derived from optical flow to enhance temporal coherence and effectively constrain the 3D structure of the scene. Third, we propose a multi-resolution rational orthogonal wavelet as a constraint, which can effectively separate the details of the endoscope and enhance the rendering performance. Extensive evaluations on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our method EndoWave achieves state-of-the-art reconstruction quality and visual accuracy compared to the baseline method.",
    "arxiv_url": "http://arxiv.org/abs/2510.23087v1",
    "pdf_url": "http://arxiv.org/pdf/2510.23087v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "dynamic",
      "3d reconstruction",
      "ar",
      "gaussian splatting",
      "nerf",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and\n  Method",
    "authors": [
      "Bohan Li",
      "Xin Jin",
      "Hu Zhu",
      "Hongsi Liu",
      "Ruikai Li",
      "Jiazhe Guo",
      "Kaiwen Cai",
      "Chao Ma",
      "Yueming Jin",
      "Hao Zhao",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "abstract": "Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
    "arxiv_url": "http://arxiv.org/abs/2510.22973v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22973v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "high-fidelity",
      "dynamic",
      "autonomous driving",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained\n  Feature Compression",
    "authors": [
      "Pranav Saxena"
    ],
    "abstract": "Modeling open-vocabulary language fields in 3D is essential for intuitive human-AI interaction and querying within physical environments. State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting to efficiently construct these language fields, encoding features distilled from high-dimensional models like CLIP. However, this efficiency is currently offset by the requirement to train a scene-specific language autoencoder for feature compression, introducing a costly, per-scene optimization bottleneck that hinders deployment scalability. In this work, we introduce Gen-LangSplat, that eliminates this requirement by replacing the scene-wise autoencoder with a generalized autoencoder, pre-trained extensively on the large-scale ScanNet dataset. This architectural shift enables the use of a fixed, compact latent space for language features across any new scene without any scene-specific training. By removing this dependency, our entire language field construction process achieves a efficiency boost while delivering querying performance comparable to, or exceeding, the original LangSplat method. To validate our design choice, we perform a thorough ablation study empirically determining the optimal latent embedding dimension and quantifying representational fidelity using Mean Squared Error and cosine similarity between the original and reprojected 512-dimensional CLIP embeddings. Our results demonstrate that generalized embeddings can efficiently and accurately support open-vocabulary querying in novel 3D scenes, paving the way for scalable, real-time interactive 3D AI applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.22930v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22930v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting\n  Data",
    "authors": [
      "Shashank N. Sridhara",
      "Birendra Kathariya",
      "Fangjun Pu",
      "Peng Yin",
      "Eduardo Pavez",
      "Antonio Ortega"
    ],
    "abstract": "We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for novel view synthesis, the size of trained models limits its deployment in bandwidth-constrained applications such as volumetric media streaming. To address this, we propose a learned hierarchical latent representation that builds upon the principles of \"overfitted\" learned image compression (e.g., Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS data have irregular spatial distributions of Gaussians (geometry) and consist of multiple attributes (signals) defined on the irregular geometry. Our codec is designed to account for these differences between images and 3DGS. Specifically, we leverage the octree structure of the voxelized 3DGS geometry to obtain a hierarchical multi-resolution representation. Our approach overfits latents to each Gaussian attribute under a global rate constraint. These latents are decoded independently through a lightweight decoder network. To estimate the bitrate during training, we employ an autoregressive probability model that leverages octree-derived contexts from the 3D point structure. The multi-resolution latents, decoder, and autoregressive entropy coding networks are jointly optimized for each Gaussian attribute. Experiments demonstrate that the proposed RALHE compression framework achieves a rendering PSNR gain of up to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS compression methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.22812v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22812v1",
    "published_date": "2025-10-26",
    "categories": [
      "eess.IV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "compression",
      "3d gaussian",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and\n  Communication",
    "authors": [
      "Yujie Wan",
      "Chenxuan Liu",
      "Shuai Wang",
      "Tong Zhang",
      "James Jianqiao Yu",
      "Kejiang Ye",
      "Dusit Niyato",
      "Chengzhong Xu"
    ],
    "abstract": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost devices. To address this issue, we present edge collaborative GS (ECO-GS), where each user can switch between a local small GS model to guarantee timeliness and a remote large GS model to guarantee fidelity. However, deciding how to engage the large GS model is nontrivial, due to the interdependency between rendering requirements and resource conditions. To this end, we propose integrated rendering and communication (IRAC), which jointly optimizes collaboration status (i.e., deciding whether to engage large GS) and edge power allocation (i.e., enabling remote rendering) under communication constraints across different users by minimizing a newly-derived GS switching function. Despite the nonconvexity of the problem, we propose an efficient penalty majorization minimization (PMM) algorithm to obtain the critical point solution. Furthermore, we develop an imitation learning optimization (ILO) algorithm, which reduces the computational time by over 100x compared to PMM. Experiments demonstrate the superiority of PMM and the real-time execution capability of ILO.",
    "arxiv_url": "http://arxiv.org/abs/2510.22718v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22718v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.IT",
      "cs.CV",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical\n  Explicit-Implicit Representation Collaboration Rendering",
    "authors": [
      "Wenkai Zhu",
      "Xu Li",
      "Qimin Xu",
      "Benwu Wang",
      "Kun Wei",
      "Yiming Peng",
      "Zihang Wang"
    ],
    "abstract": "3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human chain-of-thought process for information seeking, we introduce a hierarchical collaborative representation module that facilitates mutual reinforcement for mapping optimization, effectively mitigating scale drift and enhancing reconstruction robustness. Furthermore, to effectively eliminate the influence of dynamic objects, we propose a joint dynamic modeling module that generates fine-grained dynamic masks by fusing open-world segmentation with implicit residual constraints, guided by uncertainty estimates from DINO-Depth features. Extensive evaluations on KITTI, nuScenes, and self-collected datasets demonstrate that our approach achieves state-of-the-art performance compared to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.22669v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22669v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting",
      "human",
      "mapping",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and\n  Low-light Environment Resilience",
    "authors": [
      "Huilin Yin",
      "Zhaolin Yang",
      "Linchuan Zhang",
      "Gerhard Rigoll",
      "Johannes Betz"
    ],
    "abstract": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely constrained in environments where visual inputs suffer from noise and low illumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM frameworks achieve high-fidelity mapping under clean conditions, they remain vulnerable to compounded degradations that degrade mapping and tracking performance. A key observation underlying our work is that the original 3DGS rendering pipeline inherently behaves as an implicit low-pass filter, attenuating high-frequency noise but also risking over-smoothing. Building on this insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for noise and low-light resilience. The framework integrates three innovations: a Structure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples rendered appearance, depth, and edge cues; an adaptive tracking objective with residual balancing regularization; and a Contrastive Language-Image Pretraining (CLIP)-based enhancement module, selectively activated under compounded degradations to restore semantic and structural fidelity. Comprehensive experiments on Replica, TUM, and real-world sequences show that RoGER-SLAM consistently improves trajectory accuracy and reconstruction quality compared with other 3DGS-SLAM systems, especially under adverse imaging conditions.",
    "arxiv_url": "http://arxiv.org/abs/2510.22600v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22600v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "semantic",
      "illumination",
      "high-fidelity",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting",
      "localization",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose\n  Alignment Loss",
    "authors": [
      "Jing Yang",
      "Yufeng Yang"
    ],
    "abstract": "Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.",
    "arxiv_url": "http://arxiv.org/abs/2510.22473v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22473v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "motion",
      "animation",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum",
    "authors": [
      "Yaokun Li",
      "Lihe Ding",
      "Xiao Chen",
      "Guang Tan",
      "Tianfan Xue"
    ],
    "abstract": "Generating dynamic and interactive 3D objects, such as trees, has wide applications in virtual reality, games, and world simulation. Nevertheless, existing methods still face various challenges in generating realistic 4D motion for complex real trees. In this paper, we propose DynamicTree, the first framework that can generate long-term, interactive animation of 3D Gaussian Splatting trees. Unlike prior optimization-based methods, our approach generates dynamics in a fast feed-forward manner. The key success of our approach is the use of a compact sparse voxel spectrum to represent the tree movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline first generates mesh motion using the sparse voxel spectrum and then binds Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum can also serve as a basis for fast modal analysis under external forces, allowing real-time interactive responses. To train our model, we also introduce 4DTree, the first large-scale synthetic 4D tree dataset containing 8,786 animated tree meshes with semantic labels and 100-frame motion sequences. Extensive experiments demonstrate that our method achieves realistic and responsive tree animations, significantly outperforming existing approaches in both visual quality and computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2510.22213v1",
    "pdf_url": "http://arxiv.org/pdf/2510.22213v1",
    "published_date": "2025-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "fast",
      "motion",
      "animation",
      "face",
      "dynamic",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "authors": [
      "Bingchen Miao",
      "Rong Wei",
      "Zhiqi Ge",
      "Xiaoquan sun",
      "Shiqi Gao",
      "Jingzhe Zhu",
      "Renhan Wang",
      "Siliang Tang",
      "Jun Xiao",
      "Rui Tang",
      "Juncheng Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. The data and code will be available soon.",
    "arxiv_url": "http://arxiv.org/abs/2510.21307v1",
    "pdf_url": "http://arxiv.org/pdf/2510.21307v1",
    "published_date": "2025-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "real-time rendering",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic\n  Manipulation",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "abstract": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2510.20813v1",
    "pdf_url": "http://arxiv.org/pdf/2510.20813v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "robotics",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous\n  Parking",
    "authors": [
      "Zixuan Wu",
      "Hengyuan Zhang",
      "Ting-Hsuan Chen",
      "Yuliang Guo",
      "David Paz",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.",
    "arxiv_url": "http://arxiv.org/abs/2510.20335v1",
    "pdf_url": "http://arxiv.org/pdf/2510.20335v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "lighting",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation",
    "authors": [
      "Runsong Zhu",
      "Ka-Hei Hui",
      "Zhengzhe Liu",
      "Qianyi Wu",
      "Weiliang Tang",
      "Shi Qiu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Open-vocabulary 3D segmentation is a fundamental yet challenging task, requiring a mutual understanding of both segmentation and language. However, existing Gaussian-splatting-based methods rely either on a single 3D language field, leading to inferior segmentation, or on pre-computed class-agnostic segmentations, suffering from error accumulation. To address these limitations, we present COS3D, a new collaborative prompt-segmentation framework that contributes to effectively integrating complementary language and segmentation cues throughout its entire pipeline. We first introduce the new concept of collaborative field, comprising an instance field and a language field, as the cornerstone for collaboration. During training, to effectively construct the collaborative field, our key idea is to capture the intrinsic relationship between the instance field and language field, through a novel instance-to-language feature mapping and designing an efficient two-stage training strategy. During inference, to bridge distinct characteristics of the two fields, we further design an adaptive language-to-instance prompt refinement, promoting high-quality prompt-segmentation inference. Extensive experiments not only demonstrate COS3D's leading performance over existing methods on two widely-used benchmarks but also show its high potential to various applications,~\\ie, novel image-based 3D segmentation, hierarchical segmentation, and robotics. The code is publicly available at \\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.",
    "arxiv_url": "http://arxiv.org/abs/2510.20238v1",
    "pdf_url": "http://arxiv.org/pdf/2510.20238v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "robotics",
      "understanding",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from\n  Out-of-Distribution Camera Poses",
    "authors": [
      "Damian Bowness",
      "Charalambos Poullis"
    ],
    "abstract": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.   To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints.   Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning.   Code and results at https://damian-bowness.github.io/EV3DGS",
    "arxiv_url": "http://arxiv.org/abs/2510.20027v1",
    "pdf_url": "http://arxiv.org/pdf/2510.20027v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
    "authors": [
      "Mingrui Zhao",
      "Sauradip Nag",
      "Kai Wang",
      "Aditya Vora",
      "Guangda Ji",
      "Peter Chun",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ],
    "abstract": "We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:https://mingrui-zhao.github.io/4DRep-GMI/",
    "arxiv_url": "http://arxiv.org/abs/2510.19255v1",
    "pdf_url": "http://arxiv.org/pdf/2510.19255v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "fast",
      "motion",
      "survey",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
    "authors": [
      "In-Hwan Jin",
      "Hyeongju Mun",
      "Joonsoo Kim",
      "Kugjin Yun",
      "Kyeongbo Kong"
    ],
    "abstract": "Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency. Video demonstrations are available at https://anonymous.4open.science/w/MoE-GS-68BA/.",
    "arxiv_url": "http://arxiv.org/abs/2510.19210v1",
    "pdf_url": "http://arxiv.org/pdf/2510.19210v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "ar",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis",
    "authors": [
      "Matteo Bortolon",
      "Nuno Ferreira Duarte",
      "Plinio Moreno",
      "Fabio Poiesi",
      "José Santos-Victor",
      "Alessio Del Bue"
    ],
    "abstract": "Achieving dexterous robotic grasping with multi-fingered hands remains a significant challenge. While existing methods rely on complete 3D scans to predict grasp poses, these approaches face limitations due to the difficulty of acquiring high-quality 3D data in real-world scenarios. In this paper, we introduce GRASPLAT, a novel grasping framework that leverages consistent 3D information while being trained solely on RGB images. Our key insight is that by synthesizing physically plausible images of a hand grasping an object, we can regress the corresponding hand joints for a successful grasp. To achieve this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of real hand-object interactions, enabling end-to-end training with RGB data. Unlike prior methods, our approach incorporates a photometric loss that refines grasp predictions by minimizing discrepancies between rendered and real images. We conduct extensive experiments on both synthetic and real-world grasping datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9% over existing image-based methods. Project page: https://mbortolon97.github.io/grasplat/",
    "arxiv_url": "http://arxiv.org/abs/2510.19200v1",
    "pdf_url": "http://arxiv.org/pdf/2510.19200v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Moving Light Adaptive Colonoscopy Reconstruction via\n  Illumination-Attenuation-Aware 3D Gaussian Splatting",
    "authors": [
      "Hao Wang",
      "Ying Zhou",
      "Haoyu Zhao",
      "Rui Wang",
      "Qiang Hu",
      "Xing Zhang",
      "Qiang Li",
      "Zhiwei Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
    "arxiv_url": "http://arxiv.org/abs/2510.18739v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18739v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "tracking",
      "illumination",
      "3d reconstruction",
      "3d gaussian",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Re-Activating Frozen Primitives for 3D Gaussian Splatting",
    "authors": [
      "Yuxin Cheng",
      "Binxiao Huang",
      "Wenyong Zhou",
      "Taiqiang Wu",
      "Zhengwu Liu",
      "Graziano Chesi",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) achieves real-time photorealistic novel view synthesis, yet struggles with complex scenes due to over-reconstruction artifacts, manifesting as local blurring and needle-shape distortions. While recent approaches attribute these issues to insufficient splitting of large-scale Gaussians, we identify two fundamental limitations: gradient magnitude dilution during densification and the primitive frozen phenomenon, where essential Gaussian densification is inhibited in complex regions while suboptimally scaled Gaussians become trapped in local optima. To address these challenges, we introduce ReAct-GS, a method founded on the principle of re-activation. Our approach features: (1) an importance-aware densification criterion incorporating $\\alpha$-blending weights from multiple viewpoints to re-activate stalled primitive growth in complex regions, and (2) a re-activation mechanism that revitalizes frozen primitives through adaptive parameter perturbations. Comprehensive experiments across diverse real-world datasets demonstrate that ReAct-GS effectively eliminates over-reconstruction artifacts and achieves state-of-the-art performance on standard novel view synthesis metrics while preserving intricate geometric details. Additionally, our re-activation mechanism yields consistent improvements when integrated with other 3D-GS variants such as Pixel-GS, demonstrating its broad applicability.",
    "arxiv_url": "http://arxiv.org/abs/2510.19653v1",
    "pdf_url": "http://arxiv.org/pdf/2510.19653v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos",
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Mi Zhou",
      "Jinwen Chen",
      "Dan Xu"
    ],
    "abstract": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
    "arxiv_url": "http://arxiv.org/abs/2510.18489v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18489v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "4d",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with\n  Context-aware Cross-view Fusion",
    "authors": [
      "Tianyu Huang",
      "Runnan Chen",
      "Dongting Hu",
      "Fengming Huang",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "Understanding 3D scenes is pivotal for autonomous driving, robotics, and augmented reality. Recent semantic Gaussian Splatting approaches leverage large-scale 2D vision models to project 2D semantic features onto 3D scenes. However, they suffer from two major limitations: (1) insufficient contextual cues for individual masks during preprocessing and (2) inconsistencies and missing details when fusing multi-view features from these 2D models. In this paper, we introduce \\textbf{OpenInsGaussian}, an \\textbf{Open}-vocabulary \\textbf{Ins}tance \\textbf{Gaussian} segmentation framework with Context-aware Cross-view Fusion. Our method consists of two modules: Context-Aware Feature Extraction, which augments each mask with rich semantic context, and Attention-Driven Feature Aggregation, which selectively fuses multi-view features to mitigate alignment errors and incompleteness. Through extensive experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art results in open-vocabulary 3D Gaussian segmentation, outperforming existing baselines by a large margin. These findings underscore the robustness and generality of our proposed approach, marking a significant step forward in 3D scene understanding and its practical deployment across diverse real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2510.18253v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18253v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "3d gaussian",
      "autonomous driving",
      "robotics",
      "ar",
      "understanding",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications",
    "authors": [
      "Vitor Pereira Matias",
      "Daniel Perazzo",
      "Vinicius Silva",
      "Alberto Raposo",
      "Luiz Velho",
      "Afonso Paiva",
      "Tiago Novello"
    ],
    "abstract": "The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.",
    "arxiv_url": "http://arxiv.org/abs/2510.18101v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18101v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV",
      "68-01",
      "A.1"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "lighting",
      "survey",
      "animation",
      "face",
      "3d reconstruction",
      "3d gaussian",
      "efficient rendering",
      "ar",
      "efficient",
      "gaussian splatting",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HouseTour: A Virtual Real Estate A(I)gent",
    "authors": [
      "Ata Çelen",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ],
    "abstract": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.",
    "arxiv_url": "http://arxiv.org/abs/2510.18054v1",
    "pdf_url": "http://arxiv.org/pdf/2510.18054v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant\n  Structures with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Chung Min Kim",
      "Justin Kerr",
      "Shuangyu Xie",
      "Prithvi Akella",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Ken Goldberg"
    ],
    "abstract": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
    "arxiv_url": "http://arxiv.org/abs/2510.17783v1",
    "pdf_url": "http://arxiv.org/pdf/2510.17783v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop\n  Conditions",
    "authors": [
      "Zhiqiang Teng",
      "Beibei Lin",
      "Tingting Chen",
      "Zifeng Yuan",
      "Xuanyi Li",
      "Xuanyu Zhang",
      "Shunli Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
    "arxiv_url": "http://arxiv.org/abs/2510.17719v1",
    "pdf_url": "http://arxiv.org/pdf/2510.17719v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Initialize to Generalize: A Stronger Initialization Pipeline for\n  Sparse-View 3DGS",
    "authors": [
      "Feng Zhou",
      "Wenkai Guo",
      "Pu Cao",
      "Zhicheng Zhang",
      "Jianqin Yin"
    ],
    "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering. Prior work addresses it either by enhancing the initialization (\\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization. Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost. Given initialization's primacy, we focus our design there. Although SfM performs poorly under sparse views due to its reliance on feature matching, it still provides reliable seed points. Thus, building on SfM, our effort aims to supplement the regions it fails to cover as comprehensively as possible. Specifically, we design: (i) frequency-aware SfM that improves low-texture coverage via low-frequency view augmentation and relaxed multi-view correspondences; (ii) 3DGS self-initialization that lifts photometric supervision into additional points, compensating SfM-sparse regions with learned Gaussian centers; and (iii) point-cloud regularization that enforces multi-view consistency and uniform spatial coverage through simple geometric/visibility priors, yielding a clean and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate consistent gains in sparse-view settings, establishing our approach as a stronger initialization strategy. Code is available at https://github.com/zss171999645/ItG-GS.",
    "arxiv_url": "http://arxiv.org/abs/2510.17479v1",
    "pdf_url": "http://arxiv.org/pdf/2510.17479v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "sparse view",
      "ar",
      "gaussian splatting",
      "nerf",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured\n  Representation",
    "authors": [
      "Ruitong Gan",
      "Junran Peng",
      "Yang Liu",
      "Chuanchen Luo",
      "Qing Li",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Planes are fundamental primitives of 3D sences, especially in man-made environments such as indoor spaces and urban streets. Representing these planes in a structured and parameterized format facilitates scene editing and physical simulations in downstream applications. Recently, Gaussian Splatting (GS) has demonstrated remarkable effectiveness in the Novel View Synthesis task, with extensions showing great potential in accurate surface reconstruction. However, even state-of-the-art GS representations often struggle to reconstruct planar regions with sufficient smoothness and precision. To address this issue, we propose GSPlane, which recovers accurate geometry and produces clean and well-structured mesh connectivity for plane regions in the reconstructed scene. By leveraging off-the-shelf segmentation and normal prediction models, GSPlane extracts robust planar priors to establish structured representations for planar Gaussian coordinates, which help guide the training process by enforcing geometric consistency. To further enhance training robustness, a Dynamic Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians with persistently high gradients as non-planar, ensuring more reliable optimization. Furthermore, we utilize the optimized planar priors to refine the mesh layouts, significantly improving topological structure while reducing the number of vertices and faces. We also explore applications of the structured planar representation, which enable decoupling and flexible manipulation of objects on supportive planes. Extensive experiments demonstrate that, with no sacrifice in rendering quality, the introduction of planar priors significantly improves the geometric accuracy of the extracted meshes across various baselines.",
    "arxiv_url": "http://arxiv.org/abs/2510.17095v1",
    "pdf_url": "http://arxiv.org/pdf/2510.17095v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "segmentation",
      "face",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian\n  Splatting",
    "authors": [
      "Haofan Ren",
      "Qingsong Yan",
      "Ming Lu",
      "Rongfeng Lu",
      "Zunjie Zhu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced neural fields, as it enables high-fidelity rendering with impressive visual quality. However, 3DGS has difficulty accurately representing surfaces. In contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian disks. Despite advancements in geometric fidelity, rendering quality remains compromised, highlighting the challenge of achieving both high-quality rendering and precise geometric structures. This indicates that optimizing both geometric and rendering quality in a single training stage is currently unfeasible. To overcome this limitation, we present 2DGS-R, a new method that uses a hierarchical training approach to improve rendering quality while maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians with the normal consistency regularization. Then 2DGS-R selects the 2D Gaussians with inadequate rendering quality and applies a novel in-place cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R model with opacity frozen. Experimental results show that compared to the original 2DGS, our method requires only 1\\% more storage and minimal additional training time. Despite this negligible overhead, it achieves high-quality rendering results while preserving fine geometric structures. These findings indicate that our approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2510.16837v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16837v1",
    "published_date": "2025-10-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D\n  Avatars",
    "authors": [
      "Haocheng Tang",
      "Ruoke Yan",
      "Xinhui Yin",
      "Qi Zhang",
      "Xinfeng Zhang",
      "Siwei Ma",
      "Wen Gao",
      "Chuanmin Jia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast, photorealistic rendering of dynamic 3D scenes, showing strong potential in immersive communication. However, in digital human encoding and transmission, the compression methods based on general 3DGS representations are limited by the lack of human priors, resulting in suboptimal bitrate efficiency and reconstruction quality at the decoder side, which hinders their application in streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical Gaussian Compression framework designed for efficient transmission and high-quality rendering of dynamic avatars. Our method disentangles the Gaussian representation into a structural layer, which maps poses to Gaussians via a StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model to represent temporal pose variations compactly and semantically. This hierarchical design supports layer-wise compression, progressive decoding, and controllable rendering from diverse pose inputs such as video sequences or text. Since people are most concerned with facial realism, we incorporate a facial attention mechanism during StyleUNet training to preserve identity and expression details under low-bitrate constraints. Experimental results demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar rendering, while significantly outperforming prior methods in both visual quality and compression efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2510.16463v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16463v1",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "fast",
      "compression",
      "human",
      "dynamic",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation\n  and Editing on Gaussian Splatting",
    "authors": [
      "Changyue Shi",
      "Minghao Chen",
      "Yiping Mao",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Jiajun Ding",
      "Zhou Yu"
    ],
    "abstract": "Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.",
    "arxiv_url": "http://arxiv.org/abs/2510.16410v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16410v1",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "3d gaussian",
      "robotics",
      "ar",
      "understanding",
      "gaussian splatting",
      "localization",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proactive Scene Decomposition and Reconstruction",
    "authors": [
      "Baicheng Li",
      "Zike Yan",
      "Dong Wu",
      "Hongbin Zha"
    ],
    "abstract": "Human behaviors are the major causes of scene dynamics and inherently contain rich cues regarding the dynamics. This paper formalizes a new task of proactive scene decomposition and reconstruction, an online approach that leverages human-object interactions to iteratively disassemble and reconstruct the environment. By observing these intentional interactions, we can dynamically refine the decomposition and reconstruction process, addressing inherent ambiguities in static object-level reconstruction. The proposed system effectively integrates multiple tasks in dynamic environments such as accurate camera and object pose estimation, instance decomposition, and online map updating, capitalizing on cues from human-object interactions in egocentric live streams for a flexible, progressive alternative to conventional object-level reconstruction methods. Aided by the Gaussian splatting technique, accurate and consistent dynamic scene modeling is achieved with photorealistic and efficient rendering. The efficacy is validated in multiple real-world scenarios with promising advantages.",
    "arxiv_url": "http://arxiv.org/abs/2510.16272v1",
    "pdf_url": "http://arxiv.org/pdf/2510.16272v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient rendering",
      "ar",
      "efficient",
      "gaussian splatting",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object\n  Reconstruction",
    "authors": [
      "Ting-Yu Yen",
      "Yu-Sheng Chiu",
      "Shih-Hsuan Hung",
      "Peter Wonka",
      "Hung-Kuo Chu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality, real-time novel-view synthesis from multi-view images. However, most existing methods assume the object is captured in a single, static pose, resulting in incomplete reconstructions that miss occluded or self-occluded regions. We introduce PFGS, a pose-aware 3DGS framework that addresses the practical challenge of reconstructing complete objects from multi-pose image captures. Given images of an object in one main pose and several auxiliary poses, PFGS iteratively fuses each auxiliary set into a unified 3DGS representation of the main pose. Our pose-aware fusion strategy combines global and local registration to merge views effectively and refine the 3DGS model. While recent advances in 3D foundation models have improved registration robustness and efficiency, they remain limited by high memory demands and suboptimal accuracy. PFGS overcomes these challenges by incorporating them more intelligently into the registration process: it leverages background features for per-pose camera pose estimation and employs foundation models for cross-pose registration. This design captures the best of both approaches while resolving background inconsistency issues. Experimental results demonstrate that PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.",
    "arxiv_url": "http://arxiv.org/abs/2510.15386v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15386v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussGym: An open-source real-to-sim framework for learning locomotion\n  from pixels",
    "authors": [
      "Alejandro Escontrela",
      "Justin Kerr",
      "Arthur Allshire",
      "Jonas Frey",
      "Rocky Duan",
      "Carmelo Sferrazza",
      "Pieter Abbeel"
    ],
    "abstract": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
    "arxiv_url": "http://arxiv.org/abs/2510.15352v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15352v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "high-fidelity",
      "3d gaussian",
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from\n  Unposed Images",
    "authors": [
      "Jiaxin Guo",
      "Tongfan Guan",
      "Wenzhen Dong",
      "Wenzhao Zheng",
      "Wenting Wang",
      "Yue Wang",
      "Yeung Yam",
      "Yun-Hui Liu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Project Page: https://wrld.github.io/SaLon3R/.",
    "arxiv_url": "http://arxiv.org/abs/2510.15072v1",
    "pdf_url": "http://arxiv.org/pdf/2510.15072v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "arxiv_url": "http://arxiv.org/abs/2510.14705v1",
    "pdf_url": "http://arxiv.org/pdf/2510.14705v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian\n  Splatting Training on GPU",
    "authors": [
      "Junyi Wu",
      "Jiaming Xu",
      "Jinhao Li",
      "Yongkang Zhou",
      "Jiayi Pan",
      "Xingyang Li",
      "Guohao Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.   To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.   Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "arxiv_url": "http://arxiv.org/abs/2510.14564v1",
    "pdf_url": "http://arxiv.org/pdf/2510.14564v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "efficient",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and\n  Geometric Filtering",
    "authors": [
      "Alexander Valverde",
      "Brian Xu",
      "Yuyin Zhou",
      "Meng Xu",
      "Hongyun Wang"
    ],
    "abstract": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "arxiv_url": "http://arxiv.org/abs/2510.14270v1",
    "pdf_url": "http://arxiv.org/pdf/2510.14270v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "lighting",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models\n  with Multi-View Performance Captures",
    "authors": [
      "Yuancheng Xu",
      "Wenqi Xian",
      "Li Ma",
      "Julien Philip",
      "Ahmet Levent Taşel",
      "Yiwei Zhao",
      "Ryan Burgert",
      "Mingming He",
      "Oliver Hermann",
      "Oliver Pilarski",
      "Rahul Garg",
      "Paul Debevec",
      "Ning Yu"
    ],
    "abstract": "We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
    "arxiv_url": "http://arxiv.org/abs/2510.14179v1",
    "pdf_url": "http://arxiv.org/pdf/2510.14179v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "lighting",
      "relighting",
      "ar",
      "efficient",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive\n  3D Object Representation",
    "authors": [
      "Jungmin Lee",
      "Seonghyuk Hong",
      "Juyong Lee",
      "Jaeyoon Lee",
      "Jongwon Choi"
    ],
    "abstract": "We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.",
    "arxiv_url": "http://arxiv.org/abs/2510.17864v1",
    "pdf_url": "http://arxiv.org/pdf/2510.17864v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "medical",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from\n  Unstructured Phone Images",
    "authors": [
      "Emanuel Garbin",
      "Guy Adam",
      "Oded Krams",
      "Zohar Barzelay",
      "Eran Guendelman",
      "Michael Schwarz",
      "Matteo Presutto",
      "Moran Vatelmacher",
      "Yigal Shenkman",
      "Eli Peker",
      "Itai Druker",
      "Uri Patish",
      "Yoav Blum",
      "Max Bluvstein",
      "Junxuan Li",
      "Rawal Khirodkar",
      "Shunsuke Saito"
    ],
    "abstract": "We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This \"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.",
    "arxiv_url": "http://arxiv.org/abs/2510.14081v3",
    "pdf_url": "http://arxiv.org/pdf/2510.14081v3",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "avatar",
      "body"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications",
    "authors": [
      "Naruya Kondo",
      "Yuto Asano",
      "Yoichi Ochiai"
    ],
    "abstract": "We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: https://gaussian-vrm.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2510.13978v2",
    "pdf_url": "http://arxiv.org/pdf/2510.13978v2",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "vr",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a\n  Video Generator",
    "authors": [
      "Hyojun Go",
      "Dominik Narnhofer",
      "Goutam Bhat",
      "Prune Truong",
      "Federico Tombari",
      "Konrad Schindler"
    ],
    "abstract": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
    "arxiv_url": "http://arxiv.org/abs/2510.13454v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13454v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "human",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering",
    "authors": [
      "Siddharth Tourani",
      "Jayaram Reddy",
      "Akash Kumbar",
      "Satyajit Tourani",
      "Nishant Goyal",
      "Madhava Krishna",
      "N. Dinesh Reddy",
      "Muhammad Haris Khan"
    ],
    "abstract": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.",
    "arxiv_url": "http://arxiv.org/abs/2510.13381v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13381v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "deformation",
      "motion",
      "segmentation",
      "dynamic",
      "3d gaussian",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client\n  Selection and Power Control",
    "authors": [
      "Zhen Li",
      "Xibin Jin",
      "Guoliang Li",
      "Shuai Wang",
      "Miaowen Wen",
      "Huseyin Arslan",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients and trains a global GS model at the edge server, is an emerging paradigm for scene reconstruction. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments unveil that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. It is found that the GS-oriented objective can be accurately predicted with low sampling ratios (e.g.,10%), and our method achieves an excellent tradeoff between view contributions and communication costs.",
    "arxiv_url": "http://arxiv.org/abs/2510.13186v1",
    "pdf_url": "http://arxiv.org/pdf/2510.13186v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "head",
      "ar",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D\n  Reconstruction",
    "authors": [
      "Fengzhi Guo",
      "Chih-Chuan Hsu",
      "Sihao Ding",
      "Cheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "arxiv_url": "http://arxiv.org/abs/2510.12768v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12768v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "motion",
      "dynamic",
      "ar",
      "efficient",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "authors": [
      "An Zhao",
      "Piaopiao Yu",
      "Zhe Zhu",
      "Mingqiang Wei"
    ],
    "abstract": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction. However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur. To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images. BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions. To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages. Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.Our source code is available at https://github.com/wsxujm/bsgs",
    "arxiv_url": "http://arxiv.org/abs/2510.12493v2",
    "pdf_url": "http://arxiv.org/pdf/2510.12493v2",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
    "authors": [
      "Mohamed Omran",
      "Farhad Zanjani",
      "Davide Abati",
      "Jens Petersen",
      "Amirhossein Habibian"
    ],
    "abstract": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
    "arxiv_url": "http://arxiv.org/abs/2510.12308v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12308v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "authors": [
      "Ying A",
      "Wenzhang Sun",
      "Chang Zeng",
      "Chunfeng Wang",
      "Hao Li",
      "Jianxun Cui"
    ],
    "abstract": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "arxiv_url": "http://arxiv.org/abs/2510.12282v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12282v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "face",
      "dynamic",
      "3d reconstruction",
      "autonomous driving",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal\n  Rendering",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jianhao Jiao",
      "Dimitrios Kanoulas",
      "Jun Ma"
    ],
    "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2510.12174v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12174v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "high-fidelity",
      "3d reconstruction",
      "3d gaussian",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in\n  Virtual Reality",
    "authors": [
      "Anastasiya Pechko",
      "Piotr Borycki",
      "Joanna Waczyńska",
      "Daniel Barczyk",
      "Agata Szymańska",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce \\our{} (\\textbf{G}aussian \\textbf{S}platting for \\textbf{V}irtual \\textbf{E}nvironment \\textbf{R}endering and \\textbf{S}cene \\textbf{E}diting), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, \\our{} facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.11878v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11878v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "face",
      "ar",
      "efficient",
      "gaussian splatting",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event\n  Streams",
    "authors": [
      "Takuya Nakabayashi",
      "Navami Kairanda",
      "Hideo Saito",
      "Vladislav Golyanik"
    ],
    "abstract": "Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "arxiv_url": "http://arxiv.org/abs/2510.11717v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11717v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation",
    "authors": [
      "Maggie Wang",
      "Stephen Tian",
      "Aiden Swann",
      "Ola Shorinwa",
      "Jiajun Wu",
      "Mac Schwager"
    ],
    "abstract": "Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ .",
    "arxiv_url": "http://arxiv.org/abs/2510.11689v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11689v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via\n  View Alignment",
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "arxiv_url": "http://arxiv.org/abs/2510.11473v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11473v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "illumination",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent\n  Material Inference",
    "authors": [
      "Wenyuan Zhang",
      "Jimin Tang",
      "Weiqi Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2510.11387v2",
    "pdf_url": "http://arxiv.org/pdf/2510.11387v2",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "reflection",
      "illumination",
      "ar",
      "ray tracing",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular\n  Videos",
    "authors": [
      "Xuankai Zhang",
      "Junjin Xiao",
      "Qing Zhang"
    ],
    "abstract": "This paper presents a unified framework that allows high-quality dynamic Gaussian Splatting from both defocused and motion-blurred monocular videos. Due to the significant difference between the formation processes of defocus blur and motion blur, existing methods are tailored for either one of them, lacking the ability to simultaneously deal with both of them. Although the two can be jointly modeled as blur kernel-based convolution, the inherent difficulty in estimating accurate blur kernels greatly limits the progress in this direction. In this work, we go a step further towards this direction. Particularly, we propose to estimate per-pixel reliable blur kernels using a blur prediction network that exploits blur-related scene and camera information and is subject to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian densification strategy to mitigate the lack of Gaussians for incomplete regions, and boost the performance of novel view synthesis by incorporating unseen view information to constrain scene optimization. Extensive experiments show that our method outperforms the state-of-the-art methods in generating photorealistic novel view synthesis from defocused and motion-blurred monocular videos. Our code is available at \\href{https://github.com/hhhddddddd/dydeblur}{\\textcolor{cyan}{https://github.com/hhhddddddd/dydeblur}}.",
    "arxiv_url": "http://arxiv.org/abs/2510.10691v2",
    "pdf_url": "http://arxiv.org/pdf/2510.10691v2",
    "published_date": "2025-10-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
    "authors": [
      "Haoyu Zhao",
      "Cheng Zeng",
      "Linghao Zhuang",
      "Yaxi Zhao",
      "Shengke Xue",
      "Hao Wang",
      "Xingyue Zhao",
      "Zhongyu Li",
      "Kehan Li",
      "Siteng Huang",
      "Mingxiu Chen",
      "Xin Li",
      "Deli Zhao",
      "Hua Zou"
    ],
    "abstract": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
    "arxiv_url": "http://arxiv.org/abs/2510.10637v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10637v1",
    "published_date": "2025-10-12",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided\n  Framework",
    "authors": [
      "Shanzhi Yin",
      "Bolin Chen",
      "Xinju Wu",
      "Ru-Ling Liao",
      "Jie Chen",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "abstract": "This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.10492v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10492v1",
    "published_date": "2025-10-12",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM",
      "I.4; I.5"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "human",
      "dynamic",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "avatar",
      "body"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Opacity-Gradient Driven Density Control for Compact and Efficient\n  Few-Shot 3D Gaussian Splatting",
    "authors": [
      "Abdelrhman Elrawy",
      "Emad A. Mohammed"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions. While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count. This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency. We replace the standard positional gradient heuristic with a novel densification trigger that uses the opacity gradient as a lightweight proxy for rendering error. We find this aggressive densification is only effective when paired with a more conservative pruning schedule, which prevents destructive optimization cycles. Combined with a standard depth-correlation loss for geometric guidance, our framework demonstrates a fundamental improvement in efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a reduction of approximately 70%. This dramatic gain in compactness is achieved with a modest trade-off in reconstruction metrics, establishing a new state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2510.10257v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10257v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "lightweight",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Color3D: Controllable and Consistent 3D Colorization with Personalized\n  Colorizer",
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Renlong Wu",
      "Wangmeng Zuo"
    ],
    "abstract": "In this work, we present Color3D, a highly adaptable framework for colorizing both static and dynamic 3D scenes from monochromatic inputs, delivering visually diverse and chromatically vibrant reconstructions with flexible user-guided control. In contrast to existing methods that focus solely on static scenarios and enforce multi-view consistency by averaging color variations which inevitably sacrifice both chromatic richness and controllability, our approach is able to preserve color diversity and steerability while ensuring cross-view and cross-time consistency. In particular, the core insight of our method is to colorize only a single key view and then fine-tune a personalized colorizer to propagate its color to novel views and time steps. Through personalization, the colorizer learns a scene-specific deterministic color mapping underlying the reference view, enabling it to consistently project corresponding colors to the content in novel views and video frames via its inherent inductive bias. Once trained, the personalized colorizer can be applied to infer consistent chrominance for all other images, enabling direct reconstruction of colorful 3D scenes with a dedicated Lab color space Gaussian splatting representation. The proposed framework ingeniously recasts complicated 3D colorization as a more tractable single image paradigm, allowing seamless integration of arbitrary image colorization models with enhanced flexibility and controllability. Extensive experiments across diverse static and dynamic 3D colorization benchmarks substantiate that our method can deliver more consistent and chromatically rich renderings with precise user control. Project Page https://yecongwan.github.io/Color3D/.",
    "arxiv_url": "http://arxiv.org/abs/2510.10152v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10152v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "mapping",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian\n  Splatting",
    "authors": [
      "Jiahui Lu",
      "Haihong Xiao",
      "Xueyan Zhao",
      "Wenxiong Kang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage. These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient. To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images. Unlike prior works that rely on COLMAP for sparse point cloud initialization, we leverage the VGGT foundation model to obtain more reliable initial poses and dense point clouds. Our approach integrates several key innovations: 1) a hybrid Gaussian representation with dual position-shape optimization enhanced by inter-view matching consistency; 2) a graph-guided attribute refinement module to enhance scene details; and 3) flow-based depth regularization that improves depth estimation accuracy for more effective supervision. Comprehensive quantitative and qualitative experiments demonstrate that our approach achieves more robust performance on both forward-facing and large-scale complex datasets compared to other pose-free methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.10097v2",
    "pdf_url": "http://arxiv.org/pdf/2510.10097v2",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "P-4DGS: Predictive 4D Gaussian Splatting with 90$\\times$ Compression",
    "authors": [
      "Henan Wang",
      "Hanxin Zhu",
      "Xinliang Gong",
      "Tianyu He",
      "Xin Li",
      "Zhibo Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has garnered significant attention due to its superior scene representation fidelity and real-time rendering performance, especially for dynamic 3D scene reconstruction (\\textit{i.e.}, 4D reconstruction). However, despite achieving promising results, most existing algorithms overlook the substantial temporal and spatial redundancies inherent in dynamic scenes, leading to prohibitive memory consumption. To address this, we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene modeling. Inspired by intra- and inter-frame prediction techniques commonly used in video compression, we first design a 3D anchor point-based spatial-temporal prediction module to fully exploit the spatial-temporal correlations across different 3D Gaussian primitives. Subsequently, we employ an adaptive quantization strategy combined with context-based entropy coding to further reduce the size of the 3D anchor points, thereby achieving enhanced compression efficiency. To evaluate the rate-distortion performance of our proposed P-4DGS in comparison with other dynamic 3DGS representations, we conduct extensive experiments on both synthetic and real-world datasets. Experimental results demonstrate that our approach achieves state-of-the-art reconstruction quality and the fastest rendering speed, with a remarkably low storage footprint (around \\textbf{1MB} on average), achieving up to \\textbf{40$\\times$} and \\textbf{90$\\times$} compression on synthetic and real-world scenes, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2510.10030v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10030v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "fast",
      "compression",
      "dynamic",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting",
    "authors": [
      "Zhigang Cheng",
      "Mingchao Sun",
      "Yu Liu",
      "Zengye Ge",
      "Luyang Tang",
      "Mu Xu",
      "Yangyan Li",
      "Peng Pan"
    ],
    "abstract": "Level of Detail (LoD) is a fundamental technique in real-time computer graphics for managing the rendering costs of complex scenes while preserving visual fidelity. Traditionally, LoD is implemented using discrete levels (DLoD), where multiple, distinct versions of a model are swapped out at different distances. This long-standing paradigm, however, suffers from two major drawbacks: it requires significant storage for multiple model copies and causes jarring visual ``popping\" artifacts during transitions, degrading the user experience. We argue that the explicit, primitive-based nature of the emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm: Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality scaling within a single, unified model, thereby circumventing the core problems of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a continuous LoD mechanism directly into a 3DGS representation. Our method introduces a learnable, distance-dependent decay parameter for each Gaussian primitive, which dynamically adjusts its opacity based on viewpoint proximity. This allows for the progressive and smooth filtering of less significant primitives, effectively creating a continuous spectrum of detail within one model. To train this model to be robust across all distances, we introduce a virtual distance scaling mechanism and a novel coarse-to-fine training strategy with rendered point count regularization. Our approach not only eliminates the storage overhead and visual artifacts of discrete methods but also reduces the primitive count and memory footprint of the final model. Extensive experiments demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a single model, delivering high-fidelity results across a wide range of performance targets.",
    "arxiv_url": "http://arxiv.org/abs/2510.09997v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09997v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene\n  Mapping",
    "authors": [
      "Yicheng He",
      "Jingwen Yu",
      "Guangcheng Chen",
      "Hong Zhang"
    ],
    "abstract": "Maintaining an up-to-date map that accurately reflects recent changes in the environment is crucial, especially for robots that repeatedly traverse the same space. Failing to promptly update the changed regions can degrade map quality, resulting in poor localization, inefficient operations, and even lost robots. 3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online map reconstruction due to its dense, differentiable, and photorealistic properties, yet accurately and efficiently updating the regions of change remains a challenge. In this paper, we propose VG-Mapping, a novel online 3DGS-based mapping system tailored for such semi-static scenes. Our approach introduces a hybrid representation that augments 3DGS with a TSDF-based voxel map to efficiently identify changed regions in a scene, along with a variation-aware density control strategy that inserts or deletes Gaussian primitives in regions undergoing change. Furthermore, to address the absence of public benchmarks for this task, we construct a RGB-D dataset comprising both synthetic and real-world semi-static environments. Experimental results demonstrate that our method substantially improves the rendering quality and map update efficiency in semi-static scenes. The code and dataset are available at https://github.com/heyicheng-never/VG-Mapping.",
    "arxiv_url": "http://arxiv.org/abs/2510.09962v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09962v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "localization",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates",
    "authors": [
      "Minkwan Kim",
      "Seungmin Lee",
      "Junho Kim",
      "Young Min Kim"
    ],
    "abstract": "Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.",
    "arxiv_url": "http://arxiv.org/abs/2510.09881v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09881v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "few-shot",
      "face",
      "ar",
      "efficient",
      "gaussian splatting",
      "sparse view",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision Language Models: A Survey of 26K Papers",
    "authors": [
      "Fengming Lin"
    ],
    "abstract": "We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.",
    "arxiv_url": "http://arxiv.org/abs/2510.09586v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09586v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "understanding",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "nerf",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
    "authors": [
      "Arthur Bizzi",
      "Matias Grynberg",
      "Vitor Matias",
      "Daniel Perazzo",
      "João Paulo Lima",
      "Luiz Velho",
      "Nuno Gonçalves",
      "João Pereira",
      "Guilherme Schardong",
      "Tiago Novello"
    ],
    "abstract": "Morphing is a long-standing problem in vision and computer graphics, requiring a time-dependent warping for feature alignment and a blending for smooth interpolation. Recently, multilayer perceptrons (MLPs) have been explored as implicit neural representations (INRs) for modeling such deformations, due to their meshlessness and differentiability; however, extracting coherent and accurate morphings from standard MLPs typically relies on costly regularizations, which often lead to unstable training and prevent effective feature alignment. To overcome these limitations, we propose FLOWING (FLOW morphING), a framework that recasts warping as the construction of a differential vector flow, naturally ensuring continuity, invertibility, and temporal coherence by encoding structural flow properties directly into the network architectures. This flow-centric approach yields principled and stable transformations, enabling accurate and structure-preserving morphing of both 2D images and 3D shapes. Extensive experiments across a range of applications - including face and image morphing, as well as Gaussian Splatting morphing - show that FLOWING achieves state-of-the-art morphing quality with faster convergence. Code and pretrained models are available at http://schardong.github.io/flowing.",
    "arxiv_url": "http://arxiv.org/abs/2510.09537v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09537v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV",
      "I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "fast",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Two-Stage Gaussian Splatting Optimization for Outdoor Scene\n  Reconstruction",
    "authors": [
      "Deborah Pintani",
      "Ariel Caputo",
      "Noah Lewis",
      "Marc Stamminger",
      "Fabio Pellacini",
      "Andrea Giachetti"
    ],
    "abstract": "Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis. In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes. In stage two, foreground Gaussians are initialized from a Structure-from-Motion reconstruction, added and refined using the standard rendering loss, while the background set remains fixed but contributes to the final image formation. Experiments on diverse outdoor datasets show that our method reduces background artifacts and improves perceptual quality compared to state-of-the-art baselines. Moreover, the explicit background separation enables automatic, object-free environment map estimation, opening new possibilities for photorealistic outdoor rendering and mixed-reality applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.09489v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09489v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "illumination",
      "ar",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic\n  Urban Scenes",
    "authors": [
      "Yikang Zhang",
      "Rui Fan"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive performance in synthesizing high-fidelity novel views. Nonetheless, its effectiveness critically depends on the quality of the initialized point cloud. Specifically, achieving uniform and complete point coverage over the underlying scene structure requires overlapping observation frustums, an assumption that is often violated in unbounded, dynamic urban environments. Training Gaussian models with partially initialized point clouds often leads to distortions and artifacts, as camera rays may fail to intersect valid surfaces, resulting in incorrect gradient propagation to Gaussian primitives associated with occluded or invisible geometry. Additionally, existing densification strategies simply clone and split Gaussian primitives from existing ones, incapable of reconstructing missing structures. To address these limitations, we propose VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban scenes. Our method identifies unreliable geometry structures via voxel-based visibility reasoning, selects informative supporting views through diversity-aware view selection, and recovers missing structures via patch matching-based multi-view stereo reconstruction. This design enables the generation of new Gaussian primitives guided by reliable geometric priors, even in regions lacking initial points. Extensive experiments on the Waymo and nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS approaches and significantly improves the quality of reconstructed geometry for both static and dynamic objects. Source code will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2510.09364v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09364v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "face",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "authors": [
      "Haofei Xu",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08575v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08575v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and\n  Accurate Sparse-View Reconstruction",
    "authors": [
      "Meixi Song",
      "Xin Lin",
      "Dizhe Zhang",
      "Haodong Li",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08566v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08566v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "authors": [
      "Xilong Zhou",
      "Bao-Huy Nguyen",
      "Loïc Magne",
      "Vladislav Golyanik",
      "Thomas Leimkühler",
      "Christian Theobalt"
    ],
    "abstract": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08491v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08491v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ray marching",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D\n  Gaussian Splatting",
    "authors": [
      "Ankit Gahlawat",
      "Anirban Mukherjee",
      "Dinesh Babu Jayagopi"
    ],
    "abstract": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.",
    "arxiv_url": "http://arxiv.org/abs/2510.08096v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08096v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "segmentation",
      "face",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "human",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
    "authors": [
      "Tianrui Zhang",
      "Yichen Liu",
      "Zilin Guo",
      "Yuxin Guo",
      "Jingcheng Ni",
      "Chenjing Ding",
      "Dan Xu",
      "Lewei Lu",
      "Zehuan Wu"
    ],
    "abstract": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.",
    "arxiv_url": "http://arxiv.org/abs/2510.07944v2",
    "pdf_url": "http://arxiv.org/pdf/2510.07944v2",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "autonomous driving",
      "ar",
      "understanding",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale\n  3D Gaussian Splatting",
    "authors": [
      "Houqiang Zhong",
      "Zhenglong Wu",
      "Sihua Fu",
      "Zihan Zheng",
      "Xin Jin",
      "Xiaoyun Zhang",
      "Li Song",
      "Qiang Hu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.",
    "arxiv_url": "http://arxiv.org/abs/2510.07830v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07830v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event\n  Stream",
    "authors": [
      "Junhao He",
      "Jiaxu Wang",
      "Jia Li",
      "Mingyuan Sun",
      "Qiang Zhang",
      "Jiahang Cao",
      "Ziyi Zhang",
      "Yi Gu",
      "Jingkai Sun",
      "Renjing Xu"
    ],
    "abstract": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "arxiv_url": "http://arxiv.org/abs/2510.07752v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07752v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "deformation",
      "motion",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral\n  Probes",
    "authors": [
      "Jian Gao",
      "Mengqi Yuan",
      "Yifei Zeng",
      "Chang Zeng",
      "Zhihao Li",
      "Zhenyu Chen",
      "Weichao Qiu",
      "Xiao-Xiao Long",
      "Hao Zhu",
      "Xun Cao",
      "Yao Yao"
    ],
    "abstract": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.",
    "arxiv_url": "http://arxiv.org/abs/2510.07729v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07729v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "real-time rendering",
      "lighting",
      "efficient",
      "face",
      "ar",
      "ray tracing",
      "gaussian splatting",
      "shadow",
      "relightable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
    "authors": [
      "Huanning Dong",
      "Fan Li",
      "Ping Kuang",
      "Jianwen Min"
    ],
    "abstract": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.",
    "arxiv_url": "http://arxiv.org/abs/2510.06967v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06967v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "high-fidelity",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with\n  Gaussian Splatting in Unity",
    "authors": [
      "Islomjon Shukhratov",
      "Sergey Gorinsky"
    ],
    "abstract": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.",
    "arxiv_url": "http://arxiv.org/abs/2510.06802v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06802v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
    "authors": [
      "Jipeng Lyu",
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ],
    "abstract": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "arxiv_url": "http://arxiv.org/abs/2510.06694v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06694v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "deformation",
      "segmentation",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy\n  Reduction",
    "authors": [
      "Leshu Li",
      "Jiayin Qin",
      "Jie Peng",
      "Zishen Wan",
      "Huaizhi Qu",
      "Ye Han",
      "Pingqing Zheng",
      "Hongsen Zhang",
      "Yu Cao",
      "Tianlong Chen",
      "Yang Katie Zhao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping (SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering efficiency and accuracy, but have not yet been adopted in resource-constrained edge devices due to insufficient speed. Addressing this, we identify notable redundancies across the SLAM pipeline for acceleration. While conceptually straightforward, practical approaches are required to minimize the overhead associated with identifying and eliminating these redundancies. In response, we propose RTGS, an algorithm-hardware co-design framework that comprehensively reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline. On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to remove the redundant Gaussians by reusing gradients computed during backpropagation; and (2) a dynamic downsampling technique that directly reuses the keyframe identification and alpha computing steps to eliminate redundant pixels. On the hardware side, we propose (1) a subtile-level streaming strategy and a pixel-level pairwise scheduling strategy that mitigates workload imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates the rendering backpropagation by reusing intermediate data computed during rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory accesses caused by atomic operations while enabling pipelined aggregation. Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on four datasets and three algorithms, with up to 82.5x energy efficiency over the baseline and negligible quality loss. Code is available at https://github.com/UMN-ZhaoLab/RTGS.",
    "arxiv_url": "http://arxiv.org/abs/2510.06644v2",
    "pdf_url": "http://arxiv.org/pdf/2510.06644v2",
    "published_date": "2025-10-08",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "dynamic",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting",
      "localization",
      "mapping",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning",
    "authors": [
      "Amirhossein Mollaei Khass",
      "Guangyi Liu",
      "Vivek Pandey",
      "Wen Jiang",
      "Boshu Lei",
      "Kostas Daniilidis",
      "Nader Motee"
    ],
    "abstract": "Safe navigation in uncertain environments requires planning methods that integrate risk aversion with active perception. In this work, we present a unified framework that refines a coarse reference path by constructing tail-sensitive risk maps from Average Value-at-Risk statistics on an online-updated 3D Gaussian-splat Radiance Field. These maps enable the generation of locally safe and feasible trajectories. In parallel, we formulate Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose manifold, where Riemannian gradient descent maximizes an expected information gain objective to reduce uncertainty most critical for imminent motion. Our approach advances the state-of-the-art by coupling risk-averse path refinement with NBV planning, while introducing scalable gradient decompositions that support efficient online updates in complex environments. We demonstrate the effectiveness of the proposed framework through extensive computational studies.",
    "arxiv_url": "http://arxiv.org/abs/2510.06481v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06481v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "motion",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head\n  Avatars",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.",
    "arxiv_url": "http://arxiv.org/abs/2510.05488v1",
    "pdf_url": "http://arxiv.org/pdf/2510.05488v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "dynamic",
      "3d gaussian",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "avatar",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized Minimal 4D Gaussian Splatting",
    "authors": [
      "Minseo Lee",
      "Byeonghyeon Lee",
      "Lucas Yunkyu Lee",
      "Eunsoo Lee",
      "Sangmin Kim",
      "Seunghyeon Song",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Jaesik Park",
      "Eunbyung Park"
    ],
    "abstract": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.",
    "arxiv_url": "http://arxiv.org/abs/2510.03857v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03857v1",
    "published_date": "2025-10-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "motion",
      "compression",
      "high-fidelity",
      "dynamic",
      "face",
      "ar",
      "compact",
      "gaussian splatting",
      "4d",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches",
    "authors": [
      "Sixten Norelius",
      "Aaron O. Feldman",
      "Mac Schwager"
    ],
    "abstract": "We propose SketchPlan, a diffusion-based planner that interprets 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation. SketchPlan comprises two components: a SketchAdapter that learns to map the human sketches to projected 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and a first person view depth image. Our model achieves zero-shot sim-to-real transfer, generating accurate and safe flight paths in previously unseen real-world environments. To train the model, we build a synthetic dataset of 32k flight paths using a diverse set of photorealistic 3D Gaussian Splatting scenes. We automatically label the data by computing 2D projections of the 3D flight paths onto the camera plane, and use this to train the DiffPath diffusion model. However, since real human 2D sketches differ significantly from ideal 2D projections, we additionally label 872 of the 3D flight paths with real human sketches and use this to train the SketchAdapter to infer the 2D projection from the human sketch. We demonstrate SketchPlan's effectiveness in both simulated and real-world experiments, and show through ablations that training on a mix of human labeled and auto-labeled data together with a modular design significantly boosts its capabilities to correctly interpret human intent and infer 3D paths. In real-world drone tests, SketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen high-clutter environments, outperforming key ablations by 20-60\\% in task completion.",
    "arxiv_url": "http://arxiv.org/abs/2510.03545v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03545v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled\n  Fields",
    "authors": [
      "Zhiting Mei",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.",
    "arxiv_url": "http://arxiv.org/abs/2510.03104v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03104v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient\n  Gaussian Deformation",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.08587v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08587v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "animation",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian\n  Splatting",
    "authors": [
      "Xinran Zhang",
      "Hanqi Zhu",
      "Yifan Duan",
      "Yanyong Zhang"
    ],
    "abstract": "Constructing and sharing 3D maps is essential for many applications, including autonomous driving and augmented reality. Recently, 3D Gaussian splatting has emerged as a promising approach for accurate 3D reconstruction. However, a practical map-sharing system that features high-fidelity, continuous updates, and network efficiency remains elusive. To address these challenges, we introduce GS-Share, a photorealistic map-sharing system with a compact representation. The core of GS-Share includes anchor-based global map construction, virtual-image-based map enhancement, and incremental map update. We evaluate GS-Share against state-of-the-art methods, demonstrating that our system achieves higher fidelity, particularly for extrapolated views, with improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively. Furthermore, GS-Share is significantly more compact, reducing map transmission overhead by 36%.",
    "arxiv_url": "http://arxiv.org/abs/2510.02884v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02884v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d reconstruction",
      "3d gaussian",
      "autonomous driving",
      "ar",
      "compact",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FSFSplatter: Build Surface and Novel Views with Sparse-Views within 2min",
    "authors": [
      "Yibin Zhao",
      "Yihan Pan",
      "Jun Nan",
      "Liwei Chen",
      "Jianjun Yi"
    ],
    "abstract": "Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU, Replica, and BlendedMVS datasets.",
    "arxiv_url": "http://arxiv.org/abs/2510.02691v2",
    "pdf_url": "http://arxiv.org/pdf/2510.02691v2",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "fast",
      "face",
      "ar",
      "head",
      "gaussian splatting",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D\n  Gaussian Splatting",
    "authors": [
      "Sung-Yeon Park",
      "Adam Lee",
      "Juanwu Lu",
      "Can Cui",
      "Luyang Jiang",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ahmadreza Moradipari",
      "Ziran Wang"
    ],
    "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
    "arxiv_url": "http://arxiv.org/abs/2510.02469v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02469v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "efficient",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ],
    "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
    "arxiv_url": "http://arxiv.org/abs/2510.02314v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02314v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Performance-Guided Refinement for Visual Aerial Navigation using\n  Editable Gaussian Splatting in FalconGym 2.0",
    "authors": [
      "Yan Miao",
      "Ege Yuceel",
      "Georgios Fainekos",
      "Bardh Hoxha",
      "Hideki Okamoto",
      "Sayan Mitra"
    ],
    "abstract": "Visual policy design is crucial for aerial navigation. However, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes. We develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds. Leveraging FalconGym 2.0's editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy's training on challenging tracks while iteratively improving its performance. Across two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations. Finally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
    "arxiv_url": "http://arxiv.org/abs/2510.02248v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02248v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy\n  Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.02069v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02069v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "neural rendering",
      "reflection",
      "illumination",
      "lighting",
      "face",
      "dynamic",
      "relighting",
      "ar",
      "gaussian splatting",
      "relightable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object\n  Morphing",
    "authors": [
      "Mengtian Li",
      "Yunshu Bai",
      "Yimin Chu",
      "Yijun Shen",
      "Zhongmei Li",
      "Weifeng Ge",
      "Zhifeng Xie",
      "Chaofeng Chen"
    ],
    "abstract": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2510.02034v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02034v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "deformation",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
    "authors": [
      "Lei Liu",
      "Can Wang",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2510.01991v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01991v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "face",
      "ar",
      "understanding",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting",
    "authors": [
      "Quoc-Anh Bui",
      "Gilles Rougeron",
      "Géraldine Morin",
      "Simone Gasparini"
    ],
    "abstract": "We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\\approx 17\\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.01978v2",
    "pdf_url": "http://arxiv.org/pdf/2510.01978v2",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV",
      "68U05, 68T45 (Primary) 68T07, 68-04 (Secondary)",
      "I.2.10; I.3.3; I.3.5; I.3.7; I.4.5; I.4.6; I.4.8; I.4.10"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for\n  Mobile Robotics",
    "authors": [
      "Diram Tabaa",
      "Gianni Di Caro"
    ],
    "abstract": "Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.",
    "arxiv_url": "http://arxiv.org/abs/2510.01848v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01848v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "localization",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for\n  Large-Scale Scene Reconstruction",
    "authors": [
      "Sheng-Hsiang Hung",
      "Ting-Yu Yen",
      "Wei-Fang Sun",
      "Simon See",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2510.01767v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01767v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "3d gaussian",
      "head",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust\n  Physics-Based Dynamics",
    "authors": [
      "Changmin Lee",
      "Jihyun Lee",
      "Tae-Kyun Kim"
    ],
    "abstract": "While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/",
    "arxiv_url": "http://arxiv.org/abs/2510.01619v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01619v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "animation",
      "human",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "ar",
      "shadow",
      "gaussian splatting",
      "avatar",
      "body"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Beta Splatting",
    "authors": [
      "Rong Liu",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meida Chen",
      "Van Nguyen Nguyen",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Yue Wang",
      "Andrew Feng",
      "Ziyan Wu"
    ],
    "abstract": "We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at https://rongliu-leo.github.io/universal-beta-splatting/.",
    "arxiv_url": "http://arxiv.org/abs/2510.03312v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03312v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "real-time rendering",
      "face",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HART: Human Aligned Reconstruction Transformer",
    "authors": [
      "Xiyi Chen",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Taewon Kang",
      "Sergey Prokudin",
      "Ming Lin"
    ],
    "abstract": "We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2509.26621v1",
    "pdf_url": "http://arxiv.org/pdf/2509.26621v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "human",
      "body",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts",
    "authors": [
      "Zhenyu Shu",
      "Junlong Yu",
      "Kai Chao",
      "Shiqing Xin",
      "Ligang Liu"
    ],
    "abstract": "This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.26055v1",
    "pdf_url": "http://arxiv.org/pdf/2509.26055v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting\n  Kernels",
    "authors": [
      "Yi Hu",
      "Huiyang Zhou"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.",
    "arxiv_url": "http://arxiv.org/abs/2509.25626v2",
    "pdf_url": "http://arxiv.org/pdf/2509.25626v2",
    "published_date": "2025-09-30",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianLens: Localized High-Resolution Reconstruction via On-Demand\n  Gaussian Densification",
    "authors": [
      "Yijia Weng",
      "Zhicheng Wang",
      "Songyou Peng",
      "Saining Xie",
      "Howard Zhou",
      "Leonidas J. Guibas"
    ],
    "abstract": "We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\\times1024$ resolution.",
    "arxiv_url": "http://arxiv.org/abs/2509.25603v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25603v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
    "arxiv_url": "http://arxiv.org/abs/2509.25122v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25122v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "fast",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "vr",
      "nerf",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM\n  Reconstruction",
    "authors": [
      "Huaizhi Qu",
      "Xiao Wang",
      "Gengwei Zhang",
      "Jie Peng",
      "Tianlong Chen"
    ],
    "abstract": "Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.",
    "arxiv_url": "http://arxiv.org/abs/2509.25075v2",
    "pdf_url": "http://arxiv.org/pdf/2509.25075v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "nerf",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
    "authors": [
      "Tooba Imtiaz",
      "Lucy Chai",
      "Kathryn Heal",
      "Xuan Luo",
      "Jungyeon Park",
      "Jennifer Dy",
      "John Flynn"
    ],
    "abstract": "Large transformer models are proving to be a powerful tool for 3D vision and novel view synthesis. However, the standard Transformer's well-known quadratic complexity makes it difficult to scale these methods to large scenes. To address this challenge, we propose the Local View Transformer (LVT), a large-scale scene reconstruction and novel view synthesis architecture that circumvents the need for the quadratic attention operation. Motivated by the insight that spatially nearby views provide more useful signal about the local scene composition than distant views, our model processes all information in a local neighborhood around each view. To attend to tokens in nearby views, we leverage a novel positional encoding that conditions on the relative geometric transformation between the query and nearby views. We decode the output of our model into a 3D Gaussian Splat scene representation that includes both color and opacity view-dependence. Taken together, the Local View Transformer enables reconstruction of arbitrarily large, high-resolution scenes in a single forward pass. See our project page for results and interactive demos https://toobaimt.github.io/lvt/.",
    "arxiv_url": "http://arxiv.org/abs/2509.25001v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25001v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "large scene",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping",
    "authors": [
      "Yu Ma",
      "Guoliang Wei",
      "Haihong Xiao",
      "Yue Cheng"
    ],
    "abstract": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge in 3D reconstruction, where limited multi-view constraints lead to severe overfitting, geometric distortion, and fragmented scenes. While 3D Gaussian Splatting (3DGS) delivers real-time, high-fidelity rendering, its performance drastically deteriorates under sparse inputs, plagued by floating artifacts and structural failures. To address these challenges, we introduce HBSplat, a unified framework that elevates 3DGS by seamlessly integrating robust structural cues, virtual view constraints, and occluded region completion. Our core contributions are threefold: a Hybrid-Loss Depth Estimation module that ensures multi-view consistency by leveraging dense matching priors and integrating reprojection, point propagation, and smoothness constraints; a Bidirectional Warping Virtual View Synthesis method that enforces substantially stronger constraints by creating high-fidelity virtual views through bidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware Reconstruction component that recovers occluded areas using a depth-difference mask and a learning-based inpainting model. Extensive evaluations on LLFF, Blender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time inference. Code is available at: https://github.com/eternalland/HBSplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.24893v3",
    "pdf_url": "http://arxiv.org/pdf/2509.24893v3",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
    "authors": [
      "Jiaqi Chen",
      "Xinhao Ji",
      "Yuanyuan Gao",
      "Hao Li",
      "Yuning Gong",
      "Yifei Liu",
      "Dan Xu",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun"
    ],
    "abstract": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce ExGS, a novel feed-forward framework that unifies Universal Gaussian Compression (UGC) with GaussPainter for Extreme 3DGS compression. UGC performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas GaussPainter leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over 100X compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at: https://github.com/chenttt2001/ExGS",
    "arxiv_url": "http://arxiv.org/abs/2509.24758v4",
    "pdf_url": "http://arxiv.org/pdf/2509.24758v4",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "compression",
      "3d gaussian",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh",
    "authors": [
      "Yuanyuan Gao",
      "Yuning Gong",
      "Yifei Liu",
      "Li Jingfeng",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Yanci Zhang",
      "Dan Xu",
      "Xiao Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2509.24421v2",
    "pdf_url": "http://arxiv.org/pdf/2509.24421v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for\n  Robust Scene-Level Surface Reconstruction",
    "authors": [
      "Yuhang Cao",
      "Haojun Yan",
      "Danya Yao"
    ],
    "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2509.24308v1",
    "pdf_url": "http://arxiv.org/pdf/2509.24308v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "neural rendering",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting",
    "authors": [
      "Dragoş-Andrei Chileban",
      "Andrei-Ştefan Bulzan",
      "Cosmin Cernǎzanu-Glǎvan"
    ],
    "abstract": "Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments. However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage. Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views. In this work we introduce an automatic car damage detection pipeline that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we propose a simple yet effective learning-free approach for single-view 3D-GS segmentation. Specifically, Gaussians are projected onto the image plane using camera parameters obtained via Structure from Motion (SfM). They are then filtered through an algorithm that utilizes Z-buffering along with a normal distribution model of depth and opacities. Through experiments we found that this method is particularly effective for challenging scenarios like car damage detection, where target objects (e.g., scratches, small dents) may only be clearly visible in a single view, making multi-view consistency approaches impractical or impossible. The code is publicly available at: https://github.com/DragosChileban/CrashSplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.23947v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23947v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "segmentation",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene\n  Representations",
    "authors": [
      "Javed Ahmad",
      "Penggang Gao",
      "Donatien Delehelle",
      "Mennuti Canio",
      "Nikhil Deshpande",
      "Jesús Ortiz",
      "Darwin G. Caldwell",
      "Yonas Teodros Tefera"
    ],
    "abstract": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.23555v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23555v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "fast",
      "survey",
      "3d gaussian",
      "slam",
      "understanding",
      "ar",
      "efficient",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual\n  Videos",
    "authors": [
      "Junyi Wu",
      "Jiachen Tao",
      "Haoxuan Wang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ],
    "abstract": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2509.23492v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23492v1",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "deformation",
      "motion",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
    "authors": [
      "Atakan Topaloglu",
      "Kunyi Li",
      "Michael Niemeyer",
      "Nassir Navab",
      "A. Murat Tekalp",
      "Federico Tombari"
    ],
    "abstract": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
    "arxiv_url": "http://arxiv.org/abs/2509.23258v2",
    "pdf_url": "http://arxiv.org/pdf/2509.23258v2",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "sparse view",
      "ar",
      "gaussian splatting",
      "nerf",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Unified Representation of 3D Gaussian Splatting",
    "authors": [
      "Yuelin Xin",
      "Yuheng Liu",
      "Xiaohui Xie",
      "Xinke Li"
    ],
    "abstract": "A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2509.22917v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22917v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "I.4.10"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-Language Alignment from Compressed Image Representations using 2D\n  Gaussian Splatting",
    "authors": [
      "Yasmine Omri",
      "Connor Ding",
      "Tsachy Weissman",
      "Thierry Tambe"
    ],
    "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.",
    "arxiv_url": "http://arxiv.org/abs/2509.22615v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22615v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "fast",
      "efficient",
      "ar",
      "lightweight",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material\n  Decomposition",
    "authors": [
      "Dinh Minh Nguyen",
      "Malte Avenhaus",
      "Thomas Lindemeier"
    ],
    "abstract": "We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.22276v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22276v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting",
    "authors": [
      "Jiayu Ding",
      "Xinpeng Liu",
      "Zhiyi Pan",
      "Shiqiang Long",
      "Ge Li"
    ],
    "abstract": "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.",
    "arxiv_url": "http://arxiv.org/abs/2509.22225v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22225v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "understanding",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Material Gaussian Model for Relightable 3D Generation",
    "authors": [
      "Jingrui Ye",
      "Lingting Zhu",
      "Runze Zhang",
      "Zeyu Hu",
      "Yingda Yin",
      "Lanjiong Li",
      "Lequan Yu",
      "Qingmin Liao"
    ],
    "abstract": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
    "arxiv_url": "http://arxiv.org/abs/2509.22112v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22112v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "3d gaussian",
      "relighting",
      "ar",
      "efficient",
      "gaussian splatting",
      "relightable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drag4D: Align Your Motion with Text-Driven 3D Scene Generation",
    "authors": [
      "Minjun Kang",
      "Inkyu Shin",
      "Taeyeop Lee",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ],
    "abstract": "We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.",
    "arxiv_url": "http://arxiv.org/abs/2509.21888v1",
    "pdf_url": "http://arxiv.org/pdf/2509.21888v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d reconstruction",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Novel View Synthesis in High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Zhipeng Xiong",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2509.21853v2",
    "pdf_url": "http://arxiv.org/pdf/2509.21853v2",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in\n  Power-Constrained XR Systems",
    "authors": [
      "Weikai Lin",
      "Sushant Kondguli",
      "Carl Marshall",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.",
    "arxiv_url": "http://arxiv.org/abs/2509.21702v1",
    "pdf_url": "http://arxiv.org/pdf/2509.21702v1",
    "published_date": "2025-09-25",
    "categories": [
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian splatting holography",
    "authors": [
      "Shuhe Zhang",
      "Liangcai Cao"
    ],
    "abstract": "In-line holography offers high space-bandwidth product imaging with a simplified lens-free optical system. However, in-line holographic reconstruction is troubled by twin images arising from the Hermitian symmetry of complex fields. Twin images disrupt the reconstruction in solving the ill-posed phase retrieval problem. The known parameters are less than the unknown parameters, causing phase ambiguities. State-of-the-art deep-learning or non-learning methods face challenges in balancing data fidelity with twin-image disturbance. We propose the Gaussian splatting holography (GSH) for twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting for optical field representation and compresses the number of unknown parameters by a maximum of 15 folds, transforming the original ill-posed phase retrieval into a well-posed one with reduced phase ambiguities. Additionally, the Gaussian splatting tends to form sharp patterns rather than those with noisy twin-image backgrounds as each Gaussian has a spatially slow-varying profile. Experiments show that GSH achieves constraint-free recovery for in-line holography with accuracy comparable to state-of-the-art constraint-based methods, with an average peak signal-to-noise ratio equal to 26 dB, and structure similarity equal to 0.8. Combined with total variation, GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB, and a high compression ability of up to 15 folds.",
    "arxiv_url": "http://arxiv.org/abs/2509.20774v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20774v1",
    "published_date": "2025-09-25",
    "categories": [
      "physics.optics",
      "math.OC",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "face",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Driving Scene Generation With Stereo Forcing",
    "authors": [
      "Hao Lu",
      "Zhuang Ma",
      "Guangfeng Jiang",
      "Wenhang Ge",
      "Bohan Li",
      "Yuzhan Cai",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Yingcong Chen"
    ],
    "abstract": "Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
    "arxiv_url": "http://arxiv.org/abs/2509.20251v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20251v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "motion",
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for\n  Driving Scenes",
    "authors": [
      "Guo Chen",
      "Jiarun Liu",
      "Sicong Du",
      "Chenming Wu",
      "Deqi Li",
      "Shi-Sheng Huang",
      "Guofeng Zhang",
      "Sheng Yang"
    ],
    "abstract": "This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: https://shanzhaguoo.github.io/GS-RoadPatching/",
    "arxiv_url": "http://arxiv.org/abs/2509.19937v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19937v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based\n  Intermediate View Rendering",
    "authors": [
      "Jiangxue Yu",
      "Hui Wang",
      "San Jiang",
      "Xing Zhang",
      "Dejin Zhang",
      "Qingquan Li"
    ],
    "abstract": "The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences. The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes. First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage. Second, 3D Gaussian Splatting is then adopted for scene rendering by taking as inputs sparse points and oriented images. For accurate view rendering, a render viewpoint determination algorithm is designed by using the oriented camera poses of aerial images, which is used to generate high-quality intermediate images that can bridge the gap between aerial and ground images. Third, with the aid of intermediate images, reliable feature matching is conducted for match pairs from render-aerial and render-ground images, and final matches can be generated by transmitting correspondences through intermediate views. By using real aerial and ground datasets, the validation of the proposed solution has been verified in terms of feature matching and scene rendering and compared comprehensively with widely used methods. The experimental results demonstrate that the proposed solution can provide reliable feature matches for aerial and ground images with an obvious increase in the number of initial and refined matches, and it can provide enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based scene rendering.",
    "arxiv_url": "http://arxiv.org/abs/2509.19898v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19898v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth\n  Estimation via 3D Gaussian Splatting",
    "authors": [
      "Yixun Zhang",
      "Feng Zhou",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.19793v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19793v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface\n  Reconstruction",
    "authors": [
      "Yufei Han",
      "Bowen Tie",
      "Heng Guo",
      "Youwei Lyu",
      "Si Li",
      "Boxin Shi",
      "Yunpeng Jia",
      "Zhanyu Ma"
    ],
    "abstract": "Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2509.19726v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19726v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "shape reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian\n  Bracketing",
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ],
    "abstract": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.",
    "arxiv_url": "http://arxiv.org/abs/2509.20400v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20400v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "motion",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
    "authors": [
      "Weijie Wang",
      "Yeqing Chen",
      "Zeyu Zhang",
      "Hengyu Liu",
      "Haoxiao Wang",
      "Zhiyuan Feng",
      "Wenkang Qin",
      "Zheng Zhu",
      "Donny Y. Chen",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.19297v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19297v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
    "authors": [
      "Sherwin Bahmani",
      "Tianchang Shen",
      "Jiawei Ren",
      "Jiahui Huang",
      "Yifeng Jiang",
      "Haithem Turki",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Zan Gojcic",
      "Sanja Fidler",
      "Huan Ling",
      "Jun Gao",
      "Xuanchi Ren"
    ],
    "abstract": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2509.19296v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19296v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "dynamic",
      "3d gaussian",
      "3d reconstruction",
      "robotics",
      "autonomous driving",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian\n  Object Reconstruction",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
    "arxiv_url": "http://arxiv.org/abs/2509.19073v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19073v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "nerf",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in\n  Mirror-Containing Environments with Gaussian Splatting",
    "authors": [
      "Zijing Guo",
      "Yunyang Zhao",
      "Lin Wang"
    ],
    "abstract": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.18956v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18956v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "reflection",
      "face",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust\n  Deblurring",
    "authors": [
      "Pengteng Li",
      "Yunfan Lu",
      "Pinhao Song",
      "Weiyu Guo",
      "Huizai Yao",
      "F. Richard Yu",
      "Hui Xiong"
    ],
    "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.",
    "arxiv_url": "http://arxiv.org/abs/2509.18898v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18898v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score\n  Distillation",
    "authors": [
      "Zhaorui Wang",
      "Yi Gu",
      "Deming Zhou",
      "Renjing Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.",
    "arxiv_url": "http://arxiv.org/abs/2509.18759v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18759v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for\n  Drones",
    "authors": [
      "Maximilian Adang",
      "JunEn Low",
      "Ola Shorinwa",
      "Mac Schwager"
    ],
    "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions.",
    "arxiv_url": "http://arxiv.org/abs/2509.18610v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18610v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene\n  Reconstruction",
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Kailun Yang",
      "Jiajun Zhai",
      "Shangwei Guo",
      "Lin Wang",
      "Kaiwei Wang"
    ],
    "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
    "arxiv_url": "http://arxiv.org/abs/2509.18566v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18566v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "deformation",
      "motion",
      "fast",
      "animation",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Differentiable Light Transport with Gaussian Surfels via Adapted\n  Radiosity for Efficient Relighting and Geometry Reconstruction",
    "authors": [
      "Kaiwen Jiang",
      "Jia-Mu Sun",
      "Zilu Li",
      "Dan Wang",
      "Tzu-Mao Li",
      "Ravi Ramamoorthi"
    ],
    "abstract": "Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.",
    "arxiv_url": "http://arxiv.org/abs/2509.18497v2",
    "pdf_url": "http://arxiv.org/pdf/2509.18497v2",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "global illumination",
      "light transport",
      "illumination",
      "reflection",
      "lighting",
      "relighting",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Youmin Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Xiaohan Yu",
      "Lin Gu"
    ],
    "abstract": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
    "arxiv_url": "http://arxiv.org/abs/2509.18090v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18090v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "face",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring\n  the Pareto frontier in multi-criteria optimization",
    "authors": [
      "Phuong Mai Dinh",
      "Van-Nam Huynh"
    ],
    "abstract": "Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.",
    "arxiv_url": "http://arxiv.org/abs/2509.17889v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17889v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for\n  Underwater Scenes",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Zipeng Qi",
      "Wenjun Lu",
      "David Bull",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2509.17789v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17789v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "lightweight",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian\n  Splats from a Mobile Device",
    "authors": [
      "Gunjan Chhablani",
      "Xiaomeng Ye",
      "Muhammad Zubair Irshad",
      "Zsolt Kira"
    ],
    "abstract": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.",
    "arxiv_url": "http://arxiv.org/abs/2509.17430v2",
    "pdf_url": "http://arxiv.org/pdf/2509.17430v2",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS\n  Models to LiDAR",
    "authors": [
      "Junzhe Wu",
      "Yufei Jia",
      "Yiyi Yan",
      "Zhixing Chen",
      "Tiao Tan",
      "Zifan Wang",
      "Guangyu Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic rendering, its vast ecosystem of assets remains incompatible with high-performance LiDAR simulation, a critical tool for robotics and autonomous driving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with a truly plug-and-play approach. Our method converts \\textit{any} pretrained 3DGS model into a high-fidelity, watertight mesh without requiring LiDAR-specific supervision or architectural alterations. This conversion is achieved through a general pipeline of volumetric discretization and Truncated Signed Distance Field (TSDF) extraction. We pair this with a highly optimized, GPU-accelerated ray-casting module that simulates LiDAR returns at over 500 FPS. We validate our approach on indoor and outdoor scenes, demonstrating exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for geometrically accurate depth sensing, our framework extends their utility beyond visualization and unlocks new capabilities for scalable, multimodal simulation. Our open-source implementation is available at https://github.com/TATP-233/FGGS-LiDAR.",
    "arxiv_url": "http://arxiv.org/abs/2509.17390v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17390v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.RO",
      "68T40, 68U05",
      "I.6.8"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "3d gaussian",
      "autonomous driving",
      "robotics",
      "ar",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene\n  Reconstruction",
    "authors": [
      "Neham Jain",
      "Andrew Jong",
      "Sebastian Scherer",
      "Ioannis Gkioulekas"
    ],
    "abstract": "Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.",
    "arxiv_url": "http://arxiv.org/abs/2509.17329v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17329v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting\n  from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.",
    "arxiv_url": "http://arxiv.org/abs/2509.17246v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17246v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
    "arxiv_url": "http://arxiv.org/abs/2509.17083v2",
    "pdf_url": "http://arxiv.org/pdf/2509.17083v2",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "nerf",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic\n  Views",
    "authors": [
      "Zhenya Yang"
    ],
    "abstract": "Surgical simulation is essential for medical training, enabling practitioners to develop crucial skills in a risk-free environment while improving patient safety and surgical outcomes. However, conventional methods for building simulation environments are cumbersome, time-consuming, and difficult to scale, often resulting in poor details and unrealistic simulations. In this paper, we propose a Gaussian Splatting-based framework to directly reconstruct interactive surgical scenes from endoscopic data while ensuring efficiency, rendering quality, and realism. A key challenge in this data-driven simulation paradigm is the restricted movement of endoscopic cameras, which limits viewpoint diversity. As a result, the Gaussian Splatting representation overfits specific perspectives, leading to reduced geometric accuracy. To address this issue, we introduce a novel virtual camera-based regularization method that adaptively samples virtual viewpoints around the scene and incorporates them into the optimization process to mitigate overfitting. An effective depth-based regularization is applied to both real and virtual views to further refine the scene geometry. To enable fast deformation simulation, we propose a sparse control node-based Material Point Method, which integrates physical properties into the reconstructed scene while significantly reducing computational costs. Experimental results on representative surgical data demonstrate that our method can efficiently reconstruct and simulate surgical scenes from sparse endoscopic views. Notably, our method takes only a few minutes to reconstruct the surgical scene and is able to produce physically plausible deformations in real-time with user-defined interactions.",
    "arxiv_url": "http://arxiv.org/abs/2509.17027v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17027v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "deformation",
      "fast",
      "ar",
      "efficient",
      "gaussian splatting",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D\n  Gaussian Splatting with Pixel-Aware Density Control",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.",
    "arxiv_url": "http://arxiv.org/abs/2509.16922v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16922v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "ar",
      "lightweight",
      "gaussian splatting",
      "nerf",
      "avatar",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D\n  Gaussian Splatting SLAM",
    "authors": [
      "Amanuel T. Dufera",
      "Yuan-Li Cai"
    ],
    "abstract": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction. Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism. This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision. The resulting proxy depth guides the optimization of a deformable 3DGS map, which efficiently adapts online to maintain global consistency following pose updates from a DROID-SLAM-inspired frontend and backend optimizations (loop closure, global bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD, ScanNet) and diverse custom mobile datasets demonstrates significant improvements in reconstruction accuracy (L1 depth error) and novel view synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in challenging conditions. ConfidentSplat underscores the efficacy of principled, confidence-aware sensor fusion for advancing state-of-the-art dense visual SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2509.16863v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16863v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV",
      "68T20, 68U20"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "dynamic",
      "3d gaussian",
      "slam",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging",
    "authors": [
      "Kacper Marzol",
      "Ignacy Kolton",
      "Weronika Smolak-Dyżewska",
      "Joanna Kaleta",
      "Marcin Mazur",
      "Przemysław Spurek"
    ],
    "abstract": "Multi-modal three-dimensional (3D) medical imaging data, derived from ultrasound, magnetic resonance imaging (MRI), and potentially computed tomography (CT), provide a widely adopted approach for non-invasive anatomical visualization. Accurate modeling, registration, and visualization in this setting depend on surface reconstruction and frame-to-frame interpolation. Traditional methods often face limitations due to image noise and incomplete information between frames. To address these challenges, we present MedGS, a semi-supervised neural implicit surface reconstruction framework that employs a Gaussian Splatting (GS)-based interpolation mechanism. In this framework, medical imaging data are represented as consecutive two-dimensional (2D) frames embedded in 3D space and modeled using Gaussian-based distributions. This representation enables robust frame interpolation and high-fidelity surface reconstruction across imaging modalities. As a result, MedGS offers more efficient training than traditional neural implicit methods. Its explicit GS-based representation enhances noise robustness, allows flexible editing, and supports precise modeling of complex anatomical structures with fewer artifacts. These features make MedGS highly suitable for scalable and practical applications in medical imaging.",
    "arxiv_url": "http://arxiv.org/abs/2509.16806v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16806v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "high-fidelity",
      "ar",
      "medical",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with\n  Spatial-Temporal Gaussian Splatting",
    "authors": [
      "Xiaoyang Yan",
      "Muleilan Pei",
      "Shaojie Shen"
    ],
    "abstract": "3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.16552v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16552v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "autonomous driving",
      "ar",
      "understanding",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars",
    "authors": [
      "Weiyi Xiong",
      "Bing Zhu",
      "Tao Huang",
      "Zewei Zheng"
    ],
    "abstract": "4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2509.16119v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16119v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lighting",
      "3d gaussian",
      "autonomous driving",
      "ar",
      "efficient",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval",
    "authors": [
      "Liwei Liao",
      "Xufeng Li",
      "Xiaoyun Zheng",
      "Boning Liu",
      "Feng Gao",
      "Ronggang Wang"
    ],
    "abstract": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.",
    "arxiv_url": "http://arxiv.org/abs/2509.15871v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15871v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "robotics",
      "ar",
      "vr",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Splatting for Continuous View Optimization",
    "authors": [
      "Gahye Lee",
      "Hyomin Kim",
      "Gwangjin Ju",
      "Jooeun Son",
      "Hyejeong Yoon",
      "Seungyong Lee"
    ],
    "abstract": "We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.",
    "arxiv_url": "http://arxiv.org/abs/2509.15677v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15677v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation\n  based on 3D Gaussian Splatting",
    "authors": [
      "Yuwei Jia",
      "Yutang Lu",
      "Zhe Cui",
      "Fei Su"
    ],
    "abstract": "Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.",
    "arxiv_url": "http://arxiv.org/abs/2509.15648v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15648v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "recognition",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host\n  Offloading",
    "authors": [
      "Donghyun Lee",
      "Dawoon Jeong",
      "Jae W. Lee",
      "Hongil Yoon"
    ],
    "abstract": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
    "arxiv_url": "http://arxiv.org/abs/2509.15645v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15645v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild",
    "authors": [
      "Deming Li",
      "Kaiwen Jiang",
      "Yutao Tang",
      "Ravi Ramamoorthi",
      "Rama Chellappa",
      "Cheng Peng"
    ],
    "abstract": "In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision steps at virtual views in pixel and feature levels to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions, and outperforms existing approaches significantly across different datasets.",
    "arxiv_url": "http://arxiv.org/abs/2509.15548v4",
    "pdf_url": "http://arxiv.org/pdf/2509.15548v4",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model\n  Priors for 3D Monocular Avatar Reconstruction",
    "authors": [
      "Jinlong Fan",
      "Bingyu Hu",
      "Xingguang Li",
      "Yuxiang Yang",
      "Jing Zhang"
    ],
    "abstract": "Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.",
    "arxiv_url": "http://arxiv.org/abs/2509.14739v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14739v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "human",
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform\n  for Embodied AI",
    "authors": [
      "Cong Tai",
      "Zhaoyu Zheng",
      "Haixu Long",
      "Hansheng Wu",
      "Haodong Xiang",
      "Zhengbin Long",
      "Jun Xiong",
      "Rong Shi",
      "Shizhuang Zhang",
      "Gang Qiu",
      "He Wang",
      "Ruifeng Li",
      "Jun Huang",
      "Bin Chang",
      "Shuai Feng",
      "Tao Shen"
    ],
    "abstract": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io",
    "arxiv_url": "http://arxiv.org/abs/2509.14687v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14687v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Causal Reasoning Elicits Controllable 3D Scene Generation",
    "authors": [
      "Shen Chen",
      "Ruiyu Zhao",
      "Jiale Zhou",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "abstract": "Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.",
    "arxiv_url": "http://arxiv.org/abs/2509.15249v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15249v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perception-Integrated Safety Critical Control via Analytic Collision\n  Cone Barrier Functions on 3D Gaussian Splatting",
    "authors": [
      "Dario Tscholl",
      "Yashwanth Nakka",
      "Brian Gunter"
    ],
    "abstract": "We present a perception-driven safety filter that converts each 3D Gaussian Splat (3DGS) into a closed-form forward collision cone, which in turn yields a first-order control barrier function (CBF) embedded within a quadratic program (QP). By exploiting the analytic geometry of splats, our formulation provides a continuous, closed-form representation of collision constraints that is both simple and computationally efficient. Unlike distance-based CBFs, which tend to activate reactively only when an obstacle is already close, our collision-cone CBF activates proactively, allowing the robot to adjust earlier and thereby produce smoother and safer avoidance maneuvers at lower computational cost. We validate the method on a large synthetic scene with approximately 170k splats, where our filter reduces planning time by a factor of 3 and significantly decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while maintaining the same level of safety. The approach is entirely analytic, requires no high-order CBF extensions (HOCBFs), and generalizes naturally to robots with physical extent through a principled Minkowski-sum inflation of the splats. These properties make the method broadly applicable to real-time navigation in cluttered, perception-derived extreme environments, including space robotics and satellite systems.",
    "arxiv_url": "http://arxiv.org/abs/2509.14421v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14421v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "robotics",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for\n  High-Fidelity Mapping",
    "authors": [
      "Zhihao Cao",
      "Hanyu Wu",
      "Li Wa Tang",
      "Zizhou Luo",
      "Zihan Zhu",
      "Wei Zhang",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2509.14191v2",
    "pdf_url": "http://arxiv.org/pdf/2509.14191v2",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "slam",
      "robotics",
      "autonomous driving",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Plug-and-Play PDE Optimization for 3D Gaussian Splatting: Toward\n  High-Quality Rendering and Reconstruction",
    "authors": [
      "Yifan Mo",
      "Youcheng Cai",
      "Ligang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction by achieving high-quality novel view synthesis with fast rendering speed, introducing 3D Gaussian primitives to represent the scene. However, 3DGS encounters blurring and floaters when applied to complex scenes, caused by the reconstruction of redundant and ambiguous geometric structures. We attribute this issue to the unstable optimization of the Gaussians. To address this limitation, we present a plug-and-play PDE-based optimization method that overcomes the optimization constraints of 3DGS-based approaches in various tasks, such as novel view synthesis and surface reconstruction. Firstly, we theoretically derive that the 3DGS optimization procedure can be modeled as a PDE, and introduce a viscous term to ensure stable optimization. Secondly, we use the Material Point Method (MPM) to obtain a stable numerical solution of the PDE, which enhances both global and local constraints. Additionally, an effective Gaussian densification strategy and particle constraints are introduced to ensure fine-grained details. Extensive qualitative and quantitative experiments confirm that our method achieves state-of-the-art rendering and reconstruction quality.",
    "arxiv_url": "http://arxiv.org/abs/2509.13938v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13938v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray\n  Laminography Reconstruction",
    "authors": [
      "Chu Chen",
      "Ander Biguri",
      "Jean-Michel Morel",
      "Raymond H. Chan",
      "Carola-Bibiane Schönlieb",
      "Jizhou Li"
    ],
    "abstract": "X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$\\%$ of full views to achieve superior performance over the iterative method optimized on a full dataset.",
    "arxiv_url": "http://arxiv.org/abs/2509.13863v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13863v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "efficient",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM",
    "authors": [
      "Yinlong Bai",
      "Hongxin Zhang",
      "Sheng Zhong",
      "Junkai Niu",
      "Hai Li",
      "Yijia He",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.",
    "arxiv_url": "http://arxiv.org/abs/2509.13536v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13536v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "slam",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice\n  Vector Quantization",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its photorealistic rendering quality and real-time performance, but it generates massive amounts of data. Hence compressing 3DGS data is necessary for the cost effectiveness of 3DGS models. Recently, several anchor-based neural compression methods have been proposed, achieving good 3DGS compression performance. However, they all rely on uniform scalar quantization (USQ) due to its simplicity. A tantalizing question is whether more sophisticated quantizers can improve the current 3DGS compression methods with very little extra overhead and minimal change to the system. The answer is yes by replacing USQ with lattice vector quantization (LVQ). To better capture scene-specific characteristics, we optimize the lattice basis for each scene, improving LVQ's adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a balance between the R-D efficiency of vector quantization and the low complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS compression architectures, enhancing their R-D performance with minimal modifications and computational overhead. Moreover, by scaling the lattice basis vectors, SALVQ can dynamically adjust lattice density, enabling a single model to accommodate multiple bit rate targets. This flexibility eliminates the need to train separate models for different compression levels, significantly reducing training time and memory consumption.",
    "arxiv_url": "http://arxiv.org/abs/2509.13482v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13482v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single\n  Image",
    "authors": [
      "Gaofeng Liu",
      "Hengsen Li",
      "Ruoyu Gao",
      "Xuetong Li",
      "Zhiyuan Ma",
      "Tao Fang"
    ],
    "abstract": "With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.",
    "arxiv_url": "http://arxiv.org/abs/2509.13013v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13013v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "animation",
      "high-fidelity",
      "3d gaussian",
      "lightweight",
      "efficient",
      "ar",
      "avatar",
      "body"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian\n  Splatting and Bag of Embeddings",
    "authors": [
      "Abdalla Arafa",
      "Didier Stricker"
    ],
    "abstract": "Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.",
    "arxiv_url": "http://arxiv.org/abs/2509.12938v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12938v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "3d gaussian",
      "robotics",
      "ar",
      "understanding",
      "gaussian splatting",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
    "authors": [
      "Jiateng Liu",
      "Hao Gao",
      "Jiu-Cheng Xie",
      "Chi-Man Pun",
      "Jian Xiong",
      "Haolun Li",
      "Feng Xu"
    ],
    "abstract": "This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters.",
    "arxiv_url": "http://arxiv.org/abs/2509.12742v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12742v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "dynamic",
      "ar",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface\n  Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique for real-time, photorealistic rendering by optimizing anisotropic Gaussian primitives from view-dependent images. While 3D-GS has been extended to scientific visualization, prior work remains limited to single-GPU settings, restricting scalability for large datasets on high-performance computing (HPC) systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. To eliminate artifacts, we add ghost cells at partition boundaries and apply background masks to remove irrelevant pixels. Benchmarks on the Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup across 8 nodes on Polaris while preserving image quality. These results demonstrate that distributed 3D-GS enables scalable visualization of large-scale scientific data and provide a foundation for future in situ applications.",
    "arxiv_url": "http://arxiv.org/abs/2509.12138v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12138v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
    "authors": [
      "Yi-Hsin Li",
      "Thomas Sikora",
      "Sebastian Knorr",
      "Måarten Sjöström"
    ],
    "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.11853v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11853v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "real-time rendering",
      "fast",
      "motion",
      "segmentation",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
    "authors": [
      "Wending Liu",
      "Siyun Liang",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "abstract": "We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
    "arxiv_url": "http://arxiv.org/abs/2509.11624v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11624v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Skinning of Gaussian Avatars",
    "authors": [
      "Nikolaos Zioulis",
      "Nikolaos Kotarelas",
      "Georgios Albanis",
      "Spyridon Thermos",
      "Anargyros Chatzitofis"
    ],
    "abstract": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.",
    "arxiv_url": "http://arxiv.org/abs/2509.11411v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11411v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "fast",
      "human",
      "ar",
      "efficient",
      "gaussian splatting",
      "avatar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting",
    "authors": [
      "Lianjun Liao",
      "Chunhui Zhang",
      "Tong Wu",
      "Henglei Lv",
      "Bailin Deng",
      "Lin Gao"
    ],
    "abstract": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2509.11275v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11275v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "I.2.10; I.3"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "illumination",
      "lighting",
      "3d gaussian",
      "relighting",
      "head",
      "efficient rendering",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting",
      "nerf",
      "outdoor",
      "relightable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D\n  Gaussian Splatting",
    "authors": [
      "Ashkan Taghipour",
      "Vahid Naghshin",
      "Benjamin Southwell",
      "Farid Boussaid",
      "Hamid Laga",
      "Mohammed Bennamoun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
    "arxiv_url": "http://arxiv.org/abs/2509.11116v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11116v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "robotics",
      "ar",
      "efficient",
      "gaussian splatting",
      "vr",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting",
    "authors": [
      "Gurutva Patle",
      "Nilay Girgaonkar",
      "Nagabhushan Somraj",
      "Rajiv Soundararajan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods. The source code for our model can be found on our project page: https://gurutvapatle.github.io/publications/2025/ADGS.html .",
    "arxiv_url": "http://arxiv.org/abs/2509.11003v2",
    "pdf_url": "http://arxiv.org/pdf/2509.11003v2",
    "published_date": "2025-09-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing\n  for Physics-based Camera Effect Data Generation",
    "authors": [
      "Yi-Ruei Liu",
      "You-Zhe Xie",
      "Yu-Hsiang Hsu",
      "I-Sheng Fang",
      "Yu-Lun Liu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.",
    "arxiv_url": "http://arxiv.org/abs/2509.10759v2",
    "pdf_url": "http://arxiv.org/pdf/2509.10759v2",
    "published_date": "2025-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "ray tracing",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "T2Bs: Text-to-Character Blendshapes via Video Generation",
    "authors": [
      "Jiahao Luo",
      "Chaoyang Wang",
      "Michael Vasilkovsky",
      "Vladislav Shakhrai",
      "Di Liu",
      "Peiye Zhuang",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "James Davis",
      "Jian Wang"
    ],
    "abstract": "We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.",
    "arxiv_url": "http://arxiv.org/abs/2509.10678v2",
    "pdf_url": "http://arxiv.org/pdf/2509.10678v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "deformation",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "4d",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Geometric Accuracy of Implicit and Primitive-based\n  Representations Derived from View Rendering Constraints",
    "authors": [
      "Elias De Smijter",
      "Renaud Detry",
      "Christophe De Vleeschouwer"
    ],
    "abstract": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.10241v2",
    "pdf_url": "http://arxiv.org/pdf/2509.10241v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "robotics",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting",
    "authors": [
      "Mahtab Dahaghin",
      "Milind G. Padalkar",
      "Matteo Toso",
      "Alessio Del Bue"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.",
    "arxiv_url": "http://arxiv.org/abs/2509.07809v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07809v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting",
    "authors": [
      "Yimin Pan",
      "Matthias Nießner",
      "Tobias Kirschstein"
    ],
    "abstract": "Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.   While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.   The project page can be found at: https://yimin-pan.github.io/hair-gs/",
    "arxiv_url": "http://arxiv.org/abs/2509.07774v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07774v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accurate and Complete Surface Reconstruction from 3D Gaussians via\n  Direct SDF Learning",
    "authors": [
      "Wenzhi Guo",
      "Bing Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2509.07493v2",
    "pdf_url": "http://arxiv.org/pdf/2509.07493v2",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset\n  Generation",
    "authors": [
      "Ze-Xin Yin",
      "Jiaxiong Qiu",
      "Liu Liu",
      "Xinjie Wang",
      "Wei Sui",
      "Zhizhong Su",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.",
    "arxiv_url": "http://arxiv.org/abs/2509.07435v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07435v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "relightable"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level\n  Guidance in Large Scenes",
    "authors": [
      "Shengkai Zhang",
      "Yuhe Liu",
      "Guanjun Wu",
      "Jianhua He",
      "Xinggang Wang",
      "Mozi Chen",
      "Kezhong Liu"
    ],
    "abstract": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.",
    "arxiv_url": "http://arxiv.org/abs/2509.06685v3",
    "pdf_url": "http://arxiv.org/pdf/2509.06685v3",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "motion",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot\n  Teleoperation",
    "authors": [
      "Ian Page",
      "Pierre Susbielle",
      "Olivier Aycard",
      "Pierre-Brice Wieber"
    ],
    "abstract": "Achieving efficient remote teleoperation is particularly challenging in unknown environments, as the teleoperator must rapidly build an understanding of the site's layout. Online 3D mapping is a proven strategy to tackle this challenge, as it enables the teleoperator to progressively explore the site from multiple perspectives. However, traditional online map-based teleoperation systems struggle to generate visually accurate 3D maps in real-time due to the high computational cost involved, leading to poor teleoperation performances. In this work, we propose a solution to improve teleoperation efficiency in unknown environments. Our approach proposes a novel, modular and efficient GPU-based integration between recent advancement in gaussian splatting SLAM and existing online map-based teleoperation systems. We compare the proposed solution against state-of-the-art teleoperation systems and validate its performances through real-world experiments using an aerial vehicle. The results show significant improvements in decision-making speed and more accurate interaction with the environment, leading to greater teleoperation efficiency. In doing so, our system enhances remote teleoperation by seamlessly integrating photorealistic mapping generation with real-time performances, enabling effective teleoperation in unfamiliar environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.06433v2",
    "pdf_url": "http://arxiv.org/pdf/2509.06433v2",
    "published_date": "2025-09-08",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "slam",
      "ar",
      "understanding",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited\n  Degrees of Freedom",
    "authors": [
      "Matthieu Gendrin",
      "Stéphane Pateux",
      "Théo Ladune"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.",
    "arxiv_url": "http://arxiv.org/abs/2509.06400v1",
    "pdf_url": "http://arxiv.org/pdf/2509.06400v1",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "large scene",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians\n  and Unified Pruning",
    "authors": [
      "Jiarui Chen",
      "Yikeng Chen",
      "Yingshuang Zou",
      "Ye Huang",
      "Peng Wang",
      "Yuan Liu",
      "Yujing Sun",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: https://megs-2.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2509.07021v2",
    "pdf_url": "http://arxiv.org/pdf/2509.07021v2",
    "published_date": "2025-09-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "3d gaussian",
      "ar",
      "lightweight",
      "efficient",
      "gaussian splatting",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface\n  Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS) pipeline for scientific visualization. Building on previous work that demonstrated high-fidelity isosurface reconstruction using Gaussian primitives, we incorporate a multi-GPU training backend adapted from Grendel-GS to enable scalable processing of large datasets. By distributing optimization across GPUs, our method improves training throughput and supports high-resolution reconstructions that exceed single-GPU capacity. In our experiments, the system achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs compared to a single-GPU baseline, and successfully trains the Miranda dataset (18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays the groundwork for integrating 3D-GS into HPC-based scientific workflows, enabling real-time post hoc and in situ visualization of complex simulations.",
    "arxiv_url": "http://arxiv.org/abs/2509.05216v1",
    "pdf_url": "http://arxiv.org/pdf/2509.05216v1",
    "published_date": "2025-09-05",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
    "authors": [
      "Yangming Li",
      "Chaoyu Liu",
      "Lihao Liu",
      "Simon Masnou",
      "Carola-Bibiane Schönlieb"
    ],
    "abstract": "A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they might also be unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework, GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.",
    "arxiv_url": "http://arxiv.org/abs/2509.05075v3",
    "pdf_url": "http://arxiv.org/pdf/2509.05075v3",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "face",
      "dynamic",
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
    "authors": [
      "Hannah Schieber",
      "Dominik Frischmann",
      "Victor Schaack",
      "Simon Boche",
      "Angela Schoellig",
      "Stefan Leutenegger",
      "Daniel Roth"
    ],
    "abstract": "Mobile reconstruction has the potential to support time-critical tasks such as tele-guidance and disaster response, where operators must quickly gain an accurate understanding of the environment. Full high-fidelity scene reconstruction is computationally expensive and often unnecessary when only specific points of interest (POIs) matter for timely decision making. We address this challenge with CoRe-GS, a semantic POI-focused extension of Gaussian Splatting (GS). Instead of optimizing every scene element uniformly, CoRe-GS first produces a fast segmentation-ready GS representation and then selectively refines splats belonging to semantically relevant POIs detected during data acquisition. This targeted refinement reduces training time to 25\\% compared to full semantic GS while improving novel view synthesis quality in the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM) and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs enables faster and higher-quality mobile reconstruction tailored to operational needs.",
    "arxiv_url": "http://arxiv.org/abs/2509.04859v2",
    "pdf_url": "http://arxiv.org/pdf/2509.04859v2",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "fast",
      "segmentation",
      "high-fidelity",
      "ar",
      "understanding",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
    "authors": [
      "Jimin Xu",
      "Bosheng Qin",
      "Tao Jin",
      "Zhou Zhao",
      "Zhenhui Ye",
      "Jun Yu",
      "Fei Wu"
    ],
    "abstract": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.",
    "arxiv_url": "http://arxiv.org/abs/2509.04379v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04379v1",
    "published_date": "2025-09-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,\n  Memory-Efficient Reconstruction",
    "authors": [
      "Sankeerth Durvasula",
      "Sharanshangar Muhunthan",
      "Zain Moustafa",
      "Richard Chen",
      "Ruofan Liang",
      "Yushi Guan",
      "Nilesh Ahuja",
      "Nilesh Jain",
      "Selvakumar Panneer",
      "Nandita Vijaykumar"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.",
    "arxiv_url": "http://arxiv.org/abs/2509.03775v1",
    "pdf_url": "http://arxiv.org/pdf/2509.03775v1",
    "published_date": "2025-09-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "compact",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned\n  Residuals",
    "authors": [
      "Mohit Mendiratta",
      "Mayur Deshmukh",
      "Kartik Teotia",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Christian Theobalt"
    ],
    "abstract": "3D Morphable Models (3DMMs) enable controllable facial geometry and expression editing for reconstruction, animation, and AR/VR, but traditional PCA-based mesh models are limited in resolution, detail, and photorealism. Neural volumetric methods improve realism but remain too slow for interactive use. Recent Gaussian Splatting (3DGS) based facial models achieve fast, high-quality rendering but still depend solely on a mesh-based 3DMM prior for expression control, limiting their ability to capture fine-grained geometry, expressions, and full-head coverage. We introduce GRMM, the first full-head Gaussian 3D morphable model that augments a base 3DMM with residual geometry and appearance components, additive refinements that recover high-frequency details such as wrinkles, fine skin texture, and hairline variations. GRMM provides disentangled control through low-dimensional, interpretable parameters (e.g., identity shape, facial expressions) while separately modelling residuals that capture subject- and expression-specific detail beyond the base model's capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders represent per-Gaussian appearance, and a lightweight CNN refines rasterised images for enhanced realism, all while maintaining 75 FPS real-time rendering. To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first dataset with 60 aligned expressions across 50 identities, enabling robust disentanglement of identity and expression in Gaussian-based 3DMMs. Across monocular 3D face reconstruction, novel-view synthesis, and expression transfer, GRMM surpasses state-of-the-art methods in fidelity and expression accuracy while delivering interactive real-time performance.",
    "arxiv_url": "http://arxiv.org/abs/2509.02141v1",
    "pdf_url": "http://arxiv.org/pdf/2509.02141v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "real-time rendering",
      "fast",
      "deformation",
      "animation",
      "high-fidelity",
      "face",
      "ar",
      "lightweight",
      "vr",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
    "authors": [
      "Hongyu Li",
      "Chaofeng Chen",
      "Xiaoming Li",
      "Guangming Lu"
    ],
    "abstract": "Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.",
    "arxiv_url": "http://arxiv.org/abs/2509.01964v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01964v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "efficient",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Richard Bowden"
    ],
    "abstract": "Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.",
    "arxiv_url": "http://arxiv.org/abs/2509.01681v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01681v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "semantic",
      "fast",
      "segmentation",
      "human",
      "face",
      "ar",
      "gaussian splatting",
      "avatar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Im2Haircut: Single-view Strand-based Hair Reconstruction for Human\n  Avatars",
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Malte Prinzler",
      "Giorgio Becherini",
      "Michael J. Black",
      "Justus Thies"
    ],
    "abstract": "We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to https://im2haircut.is.tue.mpg.de.",
    "arxiv_url": "http://arxiv.org/abs/2509.01469v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01469v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "avatar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting",
    "authors": [
      "Josef Grün",
      "Lukas Meyer",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2509.00989v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00989v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "fast",
      "high-fidelity",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing\n  Redundant Sorting while Preserving Rasterization Efficiency",
    "authors": [
      "Joongho Jo",
      "Jongsun Park"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.",
    "arxiv_url": "http://arxiv.org/abs/2509.00911v2",
    "pdf_url": "http://arxiv.org/pdf/2509.00911v2",
    "published_date": "2025-08-31",
    "categories": [
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "efficient",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.",
    "arxiv_url": "http://arxiv.org/abs/2509.00800v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00800v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "high-fidelity",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "understanding",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]