[
  {
    "title": "OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction",
    "authors": [
      "Minseong Kweon",
      "Jinsun Park"
    ],
    "abstract": "We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.",
    "arxiv_url": "https://arxiv.org/abs/2601.04984v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04984v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "authors": [
      "Yen-Jen Chiou",
      "Wei-Tse Cheng",
      "Yuan-Fu Yang"
    ],
    "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
    "arxiv_url": "https://arxiv.org/abs/2601.04754v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04754v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting",
    "authors": [
      "Diego Revilla",
      "Pooja Suresh",
      "Anand Bhojan",
      "Ooi Wei Tsang"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.   In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.04348v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04348v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting",
    "authors": [
      "Wei Long",
      "Haifeng Wu",
      "Shiyin Jiang",
      "Jinhua Zhang",
      "Xinchun Ji",
      "Shuhang Gu"
    ],
    "abstract": "Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.",
    "arxiv_url": "https://arxiv.org/abs/2601.03824v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03824v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation",
    "authors": [
      "Hojun Song",
      "Chae-yeong Song",
      "Jeong-hun Hong",
      "Chaewon Moon",
      "Dong-hwi Kim",
      "Gahyeon Kim",
      "Soo Ye Kim",
      "Yiyi Liao",
      "Jaehyup Lee",
      "Sang-hyo Park"
    ],
    "abstract": "Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.",
    "arxiv_url": "https://arxiv.org/abs/2601.03510v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03510v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelightAnyone: A Generalized Relightable 3D Gaussian Head Model",
    "authors": [
      "Yingyan Xu",
      "Pramod Rao",
      "Sebastian Weiss",
      "Gaspard Zoss",
      "Markus Gross",
      "Christian Theobalt",
      "Marc Habermann",
      "Derek Bradley"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.",
    "arxiv_url": "https://arxiv.org/abs/2601.03357v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03357v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "relightable",
      "illumination",
      "relighting",
      "avatar",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting",
    "authors": [
      "Ziyang Sun",
      "Lingfan Bao",
      "Tianhu Peng",
      "Jingcheng Sun",
      "Chengxu Zhou"
    ],
    "abstract": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.",
    "arxiv_url": "https://arxiv.org/abs/2601.03200v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03200v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection",
    "authors": [
      "Kim Jun-Seong",
      "Tae-Hyun Oh",
      "Eduardo PÃ©rez-Pellitero",
      "Youngkyoon Jang"
    ],
    "abstract": "We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.",
    "arxiv_url": "https://arxiv.org/abs/2601.03024v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03024v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "authors": [
      "Eldad Matmon",
      "Amit Bracha",
      "Noam Rotstein",
      "Ron Kimmel"
    ],
    "abstract": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
    "arxiv_url": "https://arxiv.org/abs/2601.03319v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03319v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "geometry",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos",
    "authors": [
      "Taeyeon Kim",
      "Youngju Na",
      "Jumin Lee",
      "Minhyuk Sung",
      "Sung-Eui Yoon"
    ],
    "abstract": "Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2601.02716v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02716v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
    "authors": [
      "Yating Wang",
      "Yuan Sun",
      "Xuan Wang",
      "Ran Yi",
      "Boyao Zhou",
      "Yipengjing Sun",
      "Hongyu Liu",
      "Yinuo Wang",
      "Lizhuang Ma"
    ],
    "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
    "arxiv_url": "https://arxiv.org/abs/2601.02103v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02103v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "relighting",
      "real-time rendering",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images",
    "authors": [
      "Jiaqi Yao",
      "Zhongmiao Yan",
      "Jingyi Xu",
      "Songpengcheng Xia",
      "Yan Xiang",
      "Ling Pei"
    ],
    "abstract": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.02102v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02102v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse view",
      "neural rendering",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "efficient rendering",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
    "authors": [
      "Jinlong Fan",
      "Shanshan Zhao",
      "Liang Zheng",
      "Jing Zhang",
      "Yuxiang Yang",
      "Mingming Gong"
    ],
    "abstract": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2601.02098v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02098v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "ar",
      "geometry",
      "avatar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes",
    "authors": [
      "Haato Watanabe",
      "Nobuyuki Umetani"
    ],
    "abstract": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.",
    "arxiv_url": "https://arxiv.org/abs/2601.02072v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02072v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
    "authors": [
      "Chuhang Ma",
      "Shuai Tan",
      "Ye Pan",
      "Jiaolong Yang",
      "Xin Tong"
    ],
    "abstract": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.",
    "arxiv_url": "https://arxiv.org/abs/2601.01847v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01847v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "deformation",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows",
    "authors": [
      "Aymen Mir",
      "Riza Alp Guler",
      "Jian Wang",
      "Gerard Pons-Moll",
      "Bing Zhou"
    ],
    "abstract": "We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.",
    "arxiv_url": "https://arxiv.org/abs/2601.01660v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01660v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "relighting",
      "fast",
      "dynamic",
      "avatar",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery",
    "authors": [
      "Feng Luo",
      "Hongbo Pan",
      "Xiang Yang",
      "Baoyu Jiang",
      "Fengqing Liu",
      "Tao Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.",
    "arxiv_url": "https://arxiv.org/abs/2601.00939v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00939v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "illumination",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering",
      "3d reconstruction",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking",
    "authors": [
      "Xiaobao Wei",
      "Zhangjie Ye",
      "Yuxiang Gu",
      "Zunjie Zhu",
      "Yunfei Guo",
      "Yingying Shen",
      "Shan Zhao",
      "Ming Lu",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Rongfeng Lu",
      "Hangjun Ye"
    ],
    "abstract": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian",
    "arxiv_url": "https://arxiv.org/abs/2601.01386v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01386v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "autonomous driving",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting",
    "authors": [
      "Subhankar Mishra"
    ],
    "abstract": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs",
    "arxiv_url": "https://arxiv.org/abs/2601.00913v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00913v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "outdoor",
      "vr",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
    "authors": [
      "Luca Collorone",
      "Mert Kiray",
      "Indro Spinelli",
      "Fabio Galasso",
      "Benjamin Busam"
    ],
    "abstract": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.24986v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24986v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning",
    "authors": [
      "Ankit Dhiman",
      "Srinath R",
      "Jaswanth Reddy",
      "Lokesh R Boregowda",
      "Venkatesh Babu Radhakrishnan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel \"Embedding-to-Label\" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.24763v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24763v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression",
    "authors": [
      "Xiang Liu",
      "Yimin Zhou",
      "Jinxiang Wang",
      "Yujun Huang",
      "Shuzhao Xie",
      "Shiyu Qin",
      "Mingyao Hong",
      "Jiawei Li",
      "Yaowei Wang",
      "Zhi Wang",
      "Shu-Tao Xia",
      "Bin Chen"
    ],
    "abstract": "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy. To address this gap, we introduce Splatwizard, a unified benchmark toolkit designed specifically for benchmarking 3DGS compression models. Splatwizard provides an easy-to-use framework to implement new 3DGS compression model and utilize state-of-the-art techniques proposed by previous work. Besides, an integrated pipeline that automates the calculation of key performance indicators, including image-based quality metrics, chamfer distance of reconstruct mesh, rendering frame rates, and computational resource consumption is included in the framework as well. Code is available at https://github.com/splatwizard/splatwizard",
    "arxiv_url": "https://arxiv.org/abs/2512.24742v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24742v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression",
    "authors": [
      "Huanxiong Liang",
      "Yunuo Chen",
      "Yicheng Pan",
      "Sixian Wang",
      "Jincheng Dai",
      "Guo Lu",
      "Wenjun Zhang"
    ],
    "abstract": "Recent advances in 2D Gaussian Splatting (2DGS) have demonstrated its potential as a compact image representation with millisecond-level decoding. However, existing 2DGS-based pipelines allocate representation capacity and parameter precision largely oblivious to image structure, limiting their rate-distortion (RD) efficiency at low bitrates. To address this, we propose a structure-guided allocation principle for 2DGS, which explicitly couples image structure with both representation capacity and quantization precision, while preserving native decoding speed. First, we introduce a structure-guided initialization that assigns 2D Gaussians according to spatial structural priors inherent in natural images, yielding a localized and semantically meaningful distribution. Second, during quantization-aware fine-tuning, we propose adaptive bitwidth quantization of covariance parameters, which grants higher precision to small-scale Gaussians in complex regions and lower precision elsewhere, enabling RD-aware optimization, thereby reducing redundancy without degrading edge quality. Third, we impose a geometry-consistent regularization that aligns Gaussian orientations with local gradient directions to better preserve structural details. Extensive experiments demonstrate that our approach substantially improves both the representational power and the RD performance of 2DGS while maintaining over 1000 FPS decoding. Compared with the baseline GSImage, we reduce BD-rate by 43.44% on Kodak and 29.91% on DIV2K.",
    "arxiv_url": "https://arxiv.org/abs/2512.24018v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24018v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "ar",
      "geometry",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge",
    "authors": [
      "Tae Ha Park",
      "Simone D'Amico"
    ],
    "abstract": "This work presents a novel pipeline to recover the 3D structure of an unknown target spacecraft from a sequence of images captured during Rendezvous and Proximity Operations (RPO) in space. The target's geometry and appearance are represented as a 3D Gaussian Splatting (3DGS) model. However, learning 3DGS requires static scenes, an assumption in contrast to dynamic lighting conditions encountered in spaceborne imagery. The trained 3DGS model can also be used for camera pose estimation through photometric optimization. Therefore, in addition to recovering a geometrically accurate 3DGS model, the photometric accuracy of the rendered images is imperative to downstream pose estimation tasks during the RPO process. This work proposes to incorporate the prior knowledge of the Sun's position, estimated and maintained by the servicer spacecraft, into the training pipeline for improved photometric quality of 3DGS rasterization. Experimental studies demonstrate the effectiveness of the proposed solution, as 3DGS models trained on a sequence of images learn to adapt to rapidly changing illumination conditions in space and reflect global shadowing and self-occlusion.",
    "arxiv_url": "https://arxiv.org/abs/2512.23998v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23998v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Contour Information Aware 2D Gaussian Splatting for Image Representation",
    "authors": [
      "Masaya Takabe",
      "Hiroshi Watanabe",
      "Sujun Hong",
      "Tomohiro Ikai",
      "Zheming Fan",
      "Ryo Ishimoto",
      "Kakeru Sugimoto",
      "Ruri Imichi"
    ],
    "abstract": "Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.",
    "arxiv_url": "https://arxiv.org/abs/2512.23255v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23255v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "fast",
      "compression",
      "lightweight",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection",
    "authors": [
      "Yi Zhang",
      "Yi Wang",
      "Lei Yao",
      "Lap-Pui Chau"
    ],
    "abstract": "Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).",
    "arxiv_url": "https://arxiv.org/abs/2512.23176v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23176v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Differentiable Physics-Driven Human Representation for Millimeter-Wave Based Pose Estimation",
    "authors": [
      "Shuntian Zheng",
      "Guangming Wang",
      "Jiaqi Li",
      "Minzhe Ni",
      "Yu Guan"
    ],
    "abstract": "While millimeter-wave (mmWave) presents advantages for Human Pose Estimation (HPE) through its non-intrusive sensing capabilities, current mmWave-based HPE methods face limitations in two predominant input paradigms: Heatmap and Point Cloud (PC). Heatmap represents dense multi-dimensional features derived from mmWave, but is significantly affected by multipath propagation and hardware modulation noise. PC, a set of 3D points, is obtained by applying the Constant False Alarm Rate algorithm to the Heatmap, which suppresses noise but results in sparse human-related features. To address these limitations, we study the feasibility of providing an alternative input paradigm: Differentiable Physics-driven Human Representation (DIPR), which represents humans as an ensemble of Gaussian distributions with kinematic and electromagnetic parameters. Inspired by Gaussian Splatting, DIPR leverages human kinematic priors and mmWave propagation physics to enhance human features while mitigating non-human noise through two strategies: 1) We incorporate prior kinematic knowledge to initialize DIPR based on the Heatmap and establish multi-faceted optimization objectives, ensuring biomechanical validity and enhancing motion features. 2) We simulate complete mmWave processing pipelines, re-render a new Heatmap from DIPR, and compare it with the original Heatmap, avoiding spurious noise generation due to kinematic constraints overfitting. Experimental results on three datasets with four methods demonstrate that existing mmWave-based HPE methods can easily integrate DIPR and achieve superior performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.23054v2",
    "pdf_url": "https://arxiv.org/pdf/2512.23054v2",
    "published_date": "2025-12-28",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "gaussian splatting",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hash Grid Feature Pruning",
    "authors": [
      "Yangzhi Ma",
      "Bojun Liu",
      "Jie Li",
      "Li Li",
      "Dong Liu"
    ],
    "abstract": "Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.",
    "arxiv_url": "https://arxiv.org/abs/2512.22882v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22882v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "authors": [
      "Wei-Tse Cheng",
      "Yen-Jen Chiou",
      "Yuan-Fu Yang"
    ],
    "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
    "arxiv_url": "https://arxiv.org/abs/2601.00705v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00705v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "slam",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis",
    "authors": [
      "Paul Dobre",
      "Jackson Cooper",
      "Xin Wang",
      "Hongzhou Yang"
    ],
    "abstract": "3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.",
    "arxiv_url": "https://arxiv.org/abs/2512.22706v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22706v1",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "ar",
      "dynamic",
      "lighting",
      "3d gaussian",
      "autonomous driving",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tracking by Predicting 3-D Gaussians Over Time",
    "authors": [
      "Tanish Baranwal",
      "Himanshu Gaurav Singh",
      "Jathushan Rajasegaran",
      "Jitendra Malik"
    ],
    "abstract": "We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.",
    "arxiv_url": "https://arxiv.org/abs/2512.22489v2",
    "pdf_url": "https://arxiv.org/pdf/2512.22489v2",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "ar",
      "tracking",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
    "authors": [
      "Zhe Wang",
      "Jinghang Li",
      "Yifei Zhu"
    ],
    "abstract": "Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.",
    "arxiv_url": "https://arxiv.org/abs/2512.20943v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20943v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.GR",
      "cs.DC",
      "cs.LG",
      "cs.MM",
      "cs.NI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "fast",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
    "authors": [
      "Yoonwoo Jeong",
      "Cheng Sun",
      "Frank Wang",
      "Minsu Cho",
      "Jaesung Choe"
    ],
    "abstract": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2512.20927v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20927v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
    "authors": [
      "He Zhu",
      "Zheng Liu",
      "Xingyang Li",
      "Anbang Wu",
      "Jieru Zhao",
      "Fangxin Liu",
      "Yiming Gan",
      "Jingwen Leng",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.   We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.",
    "arxiv_url": "https://arxiv.org/abs/2512.20495v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20495v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "acceleration",
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
    "authors": [
      "Linfei Li",
      "Lin Zhang",
      "Zhong Wang",
      "Ying Shen"
    ],
    "abstract": "Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.",
    "arxiv_url": "https://arxiv.org/abs/2512.20377v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20377v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
    "authors": [
      "Robert van de Ven",
      "Trim Bresilla",
      "Bram Nelissen",
      "Ard Nieuwenhuizen",
      "Eldert J. van Henten",
      "Gert Kootstra"
    ],
    "abstract": "Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.",
    "arxiv_url": "https://arxiv.org/abs/2512.20148v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20148v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
    "authors": [
      "Cyrus Vachha",
      "Yixiao Kang",
      "Zach Dive",
      "Ashwat Chidambaram",
      "Anik Gupta",
      "Eunice Jun",
      "Bjoern Hartmann"
    ],
    "abstract": "Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.",
    "arxiv_url": "https://arxiv.org/abs/2512.20129v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20129v2",
    "published_date": "2025-12-23",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xiaoxu Zheng",
      "Xinchao Wang"
    ],
    "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
    "arxiv_url": "https://arxiv.org/abs/2512.19678v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "geometry",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting as a Learned Dynamical System",
    "authors": [
      "Arnold Caleb Asiimwe",
      "Carl Vondrick"
    ],
    "abstract": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
    "arxiv_url": "https://arxiv.org/abs/2512.19648v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19648v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
    "authors": [
      "Tiantian Li",
      "Xinjie Zhang",
      "Xingtong Ge",
      "Tongda Xu",
      "Dailan He",
      "Jun Zhang",
      "Yan Wang"
    ],
    "abstract": "Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.",
    "arxiv_url": "https://arxiv.org/abs/2512.19108v2",
    "pdf_url": "https://arxiv.org/pdf/2512.19108v2",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images",
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization. However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians. To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time. EcoSplat adopts a two-stage optimization process. The first stage is Pixel-aligned Gaussian Training (PGT) where our model learns initial primitive prediction. The second stage is Importance-aware Gaussian Finetuning (IGF) stage where our model learns rank primitives and adaptively adjust their parameters based on the target primitive count. Extensive experiments across multiple dense-view settings show that EcoSplat is robust and outperforms state-of-the-art methods under strict primitive-count constraints, making it well-suited for flexible downstream rendering tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.18692v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18692v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
    "authors": [
      "Kai Kohyama",
      "Yoshimitsu Aoki",
      "Guillermo Gallego",
      "Shintaro Shiba"
    ],
    "abstract": "Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2512.18640v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18640v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ray tracing",
      "ar",
      "fast",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
    "authors": [
      "Zhenhao Zhou",
      "Dan Negrut"
    ],
    "abstract": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.",
    "arxiv_url": "https://arxiv.org/abs/2512.18619v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18619v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "authors": [
      "Philipp Langsteiner",
      "Jan-Niklas Dihlmann",
      "Hendrik P. A. Lensch"
    ],
    "abstract": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2512.18314v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18314v1",
    "published_date": "2025-12-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "relightable",
      "ar",
      "relighting",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding",
    "authors": [
      "Yue Li",
      "Qi Ma",
      "Runyi Yang",
      "Mengjiao Ma",
      "Bin Ren",
      "Nikola Popovic",
      "Nicu Sebe",
      "Theo Gevers",
      "Luc Van Gool",
      "Danda Pani Paudel",
      "Martin R. Oswald"
    ],
    "abstract": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.   We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2512.17817v2",
    "pdf_url": "https://arxiv.org/pdf/2512.17817v2",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting",
    "authors": [
      "Mehdi Hosseinzadeh",
      "Shin-Fang Chng",
      "Yi Xu",
      "Simon Lucey",
      "Ian Reid",
      "Ravi Garg"
    ],
    "abstract": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).",
    "arxiv_url": "https://arxiv.org/abs/2512.17547v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17547v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Voxel-GS: Quantized Scaffold Gaussian Splatting Compression with Run-Length Coding",
    "authors": [
      "Chunyang Fu",
      "Xiangrui Liu",
      "Shiqi Wang",
      "Zhu Li"
    ],
    "abstract": "Substantial Gaussian splatting format point clouds require effective compression. In this paper, we propose Voxel-GS, a simple yet highly effective framework that departs from the complex neural entropy models of prior work, instead achieving competitive performance using only a lightweight rate proxy and run-length coding. Specifically, we employ a differentiable quantization to discretize the Gaussian attributes of Scaffold-GS. Subsequently, a Laplacian-based rate proxy is devised to impose an entropy constraint, guiding the generation of high-fidelity and compact reconstructions. Finally, this integer-type Gaussian point cloud is compressed losslessly using Octree and run-length coding. Experiments validate that the proposed rate proxy accurately estimates the bitrate of run-length coding, enabling Voxel-GS to eliminate redundancy and optimize for a more compact representation. Consequently, our method achieves a remarkable compression ratio with significantly faster coding speeds than prior art. The code is available at https://github.com/zb12138/VoxelGS.",
    "arxiv_url": "https://arxiv.org/abs/2512.17528v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17528v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "ar",
      "fast",
      "compression",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation",
    "authors": [
      "Xijie Huang",
      "Jinhan Li",
      "Tianyue Wu",
      "Xin Zhou",
      "Zhichao Han",
      "Fei Gao"
    ],
    "abstract": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.",
    "arxiv_url": "https://arxiv.org/abs/2512.17349v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17349v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "illumination",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
    "authors": [
      "Kaiwen Jiang",
      "Xueting Li",
      "Seonwook Park",
      "Ravi Ramamoorthi",
      "Shalini De Mello",
      "Koki Nagano"
    ],
    "abstract": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
    "arxiv_url": "https://arxiv.org/abs/2512.16893v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "fast",
      "avatar",
      "lightweight",
      "gaussian splatting",
      "motion",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
    "authors": [
      "Antonella Rech",
      "Nicola Conci",
      "Nicola Garau"
    ],
    "abstract": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.16706v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
    "authors": [
      "Haodi He",
      "Jihun Yu",
      "Ronald Fedkiw"
    ],
    "abstract": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.16397v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "relightable",
      "nerf",
      "ar",
      "geometry",
      "human",
      "lighting",
      "gaussian splatting",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
    "authors": [
      "Divam Gupta",
      "Anuj Pahuja",
      "Nemanja Bartolovic",
      "Tomas Simon",
      "Forrest Iandola",
      "Giljoo Nam"
    ],
    "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.",
    "arxiv_url": "https://arxiv.org/abs/2512.15711v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15711v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "avatar",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Arthur Moreau",
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jisu Shin",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Songcen Xu",
      "Eduardo PÃ©rez-Pellitero"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
    "arxiv_url": "https://arxiv.org/abs/2512.15508v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15508v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
    "authors": [
      "Yuze Wu",
      "Mo Zhu",
      "Xingxing Li",
      "Yuheng Du",
      "Yuxin Fan",
      "Wenjun Li",
      "Zhichao Han",
      "Xin Zhou",
      "Fei Gao"
    ],
    "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
    "arxiv_url": "https://arxiv.org/abs/2512.15258v2",
    "pdf_url": "https://arxiv.org/pdf/2512.15258v2",
    "published_date": "2025-12-17",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance",
    "authors": [
      "Kaizhe Zhang",
      "Shinan Chen",
      "Qian Zhao",
      "Weizhan Zhang",
      "Caixia Yan",
      "Yudeng Xin"
    ],
    "abstract": "Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.15048v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15048v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
    "authors": [
      "Le Jiang",
      "Shaotong Zhu",
      "Yedi Luo",
      "Shayda Moezzi",
      "Sarah Ostadabbas"
    ],
    "abstract": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
    "arxiv_url": "https://arxiv.org/abs/2512.14406v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
    "authors": [
      "Kaizhe Zhang",
      "Yijie Zhou",
      "Weizhan Zhang",
      "Caixia Yan",
      "Haipeng Du",
      "yugui xie",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "abstract": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
    "arxiv_url": "https://arxiv.org/abs/2512.14352v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "nerf",
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
    "authors": [
      "Zhuoxiao Li",
      "Wenzong Ma",
      "Taoyu Wu",
      "Jinjing Zhu",
      "Zhenchao Q",
      "Shuai Zhang",
      "Jing Ou",
      "Yinrui Ren",
      "Weiqing Qi",
      "Guobin Shen",
      "Hui Xiong",
      "Wufan Zhao"
    ],
    "abstract": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.14200v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "geometry",
      "ar",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "urban scene",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
    "authors": [
      "Francesco Di Sario",
      "Daniel Rebain",
      "Dor Verbin",
      "Marco Grangetto",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.14180v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
    "authors": [
      "Yang Yang",
      "Risa Shinoda",
      "Hiroaki Santo",
      "Fumio Okura"
    ],
    "abstract": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
    "arxiv_url": "https://arxiv.org/abs/2512.14087v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
    "authors": [
      "Meng Wei",
      "Cheng Zhang",
      "Jianmin Zheng",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ],
    "abstract": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
    "arxiv_url": "https://arxiv.org/abs/2512.14039v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries",
    "authors": [
      "Victor Rong",
      "Jan Held",
      "Victor Chu",
      "Daniel Rebain",
      "Marc Van Droogenbroeck",
      "Kiriakos N. Kutulakos",
      "Andrea Tagliasacchi",
      "David B. Lindell"
    ],
    "abstract": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.13796v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13796v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "outdoor",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
    "authors": [
      "Patryk NiÅ¼eniec",
      "Marcin Iwanowski"
    ],
    "abstract": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
    "arxiv_url": "https://arxiv.org/abs/2512.13411v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13411v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
    "authors": [
      "Nikolai Goncharov",
      "James L. Gray",
      "Donald G. Dansereau"
    ],
    "abstract": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
    "arxiv_url": "https://arxiv.org/abs/2512.13007v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13007v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "robotics",
      "tracking",
      "reflection",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution",
    "authors": [
      "Abhinav Kumar",
      "Tristan Aumentado-Armstrong",
      "Lazar Valkov",
      "Gopal Sharma",
      "Alex Levinshtein",
      "Radek Grzeszczuk",
      "Suren Kumar"
    ],
    "abstract": "Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.12898v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12898v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior",
    "authors": [
      "Hao Wang",
      "Ashish Bastola",
      "Chaoyi Zhou",
      "Wenhui Zhu",
      "Xiwen Chen",
      "Xuanzhao Dong",
      "Siyu Huang",
      "Abolfazl Razi"
    ],
    "abstract": "As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.",
    "arxiv_url": "https://arxiv.org/abs/2512.12774v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12774v1",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "fast",
      "real-time rendering",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
    "authors": [
      "Jan U. MÃ¼ller",
      "Robin Tim Landsgesell",
      "Leif Van Holland",
      "Patrick Stotko",
      "Reinhard Klein"
    ],
    "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.11800v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11800v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ray tracing",
      "compact",
      "ar",
      "fast",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video",
    "authors": [
      "Meng-Li Shih",
      "Ying-Huan Chen",
      "Yu-Lun Liu",
      "Brian Curless"
    ],
    "abstract": "We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings",
    "arxiv_url": "https://arxiv.org/abs/2512.11356v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11356v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightweight 3D Gaussian Splatting Compression via Video Codec",
    "authors": [
      "Qi Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "abstract": "Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .",
    "arxiv_url": "https://arxiv.org/abs/2512.11186v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11186v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "compression",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
    "authors": [
      "Madhav Agarwal",
      "Mingtian Zhang",
      "Laura Sevilla-Lara",
      "Steven McDonagh"
    ],
    "abstract": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.10939v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10939v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "avatar",
      "gaussian splatting",
      "tracking",
      "head",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting",
    "authors": [
      "Shuyi Zhou",
      "Shengze Zhong",
      "Kenshi Takayama",
      "Takafumi Taketomi",
      "Takeshi Oishi"
    ],
    "abstract": "We propose DeMapGS, a structured Gaussian Splatting framework that jointly optimizes deformable surfaces and surface-attached 2D Gaussian splats. By anchoring splats to a deformable template mesh, our method overcomes topological inconsistencies and enhances editing flexibility, addressing limitations of prior Gaussian Splatting methods that treat points independently. The unified representation in our method supports extraction of high-fidelity diffuse, normal, and displacement maps, enabling the reconstructed mesh to inherit the photorealistic rendering quality of Gaussian Splatting. To support robust optimization, we introduce a gradient diffusion strategy that propagates supervision across the surface, along with an alternating 2D/3D rendering scheme to handle concave regions. Experiments demonstrate that DeMapGS achieves state-of-the-art mesh reconstruction quality and enables downstream applications for Gaussian splats such as editing and cross-object manipulation through a shared parametric surface.",
    "arxiv_url": "https://arxiv.org/abs/2512.10572v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10572v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "mapping",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Hamiltonian Deformation Fields for Dynamic Scene Rendering",
    "authors": [
      "Hai-Long Qin",
      "Sixian Wang",
      "Guo Lu",
      "Jincheng Dai"
    ],
    "abstract": "Representing and rendering dynamic scenes with complex motions remains challenging in computer vision and graphics. Recent dynamic view synthesis methods achieve high-quality rendering but often produce physically implausible motions. We introduce NeHaD, a neural deformation field for dynamic Gaussian Splatting governed by Hamiltonian mechanics. Our key observation is that existing methods using MLPs to predict deformation fields introduce inevitable biases, resulting in unnatural dynamics. By incorporating physics priors, we achieve robust and realistic dynamic scene rendering. Hamiltonian mechanics provides an ideal framework for modeling Gaussian deformation fields due to their shared phase-space structure, where primitives evolve along energy-conserving trajectories. We employ Hamiltonian neural networks to implicitly learn underlying physical laws governing deformation. Meanwhile, we introduce Boltzmann equilibrium decomposition, an energy-aware mechanism that adaptively separates static and dynamic Gaussians based on their spatial-temporal energy states for flexible rendering. To handle real-world dissipation, we employ second-order symplectic integration and local rigidity regularization as physics-informed constraints for robust dynamics modeling. Additionally, we extend NeHaD to adaptive streaming through scale-aware mipmapping and progressive optimization. Extensive experiments demonstrate that NeHaD achieves physically plausible results with a rendering quality-efficiency trade-off. To our knowledge, this is the first exploration leveraging Hamiltonian mechanics for neural Gaussian deformation, enabling physically realistic dynamic scene rendering with streaming capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2512.10424v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10424v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views",
    "authors": [
      "Zhankuo Xu",
      "Chaoran Feng",
      "Yingtao Li",
      "Jianbin Zhao",
      "Jiashu Yang",
      "Wangbo Yu",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.",
    "arxiv_url": "https://arxiv.org/abs/2512.10369v2",
    "pdf_url": "https://arxiv.org/pdf/2512.10369v2",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
    "authors": [
      "Karthikeya KV",
      "Narendra Bandaru"
    ],
    "abstract": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360Â° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360Â° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
    "arxiv_url": "https://arxiv.org/abs/2512.10293v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10293v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction",
    "authors": [
      "Chen Ziwen",
      "Hao Tan",
      "Peng Wang",
      "Zexiang Xu",
      "Li Fuxin"
    ],
    "abstract": "Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\\times540$ resolution, achieving 360Â° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential \"decompression\" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.10267v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10267v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "compression",
      "real-time rendering",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing",
    "authors": [
      "Jiachen Tao",
      "Junyi Wu",
      "Haoxuan Wang",
      "Zongxin Yang",
      "Dawen Cai",
      "Yan Yan"
    ],
    "abstract": "We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.10095v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10095v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ray tracing",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d reconstruction",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
    "authors": [
      "Patrick Noras",
      "Jun Myeong Choi",
      "Didier Stricker",
      "Pieter Peers",
      "Roni Sengupta"
    ],
    "abstract": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/",
    "arxiv_url": "https://arxiv.org/abs/2512.09925v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09925v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "relighting",
      "sparse-view",
      "lighting",
      "gaussian splatting",
      "light transport",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
    "authors": [
      "Or Hirschorn",
      "Omer Sela",
      "Inbar Huberman-Spiegelglas",
      "Netalee Efrat",
      "Eli Alshan",
      "Ianir Ideses",
      "Frederic Devernay",
      "Yochai Zvik",
      "Lior Fritz"
    ],
    "abstract": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2512.09923v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09923v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse-view",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
    "authors": [
      "Ryan Meegan",
      "Adam D'Souza",
      "Bryan Bo Cao",
      "Shubham Jain",
      "Kristin Dana"
    ],
    "abstract": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
    "arxiv_url": "https://arxiv.org/abs/2512.09903v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09903v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "recognition",
      "ar",
      "human",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat",
    "authors": [
      "Nicolas Marticorena",
      "Tobias Fischer",
      "Niko Suenderhauf"
    ],
    "abstract": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.",
    "arxiv_url": "https://arxiv.org/abs/2512.09656v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09656v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
    "authors": [
      "Seonghwa Choi",
      "Moonkyeong Choi",
      "Mingyu Jang",
      "Jaekyung Kim",
      "Jianfei Cai",
      "Wen-Huang Cheng",
      "Sanghoon Lee"
    ],
    "abstract": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
    "arxiv_url": "https://arxiv.org/abs/2512.09335v2",
    "pdf_url": "https://arxiv.org/pdf/2512.09335v2",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "body",
      "relightable",
      "nerf",
      "ar",
      "relighting",
      "avatar",
      "dynamic",
      "human",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "authors": [
      "Sangwoon Kwak",
      "Weeyoung Kwon",
      "Jun Young Jeong",
      "Geonho Kim",
      "Won-Sik Cheong",
      "Jihyong Oh"
    ],
    "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.09270v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09270v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars",
    "authors": [
      "Kelian Baert",
      "Mae Younes",
      "Francois Bourel",
      "Marc Christie",
      "Adnane Boukhayma"
    ],
    "abstract": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.",
    "arxiv_url": "https://arxiv.org/abs/2512.09162v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09162v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "ar",
      "geometry",
      "relighting",
      "avatar",
      "lighting",
      "gaussian splatting",
      "head",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics",
    "authors": [
      "Jisang Yoo",
      "Gyeongjin Kang",
      "Hyun-kyu Ko",
      "Hyeonwoo Yu",
      "Eunbyung Park"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.",
    "arxiv_url": "https://arxiv.org/abs/2512.08625v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08625v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "vr",
      "ar",
      "geometry",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "robotics",
      "segmentation",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs",
    "authors": [
      "Yijia Guo",
      "Tong Hu",
      "Zhiwei Li",
      "Liwen Hu",
      "Keming Qian",
      "Xitong Lin",
      "Shengbo Chen",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2512.08498v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08498v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "authors": [
      "Yuning Gong",
      "Yifei Liu",
      "Yifan Zhan",
      "Muyao Niu",
      "Xueying Li",
      "Yuanjun Liao",
      "Jiaming Chen",
      "Yuanyuan Gao",
      "Jiaqi Chen",
      "Minming Chen",
      "Li Zhou",
      "Yuning Zhang",
      "Wei Wang",
      "Xiaoqing Hou",
      "Huaxi Huang",
      "Shixiang Tang",
      "Le Ma",
      "Dingwen Zhang",
      "Xue Yang",
      "Junchi Yan",
      "Yanchi Zhang",
      "Yinqiang Zheng",
      "Xiao Sun",
      "Zhihang Zhong"
    ],
    "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
    "arxiv_url": "https://arxiv.org/abs/2512.08478v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08478v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "avatar",
      "dynamic",
      "neural rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting",
    "authors": [
      "Chang Liu",
      "Hongliang Yuan",
      "Lianghao Zhang",
      "Sichao Wang",
      "Jianwei Guo",
      "Shi-Sheng Huang"
    ],
    "abstract": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.08334v2",
    "pdf_url": "https://arxiv.org/pdf/2512.08334v2",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "acceleration",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation",
    "authors": [
      "Srijan Dokania",
      "Dharini Raghavan"
    ],
    "abstract": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.",
    "arxiv_url": "https://arxiv.org/abs/2512.08271v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08271v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "authors": [
      "Gyeongjin Kang",
      "Seungkwon Yang",
      "Seungtae Nam",
      "Younggeun Lee",
      "Jungwoo Kim",
      "Eunbyung Park"
    ],
    "abstract": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
    "arxiv_url": "https://arxiv.org/abs/2512.07806v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07806v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects",
    "authors": [
      "Shuohan Tao",
      "Boyao Zhou",
      "Hanzhang Tu",
      "Yuwang Wang",
      "Yebin Liu"
    ],
    "abstract": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.07381v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07381v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "sparse-view",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing",
    "authors": [
      "Ziming Hong",
      "Tianyu Huang",
      "Runnan Chen",
      "Shanshan Ye",
      "Mingming Gong",
      "Bo Han",
      "Tongliang Liu"
    ],
    "abstract": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.",
    "arxiv_url": "https://arxiv.org/abs/2512.07247v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07247v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STRinGS: Selective Text Refinement in Gaussian Splatting",
    "authors": [
      "Abhinav Raundhal",
      "Gaurav Behera",
      "P J Narayanan",
      "Ravi Kiran Sarvadevabhatla",
      "Makarand Tapaswi"
    ],
    "abstract": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.07230v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07230v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "authors": [
      "Seokhyun Youn",
      "Soohyun Lee",
      "Geonho Kim",
      "Weeyoung Kwon",
      "Sung-Ho Bae",
      "Jihyong Oh"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
    "arxiv_url": "https://arxiv.org/abs/2512.07197v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07197v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "compact",
      "4d",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation",
    "authors": [
      "Muyu Xu",
      "Fangneng Zhan",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.07165v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07165v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse-view",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision",
    "authors": [
      "Jaeyoon Lee",
      "Hojoon Jung",
      "Sungtae Hwang",
      "Jihyong Oh",
      "Jongwon Choi"
    ],
    "abstract": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.07107v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07107v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "ar",
      "relighting",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting",
    "authors": [
      "Hoang-Nhat Tran",
      "Francesco Di Sario",
      "Gabriele Spadaro",
      "Giuseppe Valenzise",
      "Enzo Tartaglione"
    ],
    "abstract": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.",
    "arxiv_url": "https://arxiv.org/abs/2512.07052v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07052v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "authors": [
      "Jan Held",
      "Sanghyun Son",
      "Renaud Vandeghen",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Anthony Cioppa",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2512.06818v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06818v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "vr",
      "ar",
      "fast",
      "real-time rendering",
      "geometry",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting",
    "authors": [
      "Longjie Zhao",
      "Ziming Hong",
      "Zhenyang Ren",
      "Runnan Chen",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.06774v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06774v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy",
    "authors": [
      "Yumeng He",
      "Zanwei Zhou",
      "Yekun Zheng",
      "Chen Liang",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.",
    "arxiv_url": "https://arxiv.org/abs/2512.06684v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06684v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars",
    "authors": [
      "Ramazan Fazylov",
      "Sergey Zagoruyko",
      "Aleksandr Parkin",
      "Stamatis Lefkimmiatis",
      "Ivan Laptev"
    ],
    "abstract": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/",
    "arxiv_url": "https://arxiv.org/abs/2512.06438v2",
    "pdf_url": "https://arxiv.org/pdf/2512.06438v2",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "nerf",
      "vr",
      "ar",
      "human",
      "avatar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting",
    "authors": [
      "Quan Tran",
      "Tuan Dang"
    ],
    "abstract": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.",
    "arxiv_url": "https://arxiv.org/abs/2512.06269v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06269v1",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation",
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Himangi Mittal",
      "Gaurav Mittal",
      "Rohith Kukkala",
      "Yingjie Victor Chen",
      "Mei Chen"
    ],
    "abstract": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.",
    "arxiv_url": "https://arxiv.org/abs/2512.06158v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06158v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "tracking",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
    "authors": [
      "Cheng-Yuan Ho",
      "He-Bi Yang",
      "Jui-Chiu Chiang",
      "Yu-Lun Liu",
      "Wen-Hsiao Peng"
    ],
    "abstract": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.05446v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05446v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "4d",
      "ar",
      "compression",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training",
    "authors": [
      "Yang Zheng",
      "Hao Tan",
      "Kai Zhang",
      "Peng Wang",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Wang Yifan"
    ],
    "abstract": "The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \\ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.",
    "arxiv_url": "https://arxiv.org/abs/2512.05354v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05354v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "authors": [
      "Hao-Jen Chien",
      "Yi-Chuan Huang",
      "Chung-Ho Wu",
      "Wei-Lun Chao",
      "Yu-Lun Liu"
    ],
    "abstract": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
    "arxiv_url": "https://arxiv.org/abs/2512.05113v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05113v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "authors": [
      "Xianfeng Wu",
      "Yajing Bai",
      "Minghan Li",
      "Xianzu Wu",
      "Xueqi Zhao",
      "Zhongyuan Lai",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
    "arxiv_url": "https://arxiv.org/abs/2512.05060v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05060v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "4d",
      "nerf",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS",
    "authors": [
      "Chuanyu Fu",
      "Guanying Chen",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Yasuyuki Matsushita",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.",
    "arxiv_url": "https://arxiv.org/abs/2512.04815v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04815v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "illumination",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting",
    "authors": [
      "Jian Tang",
      "Pu Pang",
      "Haowen Sun",
      "Chengzhong Ma",
      "Xingyu Chen",
      "Hua Huang",
      "Xuguang Lan"
    ],
    "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.04731v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04731v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization",
    "authors": [
      "Hong Kuang",
      "Jianchen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \\replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\\&T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2512.04542v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04542v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting",
    "authors": [
      "Yonghan Lee",
      "Tsung-Wei Huang",
      "Shiv Gehlot",
      "Jaehoon Choi",
      "Guan-Ming Su",
      "Dinesh Manocha"
    ],
    "abstract": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.",
    "arxiv_url": "https://arxiv.org/abs/2512.04315v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04315v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "nerf",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding",
    "authors": [
      "Haolin Xiong",
      "Tianwen Fu",
      "Pratusha Bhuvana Prasad",
      "Yunxuan Cai",
      "Haiwei Chen",
      "Wenbin Teng",
      "Hanyuan Xiao",
      "Yajie Zhao"
    ],
    "abstract": "Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.04313v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04313v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "C3G: Learning Compact 3D Representations with 2K Gaussians",
    "authors": [
      "Honggyu An",
      "Jaewoo Jung",
      "Mungyeom Kim",
      "Sunghwan Hong",
      "Chaehyun Kim",
      "Kazumi Fukuda",
      "Minkyeong Jeon",
      "Jisang Han",
      "Takuya Narihira",
      "Hyuna Ko",
      "Junsu Kim",
      "Yuki Mitsufuji",
      "Seungryong Kim"
    ],
    "abstract": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.04021v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04021v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "understanding",
      "compact",
      "ar",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding",
    "authors": [
      "Haoran Zhou",
      "Gim Hee Lee"
    ],
    "abstract": "Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.",
    "arxiv_url": "https://arxiv.org/abs/2512.03601v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03601v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion",
      "tracking",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models",
    "authors": [
      "Tianchen Deng",
      "Yue Pan",
      "Shenghai Yuan",
      "Dong Li",
      "Chen Wang",
      "Mingrui Li",
      "Long Chen",
      "Lihua Xie",
      "Danwei Wang",
      "Jingchuan Wang",
      "Javier Civera",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.",
    "arxiv_url": "https://arxiv.org/abs/2512.03422v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03422v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "survey",
      "semantic",
      "nerf",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction",
    "authors": [
      "Jingkang Wang",
      "Henry Che",
      "Yun Chen",
      "Ze Yang",
      "Lily Goli",
      "Sivabalan Manivasagam",
      "Raquel Urtasun"
    ],
    "abstract": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.03210v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03210v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "4d",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Fast Volumetric Capture and Reconstruction Pipeline for Dynamic Point Clouds and Gaussian Splats",
    "authors": [
      "Athanasios Charisoudis",
      "Simone Croci",
      "Lam Kit Yung",
      "Pascal Frossard",
      "Aljosa Smolic"
    ],
    "abstract": "We present a fast and efficient volumetric capture and reconstruction system that processes either RGB-D or RGB-only input to generate 3D representations in the form of point clouds and Gaussian splats. For Gaussian splat reconstructions, we took the GPS-Gaussian regressor and improved it, enabling high-quality reconstructions with minimal overhead. The system is designed for easy setup and deployment, supporting in-the-wild operation under uncontrolled illumination and arbitrary backgrounds, as well as flexible camera configurations, including sparse setups, arbitrary camera numbers and baselines. Captured data can be exported in standard formats such as PLY, MPEG V-PCC, and SPLAT, and visualized through a web-based viewer or Unity/Unreal plugins. A live on-location preview of both input and reconstruction is available at 5-10 FPS. We present qualitative findings focused on deployability and targeted ablations. The complete framework is open-source, facilitating reproducibility and further research.",
    "arxiv_url": "https://arxiv.org/abs/2512.15719v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15719v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "fast",
      "ar",
      "dynamic",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
    "authors": [
      "Yancheng Zhang",
      "Guangyu Sun",
      "Chen Chen"
    ],
    "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
    "arxiv_url": "https://arxiv.org/abs/2512.02932v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02932v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "geometry",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes",
    "authors": [
      "Derui Shan",
      "Qian Qiao",
      "Hao Lu",
      "Tao Du",
      "Peng Lu"
    ],
    "abstract": "Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.02664v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02664v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "geometry",
      "real-time rendering",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking",
    "authors": [
      "Dong Li",
      "Jiahao Xiong",
      "Yingda Huang",
      "Le Chang"
    ],
    "abstract": "We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D",
    "arxiv_url": "https://arxiv.org/abs/2512.02648v2",
    "pdf_url": "https://arxiv.org/pdf/2512.02648v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Content-Aware Texturing for Gaussian Splatting",
    "authors": [
      "Panagiotis Papantonakis",
      "Georgios Kopanas",
      "Fredo Durand",
      "George Drettakis"
    ],
    "abstract": "Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.   Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/",
    "arxiv_url": "https://arxiv.org/abs/2512.02621v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02621v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "real-time rendering",
      "gaussian splatting",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline",
    "authors": [
      "Vishwesh Nath",
      "Javier G. Tejero",
      "Ruilong Li",
      "Filippo Filicori",
      "Mahdi Azizian",
      "Sean D. Huver"
    ],
    "abstract": "We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.02482v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02482v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM",
    "authors": [
      "Zihan Zhu",
      "Wei Zhang",
      "Norbert Haala",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.02293v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02293v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting",
    "authors": [
      "Pranav Asthana",
      "Alex Hanson",
      "Allen Tu",
      "Tom Goldstein",
      "Matthias Zwicker",
      "Amitabh Varshney"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.",
    "arxiv_url": "https://arxiv.org/abs/2512.02172v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02172v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
    "authors": [
      "Chenyang Gu",
      "Jiaming Liu",
      "Hao Chen",
      "Runzhong Huang",
      "Qingpo Wuwu",
      "Zhuoyang Liu",
      "Xiaoqi Li",
      "Ying Li",
      "Renrui Zhang",
      "Peng Jia",
      "Pheng-Ann Heng",
      "Shanghang Zhang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.02013v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02013v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh Modeling and Tracking",
    "authors": [
      "Hanzhi Guo",
      "Dongdong Weng",
      "Mo Su",
      "Yixiao Chen",
      "Xiaonuo Dongye",
      "Chenyu Xu"
    ],
    "abstract": "Topology-consistent dynamic model sequences are essential for applications such as animation and model editing. However, existing 4D reconstruction methods face challenges in generating high-quality topology-consistent meshes. To address this, we propose a topology-aware dynamic reconstruction framework based on Gaussian Splatting. We introduce a Gaussian topological structure that explicitly encodes spatial connectivity. This structure enables topology-aware densification and pruning, preserving the manifold consistency of the Gaussian representation. Temporal regularization terms further ensure topological coherence over time, while differentiable mesh rasterization improves mesh quality. Experimental results demonstrate that our method reconstructs topology-consistent mesh sequences with significantly higher accuracy than existing approaches. Moreover, the resulting meshes enable precise 3D keypoint tracking. Project page: https://haza628.github.io/tagSplat/",
    "arxiv_url": "https://arxiv.org/abs/2512.01329v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01329v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "tracking",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly",
    "authors": [
      "Xiaokun Pan",
      "Zhenzhe Li",
      "Zhichao Ye",
      "Hongjia Zhai",
      "Guofeng Zhang"
    ],
    "abstract": "Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/",
    "arxiv_url": "https://arxiv.org/abs/2512.01296v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01296v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "geometry",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "tracking",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency",
    "authors": [
      "Zhongbin Guo",
      "Jiahe Liu",
      "Wenyu Gao",
      "Yushan Li",
      "Chengzhi Li",
      "Ping Jian"
    ],
    "abstract": "Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.",
    "arxiv_url": "https://arxiv.org/abs/2512.01008v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01008v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head",
    "authors": [
      "Chang Liu",
      "Tianjiao Jing",
      "Chengcheng Ma",
      "Xuanqi Zhou",
      "Zhengxuan Lian",
      "Qin Jin",
      "Hongliang Yuan",
      "Shi-Sheng Huang"
    ],
    "abstract": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.",
    "arxiv_url": "https://arxiv.org/abs/2512.05991v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05991v2",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation",
    "authors": [
      "An Yang",
      "Chenyu Liu",
      "Jun Du",
      "Jianqing Gao",
      "Jia Pan",
      "Jinshui Hu",
      "Baocai Yin",
      "Bing Yin",
      "Cong Liu"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.",
    "arxiv_url": "https://arxiv.org/abs/2512.00944v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00944v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "compact",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling",
    "authors": [
      "Zhening Liu",
      "Rui Song",
      "Yushi Huang",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.",
    "arxiv_url": "https://arxiv.org/abs/2512.00877v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00877v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "abstract": "We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.",
    "arxiv_url": "https://arxiv.org/abs/2512.00850v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00850v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "compact",
      "semantic",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery",
    "authors": [
      "Bo Guo",
      "Sijia Wen",
      "Yifan Zhao",
      "Jia Li",
      "Zhiming Zheng"
    ],
    "abstract": "Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.00794v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00794v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
    "authors": [
      "Dong In Lee",
      "Hyungjun Doh",
      "Seunggeun Chi",
      "Runlin Duan",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "abstract": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
    "arxiv_url": "https://arxiv.org/abs/2512.00677v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00677v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "nerf",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions",
    "authors": [
      "Sandika Biswas",
      "Qianyi Wu",
      "Biplab Banerjee",
      "Hamid Rezatofighi"
    ],
    "abstract": "Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.00547v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00547v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "semantic",
      "vr",
      "ar",
      "geometry",
      "fast",
      "dynamic",
      "human",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
    "authors": [
      "Zeyuan An",
      "Yanghang Xiao",
      "Zhiying Leng",
      "Frederick W. B. Li",
      "Xiaohui Liang"
    ],
    "abstract": "Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
    "arxiv_url": "https://arxiv.org/abs/2512.00534v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00534v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control",
    "authors": [
      "Ji Gan",
      "Lingxu Chen",
      "Jiaxu Leng",
      "Xinbo Gao"
    ],
    "abstract": "Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.00413v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00413v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "nerf",
      "ar",
      "fast",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion",
    "authors": [
      "Rui Qian",
      "Haozhi Cao",
      "Tianchen Deng",
      "Tianxin Hu",
      "Weixiang Guo",
      "Shenghai Yuan",
      "Lihua Xie"
    ],
    "abstract": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2512.00300v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00300v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "ar",
      "geometry",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views",
    "authors": [
      "Kunwar Maheep Singh",
      "Jianchun Chen",
      "Vladislav Golyanik",
      "Stephan J. Garbin",
      "Thabo Beeler",
      "Rishabh Dabral",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/",
    "arxiv_url": "https://arxiv.org/abs/2512.00255v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00255v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "relightable",
      "illumination",
      "relighting",
      "geometry",
      "dynamic",
      "sparse view",
      "sparse-view",
      "lighting",
      "3d gaussian",
      "human",
      "motion",
      "tracking",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting",
    "authors": [
      "Tianhao Xie",
      "Linlian Jiang",
      "Xinxin Zuo",
      "Yang Wang",
      "Tiberiu Popa"
    ],
    "abstract": "Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.",
    "arxiv_url": "https://arxiv.org/abs/2511.23292v2",
    "pdf_url": "https://arxiv.org/pdf/2511.23292v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis",
    "authors": [
      "Yiwei Li",
      "Jiannong Cao",
      "Penghui Ruan",
      "Divya Saxena",
      "Songye Zhu",
      "Yinfeng Cao"
    ],
    "abstract": "Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.",
    "arxiv_url": "https://arxiv.org/abs/2511.23044v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23044v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management",
    "authors": [
      "Casimir Feldmann",
      "Maximum Wilder-Smith",
      "Vaishakh Patil",
      "Michael Oechsle",
      "Michael Niemeyer",
      "Keisuke Tateno",
      "Marco Hutter"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.23030v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23030v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis",
    "authors": [
      "Minseong Kweon",
      "Janghyun Kim",
      "Ukcheol Shin",
      "Jinsun Park"
    ],
    "abstract": "Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.",
    "arxiv_url": "https://arxiv.org/abs/2511.22997v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22997v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DenoiseGS: Gaussian Reconstruction Model for Burst Denoising",
    "authors": [
      "Yongsen Cheng",
      "Yuanhao Cai",
      "Yulun Zhang"
    ],
    "abstract": "Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving 250$\\times$ faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.22939v2",
    "pdf_url": "https://arxiv.org/pdf/2511.22939v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels",
    "authors": [
      "Bhavya Sai Nukapotula",
      "Rishabh Tripathi",
      "Seth Pregler",
      "Dileep Kalathil",
      "Srinivas Shakkottai",
      "Theodore S. Rappaport"
    ],
    "abstract": "Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \\href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.",
    "arxiv_url": "https://arxiv.org/abs/2511.22793v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22793v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction",
    "authors": [
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Zhanfeng Liao",
      "Zihan Ma",
      "Hanzhang Tu",
      "Boning Liu",
      "Yebin Liu"
    ],
    "abstract": "We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.",
    "arxiv_url": "https://arxiv.org/abs/2511.22704v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22704v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?",
    "authors": [
      "Wenkai Huang",
      "Yijia Guo",
      "Gaolei Li",
      "Lei Ma",
      "Hang Zhang",
      "Liwen Hu",
      "Jiazheng Wang",
      "Jianhua Li",
      "Tiejun Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.",
    "arxiv_url": "https://arxiv.org/abs/2511.22262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22262v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution",
    "authors": [
      "Xiang Feng",
      "Tieshi Zhong",
      "Shuo Chang",
      "Weiliu Wang",
      "Chengkai Wang",
      "Yifei Chen",
      "Yuhe Wang",
      "Zhenzhong Kuang",
      "Xuefei Yin",
      "Yanming Zhu"
    ],
    "abstract": "Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2511.22233v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22233v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-Consistent Multi-View Editing by Diffusion Guidance",
    "authors": [
      "Josef Bengtson",
      "David Nilsson",
      "Dong In Lee",
      "Fredrik Kahl"
    ],
    "abstract": "Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.22228v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22228v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks",
    "authors": [
      "Yanping Li",
      "Zhening Liu",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.",
    "arxiv_url": "https://arxiv.org/abs/2511.22147v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22147v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum",
    "authors": [
      "Xiaohan Zhang",
      "Kan Liu",
      "Yangle Liu",
      "Fengze Li",
      "Jieming Ma",
      "Yue Li"
    ],
    "abstract": "In the Virtual Reality (VR) gaming industry, maintaining immersion during real-world interruptions remains a challenge, particularly during transitions along the reality-virtuality continuum (RVC). Existing methods tend to rely on digital replicas or simple visual transitions, neglecting to address the aesthetic discontinuities between real and virtual environments, especially in highly stylized VR games. This paper introduces the Environment-Aware Stylized Transition (EAST) framework, which employs a novel style-transferred 3D Gaussian Splatting (3DGS) technique to transfer real-world interruptions into the virtual environment with seamless aesthetic consistency. Rather than merely transforming the real world into game-like visuals, EAST minimizes the disruptive impact of interruptions by integrating real-world elements within the framework. Qualitative user studies demonstrate significant enhancements in cognitive comfort and emotional continuity during transitions, while quantitative experiments highlight EAST's ability to maintain visual coherence across diverse VR styles.",
    "arxiv_url": "https://arxiv.org/abs/2511.22056v2",
    "pdf_url": "https://arxiv.org/pdf/2511.22056v2",
    "published_date": "2025-11-27",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Resolution Where It Counts: Hash-based GPU-Accelerated 3D Reconstruction via Variance-Adaptive Voxel Grids",
    "authors": [
      "Lorenzo De Rebotti",
      "Emanuele Giacomini",
      "Giorgio Grisetti",
      "Luca Di Giammarino"
    ],
    "abstract": "Efficient and scalable 3D surface reconstruction from range data remains a core challenge in computer graphics and vision, particularly in real-time and resource-constrained scenarios. Traditional volumetric methods based on fixed-resolution voxel grids or hierarchical structures like octrees often suffer from memory inefficiency, computational overhead, and a lack of GPU support. We propose a novel variance-adaptive, multi-resolution voxel grid that dynamically adjusts voxel size based on the local variance of signed distance field (SDF) observations. Unlike prior multi-resolution approaches that rely on recursive octree structures, our method leverages a flat spatial hash table to store all voxel blocks, supporting constant-time access and full GPU parallelism. This design enables high memory efficiency and real-time scalability. We further demonstrate how our representation supports GPU-accelerated rendering through a parallel quad-tree structure for Gaussian Splatting, enabling effective control over splat density. Our open-source CUDA/C++ implementation achieves up to 13x speedup and 4x lower memory usage compared to fixed-resolution baselines, while maintaining on par results in terms of reconstruction accuracy, offering a practical and extensible solution for high-performance 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.21459v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21459v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting",
      "head",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes",
    "authors": [
      "Yangle Liu",
      "Fengze Li",
      "Kan Liu",
      "Jieming Ma"
    ],
    "abstract": "Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.21367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21367v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "nerf",
      "ar",
      "geometry",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "motion",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting",
    "authors": [
      "Juncheng Chen",
      "Chao Xu",
      "Yanjun Cao"
    ],
    "abstract": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.",
    "arxiv_url": "https://arxiv.org/abs/2511.21265v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21265v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting",
    "authors": [
      "Haoliang Han",
      "Ziyuan Luo",
      "Jun Qi",
      "Anderson Rocha",
      "Renjie Wan"
    ],
    "abstract": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.",
    "arxiv_url": "https://arxiv.org/abs/2511.20354v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20354v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin",
    "authors": [
      "Andy Huynh",
      "JoÃ£o Malheiro Silva",
      "Holger Caesar",
      "Tong Duy Son"
    ],
    "abstract": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.20348v2",
    "pdf_url": "https://arxiv.org/pdf/2511.20348v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "authors": [
      "GigaWorld Team",
      "Angen Ye",
      "Boyuan Wang",
      "Chaojun Ni",
      "Guan Huang",
      "Guosheng Zhao",
      "Haoyun Li",
      "Jiagang Zhu",
      "Kerui Li",
      "Mengyuan Xu",
      "Qiuping Deng",
      "Siting Wang",
      "Wenkang Qin",
      "Xinze Chen",
      "Xiaofeng Wang",
      "Yankai Wang",
      "Yu Cao",
      "Yifan Chang",
      "Yuan Xu",
      "Yun Ye",
      "Yang Wang",
      "Yukun Zhou",
      "Zhengyuan Zhang",
      "Zhehao Dong",
      "Zheng Zhu"
    ],
    "abstract": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
    "arxiv_url": "https://arxiv.org/abs/2511.19861v2",
    "pdf_url": "https://arxiv.org/pdf/2511.19861v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction",
    "authors": [
      "Jiankuo Zhao",
      "Xiangyu Zhu",
      "Zidu Wang",
      "Zhen Lei"
    ],
    "abstract": "Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2511.19854v2",
    "pdf_url": "https://arxiv.org/pdf/2511.19854v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
    "authors": [
      "Phurtivilai Patt",
      "Leyang Huang",
      "Yinqiang Zhang",
      "Yang Lei"
    ],
    "abstract": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
    "arxiv_url": "https://arxiv.org/abs/2511.19294v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19294v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
    "authors": [
      "Carl LindstrÃ¶m",
      "Mahan Rafidashti",
      "Maryam Fatemi",
      "Lars Hammarstrand",
      "Martin R. Oswald",
      "Lennart Svensson"
    ],
    "abstract": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2511.19235v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19235v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
    "authors": [
      "Brent Zoomers",
      "Florian Hahlbohm",
      "Joni Vanherck",
      "Lode Jorissen",
      "Marcus Magnor",
      "Nick Michiels"
    ],
    "abstract": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
    "arxiv_url": "https://arxiv.org/abs/2511.19202v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19202v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
    "authors": [
      "Kehua Chen",
      "Tianlu Mao",
      "Zhuxin Ma",
      "Hao Jiang",
      "Zehao Li",
      "Zihan Liu",
      "Shuqi Gao",
      "Honglong Zhao",
      "Feng Dai",
      "Yucheng Zhang",
      "Zhaoqi Wang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.19172v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19172v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation",
    "authors": [
      "Jaeyeong Kim",
      "Seungwoo Yoo",
      "Minhyuk Sung"
    ],
    "abstract": "We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.",
    "arxiv_url": "https://arxiv.org/abs/2511.19542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19542v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "nerf",
      "ar",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction",
    "authors": [
      "Yiming Wang",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Siyu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2511.18873v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18873v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing",
    "authors": [
      "Xiaotong Huang",
      "He Zhu",
      "Tianrui Ma",
      "Yuxiang Xiong",
      "Fangxin Liu",
      "Zhezhi He",
      "Yiming Gan",
      "Zihan Liu",
      "Jingwen Leng",
      "Yu Feng",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.   This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $Î±$-checking. Together, these optimizations yield up to 121.7$\\times$ speedup on the bottleneck stages and 14.6$\\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\\times$ speedup and 4738.5$\\times$ energy savings over mobile GPUs and up to 25.2$\\times$ speedup and 241.1$\\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2511.18755v2",
    "pdf_url": "https://arxiv.org/pdf/2511.18755v2",
    "published_date": "2025-11-24",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeAR: Coupled Neural Asset-Renderer Stack",
    "authors": [
      "Hong Li",
      "Chongjie Ye",
      "Houyuan Chen",
      "Weiqing Xiao",
      "Ziyang Yan",
      "Lixing Xiao",
      "Zhaoxi Chen",
      "Jianfeng Xiang",
      "Shaocong Xu",
      "Xuhui Liu",
      "Yikai Wang",
      "Baochang Zhang",
      "Xiaoguang Han",
      "Jiaolong Yang",
      "Hao Zhao"
    ],
    "abstract": "Neural asset authoring and neural rendering have traditionally evolved as disjoint paradigms: one generates digital assets for fixed graphics pipelines, while the other maps conventional assets to images. However, treating them as independent entities limits the potential for end-to-end optimization in fidelity and consistency. In this paper, we bridge this gap with NeAR, a Coupled Neural Asset--Renderer Stack. We argue that co-designing the asset representation and the renderer creates a robust \"contract\" for superior generation. On the asset side, we introduce the Lighting-Homogenized SLAT (LH-SLAT). Leveraging a rectified-flow model, NeAR lifts casually lit single images into a canonical, illumination-invariant latent space, effectively suppressing baked-in shadows and highlights. On the renderer side, we design a lighting-aware neural decoder tailored to interpret these homogenized latents. Conditioned on HDR environment maps and camera views, it synthesizes relightable 3D Gaussian splats in real-time without per-object optimization. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit reconstruction, (3) unknown-lit relighting, and (4) novel-view relighting. Extensive experiments demonstrate that our coupled stack outperforms state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.",
    "arxiv_url": "https://arxiv.org/abs/2511.18600v2",
    "pdf_url": "https://arxiv.org/pdf/2511.18600v2",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "illumination",
      "relighting",
      "ar",
      "neural rendering",
      "lighting",
      "3d gaussian",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation",
    "authors": [
      "Samarth Chopra",
      "Jing Liang",
      "Gershom Seneviratne",
      "Dinesh Manocha"
    ],
    "abstract": "Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.",
    "arxiv_url": "https://arxiv.org/abs/2511.18570v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18570v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation",
    "authors": [
      "Samarth Chopra",
      "Jing Liang",
      "Gershom Seneviratne",
      "Yonghan Lee",
      "Jaehoon Choi",
      "Jianyu An",
      "Stephen Cheng",
      "Dinesh Manocha"
    ],
    "abstract": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io",
    "arxiv_url": "https://arxiv.org/abs/2511.18525v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18525v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "outdoor",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReCoGS: Real-time ReColoring for Gaussian Splatting scenes",
    "authors": [
      "Lorenzo Rutayisire",
      "Nicola Capodieci",
      "Fabio Pellacini"
    ],
    "abstract": "Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.",
    "arxiv_url": "https://arxiv.org/abs/2511.18441v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18441v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation",
    "authors": [
      "Peter Siegel",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \\textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.",
    "arxiv_url": "https://arxiv.org/abs/2511.18386v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18386v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "compact",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Alias-free 4D Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Huan-ang Gao",
      "Delin Qu",
      "Haohan Chi",
      "Hao Tang",
      "Kai Zhang",
      "Hao Zhao"
    ],
    "abstract": "Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/",
    "arxiv_url": "https://arxiv.org/abs/2511.18367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18367v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting",
    "authors": [
      "Yilong Wang",
      "Cheng Qian",
      "Ruomeng Fan",
      "Edward Johns"
    ],
    "abstract": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2511.18140v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18140v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse view",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion",
    "authors": [
      "Yan Xu",
      "Yixing Wang",
      "Stella X. Yu"
    ],
    "abstract": "Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \\emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \\emph{completing a natural video} unfolding through space.   We recast the task as \\emph{test-time natural video completion}, using powerful priors from \\emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \\emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \\emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \\emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.   The result is coherent, high-fidelity renderings from sparse inputs \\emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.",
    "arxiv_url": "https://arxiv.org/abs/2511.17932v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17932v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
    "authors": [
      "Youngsik Yun",
      "Dongjun Gu",
      "Youngjung Uh"
    ],
    "abstract": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
    "arxiv_url": "https://arxiv.org/abs/2511.17918v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17918v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation",
    "authors": [
      "Yuhang Ming",
      "Chenxin Fang",
      "Xingyuan Yu",
      "Fan Zhang",
      "Weichen Dai",
      "Wanzeng Kong",
      "Guofeng Zhang"
    ],
    "abstract": "Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.",
    "arxiv_url": "https://arxiv.org/abs/2511.17904v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17904v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "compact",
      "semantic",
      "ar",
      "geometry",
      "dynamic",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations",
    "authors": [
      "Dawid Wolkiewicz",
      "Anastasiya Pechko",
      "PrzemysÅaw Spurek",
      "Piotr Syga"
    ],
    "abstract": "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.",
    "arxiv_url": "https://arxiv.org/abs/2511.17747v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17747v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Yijun Xu",
      "Jingrui Zhang",
      "Hongyi Liu",
      "Yuhan Chen",
      "Yuanyang Wang",
      "Qingyao Guo",
      "Dingwen Wang",
      "Lei Yu",
      "Chu He"
    ],
    "abstract": "Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.17116v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17116v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation",
    "authors": [
      "Sergio Torregrosa",
      "David Munoz",
      "Hector Navarro",
      "Charbel Farhat",
      "Francisco Chinesta"
    ],
    "abstract": "Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.",
    "arxiv_url": "https://arxiv.org/abs/2511.17111v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17111v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPAGS: Sparse-View Articulated Object Reconstruction from Single State via Planar Gaussian Splatting",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Xueyu Yuan",
      "Qiaojun Yu",
      "Wenxiao Chen",
      "Ruilong Yan",
      "Yiming Tang",
      "Liangtu Song"
    ],
    "abstract": "Articulated objects are ubiquitous in daily environments, and their 3D reconstruction holds great significance across various fields. However, existing articulated object reconstruction methods typically require costly inputs such as multi-stage and multi-view observations. To address the limitations, we propose a category-agnostic articulated object reconstruction framework via planar Gaussian Splatting, which only uses sparse-view RGB images from a single state. Specifically, we first introduce a Gaussian information field to perceive the optimal sparse viewpoints from candidate camera poses. Then we compress 3D Gaussians into planar Gaussians to facilitate accurate estimation of normal and depth. The planar Gaussians are optimized in a coarse-to-fine manner through depth smooth regularization and few-shot diffusion. Moreover, we introduce a part segmentation probability for each Gaussian primitive and update them by back-projecting part segmentation masks of renderings. Extensive experimental results demonstrate that our method achieves higher-fidelity part-level surface reconstruction on both synthetic and real-world data than existing methods. Codes will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2511.17092v2",
    "pdf_url": "https://arxiv.org/pdf/2511.17092v2",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "ar",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Anran Huang",
      "Yuyan Liu",
      "Qiaojun Yu",
      "Shaofan Liu",
      "Liangtu Song",
      "Cewu Lu"
    ],
    "abstract": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.",
    "arxiv_url": "https://arxiv.org/abs/2511.17059v2",
    "pdf_url": "https://arxiv.org/pdf/2511.17059v2",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysMorph-GS: Differentiable Shape Morphing via Joint Optimization of Physics and Rendering Objectives",
    "authors": [
      "Chang-Yong Song",
      "David Hyde"
    ],
    "abstract": "Shape morphing with physics-based simulation naturally supports large deformations and topology changes, but existing methods suffer from a \"rendering gap\": nondifferentiable surface extraction prevents image losses from directly guiding physics optimization. We introduce PhysMorph-GS, which couples a differentiable material point method (MPM) with 3D Gaussian splatting through a deformation-aware upsampling bridge that maps sparse particle states (x, F) to dense Gaussians (mu, Sigma). Multi-modal rendering losses on silhouette and depth backpropagate along two paths, from covariances to deformation gradients via a stretch-based mapping and from Gaussian means to particle positions. Through the MPM adjoint, these gradients update deformation controls while mass is conserved at a compact set of anchor particles. A multi-pass interleaved optimization scheme repeatedly injects rendering gradients into successive physics steps, avoiding collapse to purely physics-driven solutions. On challenging morphing sequences, PhysMorph-GS improves boundary fidelity and temporal stability over a differentiable MPM baseline and better reconstructs thin structures such as ears and tails. Quantitatively, our depth-supervised variant reduces Chamfer distance by about 2.5 percent relative to the physics-only baseline. By providing a differentiable particle-to-Gaussian bridge, PhysMorph-GS closes a key gap in physics-aware rendering pipelines and enables inverse design directly from image-space supervision.",
    "arxiv_url": "https://arxiv.org/abs/2511.16988v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16988v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "deformation",
      "ar",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements",
    "authors": [
      "Yiheng Bian",
      "Zechen Li",
      "Lanqing Yang",
      "Hao Pan",
      "Yezhou Wang",
      "Longyuan Ge",
      "Jeffery Wu",
      "Ruiheng Liu",
      "Yongjian Fu",
      "Yichao chen",
      "Guangtao xue"
    ],
    "abstract": "Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.16966v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16966v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "fast",
      "dynamic",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training",
    "authors": [
      "Yipeng Wang",
      "Mengtian Yang",
      "Chieh-pu Lo",
      "Jaydeep P. Kulkarni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.",
    "arxiv_url": "https://arxiv.org/abs/2511.16831v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16831v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.AR",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "real-time rendering",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering",
    "authors": [
      "Pierrick Bournez",
      "Luca Savant Aira",
      "Thibaud Ehret",
      "Gabriele Facciolo"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models",
    "arxiv_url": "https://arxiv.org/abs/2511.16542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16542v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
    "authors": [
      "Minseok Seo",
      "Mark Hamilton",
      "Changick Kim"
    ],
    "abstract": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. \\textbf{Project page:} \\href{https://seominseok0429.github.io/Upsample-Anything/}{https://seominseok0429.github.io/Upsample-Anything/}",
    "arxiv_url": "https://arxiv.org/abs/2511.16301v2",
    "pdf_url": "https://arxiv.org/pdf/2511.16301v2",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "lightweight",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
    "authors": [
      "Md Musfiqur Rahman Sanim",
      "Zhihao Shu",
      "Bahram Afsharmanesh",
      "AmirAli Mirian",
      "Jiexiong Guan",
      "Wei Niu",
      "Bin Ren",
      "Gagan Agrawal"
    ],
    "abstract": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.16298v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16298v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM",
    "authors": [
      "Sibaek Lee",
      "Seongbo Ha",
      "Kyeongsu Kang",
      "Joonyeol Choi",
      "Seungjun Tak",
      "Hyeonwoo Yu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.",
    "arxiv_url": "https://arxiv.org/abs/2511.16144v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16144v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "compact",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "head",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Clustered Error Correction with Grouped 4D Gaussian Splatting",
    "authors": [
      "Taeho Kang",
      "Jaeyeon Park",
      "Kyungjin Lee",
      "Youngki Lee"
    ],
    "abstract": "Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.",
    "arxiv_url": "https://arxiv.org/abs/2511.16112v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16112v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis",
    "authors": [
      "Zijian Wu",
      "Mingfeng Jiang",
      "Zidian Lin",
      "Ying Song",
      "Hanjie Ma",
      "Qun Wu",
      "Dongping Zhang",
      "Guiyang Pu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/",
    "arxiv_url": "https://arxiv.org/abs/2511.16030v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16030v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting",
    "authors": [
      "Junseo Koo",
      "Jinseo Jeong",
      "Gunhee Kim"
    ],
    "abstract": "The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.",
    "arxiv_url": "https://arxiv.org/abs/2511.15102v1",
    "pdf_url": "https://arxiv.org/pdf/2511.15102v1",
    "published_date": "2025-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video",
    "authors": [
      "Yarin Bekor",
      "Gal Michael Harari",
      "Or Perel",
      "Or Litany"
    ],
    "abstract": "We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.14848v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14848v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction",
    "authors": [
      "Meiying Gu",
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Haonan Luo",
      "Jin Zheng",
      "Xiao Bai"
    ],
    "abstract": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.14633v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14633v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction",
    "authors": [
      "Hao Tian",
      "Chenyangguang Zhang",
      "Rui Liu",
      "Wen Shen",
      "Xiaolin Qin"
    ],
    "abstract": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.",
    "arxiv_url": "https://arxiv.org/abs/2511.14540v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14540v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussians Spatial Transport for Point-supervised Density Regression",
    "authors": [
      "Miao Shang",
      "Xiaopeng Hong"
    ],
    "abstract": "This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.",
    "arxiv_url": "https://arxiv.org/abs/2511.14477v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14477v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IBGS: Image-Based Gaussian Splatting",
    "authors": [
      "Hoang Chuong Nguyen",
      "Wei Mao",
      "Jose M. Alvarez",
      "Miaomiao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.",
    "arxiv_url": "https://arxiv.org/abs/2511.14357v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14357v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Ji Jiang",
      "Tong Chen",
      "Zhe Tang",
      "Zhengyong Jiang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.14343v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14343v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Tong Chen",
      "Ji Jiang",
      "Zhe Tang",
      "Zhengyong Jiang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \\textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.14315v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14315v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "illumination",
      "geometry",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEN3D: Generating Domain-Free 3D Scenes from a Single Image",
    "authors": [
      "Yuxin Zhang",
      "Ziyu Lu",
      "Hongbo Duan",
      "Keyu Fan",
      "Pengting Luo",
      "Peiyu Zhuang",
      "Mengyu Yang",
      "Houde Liu"
    ],
    "abstract": "Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.",
    "arxiv_url": "https://arxiv.org/abs/2511.14291v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14291v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery",
    "authors": [
      "Yiming Zeng",
      "Xi-Le Zhao",
      "Wei-Hao Wu",
      "Teng-Yu Ji",
      "Chao Wang"
    ],
    "abstract": "Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.",
    "arxiv_url": "https://arxiv.org/abs/2511.14270v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14270v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "authors": [
      "Xiaoquan Sun",
      "Ruijian Zhang",
      "Kang Pang",
      "Bingchen Miao",
      "Yuxiang Tan",
      "Zhen Yang",
      "Ming Li",
      "Jiayu Chen"
    ],
    "abstract": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
    "arxiv_url": "https://arxiv.org/abs/2511.14161v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14161v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
    "authors": [
      "Hao Wang",
      "Linqing Zhao",
      "Xiuwei Xu",
      "Jiwen Lu",
      "Haibin Yan"
    ],
    "abstract": "Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2Â° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
    "arxiv_url": "https://arxiv.org/abs/2511.14149v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14149v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "robotics",
      "head",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat Regression Models",
    "authors": [
      "Mara Daniels",
      "Philippe Rigollet"
    ],
    "abstract": "We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.",
    "arxiv_url": "https://arxiv.org/abs/2511.14042v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14042v1",
    "published_date": "2025-11-18",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
    "authors": [
      "Jiangnan Ye",
      "Jiedong Zhuang",
      "Lianrui Mu",
      "Wenjie Zheng",
      "Jiaqi Hu",
      "Xingze Zou",
      "Jing Wang",
      "Haoji Hu"
    ],
    "abstract": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2511.13684v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13684v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "outdoor",
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
    "authors": [
      "Ziyang Huang",
      "Jiagang Chen",
      "Jin Liu",
      "Shunping Ji"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
    "arxiv_url": "https://arxiv.org/abs/2511.13571v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13571v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Zihan Li",
      "Tengfei Wang",
      "Wentian Gan",
      "Hao Zhan",
      "Xin Wang",
      "Zongqian Zhan"
    ],
    "abstract": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
    "arxiv_url": "https://arxiv.org/abs/2511.13278v2",
    "pdf_url": "https://arxiv.org/pdf/2511.13278v2",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
    "authors": [
      "Keshav Gupta",
      "Akshat Sanghvi",
      "Shreyas Reddy Palley",
      "Astitva Srivastava",
      "Charu Sharma",
      "Avinash Sharma"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, SymGS, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at symgs.github.io",
    "arxiv_url": "https://arxiv.org/abs/2511.13264v2",
    "pdf_url": "https://arxiv.org/pdf/2511.13264v2",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis",
    "authors": [
      "Qingsen Ma",
      "Chen Zou",
      "Dianyun Wang",
      "Jia Wang",
      "Liuyu Xiang",
      "Zhaofeng He"
    ],
    "abstract": "Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.",
    "arxiv_url": "https://arxiv.org/abs/2511.13011v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13011v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models",
    "authors": [
      "Siddarth Narasimhan",
      "Matthew Lisondra",
      "Haitong Wang",
      "Goldie Nejat"
    ],
    "abstract": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.",
    "arxiv_url": "https://arxiv.org/abs/2511.12972v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12972v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration",
    "authors": [
      "Changhun Oh",
      "Seongryong Oh",
      "Jinwoo Hwang",
      "Yoonsung Kim",
      "Hardik Sharma",
      "Jongse Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.",
    "arxiv_url": "https://arxiv.org/abs/2511.12930v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12930v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "acceleration",
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing 3D Scenes in Native High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.",
    "arxiv_url": "https://arxiv.org/abs/2511.12895v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12895v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko SÃ¼nderhauf",
      "Dimity Miller"
    ],
    "abstract": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.12370v2",
    "pdf_url": "https://arxiv.org/pdf/2511.12370v2",
    "published_date": "2025-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors",
    "authors": [
      "Qifeng Chen",
      "Jiarun Liu",
      "Rengan Xie",
      "Tao Tang",
      "Sicong Du",
      "Yiru Zhao",
      "Yuchi Huo",
      "Sheng Yang"
    ],
    "abstract": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.12304v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12304v1",
    "published_date": "2025-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian and Diffusion-Based Gaze Redirection",
    "authors": [
      "Abiram Panchalingam",
      "Indu Bodala",
      "Stuart Middleton"
    ],
    "abstract": "High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.",
    "arxiv_url": "https://arxiv.org/abs/2511.11231v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11231v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting",
    "authors": [
      "Ruocheng Wu",
      "Haolan He",
      "Yufei Wang",
      "Zhihao Li",
      "Bihan Wen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.",
    "arxiv_url": "https://arxiv.org/abs/2511.11213v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11213v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "few-shot",
      "ar",
      "geometry",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos",
    "authors": [
      "Zhixin Xu",
      "Hengyu Zhou",
      "Yuan Liu",
      "Wenhan Xue",
      "Hao Pan",
      "Wenping Wang",
      "Bin Wang"
    ],
    "abstract": "Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.11175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11175v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI",
    "authors": [
      "Sun Jo",
      "Seok Young Hong",
      "JinHyun Kim",
      "Seungmin Kang",
      "Ahjin Choi",
      "Don-Gwan An",
      "Simon Song",
      "Je Hyeong Hong"
    ],
    "abstract": "4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.",
    "arxiv_url": "https://arxiv.org/abs/2511.11048v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11048v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
    "authors": [
      "Yu Deng",
      "Baozhu Zhao",
      "Junyan Su",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.10316v1",
    "pdf_url": "https://arxiv.org/pdf/2511.10316v1",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Zhiyuan Xu",
      "Nan Min",
      "Yuhang Guo",
      "Tong Wei"
    ],
    "abstract": "3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.",
    "arxiv_url": "https://arxiv.org/abs/2511.09944v1",
    "pdf_url": "https://arxiv.org/pdf/2511.09944v1",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting",
    "authors": [
      "Aymen Mir",
      "Jian Wang",
      "Riza Alp Guler",
      "Chuan Guo",
      "Gerard Pons-Moll",
      "Bing Zhou"
    ],
    "abstract": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation for animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that rendering can be decoupled from motion synthesis, and each sub-problem can be addressed independently without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework enables novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with newly animated humans, showcasing the unique advantages of 3DGS for monocular video-based human animation. To assess the full quality of our results, we encourage readers to view the supplementary material available at https://miraymen.github.io/aha/ .",
    "arxiv_url": "https://arxiv.org/abs/2511.09827v2",
    "pdf_url": "https://arxiv.org/pdf/2511.09827v2",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "avatar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Shared-Autonomy Construction Robotic System for Overhead Works",
    "authors": [
      "David Minkwan Kim",
      "K. M. Brian Lee",
      "Yong Hyeok Seo",
      "Nikola Raicevic",
      "Runfa Blark Li",
      "Kehan Long",
      "Chan Seon Yoon",
      "Dong Min Kang",
      "Byeong Jo Lim",
      "Young Pyoung Kim",
      "Nikolay Atanasov",
      "Truong Nguyen",
      "Se Woong Jun",
      "Young Wook Kim"
    ],
    "abstract": "We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.",
    "arxiv_url": "https://arxiv.org/abs/2511.09695v1",
    "pdf_url": "https://arxiv.org/pdf/2511.09695v1",
    "published_date": "2025-11-12",
    "categories": [
      "cs.RO",
      "eess.SY"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS",
    "authors": [
      "Haiyi Li",
      "Qi Chen",
      "Denis Kalkofen",
      "Hsiang-Ting Chen"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.",
    "arxiv_url": "https://arxiv.org/abs/2511.09397v2",
    "pdf_url": "https://arxiv.org/pdf/2511.09397v2",
    "published_date": "2025-11-12",
    "categories": [
      "cs.CV",
      "cs.CG",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering",
    "authors": [
      "Laura Bragagnolo",
      "Leonardo Barcellona",
      "Stefano Ghidoni"
    ],
    "abstract": "Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2511.08294v2",
    "pdf_url": "https://arxiv.org/pdf/2511.08294v2",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric",
    "authors": [
      "Zhaolin Wan",
      "Yining Diao",
      "Jingqi Xu",
      "Hao Wang",
      "Zhiyang Li",
      "Xiaopeng Fan",
      "Wangmeng Zuo",
      "Debin Zhao"
    ],
    "abstract": "With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.",
    "arxiv_url": "https://arxiv.org/abs/2511.08032v1",
    "pdf_url": "https://arxiv.org/pdf/2511.08032v1",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UltraGS: Real-Time Physically-Decoupled Gaussian Splatting for Ultrasound Novel View Synthesis",
    "authors": [
      "Yuezhe Yang",
      "Qingqing Ruan",
      "Wenjie Cai",
      "Yudang Dong",
      "Dexin Yang",
      "Xingbo Dong",
      "Zhe Jin",
      "Yong Dai"
    ],
    "abstract": "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view poses challenges for novel view synthesis. We present UltraGS, a real-time framework that adapts Gaussian Splatting to sensorless ultrasound imaging by integrating explicit radiance fields with lightweight, physics-inspired acoustic modeling. UltraGS employs depth-aware Gaussian primitives with learnable fields of view to improve geometric consistency under unconstrained probe motion, and introduces PD Rendering, a differentiable acoustic operator that combines low-order spherical harmonics with first-order wave effects for efficient intensity synthesis. We further present a clinical ultrasound dataset acquired under real-world scanning protocols. Extensive evaluations across three datasets demonstrate that UltraGS establishes a new performance-efficiency frontier, achieving state-of-the-art results in PSNR (up to 29.55) and SSIM (up to 0.89) while achieving real-time synthesis at 64.69 fps on a single GPU. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.07743v2",
    "pdf_url": "https://arxiv.org/pdf/2511.07743v2",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "lightweight",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
    "authors": [
      "Botao Ye",
      "Boqi Chen",
      "Haofei Xu",
      "Daniel Barath",
      "Marc Pollefeys"
    ],
    "abstract": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
    "arxiv_url": "https://arxiv.org/abs/2511.07321v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07321v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
    "authors": [
      "Mengmeng Liu",
      "Jiuming Liu",
      "Yunpeng Zhang",
      "Jiangtao Li",
      "Michael Ying Yang",
      "Francesco Nex",
      "Hao Cheng"
    ],
    "abstract": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.",
    "arxiv_url": "https://arxiv.org/abs/2511.07241v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07241v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
    "authors": [
      "Changyue Shi",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Minghao Chen",
      "Wenwen Pan",
      "Yan Yang",
      "Jiajun Ding",
      "Zhou Yu",
      "Jun Yu"
    ],
    "abstract": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
    "arxiv_url": "https://arxiv.org/abs/2511.07122v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07122v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "nerf",
      "few-shot",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GFix: Perceptually Enhanced Gaussian Splatting Video Compression",
    "authors": [
      "Siyue Teng",
      "Ge Gao",
      "Duolikun Danier",
      "Yuxuan Jiang",
      "Fan Zhang",
      "Thomas Davis",
      "Zoe Liu",
      "David Bull"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.",
    "arxiv_url": "https://arxiv.org/abs/2511.06953v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06953v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "quality enhancement"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks",
    "authors": [
      "Tianang Chen",
      "Jian Jin",
      "Shilv Cai",
      "Zhuangzi Li",
      "Weisi Lin"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.",
    "arxiv_url": "https://arxiv.org/abs/2511.06830v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06830v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives",
    "authors": [
      "BartÅomiej Baranowski",
      "Stefano Esposito",
      "Patricia GschoÃmann",
      "Anpei Chen",
      "Andreas Geiger"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.",
    "arxiv_url": "https://arxiv.org/abs/2511.06810v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06810v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes",
    "authors": [
      "Meijun Guo",
      "Yongliang Shi",
      "Caiyun Liu",
      "Yixiao Feng",
      "Ming Ma",
      "Tinghai Yan",
      "Weining Lu",
      "Bin Liang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.",
    "arxiv_url": "https://arxiv.org/abs/2511.06765v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06765v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning",
    "authors": [
      "Qianfeng Yang",
      "Xiang Chen",
      "Pengpeng Li",
      "Qiyuan Guan",
      "Guiyue Jin",
      "Jiyu Jin"
    ],
    "abstract": "Rain degrades the visual quality of multi-view images, which are essential for 3D scene reconstruction, resulting in inaccurate and incomplete reconstruction results. Existing datasets often overlook two critical characteristics of real rainy 3D scenes: the viewpoint-dependent variation in the appearance of rain streaks caused by their projection onto 2D images, and the reduction in ambient brightness resulting from cloud coverage during rainfall. To improve data realism, we construct a new dataset named OmniRain3D that incorporates perspective heterogeneity and brightness dynamicity, enabling more faithful simulation of rain degradation in 3D scenes. Based on this dataset, we propose an end-to-end reconstruction framework named REVR-GSNet (Rain Elimination and Visibility Recovery for 3D Gaussian Splatting). Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian primitive optimization, and GS-guided rain elimination into a unified architecture through joint alternating optimization, achieving high-fidelity reconstruction of clean 3D scenes from rain-degraded inputs. Extensive experiments show the effectiveness of our dataset and method. Our dataset and method provide a foundation for future research on multi-view image deraining and rainy 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.06734v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06734v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "vr",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting",
    "authors": [
      "Chenpeng Su",
      "Wenhua Wu",
      "Chensheng Peng",
      "Tianchen Deng",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Urban scene reconstruction is critical for autonomous driving, enabling structured 3D representations for data synthesis and closed-loop testing. Supervised approaches rely on costly human annotations and lack scalability, while current self-supervised methods often confuse static and dynamic elements and fail to distinguish individual dynamic objects, limiting fine-grained editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction method for label-free street scenes with 4D Gaussian Splatting. We first accurately identify dynamic instances by exploiting appearance-position inconsistency between warped rendering and actual observation. Guided by instance-level dynamic perception, we employ instance-aware 4D Gaussians as the unified volumetric representation, realizing dynamic-adaptive and instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism through which identity and dynamics reinforce each other, enhancing both integrity and consistency. Experiments on urban driving scenarios show that DIAL-GS surpasses existing self-supervised baselines in reconstruction quality and instance-level editing, offering a concise yet powerful solution for urban scene modeling.",
    "arxiv_url": "https://arxiv.org/abs/2511.06632v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06632v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "urban scene",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360Â° Scenes",
    "authors": [
      "Shaoxiang Wang",
      "Shihong Zhang",
      "Christen Millerdurai",
      "RÃ¼diger Westermann",
      "Didier Stricker",
      "Alain Pagani"
    ],
    "abstract": "Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360Â° scenes remains largely underexplored. This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360Â° environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively. To tackle these challenges, we propose Inpaint360GS, a flexible 360Â° editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space. By distilling 2D segmentation into 3D and leveraging virtual camera views for contextual guidance, our method enables accurate object-level editing and consistent scene completion. We further introduce a new dataset tailored for 360Â° inpainting, addressing the lack of ground truth object-free scenes. Experiments demonstrate that Inpaint360GS outperforms existing baselines and achieves state-of-the-art performance. Project page: https://dfki-av.github.io/inpaint360gs/",
    "arxiv_url": "https://arxiv.org/abs/2511.06457v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06457v1",
    "published_date": "2025-11-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field",
    "authors": [
      "Haoqin Hong",
      "Ding Fan",
      "Fubin Dou",
      "Zhi-Li Zhou",
      "Haoran Sun",
      "Congcong Zhu",
      "Jingrun Chen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2511.06299v3",
    "pdf_url": "https://arxiv.org/pdf/2511.06299v3",
    "published_date": "2025-11-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video",
    "authors": [
      "Zhihui Ke",
      "Yuyang Liu",
      "Xiaobo Zhou",
      "Tie Qiu"
    ],
    "abstract": "Streaming free-viewpoint video~(FVV) in real-time still faces significant challenges, particularly in training, rendering, and transmission efficiency. Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent 3DGS-based FVV methods have achieved notable breakthroughs in both training and rendering. However, the storage requirements of these methods can reach up to $10$MB per frame, making stream FVV in real-time impossible. To address this problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D Gaussians, temporal features, and a deformation field. For high compression efficiency, we encode canonical Gaussian attributes as 2D images and temporal features as a video. This design not only enables real-time streaming, but also inherently supports adaptive bitrate control based on network condition without any extra training. Moreover, we propose a sliding window scheme to aggregate adjacent temporal features to learn local motions, and then introduce a transformer-guided auxiliary training module to learn global motions. On diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the PSNR by an average of $1$dB while reducing the average frame size to just $170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.06046v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06046v1",
    "published_date": "2025-11-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
    "authors": [
      "Mengqi Guo",
      "Bo Xu",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.",
    "arxiv_url": "https://arxiv.org/abs/2511.05229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05229v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "nerf",
      "ar",
      "geometry",
      "dynamic",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "abstract": "Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t=0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: https://interims-git.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.05152v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05152v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient representation of 3D spatial data for defense-related applications",
    "authors": [
      "Benjamin Kahl",
      "Marcus Hebel",
      "Michael Arens"
    ],
    "abstract": "Geospatial sensor data is essential for modern defense and security, offering indispensable 3D information for situational awareness. This data, gathered from sources like lidar sensors and optical cameras, allows for the creation of detailed models of operational environments. In this paper, we provide a comparative analysis of traditional representation methods, such as point clouds, voxel grids, and triangle meshes, alongside modern neural and implicit techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS). Our evaluation reveals a fundamental trade-off: traditional models offer robust geometric accuracy ideal for functional tasks like line-of-sight analysis and physics simulations, while modern methods excel at producing high-fidelity, photorealistic visuals but often lack geometric reliability. Based on these findings, we conclude that a hybrid approach is the most promising path forward. We propose a system architecture that combines a traditional mesh scaffold for geometric integrity with a neural representation like 3DGS for visual detail, managed within a hierarchical scene structure to ensure scalability and performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.05109v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05109v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting",
    "authors": [
      "Hexu Zhao",
      "Xiwen Min",
      "Xiaoteng Liu",
      "Moonjun Gong",
      "Yiming Li",
      "Ang Li",
      "Saining Xie",
      "Jinyang Li",
      "Aurojit Panda"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2511.04951v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04951v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Channel Knowledge Map Construction: Recent Advances and Open Challenges",
    "authors": [
      "Zixiang Ren",
      "Juncong Zhou",
      "Jie Xu",
      "Ling Qiu",
      "Yong Zeng",
      "Han Hu",
      "Juyong Zhang",
      "Rui Zhang"
    ],
    "abstract": "Channel knowledge map (CKM) has emerged as a pivotal technology for environment-aware wireless communications and sensing, which provides a priori location-specific channel knowledge to facilitate network optimization. Efficient CKM construction is an important technical problem for its effective implementation. This article provides a comprehensive overview of recent advances in CKM construction. First, we examine classical interpolation-based CKM construction methods, highlighting their limitations in practical deployments. Next, we explore image processing and generative artificial intelligence (AI) techniques, which leverage feature extraction to construct CKMs based on environmental knowledge. Furthermore, we present emerging wireless radiance field (WRF) frameworks that exploit neural radiance fields or Gaussian splatting to construct high-fidelity CKMs from sparse measurement data. Finally, we outline various future research directions in real-time and cross-domain CKM construction, as well as cost-efficient deployment of CKMs.",
    "arxiv_url": "https://arxiv.org/abs/2511.04944v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04944v1",
    "published_date": "2025-11-07",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Point Encoders",
    "authors": [
      "Jim James",
      "Ben Wilson",
      "Simon Lucey",
      "James Hays"
    ],
    "abstract": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.",
    "arxiv_url": "https://arxiv.org/abs/2511.04797v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04797v1",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "recognition",
      "ar",
      "fast",
      "geometry",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions",
    "authors": [
      "Kaifeng Zhang",
      "Shuo Sha",
      "Hanxiao Jiang",
      "Matthew Loper",
      "Hyunjong Song",
      "Guangyan Cai",
      "Zhuo Xu",
      "Xiaochen Hu",
      "Changxi Zheng",
      "Yunzhu Li"
    ],
    "abstract": "Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.04665v2",
    "pdf_url": "https://arxiv.org/pdf/2511.04665v2",
    "published_date": "2025-11-06",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "body"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds",
    "authors": [
      "Shiwei Ren",
      "Tianci Wen",
      "Yongchun Fang",
      "Biao Lu"
    ],
    "abstract": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.04283v3",
    "pdf_url": "https://arxiv.org/pdf/2511.04283v3",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "acceleration",
      "ar",
      "fast",
      "mapping",
      "dynamic",
      "sparse-view",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation",
    "authors": [
      "Yuwen Tao",
      "Kanglei Zhou",
      "Xin Tan",
      "Yuan Xie"
    ],
    "abstract": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.",
    "arxiv_url": "https://arxiv.org/abs/2511.03992v1",
    "pdf_url": "https://arxiv.org/pdf/2511.03992v1",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "vr",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Tong Chen",
      "Sihao Li",
      "Ji Jiang",
      "Youpeng Yang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "In orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.",
    "arxiv_url": "https://arxiv.org/abs/2511.03099v1",
    "pdf_url": "https://arxiv.org/pdf/2511.03099v1",
    "published_date": "2025-11-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing",
    "authors": [
      "Antonio Oroz",
      "Matthias NieÃner",
      "Tobias Kirschstein"
    ],
    "abstract": "We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE",
    "arxiv_url": "https://arxiv.org/abs/2511.02777v1",
    "pdf_url": "https://arxiv.org/pdf/2511.02777v1",
    "published_date": "2025-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "geometry",
      "lightweight",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping",
    "authors": [
      "Jiajia Li",
      "Keyi Zhu",
      "Qianwen Zhang",
      "Dong Chen",
      "Qi Sun",
      "Zhaojian Li"
    ],
    "abstract": "Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.",
    "arxiv_url": "https://arxiv.org/abs/2511.02207v1",
    "pdf_url": "https://arxiv.org/pdf/2511.02207v1",
    "published_date": "2025-11-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "dynamic",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting",
    "authors": [
      "Chun-Tin Wu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2511.00560v1",
    "pdf_url": "https://arxiv.org/pdf/2511.00560v1",
    "published_date": "2025-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "4d",
      "ar",
      "fast",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Detail Enhanced Gaussian Splatting for Large-Scale Volumetric Capture",
    "authors": [
      "Julien Philip",
      "Li Ma",
      "Pascal Clausen",
      "Wenqi Xian",
      "Ahmet Levent TaÅel",
      "Mingming He",
      "Xueming Yu",
      "David M. George",
      "Ning Yu",
      "Oliver Pilarski",
      "Paul Debevec"
    ],
    "abstract": "We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig, which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig, which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig, with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production.",
    "arxiv_url": "https://arxiv.org/abs/2511.21697v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21697v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction",
    "authors": [
      "Wenfeng Huang",
      "Xiangyun Liao",
      "Yinling Qian",
      "Hao Liu",
      "Yongming Yang",
      "Wenjing Jia",
      "Qiong Wang"
    ],
    "abstract": "Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery. The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences. However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has improved reconstruction efficiency by enabling a faster rendering pipeline. Nevertheless, existing 3DGS methods often prioritize rendering speed while neglecting these critical issues. To address these challenges, we propose SAGS, a self-adaptive alias-free Gaussian splatting framework. We introduce an attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue reconstruction and better capture the fine details of tissue movement. Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate that our method achieves superior performance in all metrics of PSNR, SSIM, and LPIPS compared to the state of the art while also delivering better visualization quality.",
    "arxiv_url": "https://arxiv.org/abs/2510.27318v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27318v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "nerf",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond",
    "authors": [
      "Zhicong Sun",
      "Jacqueline Lo",
      "Jinxing Hu"
    ],
    "abstract": "3D Gaussian splatting (3DGS) and its subsequent variants have led to remarkable progress in simultaneous localization and mapping (SLAM). While most recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing 3DGS-based SLAM methods for large-scale forest scenes holds great potential for many real-world applications, especially for wildfire emergency response and forest management. However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, and collecting such a dataset over real-world scenes is costly and technically infeasible. To this end, we have built a large-scale, comprehensive, and high-quality synthetic dataset for SLAM in wildfire and forest environments. Leveraging the Unreal Engine 5 Electric Dreams Environment Sample Project, we developed a pipeline to easily collect aerial and ground views, including ground-truth camera poses and a range of additional data modalities from unmanned aerial vehicle. Our pipeline also provides flexible controls on environmental factors such as light, weather, and types and conditions of wildfire, supporting the need for various tasks covering forest mapping, wildfire emergency response, and beyond. The resulting pilot dataset, WildfireX-SLAM, contains 5.5k low-altitude RGB-D aerial images from a large-scale forest map with a total size of 16 km2. On top of WildfireX-SLAM, a thorough benchmark is also conducted, which not only reveals the unique challenges of 3DGS-based SLAM in the forest but also highlights potential improvements for future works. The dataset and code will be publicly available. Project page: https://zhicongsun.github.io/wildfirexslam.",
    "arxiv_url": "https://arxiv.org/abs/2510.27133v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27133v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting",
    "authors": [
      "Moonsoo Jeong",
      "Dongbeen Kim",
      "Minseong Kim",
      "Sungkil Lee"
    ],
    "abstract": "We present a Directional Consistency (DC)-driven Adaptive Density Control (ADC) for 3D Gaussian Splatting (DC4GS). Whereas the conventional ADC bases its primitive splitting on the magnitudes of positional gradients, we further incorporate the DC of the gradients into ADC, and realize it through the angular coherence of the gradients. Our DC better captures local structural complexities in ADC, avoiding redundant splitting. When splitting is required, we again utilize the DC to define optimal split positions so that sub-primitives best align with the local structures than the conventional random placement. As a consequence, our DC4GS greatly reduces the number of primitives (up to 30% in our experiments) than the existing ADC, and also enhances reconstruction fidelity greatly.",
    "arxiv_url": "https://arxiv.org/abs/2510.26921v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26921v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HEIR: Learning Graph-Based Motion Hierarchies",
    "authors": [
      "Cheng Zheng",
      "William Koch",
      "Baiang Li",
      "Felix Heide"
    ],
    "abstract": "Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",
    "arxiv_url": "https://arxiv.org/abs/2510.26786v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26786v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Impact and Outlook of 3D Gaussian Splatting",
    "authors": [
      "Bernhard Kerbl"
    ],
    "abstract": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.",
    "arxiv_url": "https://arxiv.org/abs/2510.26694v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26694v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM",
    "authors": [
      "Mirko Usuelli",
      "David Rapado-Rincon",
      "Gert Kootstra",
      "Matteo Matteucci"
    ],
    "abstract": "Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, back-propagated through the camera projection to tighten geometry-appearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance on-tractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring robust multimodal perception.",
    "arxiv_url": "https://arxiv.org/abs/2510.26358v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26358v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "outdoor",
      "ar",
      "geometry",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting",
    "authors": [
      "Juncong Zhou",
      "Chao Hu",
      "Guanlin Wu",
      "Zixiang Ren",
      "Han Hu",
      "Juyong Zhang",
      "Rui Zhang",
      "Jie Xu"
    ],
    "abstract": "This paper investigates the construction of channel knowledge map (CKM) from sparse channel measurements. Dif ferent from conventional two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station configurations, we present a six-dimensional (6D) CKM framework named bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod eling wireless channels across dynamic transmitter (Tx) and receiver (Rx) positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual scatterer clusters and environmental obstacles in the wireless environment. By properly learning the bidirectional scattering patterns and complex attenuation profiles based on channel measurements, these ellipsoids inherently cap ture the electromagnetic transmission characteristics of wireless environments, thereby accurately modeling signal transmission under varying transceiver configurations. Experiment results show that BiWGS significantly outperforms classic multi-layer perception (MLP) for the construction of 6D channel power gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction accuracy comparable to the state-of-the art wireless radiation field Gaussian splatting (WRF-GS) for 3D CKM construction. This validates the capability of the proposed BiWGS in accomplishing dimensional expansion of 6D CKM construction, without compromising fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2510.26166v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26166v1",
    "published_date": "2025-10-30",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction",
    "authors": [
      "Kejing Xia",
      "Jidong Jia",
      "Ke Jin",
      "Yucai Bai",
      "Li Sun",
      "Dacheng Tao",
      "Youjian Zhang"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.",
    "arxiv_url": "https://arxiv.org/abs/2510.25173v2",
    "pdf_url": "https://arxiv.org/pdf/2510.25173v2",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "urban scene",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians",
    "authors": [
      "Xiyu Zhang",
      "Chong Bao",
      "Yipeng Chen",
      "Hongjia Zhai",
      "Yitong Dong",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ],
    "abstract": "3D reconstruction of indoor and urban environments is a prominent research topic with various downstream applications. However, existing geometric priors for addressing low-texture regions in indoor and urban settings often lack global consistency. Moreover, Gaussian Splatting and implicit SDF fields often suffer from discontinuities or exhibit computational inefficiencies, resulting in a loss of detail. To address these issues, we propose an Atlanta-world guided implicit-structured Gaussian Splatting that achieves smooth indoor and urban scene reconstruction while preserving high-frequency details and rendering efficiency. By leveraging the Atlanta-world model, we ensure the accurate surface reconstruction for low-texture regions, while the proposed novel implicit-structured GS representations provide smoothness without sacrificing efficiency and high-frequency details. Specifically, we propose a semantic GS representation to predict the probability of all semantic regions and deploy a structure plane regularization with learnable plane indicators for global accurate surface reconstruction. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2510.25129v1",
    "pdf_url": "https://arxiv.org/pdf/2510.25129v1",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "urban scene",
      "ar",
      "gaussian splatting",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation",
    "authors": [
      "Mingyu Jeong",
      "Eunsung Kim",
      "Sehun Park",
      "Andrew Jaeyong Choi"
    ],
    "abstract": "We present NVSim, a framework that automatically constructs large-scale, navigable indoor simulators from only common image sequences, overcoming the cost and scalability limitations of traditional 3D scanning. Our approach adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed floors a common issue in robotic traversal data. We introduce Floor-Aware Gaussian Splatting to ensure a clean, navigable ground plane, and a novel mesh-free traversability checking algorithm that constructs a topological graph by directly analyzing rendered views. We demonstrate our system's ability to generate valid, large-scale navigation graphs from real-world data. A video demonstration is avilable at https://youtu.be/tTiIQt6nXC8",
    "arxiv_url": "https://arxiv.org/abs/2510.24335v1",
    "pdf_url": "https://arxiv.org/pdf/2510.24335v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation",
    "authors": [
      "Haotian Zhou",
      "Xiaole Wang",
      "He Li",
      "Fusheng Sun",
      "Shengyu Guo",
      "Guolei Qi",
      "Jianghuan Xu",
      "Huijing Zhao"
    ],
    "abstract": "Navigating to a designated goal using visual information is a fundamental capability for intelligent robots. Most classical visual navigation methods are restricted to single-goal, single-modality, and closed set goal settings. To address the practical demands of multi-modal, open-vocabulary goal queries and multi-goal visual navigation, we propose LagMemo, a navigation system that leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo constructs a unified 3D language memory. With incoming task goals, the system queries the memory, predicts candidate goal locations, and integrates a local perception-based verification mechanism to dynamically match and validate goals during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a high-quality core split distilled from GOAT-Bench tailored to multi-modal open-vocabulary multi-goal visual navigation. Experimental results show that LagMemo's memory module enables effective multi-modal open-vocabulary goal localization, and that LagMemo outperforms state-of-the-art methods in multi-goal visual navigation. Project page: https://weekgoodday.github.io/lagmemo",
    "arxiv_url": "https://arxiv.org/abs/2510.24118v1",
    "pdf_url": "https://arxiv.org/pdf/2510.24118v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting",
    "authors": [
      "Phuc Nguyen Xuan",
      "Thanh Nguyen Canh",
      "Huu-Hung Nguyen",
      "Nak Young Chong",
      "Xiem HoangVan"
    ],
    "abstract": "This survey comprehensively reviews the evolving field of multi-robot collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian Splatting (3DGS). As an explicit scene representation, 3DGS has enabled unprecedented real-time, high-fidelity rendering, ideal for robotics. However, its use in multi-robot systems introduces significant challenges in maintaining global consistency, managing communication, and fusing data from heterogeneous sources. We systematically categorize approaches by their architecture -- centralized, distributed -- and analyze core components like multi-agent consistency and alignment, communication-efficient, Gaussian representation, semantic distillation, fusion and pose optimization, and real-time scalability. In addition, a summary of critical datasets and evaluation metrics is provided to contextualize performance. Finally, we identify key open challenges and chart future research directions, including lifelong mapping, semantic association and mapping, multi-model for robustness, and bridging the Sim2Real gap.",
    "arxiv_url": "https://arxiv.org/abs/2510.23988v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23988v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "semantic",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors",
    "authors": [
      "Xirui Jin",
      "Renbiao Jin",
      "Boying Li",
      "Danping Zou",
      "Wenxian Yu"
    ],
    "abstract": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an efficient representation for novel-view synthesis, achieving impressive visual quality. However, in scenes dominated by large and low-texture regions, common in indoor environments, the photometric loss used to optimize 3DGS yields ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for indoor scene reconstruction. Specifically, we design a pipeline for Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language segmentation model and refines its region proposals via cross-view fusion and inspection with geometric priors. 3D Gaussians in our framework are optimized with two additional terms: a planar prior supervision term that enforces planar consistency, and a geometric prior supervision term that steers the Gaussians toward the depth and normal cues. We have conducted extensive experiments on standard indoor benchmarks. The results show that PlanarGS reconstructs accurate and detailed 3D surfaces, consistently outperforming state-of-the-art methods by a large margin. Project page: https://planargs.github.io",
    "arxiv_url": "https://arxiv.org/abs/2510.23930v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23930v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation",
    "authors": [
      "Anthony Opipari",
      "Aravindhan K Krishnan",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Remembering where object segments were predicted in the past is useful for improving the accuracy and consistency of class-agnostic video segmentation algorithms. Existing video segmentation algorithms typically use either no object-level memory (e.g. FastSAM) or they use implicit memories in the form of recurrent neural network features (e.g. SAM2). In this paper, we augment both types of segmentation models using an explicit 3D memory and show that the resulting models have more accurate and consistent predictions. For this, we develop an online 3D Gaussian Splatting (3DGS) technique to store predicted object-level segments generated throughout the duration of a video. Based on this 3DGS representation, a set of fusion techniques are developed, named FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve their respective foundation models' predictions. Ablation experiments are used to validate the proposed techniques' design and hyperparameter settings. Results from both real-world and simulated benchmarking experiments show that models which use explicit 3D memories result in more accurate and consistent predictions than those which use no memory or only implicit neural network memories. Project Page: https://topipari.com/projects/FastSAM-Splat/",
    "arxiv_url": "https://arxiv.org/abs/2510.23521v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23521v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Jiaxin Guo",
      "Ziyan Chen",
      "Sihang Zhao",
      "Zhuoxiao Li",
      "Zhe Tang",
      "Baoru Huang",
      "Limin Yu"
    ],
    "abstract": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from endoscopic video is vital for downstream tasks and improved outcomes. However, endoscopic scenarios present unique challenges, including photometric inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most 3DGS-based methods that rely solely on appearance constraints for optimizing 3DGS are often insufficient in this context, as these dynamic visual artifacts can mislead the optimization process and lead to inaccurate reconstructions. To address these limitations, we present EndoWave, a unified spatio-temporal Gaussian Splatting framework by incorporating an optical flow-based geometric constraint and a multi-resolution rational wavelet supervision. First, we adopt a unified spatio-temporal Gaussian representation that directly optimizes primitives in a 4D domain. Second, we propose a geometric constraint derived from optical flow to enhance temporal coherence and effectively constrain the 3D structure of the scene. Third, we propose a multi-resolution rational orthogonal wavelet as a constraint, which can effectively separate the details of the endoscope and enhance the rendering performance. Extensive evaluations on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our method EndoWave achieves state-of-the-art reconstruction quality and visual accuracy compared to the baseline method.",
    "arxiv_url": "https://arxiv.org/abs/2510.23087v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23087v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
    "authors": [
      "Bohan Li",
      "Xin Jin",
      "Hu Zhu",
      "Hongsi Liu",
      "Ruikai Li",
      "Jiazhe Guo",
      "Kaiwen Cai",
      "Chao Ma",
      "Yueming Jin",
      "Hao Zhao",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "abstract": "Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
    "arxiv_url": "https://arxiv.org/abs/2510.22973v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22973v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
    "authors": [
      "Pranav Saxena"
    ],
    "abstract": "Modeling open-vocabulary language fields in 3D is essential for intuitive human-AI interaction and querying within physical environments. State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting to efficiently construct these language fields, encoding features distilled from high-dimensional models like CLIP. However, this efficiency is currently offset by the requirement to train a scene-specific language autoencoder for feature compression, introducing a costly, per-scene optimization bottleneck that hinders deployment scalability. In this work, we introduce Gen-LangSplat, that eliminates this requirement by replacing the scene-wise autoencoder with a generalized autoencoder, pre-trained extensively on the large-scale ScanNet dataset. This architectural shift enables the use of a fixed, compact latent space for language features across any new scene without any scene-specific training. By removing this dependency, our entire language field construction process achieves a efficiency boost while delivering querying performance comparable to, or exceeding, the original LangSplat method. To validate our design choice, we perform a thorough ablation study empirically determining the optimal latent embedding dimension and quantifying representational fidelity using Mean Squared Error and cosine similarity between the original and reprojected 512-dimensional CLIP embeddings. Our results demonstrate that generalized embeddings can efficiently and accurately support open-vocabulary querying in novel 3D scenes, paving the way for scalable, real-time interactive 3D AI applications.",
    "arxiv_url": "https://arxiv.org/abs/2510.22930v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22930v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "human",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting Data",
    "authors": [
      "Shashank N. Sridhara",
      "Birendra Kathariya",
      "Fangjun Pu",
      "Peng Yin",
      "Eduardo Pavez",
      "Antonio Ortega"
    ],
    "abstract": "We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for novel view synthesis, the size of trained models limits its deployment in bandwidth-constrained applications such as volumetric media streaming. To address this, we propose a learned hierarchical latent representation that builds upon the principles of \"overfitted\" learned image compression (e.g., Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS data have irregular spatial distributions of Gaussians (geometry) and consist of multiple attributes (signals) defined on the irregular geometry. Our codec is designed to account for these differences between images and 3DGS. Specifically, we leverage the octree structure of the voxelized 3DGS geometry to obtain a hierarchical multi-resolution representation. Our approach overfits latents to each Gaussian attribute under a global rate constraint. These latents are decoded independently through a lightweight decoder network. To estimate the bitrate during training, we employ an autoregressive probability model that leverages octree-derived contexts from the 3D point structure. The multi-resolution latents, decoder, and autoregressive entropy coding networks are jointly optimized for each Gaussian attribute. Experiments demonstrate that the proposed RALHE compression framework achieves a rendering PSNR gain of up to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS compression methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.22812v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22812v1",
    "published_date": "2025-10-26",
    "categories": [
      "eess.IV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "compression",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication",
    "authors": [
      "Yujie Wan",
      "Chenxuan Liu",
      "Shuai Wang",
      "Tong Zhang",
      "James Jianqiao Yu",
      "Kejiang Ye",
      "Dusit Niyato",
      "Chengzhong Xu"
    ],
    "abstract": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost devices. To address this issue, we present edge collaborative GS (ECO-GS), where each user can switch between a local small GS model to guarantee timeliness and a remote large GS model to guarantee fidelity. However, deciding how to engage the large GS model is nontrivial, due to the interdependency between rendering requirements and resource conditions. To this end, we propose integrated rendering and communication (IRAC), which jointly optimizes collaboration status (i.e., deciding whether to engage large GS) and edge power allocation (i.e., enabling remote rendering) under communication constraints across different users by minimizing a newly-derived GS switching function. Despite the nonconvexity of the problem, we propose an efficient penalty majorization minimization (PMM) algorithm to obtain the critical point solution. Furthermore, we develop an imitation learning optimization (ILO) algorithm, which reduces the computational time by over 100x compared to PMM. Experiments demonstrate the superiority of PMM and the real-time execution capability of ILO.",
    "arxiv_url": "https://arxiv.org/abs/2510.22718v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22718v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.IT",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering",
    "authors": [
      "Wenkai Zhu",
      "Xu Li",
      "Qimin Xu",
      "Benwu Wang",
      "Kun Wei",
      "Yiming Peng",
      "Zihang Wang"
    ],
    "abstract": "3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human chain-of-thought process for information seeking, we introduce a hierarchical collaborative representation module that facilitates mutual reinforcement for mapping optimization, effectively mitigating scale drift and enhancing reconstruction robustness. Furthermore, to effectively eliminate the influence of dynamic objects, we propose a joint dynamic modeling module that generates fine-grained dynamic masks by fusing open-world segmentation with implicit residual constraints, guided by uncertainty estimates from DINO-Depth features. Extensive evaluations on KITTI, nuScenes, and self-collected datasets demonstrate that our approach achieves state-of-the-art performance compared to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.22669v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22669v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "human",
      "mapping",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience",
    "authors": [
      "Huilin Yin",
      "Zhaolin Yang",
      "Linchuan Zhang",
      "Gerhard Rigoll",
      "Johannes Betz"
    ],
    "abstract": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely constrained in environments where visual inputs suffer from noise and low illumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM frameworks achieve high-fidelity mapping under clean conditions, they remain vulnerable to compounded degradations that degrade mapping and tracking performance. A key observation underlying our work is that the original 3DGS rendering pipeline inherently behaves as an implicit low-pass filter, attenuating high-frequency noise but also risking over-smoothing. Building on this insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for noise and low-light resilience. The framework integrates three innovations: a Structure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples rendered appearance, depth, and edge cues; an adaptive tracking objective with residual balancing regularization; and a Contrastive Language-Image Pretraining (CLIP)-based enhancement module, selectively activated under compounded degradations to restore semantic and structural fidelity. Comprehensive experiments on Replica, TUM, and real-world sequences show that RoGER-SLAM consistently improves trajectory accuracy and reconstruction quality compared with other 3DGS-SLAM systems, especially under adverse imaging conditions.",
    "arxiv_url": "https://arxiv.org/abs/2510.22600v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22600v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "illumination",
      "mapping",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
    "authors": [
      "Jing Yang",
      "Yufeng Yang"
    ],
    "abstract": "Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.",
    "arxiv_url": "https://arxiv.org/abs/2510.22473v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22473v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum",
    "authors": [
      "Yaokun Li",
      "Lihe Ding",
      "Xiao Chen",
      "Guang Tan",
      "Tianfan Xue"
    ],
    "abstract": "Generating dynamic and interactive 3D trees has wide applications in virtual reality, games, and world simulation. However, existing methods still face various challenges in generating structurally consistent and realistic 4D motion for complex real trees. In this paper, we propose DynamicTree, the first framework that can generate long-term, interactive 3D motion for 3DGS reconstructions of real trees. Unlike prior optimization-based methods, our approach generates dynamics in a fast feed-forward manner. The key success of our approach is the use of a compact sparse voxel spectrum to represent the tree movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline first generates mesh motion using the sparse voxel spectrum and then binds Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum can also serve as a basis for fast modal analysis under external forces, allowing real-time interactive responses. To train our model, we also introduce 4DTree, the first large-scale synthetic 4D tree dataset containing 8,786 animated tree meshes with 100-frame motion sequences. Extensive experiments demonstrate that our method achieves realistic and responsive tree animations, significantly outperforming existing approaches in both visual quality and computational efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2510.22213v2",
    "pdf_url": "https://arxiv.org/pdf/2510.22213v2",
    "published_date": "2025-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "4d",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "motion",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "authors": [
      "Bingchen Miao",
      "Rong Wei",
      "Zhiqi Ge",
      "Xiaoquan sun",
      "Shiqi Gao",
      "Jingzhe Zhu",
      "Renhan Wang",
      "Siliang Tang",
      "Jun Xiao",
      "Rui Tang",
      "Juncheng Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2510.21307v2",
    "pdf_url": "https://arxiv.org/pdf/2510.21307v2",
    "published_date": "2025-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "abstract": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2510.20813v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20813v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking",
    "authors": [
      "Zixuan Wu",
      "Hengyuan Zhang",
      "Ting-Hsuan Chen",
      "Yuliang Guo",
      "David Paz",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.",
    "arxiv_url": "https://arxiv.org/abs/2510.20335v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20335v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation",
    "authors": [
      "Runsong Zhu",
      "Ka-Hei Hui",
      "Zhengzhe Liu",
      "Qianyi Wu",
      "Weiliang Tang",
      "Shi Qiu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Open-vocabulary 3D segmentation is a fundamental yet challenging task, requiring a mutual understanding of both segmentation and language. However, existing Gaussian-splatting-based methods rely either on a single 3D language field, leading to inferior segmentation, or on pre-computed class-agnostic segmentations, suffering from error accumulation. To address these limitations, we present COS3D, a new collaborative prompt-segmentation framework that contributes to effectively integrating complementary language and segmentation cues throughout its entire pipeline. We first introduce the new concept of collaborative field, comprising an instance field and a language field, as the cornerstone for collaboration. During training, to effectively construct the collaborative field, our key idea is to capture the intrinsic relationship between the instance field and language field, through a novel instance-to-language feature mapping and designing an efficient two-stage training strategy. During inference, to bridge distinct characteristics of the two fields, we further design an adaptive language-to-instance prompt refinement, promoting high-quality prompt-segmentation inference. Extensive experiments not only demonstrate COS3D's leading performance over existing methods on two widely-used benchmarks but also show its high potential to various applications,~\\ie, novel image-based 3D segmentation, hierarchical segmentation, and robotics. The code is publicly available at \\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.",
    "arxiv_url": "https://arxiv.org/abs/2510.20238v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20238v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "ar",
      "robotics",
      "segmentation",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses",
    "authors": [
      "Damian Bowness",
      "Charalambos Poullis"
    ],
    "abstract": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.   To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints.   Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning.   Code and results at https://damian-bowness.github.io/EV3DGS",
    "arxiv_url": "https://arxiv.org/abs/2510.20027v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20027v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
    "authors": [
      "Mingrui Zhao",
      "Sauradip Nag",
      "Kai Wang",
      "Aditya Vora",
      "Guangda Ji",
      "Peter Chun",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ],
    "abstract": "We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:https://mingrui-zhao.github.io/4DRep-GMI/",
    "arxiv_url": "https://arxiv.org/abs/2510.19255v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19255v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "4d",
      "nerf",
      "ar",
      "fast",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
    "authors": [
      "In-Hwan Jin",
      "Hyeongju Mun",
      "Joonsoo Kim",
      "Kugjin Yun",
      "Kyeongbo Kong"
    ],
    "abstract": "Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency. Video demonstrations are available at https://anonymous.4open.science/w/MoE-GS-68BA/.",
    "arxiv_url": "https://arxiv.org/abs/2510.19210v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19210v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis",
    "authors": [
      "Matteo Bortolon",
      "Nuno Ferreira Duarte",
      "Plinio Moreno",
      "Fabio Poiesi",
      "JosÃ© Santos-Victor",
      "Alessio Del Bue"
    ],
    "abstract": "Achieving dexterous robotic grasping with multi-fingered hands remains a significant challenge. While existing methods rely on complete 3D scans to predict grasp poses, these approaches face limitations due to the difficulty of acquiring high-quality 3D data in real-world scenarios. In this paper, we introduce GRASPLAT, a novel grasping framework that leverages consistent 3D information while being trained solely on RGB images. Our key insight is that by synthesizing physically plausible images of a hand grasping an object, we can regress the corresponding hand joints for a successful grasp. To achieve this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of real hand-object interactions, enabling end-to-end training with RGB data. Unlike prior methods, our approach incorporates a photometric loss that refines grasp predictions by minimizing discrepancies between rendered and real images. We conduct extensive experiments on both synthetic and real-world grasping datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9% over existing image-based methods. Project page: https://mbortolon97.github.io/grasplat/",
    "arxiv_url": "https://arxiv.org/abs/2510.19200v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19200v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting",
    "authors": [
      "Hao Wang",
      "Ying Zhou",
      "Haoyu Zhao",
      "Rui Wang",
      "Qiang Hu",
      "Xing Zhang",
      "Qiang Li",
      "Zhiwei Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
    "arxiv_url": "https://arxiv.org/abs/2510.18739v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18739v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Re-Activating Frozen Primitives for 3D Gaussian Splatting",
    "authors": [
      "Yuxin Cheng",
      "Binxiao Huang",
      "Wenyong Zhou",
      "Taiqiang Wu",
      "Zhengwu Liu",
      "Graziano Chesi",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) achieves real-time photorealistic novel view synthesis, yet struggles with complex scenes due to over-reconstruction artifacts, manifesting as local blurring and needle-shape distortions. While recent approaches attribute these issues to insufficient splitting of large-scale Gaussians, we identify two fundamental limitations: gradient magnitude dilution during densification and the primitive frozen phenomenon, where essential Gaussian densification is inhibited in complex regions while suboptimally scaled Gaussians become trapped in local optima. To address these challenges, we introduce ReAct-GS, a method founded on the principle of re-activation. Our approach features: (1) an importance-aware densification criterion incorporating $Î±$-blending weights from multiple viewpoints to re-activate stalled primitive growth in complex regions, and (2) a re-activation mechanism that revitalizes frozen primitives through adaptive parameter perturbations. Comprehensive experiments across diverse real-world datasets demonstrate that ReAct-GS effectively eliminates over-reconstruction artifacts and achieves state-of-the-art performance on standard novel view synthesis metrics while preserving intricate geometric details. Additionally, our re-activation mechanism yields consistent improvements when integrated with other 3D-GS variants such as Pixel-GS, demonstrating its broad applicability.",
    "arxiv_url": "https://arxiv.org/abs/2510.19653v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19653v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos",
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Mi Zhou",
      "Jinwen Chen",
      "Dan Xu"
    ],
    "abstract": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
    "arxiv_url": "https://arxiv.org/abs/2510.18489v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18489v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion",
    "authors": [
      "Tianyu Huang",
      "Runnan Chen",
      "Dongting Hu",
      "Fengming Huang",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "Understanding 3D scenes is pivotal for autonomous driving, robotics, and augmented reality. Recent semantic Gaussian Splatting approaches leverage large-scale 2D vision models to project 2D semantic features onto 3D scenes. However, they suffer from two major limitations: (1) insufficient contextual cues for individual masks during preprocessing and (2) inconsistencies and missing details when fusing multi-view features from these 2D models. In this paper, we introduce \\textbf{OpenInsGaussian}, an \\textbf{Open}-vocabulary \\textbf{Ins}tance \\textbf{Gaussian} segmentation framework with Context-aware Cross-view Fusion. Our method consists of two modules: Context-Aware Feature Extraction, which augments each mask with rich semantic context, and Attention-Driven Feature Aggregation, which selectively fuses multi-view features to mitigate alignment errors and incompleteness. Through extensive experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art results in open-vocabulary 3D Gaussian segmentation, outperforming existing baselines by a large margin. These findings underscore the robustness and generality of our proposed approach, marking a significant step forward in 3D scene understanding and its practical deployment across diverse real-world scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2510.18253v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18253v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications",
    "authors": [
      "Vitor Pereira Matias",
      "Daniel Perazzo",
      "Vinicius Silva",
      "Alberto Raposo",
      "Luiz Velho",
      "Afonso Paiva",
      "Tiago Novello"
    ],
    "abstract": "The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2510.18101v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18101v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "ar",
      "real-time rendering",
      "avatar",
      "face",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "efficient rendering",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HouseTour: A Virtual Real Estate A(I)gent",
    "authors": [
      "Ata Ãelen",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ],
    "abstract": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.",
    "arxiv_url": "https://arxiv.org/abs/2510.18054v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18054v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Chung Min Kim",
      "Justin Kerr",
      "Shuangyu Xie",
      "Prithvi Akella",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Ken Goldberg"
    ],
    "abstract": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
    "arxiv_url": "https://arxiv.org/abs/2510.17783v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17783v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaindropGS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
    "authors": [
      "Zhiqiang Teng",
      "Tingting Chen",
      "Beibei Lin",
      "Zifeng Yuan",
      "Xuanyi Li",
      "Xuanyu Zhang",
      "Shunli Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
    "arxiv_url": "https://arxiv.org/abs/2510.17719v2",
    "pdf_url": "https://arxiv.org/pdf/2510.17719v2",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
    "authors": [
      "Feng Zhou",
      "Wenkai Guo",
      "Pu Cao",
      "Zhicheng Zhang",
      "Jianqin Yin"
    ],
    "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering. Prior work addresses it either by enhancing the initialization (\\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization. Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost. Given initialization's primacy, we focus our design there. Although SfM performs poorly under sparse views due to its reliance on feature matching, it still provides reliable seed points. Thus, building on SfM, our effort aims to supplement the regions it fails to cover as comprehensively as possible. Specifically, we design: (i) frequency-aware SfM that improves low-texture coverage via low-frequency view augmentation and relaxed multi-view correspondences; (ii) 3DGS self-initialization that lifts photometric supervision into additional points, compensating SfM-sparse regions with learned Gaussian centers; and (iii) point-cloud regularization that enforces multi-view consistency and uniform spatial coverage through simple geometric/visibility priors, yielding a clean and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate consistent gains in sparse-view settings, establishing our approach as a stronger initialization strategy. Code is available at https://github.com/zss171999645/ItG-GS.",
    "arxiv_url": "https://arxiv.org/abs/2510.17479v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17479v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
    "authors": [
      "Ruitong Gan",
      "Junran Peng",
      "Yang Liu",
      "Chuanchen Luo",
      "Qing Li",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Planes are fundamental primitives of 3D sences, especially in man-made environments such as indoor spaces and urban streets. Representing these planes in a structured and parameterized format facilitates scene editing and physical simulations in downstream applications. Recently, Gaussian Splatting (GS) has demonstrated remarkable effectiveness in the Novel View Synthesis task, with extensions showing great potential in accurate surface reconstruction. However, even state-of-the-art GS representations often struggle to reconstruct planar regions with sufficient smoothness and precision. To address this issue, we propose GSPlane, which recovers accurate geometry and produces clean and well-structured mesh connectivity for plane regions in the reconstructed scene. By leveraging off-the-shelf segmentation and normal prediction models, GSPlane extracts robust planar priors to establish structured representations for planar Gaussian coordinates, which help guide the training process by enforcing geometric consistency. To further enhance training robustness, a Dynamic Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians with persistently high gradients as non-planar, ensuring more reliable optimization. Furthermore, we utilize the optimized planar priors to refine the mesh layouts, significantly improving topological structure while reducing the number of vertices and faces. We also explore applications of the structured planar representation, which enable decoupling and flexible manipulation of objects on supportive planes. Extensive experiments demonstrate that, with no sacrifice in rendering quality, the introduction of planar priors significantly improves the geometric accuracy of the extracted meshes across various baselines.",
    "arxiv_url": "https://arxiv.org/abs/2510.17095v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17095v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting",
    "authors": [
      "Haofan Ren",
      "Qingsong Yan",
      "Ming Lu",
      "Rongfeng Lu",
      "Zunjie Zhu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced neural fields, as it enables high-fidelity rendering with impressive visual quality. However, 3DGS has difficulty accurately representing surfaces. In contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian disks. Despite advancements in geometric fidelity, rendering quality remains compromised, highlighting the challenge of achieving both high-quality rendering and precise geometric structures. This indicates that optimizing both geometric and rendering quality in a single training stage is currently unfeasible. To overcome this limitation, we present 2DGS-R, a new method that uses a hierarchical training approach to improve rendering quality while maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians with the normal consistency regularization. Then 2DGS-R selects the 2D Gaussians with inadequate rendering quality and applies a novel in-place cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R model with opacity frozen. Experimental results show that compared to the original 2DGS, our method requires only 1\\% more storage and minimal additional training time. Despite this negligible overhead, it achieves high-quality rendering results while preserving fine geometric structures. These findings indicate that our approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2510.16837v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16837v1",
    "published_date": "2025-10-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars",
    "authors": [
      "Haocheng Tang",
      "Ruoke Yan",
      "Xinhui Yin",
      "Qi Zhang",
      "Xinfeng Zhang",
      "Siwei Ma",
      "Wen Gao",
      "Chuanmin Jia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast, photorealistic rendering of dynamic 3D scenes, showing strong potential in immersive communication. However, in digital human encoding and transmission, the compression methods based on general 3DGS representations are limited by the lack of human priors, resulting in suboptimal bitrate efficiency and reconstruction quality at the decoder side, which hinders their application in streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical Gaussian Compression framework designed for efficient transmission and high-quality rendering of dynamic avatars. Our method disentangles the Gaussian representation into a structural layer, which maps poses to Gaussians via a StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model to represent temporal pose variations compactly and semantically. This hierarchical design supports layer-wise compression, progressive decoding, and controllable rendering from diverse pose inputs such as video sequences or text. Since people are most concerned with facial realism, we incorporate a facial attention mechanism during StyleUNet training to preserve identity and expression details under low-bitrate constraints. Experimental results demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar rendering, while significantly outperforming prior methods in both visual quality and compression efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2510.16463v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16463v1",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "semantic",
      "ar",
      "fast",
      "compression",
      "dynamic",
      "avatar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
    "authors": [
      "Changyue Shi",
      "Minghao Chen",
      "Yiping Mao",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Jiajun Ding",
      "Zhou Yu"
    ],
    "abstract": "Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.",
    "arxiv_url": "https://arxiv.org/abs/2510.16410v2",
    "pdf_url": "https://arxiv.org/pdf/2510.16410v2",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "human",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proactive Scene Decomposition and Reconstruction",
    "authors": [
      "Baicheng Li",
      "Zike Yan",
      "Dong Wu",
      "Hongbin Zha"
    ],
    "abstract": "Human behaviors are the major causes of scene dynamics and inherently contain rich cues regarding the dynamics. This paper formalizes a new task of proactive scene decomposition and reconstruction, an online approach that leverages human-object interactions to iteratively disassemble and reconstruct the environment. By observing these intentional interactions, we can dynamically refine the decomposition and reconstruction process, addressing inherent ambiguities in static object-level reconstruction. The proposed system effectively integrates multiple tasks in dynamic environments such as accurate camera and object pose estimation, instance decomposition, and online map updating, capitalizing on cues from human-object interactions in egocentric live streams for a flexible, progressive alternative to conventional object-level reconstruction methods. Aided by the Gaussian splatting technique, accurate and consistent dynamic scene modeling is achieved with photorealistic and efficient rendering. The efficacy is validated in multiple real-world scenarios with promising advantages.",
    "arxiv_url": "https://arxiv.org/abs/2510.16272v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16272v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction",
    "authors": [
      "Ting-Yu Yen",
      "Yu-Sheng Chiu",
      "Shih-Hsuan Hung",
      "Peter Wonka",
      "Hung-Kuo Chu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality, real-time novel-view synthesis from multi-view images. However, most existing methods assume the object is captured in a single, static pose, resulting in incomplete reconstructions that miss occluded or self-occluded regions. We introduce PFGS, a pose-aware 3DGS framework that addresses the practical challenge of reconstructing complete objects from multi-pose image captures. Given images of an object in one main pose and several auxiliary poses, PFGS iteratively fuses each auxiliary set into a unified 3DGS representation of the main pose. Our pose-aware fusion strategy combines global and local registration to merge views effectively and refine the 3DGS model. While recent advances in 3D foundation models have improved registration robustness and efficiency, they remain limited by high memory demands and suboptimal accuracy. PFGS overcomes these challenges by incorporating them more intelligently into the registration process: it leverages background features for per-pose camera pose estimation and employs foundation models for cross-pose registration. This design captures the best of both approaches while resolving background inconsistency issues. Experimental results demonstrate that PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.",
    "arxiv_url": "https://arxiv.org/abs/2510.15386v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15386v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels",
    "authors": [
      "Alejandro Escontrela",
      "Justin Kerr",
      "Arthur Allshire",
      "Jonas Frey",
      "Rocky Duan",
      "Carmelo Sferrazza",
      "Pieter Abbeel"
    ],
    "abstract": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
    "arxiv_url": "https://arxiv.org/abs/2510.15352v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15352v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images",
    "authors": [
      "Jiaxin Guo",
      "Tongfan Guan",
      "Wenzhen Dong",
      "Wenzhao Zheng",
      "Wenting Wang",
      "Yue Wang",
      "Yeung Yam",
      "Yun-Hui Liu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Project Page: https://wrld.github.io/SaLon3R/.",
    "arxiv_url": "https://arxiv.org/abs/2510.15072v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15072v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "arxiv_url": "https://arxiv.org/abs/2510.14705v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14705v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
    "authors": [
      "Junyi Wu",
      "Jiaming Xu",
      "Jinhao Li",
      "Yongkang Zhou",
      "Jiayi Pan",
      "Xingyang Li",
      "Guohao Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.   To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.   Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "arxiv_url": "https://arxiv.org/abs/2510.14564v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14564v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
    "authors": [
      "Alexander Valverde",
      "Brian Xu",
      "Yuyin Zhou",
      "Meng Xu",
      "Hongyun Wang"
    ],
    "abstract": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "arxiv_url": "https://arxiv.org/abs/2510.14270v3",
    "pdf_url": "https://arxiv.org/pdf/2510.14270v3",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "nerf",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
    "authors": [
      "Yuancheng Xu",
      "Wenqi Xian",
      "Li Ma",
      "Julien Philip",
      "Ahmet Levent TaÅel",
      "Yiwei Zhao",
      "Ryan Burgert",
      "Mingming He",
      "Oliver Hermann",
      "Oliver Pilarski",
      "Rahul Garg",
      "Paul Debevec",
      "Ning Yu"
    ],
    "abstract": "We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
    "arxiv_url": "https://arxiv.org/abs/2510.14179v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14179v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "relighting",
      "lighting",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation",
    "authors": [
      "Jungmin Lee",
      "Seonghyuk Hong",
      "Juyong Lee",
      "Jaeyoon Lee",
      "Jongwon Choi"
    ],
    "abstract": "We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.",
    "arxiv_url": "https://arxiv.org/abs/2510.17864v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17864v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images",
    "authors": [
      "Emanuel Garbin",
      "Guy Adam",
      "Oded Krams",
      "Zohar Barzelay",
      "Eran Guendelman",
      "Michael Schwarz",
      "Matteo Presutto",
      "Moran Vatelmacher",
      "Yigal Shenkman",
      "Eli Peker",
      "Itai Druker",
      "Uri Patish",
      "Yoav Blum",
      "Max Bluvstein",
      "Junxuan Li",
      "Rawal Khirodkar",
      "Shunsuke Saito"
    ],
    "abstract": "We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This \"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.",
    "arxiv_url": "https://arxiv.org/abs/2510.14081v3",
    "pdf_url": "https://arxiv.org/pdf/2510.14081v3",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "ar",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications",
    "authors": [
      "Naruya Kondo",
      "Yuto Asano",
      "Yoichi Ochiai"
    ],
    "abstract": "We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: https://gaussian-vrm.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2510.13978v2",
    "pdf_url": "https://arxiv.org/pdf/2510.13978v2",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "avatar",
      "dynamic",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
    "authors": [
      "Hyojun Go",
      "Dominik Narnhofer",
      "Goutam Bhat",
      "Prune Truong",
      "Federico Tombari",
      "Konrad Schindler"
    ],
    "abstract": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation.",
    "arxiv_url": "https://arxiv.org/abs/2510.13454v1",
    "pdf_url": "https://arxiv.org/pdf/2510.13454v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "human",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering",
    "authors": [
      "Siddharth Tourani",
      "Jayaram Reddy",
      "Akash Kumbar",
      "Satyajit Tourani",
      "Nishant Goyal",
      "Madhava Krishna",
      "N. Dinesh Reddy",
      "Muhammad Haris Khan"
    ],
    "abstract": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.",
    "arxiv_url": "https://arxiv.org/abs/2510.13381v1",
    "pdf_url": "https://arxiv.org/pdf/2510.13381v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "segmentation",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control",
    "authors": [
      "Zhen Li",
      "Xibin Jin",
      "Guoliang Li",
      "Shuai Wang",
      "Miaowen Wen",
      "Huseyin Arslan",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients (e.g., drones) and trains a global GS model at the edge (e.g., ground server), is an emerging paradigm for scene reconstruction in low-altitude economy. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead. Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments reveal that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. The GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%), and our method achieves an excellent tradeoff between view contributions and communication costs.",
    "arxiv_url": "https://arxiv.org/abs/2510.13186v4",
    "pdf_url": "https://arxiv.org/pdf/2510.13186v4",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "authors": [
      "Fengzhi Guo",
      "Chih-Chuan Hsu",
      "Sihao Ding",
      "Cheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2510.12768v1",
    "pdf_url": "https://arxiv.org/pdf/2510.12768v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "authors": [
      "An Zhao",
      "Piaopiao Yu",
      "Zhe Zhu",
      "Mingqiang Wei"
    ],
    "abstract": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction. However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur. To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images. BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions. To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages. Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.Our source code is available at https://github.com/wsxujm/bsgs",
    "arxiv_url": "https://arxiv.org/abs/2510.12493v2",
    "pdf_url": "https://arxiv.org/pdf/2510.12493v2",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
    "authors": [
      "Mohamed Omran",
      "Farhad Zanjani",
      "Davide Abati",
      "Jens Petersen",
      "Amirhossein Habibian"
    ],
    "abstract": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
    "arxiv_url": "https://arxiv.org/abs/2510.12308v1",
    "pdf_url": "https://arxiv.org/pdf/2510.12308v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "authors": [
      "Ying A",
      "Wenzhang Sun",
      "Chang Zeng",
      "Chunfeng Wang",
      "Hao Li",
      "Jianxun Cui"
    ],
    "abstract": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "arxiv_url": "https://arxiv.org/abs/2510.12282v1",
    "pdf_url": "https://arxiv.org/pdf/2510.12282v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "urban scene",
      "ar",
      "autonomous driving",
      "dynamic",
      "gaussian splatting",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jianhao Jiao",
      "Dimitrios Kanoulas",
      "Jun Ma"
    ],
    "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "arxiv_url": "https://arxiv.org/abs/2510.12174v2",
    "pdf_url": "https://arxiv.org/pdf/2510.12174v2",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality",
    "authors": [
      "Anastasiya Pechko",
      "Piotr Borycki",
      "Joanna WaczyÅska",
      "Daniel Barczyk",
      "Agata SzymaÅska",
      "SÅawomir Tadeja",
      "PrzemysÅaw Spurek"
    ],
    "abstract": "As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce GS-Verse (Gaussian Splatting for Virtual Environment Rendering and Scene Editing), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, GS-Verse facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.11878v2",
    "pdf_url": "https://arxiv.org/pdf/2510.11878v2",
    "published_date": "2025-10-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "vr",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams",
    "authors": [
      "Takuya Nakabayashi",
      "Navami Kairanda",
      "Hideo Saito",
      "Vladislav Golyanik"
    ],
    "abstract": "Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "arxiv_url": "https://arxiv.org/abs/2510.11717v1",
    "pdf_url": "https://arxiv.org/pdf/2510.11717v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation",
    "authors": [
      "Maggie Wang",
      "Stephen Tian",
      "Aiden Swann",
      "Ola Shorinwa",
      "Jiajun Wu",
      "Mac Schwager"
    ],
    "abstract": "Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ .",
    "arxiv_url": "https://arxiv.org/abs/2510.11689v1",
    "pdf_url": "https://arxiv.org/pdf/2510.11689v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "arxiv_url": "https://arxiv.org/abs/2510.11473v2",
    "pdf_url": "https://arxiv.org/pdf/2510.11473v2",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
    "authors": [
      "Wenyuan Zhang",
      "Jimin Tang",
      "Weiqi Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2510.11387v2",
    "pdf_url": "https://arxiv.org/pdf/2510.11387v2",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "illumination",
      "geometry",
      "ar",
      "gaussian splatting",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular Videos",
    "authors": [
      "Xuankai Zhang",
      "Junjin Xiao",
      "Qing Zhang"
    ],
    "abstract": "This paper presents a unified framework that allows high-quality dynamic Gaussian Splatting from both defocused and motion-blurred monocular videos. Due to the significant difference between the formation processes of defocus blur and motion blur, existing methods are tailored for either one of them, lacking the ability to simultaneously deal with both of them. Although the two can be jointly modeled as blur kernel-based convolution, the inherent difficulty in estimating accurate blur kernels greatly limits the progress in this direction. In this work, we go a step further towards this direction. Particularly, we propose to estimate per-pixel reliable blur kernels using a blur prediction network that exploits blur-related scene and camera information and is subject to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian densification strategy to mitigate the lack of Gaussians for incomplete regions, and boost the performance of novel view synthesis by incorporating unseen view information to constrain scene optimization. Extensive experiments show that our method outperforms the state-of-the-art methods in generating photorealistic novel view synthesis from defocused and motion-blurred monocular videos. Our code is available at https://github.com/hhhddddddd/dydeblur.",
    "arxiv_url": "https://arxiv.org/abs/2510.10691v3",
    "pdf_url": "https://arxiv.org/pdf/2510.10691v3",
    "published_date": "2025-10-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting",
    "authors": [
      "Haoyu Zhao",
      "Cheng Zeng",
      "Linghao Zhuang",
      "Yaxi Zhao",
      "Shengke Xue",
      "Hao Wang",
      "Xingyue Zhao",
      "Zhongyu Li",
      "Kehan Li",
      "Siteng Huang",
      "Mingxiu Chen",
      "Xin Li",
      "Deli Zhao",
      "Hua Zou"
    ],
    "abstract": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
    "arxiv_url": "https://arxiv.org/abs/2510.10637v1",
    "pdf_url": "https://arxiv.org/pdf/2510.10637v1",
    "published_date": "2025-10-12",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided Framework",
    "authors": [
      "Shanzhi Yin",
      "Bolin Chen",
      "Xinju Wu",
      "Ru-Ling Liao",
      "Jie Chen",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "abstract": "This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications.",
    "arxiv_url": "https://arxiv.org/abs/2510.10492v1",
    "pdf_url": "https://arxiv.org/pdf/2510.10492v1",
    "published_date": "2025-10-12",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "body",
      "ar",
      "human",
      "compression",
      "dynamic",
      "avatar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting",
    "authors": [
      "Abdelrhman Elrawy",
      "Emad A. Mohammed"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions. While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count. This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency. We replace the standard positional gradient heuristic with a novel densification trigger that uses the opacity gradient as a lightweight proxy for rendering error. We find this aggressive densification is only effective when paired with a more conservative pruning schedule, which prevents destructive optimization cycles. Combined with a standard depth-correlation loss for geometric guidance, our framework demonstrates a fundamental improvement in efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a reduction of approximately 70%. This dramatic gain in compactness is achieved with a modest trade-off in reconstruction metrics, establishing a new state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2510.10257v1",
    "pdf_url": "https://arxiv.org/pdf/2510.10257v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "nerf",
      "few-shot",
      "ar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer",
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Renlong Wu",
      "Wangmeng Zuo"
    ],
    "abstract": "In this work, we present Color3D, a highly adaptable framework for colorizing both static and dynamic 3D scenes from monochromatic inputs, delivering visually diverse and chromatically vibrant reconstructions with flexible user-guided control. In contrast to existing methods that focus solely on static scenarios and enforce multi-view consistency by averaging color variations which inevitably sacrifice both chromatic richness and controllability, our approach is able to preserve color diversity and steerability while ensuring cross-view and cross-time consistency. In particular, the core insight of our method is to colorize only a single key view and then fine-tune a personalized colorizer to propagate its color to novel views and time steps. Through personalization, the colorizer learns a scene-specific deterministic color mapping underlying the reference view, enabling it to consistently project corresponding colors to the content in novel views and video frames via its inherent inductive bias. Once trained, the personalized colorizer can be applied to infer consistent chrominance for all other images, enabling direct reconstruction of colorful 3D scenes with a dedicated Lab color space Gaussian splatting representation. The proposed framework ingeniously recasts complicated 3D colorization as a more tractable single image paradigm, allowing seamless integration of arbitrary image colorization models with enhanced flexibility and controllability. Extensive experiments across diverse static and dynamic 3D colorization benchmarks substantiate that our method can deliver more consistent and chromatically rich renderings with precise user control. Project Page https://yecongwan.github.io/Color3D/.",
    "arxiv_url": "https://arxiv.org/abs/2510.10152v1",
    "pdf_url": "https://arxiv.org/pdf/2510.10152v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting",
    "authors": [
      "Jiahui Lu",
      "Haihong Xiao",
      "Xueyan Zhao",
      "Wenxiong Kang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage. These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient. To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images. Unlike prior works that rely on COLMAP for sparse point cloud initialization, we leverage the VGGT foundation model to obtain more reliable initial poses and dense point clouds. Our approach integrates several key innovations: 1) a hybrid Gaussian representation with dual position-shape optimization enhanced by inter-view matching consistency; 2) a graph-guided attribute refinement module to enhance scene details; and 3) flow-based depth regularization that improves depth estimation accuracy for more effective supervision. Comprehensive quantitative and qualitative experiments demonstrate that our approach achieves more robust performance on both forward-facing and large-scale complex datasets compared to other pose-free methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.10097v2",
    "pdf_url": "https://arxiv.org/pdf/2510.10097v2",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "P-4DGS: Predictive 4D Gaussian Splatting with 90$\\times$ Compression",
    "authors": [
      "Henan Wang",
      "Hanxin Zhu",
      "Xinliang Gong",
      "Tianyu He",
      "Xin Li",
      "Zhibo Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has garnered significant attention due to its superior scene representation fidelity and real-time rendering performance, especially for dynamic 3D scene reconstruction (\\textit{i.e.}, 4D reconstruction). However, despite achieving promising results, most existing algorithms overlook the substantial temporal and spatial redundancies inherent in dynamic scenes, leading to prohibitive memory consumption. To address this, we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene modeling. Inspired by intra- and inter-frame prediction techniques commonly used in video compression, we first design a 3D anchor point-based spatial-temporal prediction module to fully exploit the spatial-temporal correlations across different 3D Gaussian primitives. Subsequently, we employ an adaptive quantization strategy combined with context-based entropy coding to further reduce the size of the 3D anchor points, thereby achieving enhanced compression efficiency. To evaluate the rate-distortion performance of our proposed P-4DGS in comparison with other dynamic 3DGS representations, we conduct extensive experiments on both synthetic and real-world datasets. Experimental results demonstrate that our approach achieves state-of-the-art reconstruction quality and the fastest rendering speed, with a remarkably low storage footprint (around \\textbf{1MB} on average), achieving up to \\textbf{40$\\times$} and \\textbf{90$\\times$} compression on synthetic and real-world scenes, respectively.",
    "arxiv_url": "https://arxiv.org/abs/2510.10030v1",
    "pdf_url": "https://arxiv.org/pdf/2510.10030v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "4d",
      "ar",
      "fast",
      "real-time rendering",
      "dynamic",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting",
    "authors": [
      "Zhigang Cheng",
      "Mingchao Sun",
      "Yu Liu",
      "Zengye Ge",
      "Luyang Tang",
      "Mu Xu",
      "Yangyan Li",
      "Peng Pan"
    ],
    "abstract": "Level of Detail (LoD) is a fundamental technique in real-time computer graphics for managing the rendering costs of complex scenes while preserving visual fidelity. Traditionally, LoD is implemented using discrete levels (DLoD), where multiple, distinct versions of a model are swapped out at different distances. This long-standing paradigm, however, suffers from two major drawbacks: it requires significant storage for multiple model copies and causes jarring visual ``popping\" artifacts during transitions, degrading the user experience. We argue that the explicit, primitive-based nature of the emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm: Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality scaling within a single, unified model, thereby circumventing the core problems of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a continuous LoD mechanism directly into a 3DGS representation. Our method introduces a learnable, distance-dependent decay parameter for each Gaussian primitive, which dynamically adjusts its opacity based on viewpoint proximity. This allows for the progressive and smooth filtering of less significant primitives, effectively creating a continuous spectrum of detail within one model. To train this model to be robust across all distances, we introduce a virtual distance scaling mechanism and a novel coarse-to-fine training strategy with rendered point count regularization. Our approach not only eliminates the storage overhead and visual artifacts of discrete methods but also reduces the primitive count and memory footprint of the final model. Extensive experiments demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a single model, delivering high-fidelity results across a wide range of performance targets.",
    "arxiv_url": "https://arxiv.org/abs/2510.09997v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09997v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping",
    "authors": [
      "Yicheng He",
      "Jingwen Yu",
      "Guangcheng Chen",
      "Hong Zhang"
    ],
    "abstract": "Maintaining an up-to-date map that accurately reflects recent changes in the environment is crucial, especially for robots that repeatedly traverse the same space. Failing to promptly update the changed regions can degrade map quality, resulting in poor localization, inefficient operations, and even lost robots. 3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online map reconstruction due to its dense, differentiable, and photorealistic properties, yet accurately and efficiently updating the regions of change remains a challenge. In this paper, we propose VG-Mapping, a novel online 3DGS-based mapping system tailored for such semi-static scenes. Our approach introduces a hybrid representation that augments 3DGS with a TSDF-based voxel map to efficiently identify changed regions in a scene, along with a variation-aware density control strategy that inserts or deletes Gaussian primitives in regions undergoing change. Furthermore, to address the absence of public benchmarks for this task, we construct a RGB-D dataset comprising both synthetic and real-world semi-static environments. Experimental results demonstrate that our method substantially improves the rendering quality and map update efficiency in semi-static scenes. The code and dataset are available at https://github.com/heyicheng-never/VG-Mapping.",
    "arxiv_url": "https://arxiv.org/abs/2510.09962v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09962v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates",
    "authors": [
      "Minkwan Kim",
      "Seungmin Lee",
      "Junho Kim",
      "Young Min Kim"
    ],
    "abstract": "Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.",
    "arxiv_url": "https://arxiv.org/abs/2510.09881v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09881v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "few-shot",
      "ar",
      "fast",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision Language Models: A Survey of 26K Papers",
    "authors": [
      "Fengming Lin"
    ],
    "abstract": "We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.",
    "arxiv_url": "https://arxiv.org/abs/2510.09586v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09586v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "understanding",
      "nerf",
      "ar",
      "human",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
    "authors": [
      "Arthur Bizzi",
      "Matias Grynberg",
      "Vitor Matias",
      "Daniel Perazzo",
      "JoÃ£o Paulo Lima",
      "Luiz Velho",
      "Nuno GonÃ§alves",
      "JoÃ£o Pereira",
      "Guilherme Schardong",
      "Tiago Novello"
    ],
    "abstract": "Morphing is a long-standing problem in vision and computer graphics, requiring a time-dependent warping for feature alignment and a blending for smooth interpolation. Recently, multilayer perceptrons (MLPs) have been explored as implicit neural representations (INRs) for modeling such deformations, due to their meshlessness and differentiability; however, extracting coherent and accurate morphings from standard MLPs typically relies on costly regularizations, which often lead to unstable training and prevent effective feature alignment. To overcome these limitations, we propose FLOWING (FLOW morphING), a framework that recasts warping as the construction of a differential vector flow, naturally ensuring continuity, invertibility, and temporal coherence by encoding structural flow properties directly into the network architectures. This flow-centric approach yields principled and stable transformations, enabling accurate and structure-preserving morphing of both 2D images and 3D shapes. Extensive experiments across a range of applications - including face and image morphing, as well as Gaussian Splatting morphing - show that FLOWING achieves state-of-the-art morphing quality with faster convergence. Code and pretrained models are available at http://schardong.github.io/flowing.",
    "arxiv_url": "https://arxiv.org/abs/2510.09537v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09537v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "fast",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction",
    "authors": [
      "Deborah Pintani",
      "Ariel Caputo",
      "Noah Lewis",
      "Marc Stamminger",
      "Fabio Pellacini",
      "Andrea Giachetti"
    ],
    "abstract": "Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis. In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes. In stage two, foreground Gaussians are initialized from a Structure-from-Motion reconstruction, added and refined using the standard rendering loss, while the background set remains fixed but contributes to the final image formation. Experiments on diverse outdoor datasets show that our method reduces background artifacts and improves perceptual quality compared to state-of-the-art baselines. Moreover, the explicit background separation enables automatic, object-free environment map estimation, opening new possibilities for photorealistic outdoor rendering and mixed-reality applications.",
    "arxiv_url": "https://arxiv.org/abs/2510.09489v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09489v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "illumination",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes",
    "authors": [
      "Yikang Zhang",
      "Rui Fan"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive performance in synthesizing high-fidelity novel views. Nonetheless, its effectiveness critically depends on the quality of the initialized point cloud. Specifically, achieving uniform and complete point coverage over the underlying scene structure requires overlapping observation frustums, an assumption that is often violated in unbounded, dynamic urban environments. Training Gaussian models with partially initialized point clouds often leads to distortions and artifacts, as camera rays may fail to intersect valid surfaces, resulting in incorrect gradient propagation to Gaussian primitives associated with occluded or invisible geometry. Additionally, existing densification strategies simply clone and split Gaussian primitives from existing ones, incapable of reconstructing missing structures. To address these limitations, we propose VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban scenes. Our method identifies unreliable geometry structures via voxel-based visibility reasoning, selects informative supporting views through diversity-aware view selection, and recovers missing structures via patch matching-based multi-view stereo reconstruction. This design enables the generation of new Gaussian primitives guided by reliable geometric priors, even in regions lacking initial points. Extensive experiments on the Waymo and nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS approaches and significantly improves the quality of reconstructed geometry for both static and dynamic objects. Source code will be released upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2510.09364v1",
    "pdf_url": "https://arxiv.org/pdf/2510.09364v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "dynamic",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "authors": [
      "Haofei Xu",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "While feed-forward Gaussian splatting models offer computational efficiency and can generalize to sparse input settings, their performance is fundamentally constrained by relying on a single forward pass for inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization across datasets, view counts and image resolutions. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16, 32), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV, RealEstate10K and ACID) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "arxiv_url": "https://arxiv.org/abs/2510.08575v2",
    "pdf_url": "https://arxiv.org/pdf/2510.08575v2",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction",
    "authors": [
      "Meixi Song",
      "Xin Lin",
      "Dizhe Zhang",
      "Haodong Li",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "arxiv_url": "https://arxiv.org/abs/2510.08566v1",
    "pdf_url": "https://arxiv.org/pdf/2510.08566v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "authors": [
      "Xilong Zhou",
      "Bao-Huy Nguyen",
      "LoÃ¯c Magne",
      "Vladislav Golyanik",
      "Thomas LeimkÃ¼hler",
      "Christian Theobalt"
    ],
    "abstract": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "arxiv_url": "https://arxiv.org/abs/2510.08491v1",
    "pdf_url": "https://arxiv.org/pdf/2510.08491v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting",
    "authors": [
      "Ankit Gahlawat",
      "Anirban Mukherjee",
      "Dinesh Babu Jayagopi"
    ],
    "abstract": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.",
    "arxiv_url": "https://arxiv.org/abs/2510.08096v1",
    "pdf_url": "https://arxiv.org/pdf/2510.08096v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "ar",
      "geometry",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
    "authors": [
      "Tianrui Zhang",
      "Yichen Liu",
      "Zilin Guo",
      "Yuxin Guo",
      "Jingcheng Ni",
      "Chenjing Ding",
      "Dan Xu",
      "Lewei Lu",
      "Zehuan Wu"
    ],
    "abstract": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.",
    "arxiv_url": "https://arxiv.org/abs/2510.07944v2",
    "pdf_url": "https://arxiv.org/pdf/2510.07944v2",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting",
    "authors": [
      "Houqiang Zhong",
      "Zhenglong Wu",
      "Sihua Fu",
      "Zihan Zheng",
      "Xin Jin",
      "Xiaoyun Zhang",
      "Li Song",
      "Qiang Hu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.",
    "arxiv_url": "https://arxiv.org/abs/2510.07830v1",
    "pdf_url": "https://arxiv.org/pdf/2510.07830v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream",
    "authors": [
      "Junhao He",
      "Jiaxu Wang",
      "Jia Li",
      "Mingyuan Sun",
      "Qiang Zhang",
      "Jiahang Cao",
      "Ziyi Zhang",
      "Yi Gu",
      "Jingkai Sun",
      "Renjing Xu"
    ],
    "abstract": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "arxiv_url": "https://arxiv.org/abs/2510.07752v2",
    "pdf_url": "https://arxiv.org/pdf/2510.07752v2",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes",
    "authors": [
      "Jian Gao",
      "Mengqi Yuan",
      "Yifei Zeng",
      "Chang Zeng",
      "Zhihao Li",
      "Zhenyu Chen",
      "Weichao Qiu",
      "Xiao-Xiao Long",
      "Hao Zhu",
      "Xun Cao",
      "Yao Yao"
    ],
    "abstract": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.",
    "arxiv_url": "https://arxiv.org/abs/2510.07729v1",
    "pdf_url": "https://arxiv.org/pdf/2510.07729v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "relightable",
      "ar",
      "real-time rendering",
      "lighting",
      "gaussian splatting",
      "light transport",
      "face",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
    "authors": [
      "Huanning Dong",
      "Fan Li",
      "Ping Kuang",
      "Jianwen Min"
    ],
    "abstract": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.",
    "arxiv_url": "https://arxiv.org/abs/2510.06967v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06967v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "high-fidelity",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity",
    "authors": [
      "Islomjon Shukhratov",
      "Sergey Gorinsky"
    ],
    "abstract": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.",
    "arxiv_url": "https://arxiv.org/abs/2510.06802v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06802v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis",
    "authors": [
      "Jipeng Lyu",
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ],
    "abstract": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "arxiv_url": "https://arxiv.org/abs/2510.06694v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06694v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction",
    "authors": [
      "Leshu Li",
      "Jiayin Qin",
      "Jie Peng",
      "Zishen Wan",
      "Huaizhi Qu",
      "Ye Han",
      "Pingqing Zheng",
      "Hongsen Zhang",
      "Yu Cao",
      "Tianlong Chen",
      "Yang Katie Zhao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping (SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering efficiency and accuracy, but have not yet been adopted in resource-constrained edge devices due to insufficient speed. Addressing this, we identify notable redundancies across the SLAM pipeline for acceleration. While conceptually straightforward, practical approaches are required to minimize the overhead associated with identifying and eliminating these redundancies. In response, we propose RTGS, an algorithm-hardware co-design framework that comprehensively reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline. On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to remove the redundant Gaussians by reusing gradients computed during backpropagation; and (2) a dynamic downsampling technique that directly reuses the keyframe identification and alpha computing steps to eliminate redundant pixels. On the hardware side, we propose (1) a subtile-level streaming strategy and a pixel-level pairwise scheduling strategy that mitigates workload imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates the rendering backpropagation by reusing intermediate data computed during rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory accesses caused by atomic operations while enabling pipelined aggregation. Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on four datasets and three algorithms, with up to 82.5x energy efficiency over the baseline and negligible quality loss. Code is available at https://github.com/UMN-ZhaoLab/RTGS.",
    "arxiv_url": "https://arxiv.org/abs/2510.06644v2",
    "pdf_url": "https://arxiv.org/pdf/2510.06644v2",
    "published_date": "2025-10-08",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "ar",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning",
    "authors": [
      "Amirhossein Mollaei Khass",
      "Guangyi Liu",
      "Vivek Pandey",
      "Wen Jiang",
      "Boshu Lei",
      "Kostas Daniilidis",
      "Nader Motee"
    ],
    "abstract": "Safe navigation in uncertain environments requires planning methods that integrate risk aversion with active perception. In this work, we present a unified framework that refines a coarse reference path by constructing tail-sensitive risk maps from Average Value-at-Risk statistics on an online-updated 3D Gaussian-splat Radiance Field. These maps enable the generation of locally safe and feasible trajectories. In parallel, we formulate Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose manifold, where Riemannian gradient descent maximizes an expected information gain objective to reduce uncertainty most critical for imminent motion. Our approach advances the state-of-the-art by coupling risk-averse path refinement with NBV planning, while introducing scalable gradient decompositions that support efficient online updates in complex environments. We demonstrate the effectiveness of the proposed framework through extensive computational studies.",
    "arxiv_url": "https://arxiv.org/abs/2510.06481v1",
    "pdf_url": "https://arxiv.org/pdf/2510.06481v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.",
    "arxiv_url": "https://arxiv.org/abs/2510.05488v1",
    "pdf_url": "https://arxiv.org/pdf/2510.05488v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "real-time rendering",
      "dynamic",
      "avatar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized Minimal 4D Gaussian Splatting",
    "authors": [
      "Minseo Lee",
      "Byeonghyeon Lee",
      "Lucas Yunkyu Lee",
      "Eunsoo Lee",
      "Sangmin Kim",
      "Seunghyeon Song",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Jaesik Park",
      "Eunbyung Park"
    ],
    "abstract": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.",
    "arxiv_url": "https://arxiv.org/abs/2510.03857v1",
    "pdf_url": "https://arxiv.org/pdf/2510.03857v1",
    "published_date": "2025-10-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "compression",
      "gaussian splatting",
      "motion",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches",
    "authors": [
      "Sixten Norelius",
      "Aaron O. Feldman",
      "Mac Schwager"
    ],
    "abstract": "We propose SketchPlan, a diffusion-based planner that interprets 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation. SketchPlan comprises two components: a SketchAdapter that learns to map the human sketches to projected 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and a first person view depth image. Our model achieves zero-shot sim-to-real transfer, generating accurate and safe flight paths in previously unseen real-world environments. To train the model, we build a synthetic dataset of 32k flight paths using a diverse set of photorealistic 3D Gaussian Splatting scenes. We automatically label the data by computing 2D projections of the 3D flight paths onto the camera plane, and use this to train the DiffPath diffusion model. However, since real human 2D sketches differ significantly from ideal 2D projections, we additionally label 872 of the 3D flight paths with real human sketches and use this to train the SketchAdapter to infer the 2D projection from the human sketch. We demonstrate SketchPlan's effectiveness in both simulated and real-world experiments, and show through ablations that training on a mix of human labeled and auto-labeled data together with a modular design significantly boosts its capabilities to correctly interpret human intent and infer 3D paths. In real-world drone tests, SketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen high-clutter environments, outperforming key ablations by 20-60\\% in task completion.",
    "arxiv_url": "https://arxiv.org/abs/2510.03545v1",
    "pdf_url": "https://arxiv.org/pdf/2510.03545v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields",
    "authors": [
      "Zhiting Mei",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.",
    "arxiv_url": "https://arxiv.org/abs/2510.03104v1",
    "pdf_url": "https://arxiv.org/pdf/2510.03104v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "localization",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.",
    "arxiv_url": "https://arxiv.org/abs/2510.08587v1",
    "pdf_url": "https://arxiv.org/pdf/2510.08587v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "deformation",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting",
    "authors": [
      "Xinran Zhang",
      "Hanqi Zhu",
      "Yifan Duan",
      "Yanyong Zhang"
    ],
    "abstract": "Constructing and sharing 3D maps is essential for many applications, including autonomous driving and augmented reality. Recently, 3D Gaussian splatting has emerged as a promising approach for accurate 3D reconstruction. However, a practical map-sharing system that features high-fidelity, continuous updates, and network efficiency remains elusive. To address these challenges, we introduce GS-Share, a photorealistic map-sharing system with a compact representation. The core of GS-Share includes anchor-based global map construction, virtual-image-based map enhancement, and incremental map update. We evaluate GS-Share against state-of-the-art methods, demonstrating that our system achieves higher fidelity, particularly for extrapolated views, with improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively. Furthermore, GS-Share is significantly more compact, reducing map transmission overhead by 36%.",
    "arxiv_url": "https://arxiv.org/abs/2510.02884v1",
    "pdf_url": "https://arxiv.org/pdf/2510.02884v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "ar",
      "autonomous driving",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FSFSplatter: Build Surface and Novel Views with Sparse-Views within 2min",
    "authors": [
      "Yibin Zhao",
      "Yihan Pan",
      "Jun Nan",
      "Liwei Chen",
      "Jianjun Yi"
    ],
    "abstract": "Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU, Replica, and BlendedMVS datasets.",
    "arxiv_url": "https://arxiv.org/abs/2510.02691v2",
    "pdf_url": "https://arxiv.org/pdf/2510.02691v2",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "sparse-view",
      "gaussian splatting",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
    "authors": [
      "Sung-Yeon Park",
      "Adam Lee",
      "Juanwu Lu",
      "Can Cui",
      "Luyang Jiang",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ahmadreza Moradipari",
      "Ziran Wang"
    ],
    "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
    "arxiv_url": "https://arxiv.org/abs/2510.02469v1",
    "pdf_url": "https://arxiv.org/pdf/2510.02469v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ],
    "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
    "arxiv_url": "https://arxiv.org/abs/2510.02314v1",
    "pdf_url": "https://arxiv.org/pdf/2510.02314v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0",
    "authors": [
      "Yan Miao",
      "Ege Yuceel",
      "Georgios Fainekos",
      "Bardh Hoxha",
      "Hideki Okamoto",
      "Sayan Mitra"
    ],
    "abstract": "Visual policy design is crucial for aerial navigation. However, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes. We develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds. Leveraging FalconGym 2.0's editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy's training on challenging tracks while iteratively improving its performance. Across two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations. Finally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
    "arxiv_url": "https://arxiv.org/abs/2510.02248v1",
    "pdf_url": "https://arxiv.org/pdf/2510.02248v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "geometry",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.02069v3",
    "pdf_url": "https://arxiv.org/pdf/2510.02069v3",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "illumination",
      "relighting",
      "geometry",
      "dynamic",
      "ar",
      "neural rendering",
      "lighting",
      "gaussian splatting",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing",
    "authors": [
      "Mengtian Li",
      "Yunshu Bai",
      "Yimin Chu",
      "Yijun Shen",
      "Zhongmei Li",
      "Weifeng Ge",
      "Zhifeng Xie",
      "Chaofeng Chen"
    ],
    "abstract": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($ÎE$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2510.02034v1",
    "pdf_url": "https://arxiv.org/pdf/2510.02034v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
    "authors": [
      "Lei Liu",
      "Can Wang",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2510.01991v1",
    "pdf_url": "https://arxiv.org/pdf/2510.01991v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "4d",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting",
    "authors": [
      "Quoc-Anh Bui",
      "Gilles Rougeron",
      "GÃ©raldine Morin",
      "Simone Gasparini"
    ],
    "abstract": "We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\\approx 17\\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.01978v2",
    "pdf_url": "https://arxiv.org/pdf/2510.01978v2",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics",
    "authors": [
      "Diram Tabaa",
      "Gianni Di Caro"
    ],
    "abstract": "Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.",
    "arxiv_url": "https://arxiv.org/abs/2510.01848v1",
    "pdf_url": "https://arxiv.org/pdf/2510.01848v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction",
    "authors": [
      "Sheng-Hsiang Hung",
      "Ting-Yu Yen",
      "Wei-Fang Sun",
      "Simon See",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2510.01767v1",
    "pdf_url": "https://arxiv.org/pdf/2510.01767v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "outdoor",
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
    "authors": [
      "Changmin Lee",
      "Jihyun Lee",
      "Tae-Kyun Kim"
    ],
    "abstract": "While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/",
    "arxiv_url": "https://arxiv.org/abs/2510.01619v1",
    "pdf_url": "https://arxiv.org/pdf/2510.01619v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "body",
      "ar",
      "human",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Beta Splatting",
    "authors": [
      "Rong Liu",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meida Chen",
      "Van Nguyen Nguyen",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Yue Wang",
      "Andrew Feng",
      "Ziyan Wu"
    ],
    "abstract": "We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at https://rongliu-leo.github.io/universal-beta-splatting/.",
    "arxiv_url": "https://arxiv.org/abs/2510.03312v1",
    "pdf_url": "https://arxiv.org/pdf/2510.03312v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "light transport",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HART: Human Aligned Reconstruction Transformer",
    "authors": [
      "Xiyi Chen",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Taewon Kang",
      "Sergey Prokudin",
      "Ming Lin"
    ],
    "abstract": "We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
    "arxiv_url": "https://arxiv.org/abs/2509.26621v1",
    "pdf_url": "https://arxiv.org/pdf/2509.26621v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "ar",
      "human",
      "geometry",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts",
    "authors": [
      "Zhenyu Shu",
      "Junlong Yu",
      "Kai Chao",
      "Shiqing Xin",
      "Ligang Liu"
    ],
    "abstract": "This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.",
    "arxiv_url": "https://arxiv.org/abs/2509.26055v1",
    "pdf_url": "https://arxiv.org/pdf/2509.26055v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels",
    "authors": [
      "Yi Hu",
      "Huiyang Zhou"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.",
    "arxiv_url": "https://arxiv.org/abs/2509.25626v2",
    "pdf_url": "https://arxiv.org/pdf/2509.25626v2",
    "published_date": "2025-09-30",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification",
    "authors": [
      "Yijia Weng",
      "Zhicheng Wang",
      "Songyou Peng",
      "Saining Xie",
      "Howard Zhou",
      "Leonidas J. Guibas"
    ],
    "abstract": "We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\\times1024$ resolution.",
    "arxiv_url": "https://arxiv.org/abs/2509.25603v1",
    "pdf_url": "https://arxiv.org/pdf/2509.25603v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "fast",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
    "arxiv_url": "https://arxiv.org/abs/2509.25122v1",
    "pdf_url": "https://arxiv.org/pdf/2509.25122v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "vr",
      "ar",
      "fast",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction",
    "authors": [
      "Huaizhi Qu",
      "Xiao Wang",
      "Gengwei Zhang",
      "Jie Peng",
      "Tianlong Chen"
    ],
    "abstract": "Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.",
    "arxiv_url": "https://arxiv.org/abs/2509.25075v2",
    "pdf_url": "https://arxiv.org/pdf/2509.25075v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
    "authors": [
      "Tooba Imtiaz",
      "Lucy Chai",
      "Kathryn Heal",
      "Xuan Luo",
      "Jungyeon Park",
      "Jennifer Dy",
      "John Flynn"
    ],
    "abstract": "Large transformer models are proving to be a powerful tool for 3D vision and novel view synthesis. However, the standard Transformer's well-known quadratic complexity makes it difficult to scale these methods to large scenes. To address this challenge, we propose the Local View Transformer (LVT), a large-scale scene reconstruction and novel view synthesis architecture that circumvents the need for the quadratic attention operation. Motivated by the insight that spatially nearby views provide more useful signal about the local scene composition than distant views, our model processes all information in a local neighborhood around each view. To attend to tokens in nearby views, we leverage a novel positional encoding that conditions on the relative geometric transformation between the query and nearby views. We decode the output of our model into a 3D Gaussian Splat scene representation that includes both color and opacity view-dependence. Taken together, the Local View Transformer enables reconstruction of arbitrarily large, high-resolution scenes in a single forward pass. See our project page for results and interactive demos https://toobaimt.github.io/lvt/.",
    "arxiv_url": "https://arxiv.org/abs/2509.25001v1",
    "pdf_url": "https://arxiv.org/pdf/2509.25001v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss Guided Depth and Bidirectional Warping",
    "authors": [
      "Yu Ma",
      "Guoliang Wei",
      "Haihong Xiao",
      "Yue Cheng"
    ],
    "abstract": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge in 3D reconstruction, where limited multi-view constraints lead to severe overfitting, geometric distortion, and fragmented scenes. While 3D Gaussian Splatting (3DGS) delivers real-time, high-fidelity rendering, its performance drastically deteriorates under sparse inputs, plagued by floating artifacts and structural failures. To address these challenges, we introduce HBSplat, a unified framework that elevates 3DGS by seamlessly integrating robust structural cues, virtual view constraints, and occluded region completion. Our core contributions are threefold: a Hybrid-Loss Depth Estimation module that ensures multi-view consistency by leveraging dense matching priors and integrating reprojection, point propagation, and smoothness constraints; a Bidirectional Warping Virtual View Synthesis method that enforces substantially stronger constraints by creating high-fidelity virtual views through bidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware Reconstruction component that recovers occluded areas using a depth-difference mask and a learning-based inpainting model. Extensive evaluations on LLFF, Blender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time inference. Code is available at: https://github.com/eternalland/HBSplat.",
    "arxiv_url": "https://arxiv.org/abs/2509.24893v3",
    "pdf_url": "https://arxiv.org/pdf/2509.24893v3",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
    "authors": [
      "Jiaqi Chen",
      "Xinhao Ji",
      "Yuanyuan Gao",
      "Hao Li",
      "Yuning Gong",
      "Yifei Liu",
      "Dan Xu",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun"
    ],
    "abstract": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce ExGS, a novel feed-forward framework that unifies Universal Gaussian Compression (UGC) with GaussPainter for Extreme 3DGS compression. UGC performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas GaussPainter leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over 100X compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at: https://github.com/chenttt2001/ExGS",
    "arxiv_url": "https://arxiv.org/abs/2509.24758v4",
    "pdf_url": "https://arxiv.org/pdf/2509.24758v4",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "neural rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh",
    "authors": [
      "Yuanyuan Gao",
      "Yuning Gong",
      "Yifei Liu",
      "Li Jingfeng",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Yanci Zhang",
      "Dan Xu",
      "Xiao Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2509.24421v2",
    "pdf_url": "https://arxiv.org/pdf/2509.24421v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
    "authors": [
      "Yuhang Cao",
      "Haojun Yan",
      "Danya Yao"
    ],
    "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2509.24308v1",
    "pdf_url": "https://arxiv.org/pdf/2509.24308v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "neural rendering",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting",
    "authors": [
      "DragoÅ-Andrei Chileban",
      "Andrei-Åtefan Bulzan",
      "Cosmin CernÇzanu-GlÇvan"
    ],
    "abstract": "Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments. However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage. Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views. In this work we introduce an automatic car damage detection pipeline that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we propose a simple yet effective learning-free approach for single-view 3D-GS segmentation. Specifically, Gaussians are projected onto the image plane using camera parameters obtained via Structure from Motion (SfM). They are then filtered through an algorithm that utilizes Z-buffering along with a normal distribution model of depth and opacities. Through experiments we found that this method is particularly effective for challenging scenarios like car damage detection, where target objects (e.g., scratches, small dents) may only be clearly visible in a single view, making multi-view consistency approaches impractical or impossible. The code is publicly available at: https://github.com/DragosChileban/CrashSplat.",
    "arxiv_url": "https://arxiv.org/abs/2509.23947v1",
    "pdf_url": "https://arxiv.org/pdf/2509.23947v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations",
    "authors": [
      "Javed Ahmad",
      "Penggang Gao",
      "Donatien Delehelle",
      "Mennuti Canio",
      "Nikhil Deshpande",
      "JesÃºs Ortiz",
      "Darwin G. Caldwell",
      "Yonas Teodros Tefera"
    ],
    "abstract": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.",
    "arxiv_url": "https://arxiv.org/abs/2509.23555v1",
    "pdf_url": "https://arxiv.org/pdf/2509.23555v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "understanding",
      "nerf",
      "ar",
      "fast",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos",
    "authors": [
      "Junyi Wu",
      "Jiachen Tao",
      "Haoxuan Wang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ],
    "abstract": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.",
    "arxiv_url": "https://arxiv.org/abs/2509.23492v1",
    "pdf_url": "https://arxiv.org/pdf/2509.23492v1",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
    "authors": [
      "Atakan Topaloglu",
      "Kunyi Li",
      "Michael Niemeyer",
      "Nassir Navab",
      "A. Murat Tekalp",
      "Federico Tombari"
    ],
    "abstract": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
    "arxiv_url": "https://arxiv.org/abs/2509.23258v2",
    "pdf_url": "https://arxiv.org/pdf/2509.23258v2",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Unified Representation of 3D Gaussian Splatting",
    "authors": [
      "Yuelin Xin",
      "Yuheng Liu",
      "Xiaohui Xie",
      "Xinke Li"
    ],
    "abstract": "A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2509.22917v1",
    "pdf_url": "https://arxiv.org/pdf/2509.22917v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVision: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting",
    "authors": [
      "Yasmine Omri",
      "Connor Ding",
      "Tsachy Weissman",
      "Thierry Tambe"
    ],
    "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero-shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy-intensive and costly, and (ii) patch-based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance-aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language-image pre-training (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat-aware input stem and a perceiver resampler, training only 9.7% to 13.8% of the total parameters. On a 12.8M dataset from DataComp, GS encoders yield competitive zero-shot performance on 38 datasets from the CLIP benchmark while compressing inputs 3x to 23.5x relative to pixels. Our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission-efficient for edge-cloud learning.",
    "arxiv_url": "https://arxiv.org/abs/2509.22615v2",
    "pdf_url": "https://arxiv.org/pdf/2509.22615v2",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "semantic",
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition",
    "authors": [
      "Dinh Minh Nguyen",
      "Malte Avenhaus",
      "Thomas Lindemeier"
    ],
    "abstract": "We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.",
    "arxiv_url": "https://arxiv.org/abs/2509.22276v1",
    "pdf_url": "https://arxiv.org/pdf/2509.22276v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting",
    "authors": [
      "Jiayu Ding",
      "Xinpeng Liu",
      "Zhiyi Pan",
      "Shiqiang Long",
      "Ge Li"
    ],
    "abstract": "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.",
    "arxiv_url": "https://arxiv.org/abs/2509.22225v1",
    "pdf_url": "https://arxiv.org/pdf/2509.22225v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Material Gaussian Model for Relightable 3D Generation",
    "authors": [
      "Jingrui Ye",
      "Lingting Zhu",
      "Runze Zhang",
      "Zeyu Hu",
      "Yingda Yin",
      "Lanjiong Li",
      "Lequan Yu",
      "Qingmin Liao"
    ],
    "abstract": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
    "arxiv_url": "https://arxiv.org/abs/2509.22112v1",
    "pdf_url": "https://arxiv.org/pdf/2509.22112v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "ar",
      "relighting",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drag4D: Align Your Motion with Text-Driven 3D Scene Generation",
    "authors": [
      "Minjun Kang",
      "Inkyu Shin",
      "Taeyeop Lee",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ],
    "abstract": "We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.",
    "arxiv_url": "https://arxiv.org/abs/2509.21888v1",
    "pdf_url": "https://arxiv.org/pdf/2509.21888v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "gaussian splatting",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Novel View Synthesis in High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Zhipeng Xiong",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2509.21853v2",
    "pdf_url": "https://arxiv.org/pdf/2509.21853v2",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems",
    "authors": [
      "Weikai Lin",
      "Sushant Kondguli",
      "Carl Marshall",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.",
    "arxiv_url": "https://arxiv.org/abs/2509.21702v1",
    "pdf_url": "https://arxiv.org/pdf/2509.21702v1",
    "published_date": "2025-09-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian splatting holography",
    "authors": [
      "Shuhe Zhang",
      "Liangcai Cao"
    ],
    "abstract": "In-line holography offers high space-bandwidth product imaging with a simplified lens-free optical system. However, in-line holographic reconstruction is troubled by twin images arising from the Hermitian symmetry of complex fields. Twin images disrupt the reconstruction in solving the ill-posed phase retrieval problem. The known parameters are less than the unknown parameters, causing phase ambiguities. State-of-the-art deep-learning or non-learning methods face challenges in balancing data fidelity with twin-image disturbance. We propose the Gaussian splatting holography (GSH) for twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting for optical field representation and compresses the number of unknown parameters by a maximum of 15 folds, transforming the original ill-posed phase retrieval into a well-posed one with reduced phase ambiguities. Additionally, the Gaussian splatting tends to form sharp patterns rather than those with noisy twin-image backgrounds as each Gaussian has a spatially slow-varying profile. Experiments show that GSH achieves constraint-free recovery for in-line holography with accuracy comparable to state-of-the-art constraint-based methods, with an average peak signal-to-noise ratio equal to 26 dB, and structure similarity equal to 0.8. Combined with total variation, GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB, and a high compression ability of up to 15 folds.",
    "arxiv_url": "https://arxiv.org/abs/2509.20774v1",
    "pdf_url": "https://arxiv.org/pdf/2509.20774v1",
    "published_date": "2025-09-25",
    "categories": [
      "physics.optics",
      "math.OC",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Driving Scene Generation With Stereo Forcing",
    "authors": [
      "Hao Lu",
      "Zhuang Ma",
      "Guangfeng Jiang",
      "Wenhang Ge",
      "Bohan Li",
      "Yuzhan Cai",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Yingcong Chen"
    ],
    "abstract": "Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
    "arxiv_url": "https://arxiv.org/abs/2509.20251v1",
    "pdf_url": "https://arxiv.org/pdf/2509.20251v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes",
    "authors": [
      "Guo Chen",
      "Jiarun Liu",
      "Sicong Du",
      "Chenming Wu",
      "Deqi Li",
      "Shi-Sheng Huang",
      "Guofeng Zhang",
      "Sheng Yang"
    ],
    "abstract": "This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: https://shanzhaguoo.github.io/GS-RoadPatching/",
    "arxiv_url": "https://arxiv.org/abs/2509.19937v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19937v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering",
    "authors": [
      "Jiangxue Yu",
      "Hui Wang",
      "San Jiang",
      "Xing Zhang",
      "Dejin Zhang",
      "Qingquan Li"
    ],
    "abstract": "The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences. The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes. First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage. Second, 3D Gaussian Splatting is then adopted for scene rendering by taking as inputs sparse points and oriented images. For accurate view rendering, a render viewpoint determination algorithm is designed by using the oriented camera poses of aerial images, which is used to generate high-quality intermediate images that can bridge the gap between aerial and ground images. Third, with the aid of intermediate images, reliable feature matching is conducted for match pairs from render-aerial and render-ground images, and final matches can be generated by transmitting correspondences through intermediate views. By using real aerial and ground datasets, the validation of the proposed solution has been verified in terms of feature matching and scene rendering and compared comprehensively with widely used methods. The experimental results demonstrate that the proposed solution can provide reliable feature matches for aerial and ground images with an obvious increase in the number of initial and refined matches, and it can provide enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based scene rendering.",
    "arxiv_url": "https://arxiv.org/abs/2509.19898v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19898v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting",
    "authors": [
      "Yixun Zhang",
      "Feng Zhou",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2509.19793v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19793v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction",
    "authors": [
      "Yufei Han",
      "Bowen Tie",
      "Heng Guo",
      "Youwei Lyu",
      "Si Li",
      "Boxin Shi",
      "Yunpeng Jia",
      "Zhanyu Ma"
    ],
    "abstract": "Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method.",
    "arxiv_url": "https://arxiv.org/abs/2509.19726v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19726v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "shape reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing",
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ],
    "abstract": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.",
    "arxiv_url": "https://arxiv.org/abs/2509.20400v1",
    "pdf_url": "https://arxiv.org/pdf/2509.20400v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
    "authors": [
      "Weijie Wang",
      "Yeqing Chen",
      "Zeyu Zhang",
      "Hengyu Liu",
      "Haoxiao Wang",
      "Zhiyuan Feng",
      "Wenkang Qin",
      "Zheng Zhu",
      "Donny Y. Chen",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
    "arxiv_url": "https://arxiv.org/abs/2509.19297v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19297v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation",
    "authors": [
      "Sherwin Bahmani",
      "Tianchang Shen",
      "Jiawei Ren",
      "Jiahui Huang",
      "Yifeng Jiang",
      "Haithem Turki",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Zan Gojcic",
      "Sanja Fidler",
      "Huan Ling",
      "Jun Gao",
      "Xuanchi Ren"
    ],
    "abstract": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
    "arxiv_url": "https://arxiv.org/abs/2509.19296v1",
    "pdf_url": "https://arxiv.org/pdf/2509.19296v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "dynamic",
      "autonomous driving",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
    "arxiv_url": "https://arxiv.org/abs/2509.19073v2",
    "pdf_url": "https://arxiv.org/pdf/2509.19073v2",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "sparse-view",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting",
    "authors": [
      "Zijing Guo",
      "Yunyang Zhao",
      "Lin Wang"
    ],
    "abstract": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.",
    "arxiv_url": "https://arxiv.org/abs/2509.18956v1",
    "pdf_url": "https://arxiv.org/pdf/2509.18956v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
    "authors": [
      "Pengteng Li",
      "Yunfan Lu",
      "Pinhao Song",
      "Weiyu Guo",
      "Huizai Yao",
      "F. Richard Yu",
      "Hui Xiong"
    ],
    "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.",
    "arxiv_url": "https://arxiv.org/abs/2509.18898v1",
    "pdf_url": "https://arxiv.org/pdf/2509.18898v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation",
    "authors": [
      "Zhaorui Wang",
      "Yi Gu",
      "Deming Zhou",
      "Renjing Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.",
    "arxiv_url": "https://arxiv.org/abs/2509.18759v1",
    "pdf_url": "https://arxiv.org/pdf/2509.18759v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
    "authors": [
      "Maximilian Adang",
      "JunEn Low",
      "Ola Shorinwa",
      "Mac Schwager"
    ],
    "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions.",
    "arxiv_url": "https://arxiv.org/abs/2509.18610v1",
    "pdf_url": "https://arxiv.org/pdf/2509.18610v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Kailun Yang",
      "Jiajun Zhai",
      "Shangwei Guo",
      "Lin Wang",
      "Kaiwei Wang"
    ],
    "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
    "arxiv_url": "https://arxiv.org/abs/2509.18566v1",
    "pdf_url": "https://arxiv.org/pdf/2509.18566v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "semantic",
      "ar",
      "fast",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction",
    "authors": [
      "Kaiwen Jiang",
      "Jia-Mu Sun",
      "Zilu Li",
      "Dan Wang",
      "Tzu-Mao Li",
      "Ravi Ramamoorthi"
    ],
    "abstract": "Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.",
    "arxiv_url": "https://arxiv.org/abs/2509.18497v2",
    "pdf_url": "https://arxiv.org/pdf/2509.18497v2",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "light transport",
      "global illumination",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Youmin Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Xiaohan Yu",
      "Lin Gu"
    ],
    "abstract": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
    "arxiv_url": "https://arxiv.org/abs/2509.18090v2",
    "pdf_url": "https://arxiv.org/pdf/2509.18090v2",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization",
    "authors": [
      "Phuong Mai Dinh",
      "Van-Nam Huynh"
    ],
    "abstract": "Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.",
    "arxiv_url": "https://arxiv.org/abs/2509.17889v1",
    "pdf_url": "https://arxiv.org/pdf/2509.17889v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Zipeng Qi",
      "Wenjun Lu",
      "David Bull",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2509.17789v1",
    "pdf_url": "https://arxiv.org/pdf/2509.17789v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "illumination",
      "ar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
    "authors": [
      "Gunjan Chhablani",
      "Xiaomeng Ye",
      "Muhammad Zubair Irshad",
      "Zsolt Kira"
    ],
    "abstract": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.",
    "arxiv_url": "https://arxiv.org/abs/2509.17430v2",
    "pdf_url": "https://arxiv.org/pdf/2509.17430v2",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR",
    "authors": [
      "Junzhe Wu",
      "Yufei Jia",
      "Yiyi Yan",
      "Zhixing Chen",
      "Tiao Tan",
      "Zifan Wang",
      "Guangyu Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic rendering, its vast ecosystem of assets remains incompatible with high-performance LiDAR simulation, a critical tool for robotics and autonomous driving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with a truly plug-and-play approach. Our method converts \\textit{any} pretrained 3DGS model into a high-fidelity, watertight mesh without requiring LiDAR-specific supervision or architectural alterations. This conversion is achieved through a general pipeline of volumetric discretization and Truncated Signed Distance Field (TSDF) extraction. We pair this with a highly optimized, GPU-accelerated ray-casting module that simulates LiDAR returns at over 500 FPS. We validate our approach on indoor and outdoor scenes, demonstrating exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for geometrically accurate depth sensing, our framework extends their utility beyond visualization and unlocks new capabilities for scalable, multimodal simulation. Our open-source implementation is available at https://github.com/TATP-233/FGGS-LiDAR.",
    "arxiv_url": "https://arxiv.org/abs/2509.17390v1",
    "pdf_url": "https://arxiv.org/pdf/2509.17390v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction",
    "authors": [
      "Neham Jain",
      "Andrew Jong",
      "Sebastian Scherer",
      "Ioannis Gkioulekas"
    ],
    "abstract": "Smoke in real-world scenes can severely degrade image quality and hamper visibility. Recent image restoration methods either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from multi-view video sequences. Our method uses thermal and RGB images, leveraging the reduced scattering in thermal images to see through smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene into smoke and non-smoke components. Unlike prior work, SmokeSeer handles a broad range of smoke densities and adapts to temporally varying smoke. We validate our method on synthetic data and a new real-world smoke dataset with RGB and thermal images. We provide an open-source implementation and data on the project website.",
    "arxiv_url": "https://arxiv.org/abs/2509.17329v3",
    "pdf_url": "https://arxiv.org/pdf/2509.17329v3",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.",
    "arxiv_url": "https://arxiv.org/abs/2509.17246v1",
    "pdf_url": "https://arxiv.org/pdf/2509.17246v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
    "arxiv_url": "https://arxiv.org/abs/2509.17083v2",
    "pdf_url": "https://arxiv.org/pdf/2509.17083v2",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views",
    "authors": [
      "Zhenya Yang"
    ],
    "abstract": "Surgical simulation is essential for medical training, enabling practitioners to develop crucial skills in a risk-free environment while improving patient safety and surgical outcomes. However, conventional methods for building simulation environments are cumbersome, time-consuming, and difficult to scale, often resulting in poor details and unrealistic simulations. In this paper, we propose a Gaussian Splatting-based framework to directly reconstruct interactive surgical scenes from endoscopic data while ensuring efficiency, rendering quality, and realism. A key challenge in this data-driven simulation paradigm is the restricted movement of endoscopic cameras, which limits viewpoint diversity. As a result, the Gaussian Splatting representation overfits specific perspectives, leading to reduced geometric accuracy. To address this issue, we introduce a novel virtual camera-based regularization method that adaptively samples virtual viewpoints around the scene and incorporates them into the optimization process to mitigate overfitting. An effective depth-based regularization is applied to both real and virtual views to further refine the scene geometry. To enable fast deformation simulation, we propose a sparse control node-based Material Point Method, which integrates physical properties into the reconstructed scene while significantly reducing computational costs. Experimental results on representative surgical data demonstrate that our method can efficiently reconstruct and simulate surgical scenes from sparse endoscopic views. Notably, our method takes only a few minutes to reconstruct the surgical scene and is able to produce physically plausible deformations in real-time with user-defined interactions.",
    "arxiv_url": "https://arxiv.org/abs/2509.17027v1",
    "pdf_url": "https://arxiv.org/pdf/2509.17027v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.",
    "arxiv_url": "https://arxiv.org/abs/2509.16922v1",
    "pdf_url": "https://arxiv.org/pdf/2509.16922v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "nerf",
      "ar",
      "avatar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM",
    "authors": [
      "Amanuel T. Dufera",
      "Yuan-Li Cai"
    ],
    "abstract": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction. Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism. This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision. The resulting proxy depth guides the optimization of a deformable 3DGS map, which efficiently adapts online to maintain global consistency following pose updates from a DROID-SLAM-inspired frontend and backend optimizations (loop closure, global bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD, ScanNet) and diverse custom mobile datasets demonstrates significant improvements in reconstruction accuracy (L1 depth error) and novel view synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in challenging conditions. ConfidentSplat underscores the efficacy of principled, confidence-aware sensor fusion for advancing state-of-the-art dense visual SLAM.",
    "arxiv_url": "https://arxiv.org/abs/2509.16863v1",
    "pdf_url": "https://arxiv.org/pdf/2509.16863v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging",
    "authors": [
      "Kacper Marzol",
      "Ignacy Kolton",
      "Weronika Smolak-DyÅ¼ewska",
      "Joanna Kaleta",
      "Marcin Mazur",
      "PrzemysÅaw Spurek"
    ],
    "abstract": "Multi-modal three-dimensional (3D) medical imaging data, derived from ultrasound, magnetic resonance imaging (MRI), and potentially computed tomography (CT), provide a widely adopted approach for non-invasive anatomical visualization. Accurate modeling, registration, and visualization in this setting depend on surface reconstruction and frame-to-frame interpolation. Traditional methods often face limitations due to image noise and incomplete information between frames. To address these challenges, we present MedGS, a semi-supervised neural implicit surface reconstruction framework that employs a Gaussian Splatting (GS)-based interpolation mechanism. In this framework, medical imaging data are represented as consecutive two-dimensional (2D) frames embedded in 3D space and modeled using Gaussian-based distributions. This representation enables robust frame interpolation and high-fidelity surface reconstruction across imaging modalities. As a result, MedGS offers more efficient training than traditional neural implicit methods. Its explicit GS-based representation enhances noise robustness, allows flexible editing, and supports precise modeling of complex anatomical structures with fewer artifacts. These features make MedGS highly suitable for scalable and practical applications in medical imaging.",
    "arxiv_url": "https://arxiv.org/abs/2509.16806v1",
    "pdf_url": "https://arxiv.org/pdf/2509.16806v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "face",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting",
    "authors": [
      "Xiaoyang Yan",
      "Muleilan Pei",
      "Shaojie Shen"
    ],
    "abstract": "3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.",
    "arxiv_url": "https://arxiv.org/abs/2509.16552v1",
    "pdf_url": "https://arxiv.org/pdf/2509.16552v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "head",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars",
    "authors": [
      "Weiyi Xiong",
      "Bing Zhu",
      "Tao Huang",
      "Zewei Zheng"
    ],
    "abstract": "4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.",
    "arxiv_url": "https://arxiv.org/abs/2509.16119v1",
    "pdf_url": "https://arxiv.org/pdf/2509.16119v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "fast",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval",
    "authors": [
      "Liwei Liao",
      "Xufeng Li",
      "Xiaoyun Zheng",
      "Boning Liu",
      "Feng Gao",
      "Ronggang Wang"
    ],
    "abstract": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.",
    "arxiv_url": "https://arxiv.org/abs/2509.15871v1",
    "pdf_url": "https://arxiv.org/pdf/2509.15871v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Splatting for Continuous View Optimization",
    "authors": [
      "Gahye Lee",
      "Hyomin Kim",
      "Gwangjin Ju",
      "Jooeun Son",
      "Hyejeong Yoon",
      "Seungyong Lee"
    ],
    "abstract": "We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.",
    "arxiv_url": "https://arxiv.org/abs/2509.15677v1",
    "pdf_url": "https://arxiv.org/pdf/2509.15677v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting",
    "authors": [
      "Yuwei Jia",
      "Yutang Lu",
      "Zhe Cui",
      "Fei Su"
    ],
    "abstract": "Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.",
    "arxiv_url": "https://arxiv.org/abs/2509.15648v1",
    "pdf_url": "https://arxiv.org/pdf/2509.15648v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading",
    "authors": [
      "Donghyun Lee",
      "Dawoon Jeong",
      "Jae W. Lee",
      "Hongil Yoon"
    ],
    "abstract": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
    "arxiv_url": "https://arxiv.org/abs/2509.15645v1",
    "pdf_url": "https://arxiv.org/pdf/2509.15645v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild",
    "authors": [
      "Deming Li",
      "Kaiwen Jiang",
      "Yutao Tang",
      "Ravi Ramamoorthi",
      "Rama Chellappa",
      "Cheng Peng"
    ],
    "abstract": "In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision steps at virtual views in pixel and feature levels to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions, and outperforms existing approaches significantly across different datasets.",
    "arxiv_url": "https://arxiv.org/abs/2509.15548v4",
    "pdf_url": "https://arxiv.org/pdf/2509.15548v4",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "nerf",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction",
    "authors": [
      "Jinlong Fan",
      "Bingyu Hu",
      "Xingguang Li",
      "Yuxiang Yang",
      "Jing Zhang"
    ],
    "abstract": "Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.",
    "arxiv_url": "https://arxiv.org/abs/2509.14739v1",
    "pdf_url": "https://arxiv.org/pdf/2509.14739v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "ar",
      "human",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI",
    "authors": [
      "Cong Tai",
      "Zhaoyu Zheng",
      "Haixu Long",
      "Hansheng Wu",
      "Haodong Xiang",
      "Zhengbin Long",
      "Jun Xiong",
      "Rong Shi",
      "Shizhuang Zhang",
      "Gang Qiu",
      "He Wang",
      "Ruifeng Li",
      "Jun Huang",
      "Bin Chang",
      "Shuai Feng",
      "Tao Shen"
    ],
    "abstract": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io",
    "arxiv_url": "https://arxiv.org/abs/2509.14687v1",
    "pdf_url": "https://arxiv.org/pdf/2509.14687v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Causal Reasoning Elicits Controllable 3D Scene Generation",
    "authors": [
      "Shen Chen",
      "Ruiyu Zhao",
      "Jiale Zhou",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "abstract": "Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.",
    "arxiv_url": "https://arxiv.org/abs/2509.15249v1",
    "pdf_url": "https://arxiv.org/pdf/2509.15249v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting",
    "authors": [
      "Dario Tscholl",
      "Yashwanth Nakka",
      "Brian Gunter"
    ],
    "abstract": "We present a perception-driven safety filter that converts each 3D Gaussian Splat (3DGS) into a closed-form forward collision cone, which in turn yields a first-order control barrier function (CBF) embedded within a quadratic program (QP). By exploiting the analytic geometry of splats, our formulation provides a continuous, closed-form representation of collision constraints that is both simple and computationally efficient. Unlike distance-based CBFs, which tend to activate reactively only when an obstacle is already close, our collision-cone CBF activates proactively, allowing the robot to adjust earlier and thereby produce smoother and safer avoidance maneuvers at lower computational cost. We validate the method on a large synthetic scene with approximately 170k splats, where our filter reduces planning time by a factor of 3 and significantly decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while maintaining the same level of safety. The approach is entirely analytic, requires no high-order CBF extensions (HOCBFs), and generalizes naturally to robots with physical extent through a principled Minkowski-sum inflation of the splats. These properties make the method broadly applicable to real-time navigation in cluttered, perception-derived extreme environments, including space robotics and satellite systems.",
    "arxiv_url": "https://arxiv.org/abs/2509.14421v1",
    "pdf_url": "https://arxiv.org/pdf/2509.14421v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping",
    "authors": [
      "Zhihao Cao",
      "Hanyu Wu",
      "Li Wa Tang",
      "Zizhou Luo",
      "Zihan Zhu",
      "Wei Zhang",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.",
    "arxiv_url": "https://arxiv.org/abs/2509.14191v2",
    "pdf_url": "https://arxiv.org/pdf/2509.14191v2",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "robotics",
      "mapping",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Plug-and-Play PDE Optimization for 3D Gaussian Splatting: Toward High-Quality Rendering and Reconstruction",
    "authors": [
      "Yifan Mo",
      "Youcheng Cai",
      "Ligang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction by achieving high-quality novel view synthesis with fast rendering speed, introducing 3D Gaussian primitives to represent the scene. However, 3DGS encounters blurring and floaters when applied to complex scenes, caused by the reconstruction of redundant and ambiguous geometric structures. We attribute this issue to the unstable optimization of the Gaussians. To address this limitation, we present a plug-and-play PDE-based optimization method that overcomes the optimization constraints of 3DGS-based approaches in various tasks, such as novel view synthesis and surface reconstruction. Firstly, we theoretically derive that the 3DGS optimization procedure can be modeled as a PDE, and introduce a viscous term to ensure stable optimization. Secondly, we use the Material Point Method (MPM) to obtain a stable numerical solution of the PDE, which enhances both global and local constraints. Additionally, an effective Gaussian densification strategy and particle constraints are introduced to ensure fine-grained details. Extensive qualitative and quantitative experiments confirm that our method achieves state-of-the-art rendering and reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2509.13938v1",
    "pdf_url": "https://arxiv.org/pdf/2509.13938v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction",
    "authors": [
      "Chu Chen",
      "Ander Biguri",
      "Jean-Michel Morel",
      "Raymond H. Chan",
      "Carola-Bibiane SchÃ¶nlieb",
      "Jizhou Li"
    ],
    "abstract": "X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$\\%$ of full views to achieve superior performance over the iterative method optimized on a full dataset.",
    "arxiv_url": "https://arxiv.org/abs/2509.13863v1",
    "pdf_url": "https://arxiv.org/pdf/2509.13863v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM",
    "authors": [
      "Yinlong Bai",
      "Hongxin Zhang",
      "Sheng Zhong",
      "Junkai Niu",
      "Hai Li",
      "Yijia He",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.",
    "arxiv_url": "https://arxiv.org/abs/2509.13536v1",
    "pdf_url": "https://arxiv.org/pdf/2509.13536v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its photorealistic rendering quality and real-time performance, but it generates massive amounts of data. Hence compressing 3DGS data is necessary for the cost effectiveness of 3DGS models. Recently, several anchor-based neural compression methods have been proposed, achieving good 3DGS compression performance. However, they all rely on uniform scalar quantization (USQ) due to its simplicity. A tantalizing question is whether more sophisticated quantizers can improve the current 3DGS compression methods with very little extra overhead and minimal change to the system. The answer is yes by replacing USQ with lattice vector quantization (LVQ). To better capture scene-specific characteristics, we optimize the lattice basis for each scene, improving LVQ's adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a balance between the R-D efficiency of vector quantization and the low complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS compression architectures, enhancing their R-D performance with minimal modifications and computational overhead. Moreover, by scaling the lattice basis vectors, SALVQ can dynamically adjust lattice density, enabling a single model to accommodate multiple bit rate targets. This flexibility eliminates the need to train separate models for different compression levels, significantly reducing training time and memory consumption.",
    "arxiv_url": "https://arxiv.org/abs/2509.13482v1",
    "pdf_url": "https://arxiv.org/pdf/2509.13482v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image",
    "authors": [
      "Gaofeng Liu",
      "Hengsen Li",
      "Ruoyu Gao",
      "Xuetong Li",
      "Zhiyuan Ma",
      "Tao Fang"
    ],
    "abstract": "With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.",
    "arxiv_url": "https://arxiv.org/abs/2509.13013v1",
    "pdf_url": "https://arxiv.org/pdf/2509.13013v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "body",
      "ar",
      "geometry",
      "avatar",
      "lightweight",
      "3d gaussian",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings",
    "authors": [
      "Abdalla Arafa",
      "Didier Stricker"
    ],
    "abstract": "Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.",
    "arxiv_url": "https://arxiv.org/abs/2509.12938v1",
    "pdf_url": "https://arxiv.org/pdf/2509.12938v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A-TDOM: Active TDOM via On-the-Fly 3DGS",
    "authors": [
      "Yiwei Xu",
      "Xiang Wang",
      "Yifei Yu",
      "Wentian Gan",
      "Luca Morelli",
      "Giulio Perda",
      "Xin Wang",
      "Zongqian Zhan",
      "Fabio Remondino"
    ],
    "abstract": "True Digital Orthophoto Map (TDOM), a 2D objective representation of the Earth's surface, is an essential geospatial product widely used in urban management, city planning, land surveying, and related applications. However, traditional TDOM generation typically relies on a complex offline photogrammetric pipeline, leading to substantial latency and making it unsuitable for time-critical or real-time scenarios. Moreover, the quality of TDOM may deteriorate due to inaccurate camera poses, imperfect Digital Surface Model (DSM), and incorrect occlusions detection. To address these challenges, this work introduces A-TDOM, a near real-time TDOM generation method built upon On-the-Fly 3DGS (3D Gaussian Splatting) optimization. As each incoming image arrives, its pose and sparse point cloud are computed via On-the-Fly SfM. Newly observed regions are then incrementally reconstructed as additional 3D Gaussians are inserted using a Delaunay triangulated Gaussian sampling and integration and are further optimized via adaptive training iterations and learning rate, especially in previously unseen or coarsely modeled areas. With orthogonal splatting integrated into the rendering pipeline, A-TDOM can actively produce updated TDOM outputs immediately after each 3DGS update. Code is now available at https://github.com/xywjohn/A-TDOM.",
    "arxiv_url": "https://arxiv.org/abs/2509.12759v3",
    "pdf_url": "https://arxiv.org/pdf/2509.12759v3",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
    "authors": [
      "Jiateng Liu",
      "Hao Gao",
      "Jiu-Cheng Xie",
      "Chi-Man Pun",
      "Jian Xiong",
      "Haolun Li",
      "Junxin Chen",
      "Feng Xu"
    ],
    "abstract": "This paper presents an effective Gaussian management framework for high-fidelity scene reconstruction of appearance and geometry. Departing from recent Gaussian Splatting (GS) methods that rely on indiscriminate attribute assignment, our approach introduces a novel densification strategy called \\emph{GauSep} that selectively activates Gaussian color or normal attributes. Together with a tailored rendering pipeline, termed \\emph{Separate Rendering}, this strategy alleviates gradient conflicts arising from dual supervision and yields improved reconstruction quality. In addition, we develop \\emph{GauRep}, an adaptive and integrated Gaussian representation that reduces redundancy both at the individual and global levels, effectively balancing model capacity and number of parameters. To provide reliable geometric supervision essential for effective management, we also introduce \\emph{CoRe}, a novel surface reconstruction module that distills normal fields from the SDF branch to the Gaussian branch through a confidence mechanism. Notably, our management framework is model-agnostic and can be seamlessly incorporated into other architectures, simultaneously improving performance and reducing model size. Extensive experiments demonstrate that our approach achieves superior performance in reconstructing both appearance and geometry compared with state-of-the-art methods, while using significantly fewer parameters.",
    "arxiv_url": "https://arxiv.org/abs/2509.12742v2",
    "pdf_url": "https://arxiv.org/pdf/2509.12742v2",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique for real-time, photorealistic rendering by optimizing anisotropic Gaussian primitives from view-dependent images. While 3D-GS has been extended to scientific visualization, prior work remains limited to single-GPU settings, restricting scalability for large datasets on high-performance computing (HPC) systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. To eliminate artifacts, we add ghost cells at partition boundaries and apply background masks to remove irrelevant pixels. Benchmarks on the Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup across 8 nodes on Polaris while preserving image quality. These results demonstrate that distributed 3D-GS enables scalable visualization of large-scale scientific data and provide a foundation for future in situ applications.",
    "arxiv_url": "https://arxiv.org/abs/2509.12138v1",
    "pdf_url": "https://arxiv.org/pdf/2509.12138v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
    "authors": [
      "Yi-Hsin Li",
      "Thomas Sikora",
      "Sebastian Knorr",
      "MÃ¥rten SjÃ¶strÃ¶m"
    ],
    "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2509.11853v2",
    "pdf_url": "https://arxiv.org/pdf/2509.11853v2",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "real-time rendering",
      "fast",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
    "authors": [
      "Wending Liu",
      "Siyun Liang",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "abstract": "We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
    "arxiv_url": "https://arxiv.org/abs/2509.11624v1",
    "pdf_url": "https://arxiv.org/pdf/2509.11624v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Skinning of Gaussian Avatars",
    "authors": [
      "Nikolaos Zioulis",
      "Nikolaos Kotarelas",
      "Georgios Albanis",
      "Spyridon Thermos",
      "Anargyros Chatzitofis"
    ],
    "abstract": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.",
    "arxiv_url": "https://arxiv.org/abs/2509.11411v1",
    "pdf_url": "https://arxiv.org/pdf/2509.11411v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "fast",
      "avatar",
      "human",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting",
    "authors": [
      "Lianjun Liao",
      "Chunhui Zhang",
      "Tong Wu",
      "Henglei Lv",
      "Bailin Deng",
      "Lin Gao"
    ],
    "abstract": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2509.11275v1",
    "pdf_url": "https://arxiv.org/pdf/2509.11275v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "outdoor",
      "relightable",
      "nerf",
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting",
    "authors": [
      "Ashkan Taghipour",
      "Vahid Naghshin",
      "Benjamin Southwell",
      "Farid Boussaid",
      "Hamid Laga",
      "Mohammed Bennamoun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
    "arxiv_url": "https://arxiv.org/abs/2509.11116v1",
    "pdf_url": "https://arxiv.org/pdf/2509.11116v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "vr",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting",
    "authors": [
      "Gurutva Patle",
      "Nilay Girgaonkar",
      "Nagabhushan Somraj",
      "Rajiv Soundararajan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods. The source code for our model can be found on our project page: https://gurutvapatle.github.io/publications/2025/ADGS.html .",
    "arxiv_url": "https://arxiv.org/abs/2509.11003v2",
    "pdf_url": "https://arxiv.org/pdf/2509.11003v2",
    "published_date": "2025-09-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation",
    "authors": [
      "Yi-Ruei Liu",
      "You-Zhe Xie",
      "Yu-Hsiang Hsu",
      "I-Sheng Fang",
      "Yu-Lun Liu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.",
    "arxiv_url": "https://arxiv.org/abs/2509.10759v2",
    "pdf_url": "https://arxiv.org/pdf/2509.10759v2",
    "published_date": "2025-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "4d",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "T2Bs: Text-to-Character Blendshapes via Video Generation",
    "authors": [
      "Jiahao Luo",
      "Chaoyang Wang",
      "Michael Vasilkovsky",
      "Vladislav Shakhrai",
      "Di Liu",
      "Peiye Zhuang",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "James Davis",
      "Jian Wang"
    ],
    "abstract": "We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.",
    "arxiv_url": "https://arxiv.org/abs/2509.10678v2",
    "pdf_url": "https://arxiv.org/pdf/2509.10678v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints",
    "authors": [
      "Elias De Smijter",
      "Renaud Detry",
      "Christophe De Vleeschouwer"
    ],
    "abstract": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2509.10241v2",
    "pdf_url": "https://arxiv.org/pdf/2509.10241v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting",
    "authors": [
      "Mahtab Dahaghin",
      "Milind G. Padalkar",
      "Matteo Toso",
      "Alessio Del Bue"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2509.07809v1",
    "pdf_url": "https://arxiv.org/pdf/2509.07809v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting",
    "authors": [
      "Yimin Pan",
      "Matthias NieÃner",
      "Tobias Kirschstein"
    ],
    "abstract": "Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.   While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.   The project page can be found at: https://yimin-pan.github.io/hair-gs/",
    "arxiv_url": "https://arxiv.org/abs/2509.07774v1",
    "pdf_url": "https://arxiv.org/pdf/2509.07774v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "human",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning",
    "authors": [
      "Wenzhi Guo",
      "Bing Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2509.07493v2",
    "pdf_url": "https://arxiv.org/pdf/2509.07493v2",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation",
    "authors": [
      "Ze-Xin Yin",
      "Jiaxiong Qiu",
      "Liu Liu",
      "Xinjie Wang",
      "Wei Sui",
      "Zhizhong Su",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.",
    "arxiv_url": "https://arxiv.org/abs/2509.07435v1",
    "pdf_url": "https://arxiv.org/pdf/2509.07435v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "ar",
      "geometry",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes",
    "authors": [
      "Shengkai Zhang",
      "Yuhe Liu",
      "Guanjun Wu",
      "Jianhua He",
      "Xinggang Wang",
      "Mozi Chen",
      "Kezhong Liu"
    ],
    "abstract": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.",
    "arxiv_url": "https://arxiv.org/abs/2509.06685v3",
    "pdf_url": "https://arxiv.org/pdf/2509.06685v3",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation",
    "authors": [
      "Ian Page",
      "Pierre Susbielle",
      "Olivier Aycard",
      "Pierre-Brice Wieber"
    ],
    "abstract": "Achieving efficient remote teleoperation is particularly challenging in unknown environments, as the teleoperator must rapidly build an understanding of the site's layout. Online 3D mapping is a proven strategy to tackle this challenge, as it enables the teleoperator to progressively explore the site from multiple perspectives. However, traditional online map-based teleoperation systems struggle to generate visually accurate 3D maps in real-time due to the high computational cost involved, leading to poor teleoperation performances. In this work, we propose a solution to improve teleoperation efficiency in unknown environments. Our approach proposes a novel, modular and efficient GPU-based integration between recent advancement in gaussian splatting SLAM and existing online map-based teleoperation systems. We compare the proposed solution against state-of-the-art teleoperation systems and validate its performances through real-world experiments using an aerial vehicle. The results show significant improvements in decision-making speed and more accurate interaction with the environment, leading to greater teleoperation efficiency. In doing so, our system enhances remote teleoperation by seamlessly integrating photorealistic mapping generation with real-time performances, enabling effective teleoperation in unfamiliar environments.",
    "arxiv_url": "https://arxiv.org/abs/2509.06433v2",
    "pdf_url": "https://arxiv.org/pdf/2509.06433v2",
    "published_date": "2025-09-08",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "efficient",
      "ar",
      "gaussian splatting",
      "slam",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom",
    "authors": [
      "Matthieu Gendrin",
      "StÃ©phane Pateux",
      "ThÃ©o Ladune"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.",
    "arxiv_url": "https://arxiv.org/abs/2509.06400v1",
    "pdf_url": "https://arxiv.org/pdf/2509.06400v1",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning",
    "authors": [
      "Jiarui Chen",
      "Yikeng Chen",
      "Yingshuang Zou",
      "Ye Huang",
      "Peng Wang",
      "Yuan Liu",
      "Yujing Sun",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: https://megs-2.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2509.07021v2",
    "pdf_url": "https://arxiv.org/pdf/2509.07021v2",
    "published_date": "2025-09-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "compression",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS) pipeline for scientific visualization. Building on previous work that demonstrated high-fidelity isosurface reconstruction using Gaussian primitives, we incorporate a multi-GPU training backend adapted from Grendel-GS to enable scalable processing of large datasets. By distributing optimization across GPUs, our method improves training throughput and supports high-resolution reconstructions that exceed single-GPU capacity. In our experiments, the system achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs compared to a single-GPU baseline, and successfully trains the Miranda dataset (18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays the groundwork for integrating 3D-GS into HPC-based scientific workflows, enabling real-time post hoc and in situ visualization of complex simulations.",
    "arxiv_url": "https://arxiv.org/abs/2509.05216v1",
    "pdf_url": "https://arxiv.org/pdf/2509.05216v1",
    "published_date": "2025-09-05",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
    "authors": [
      "Yangming Li",
      "Chaoyu Liu",
      "Lihao Liu",
      "Simon Masnou",
      "Carola-Bibiane SchÃ¶nlieb"
    ],
    "abstract": "A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they might also be unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework, GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.",
    "arxiv_url": "https://arxiv.org/abs/2509.05075v3",
    "pdf_url": "https://arxiv.org/pdf/2509.05075v3",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
    "authors": [
      "Hannah Schieber",
      "Dominik Frischmann",
      "Victor Schaack",
      "Simon Boche",
      "Angela Schoellig",
      "Stefan Leutenegger",
      "Daniel Roth"
    ],
    "abstract": "Mobile reconstruction has the potential to support time-critical tasks such as tele-guidance and disaster response, where operators must quickly gain an accurate understanding of the environment. Full high-fidelity scene reconstruction is computationally expensive and often unnecessary when only specific points of interest (POIs) matter for timely decision making. We address this challenge with CoRe-GS, a semantic POI-focused extension of Gaussian Splatting (GS). Instead of optimizing every scene element uniformly, CoRe-GS first produces a fast segmentation-ready GS representation and then selectively refines splats belonging to semantically relevant POIs detected during data acquisition. This targeted refinement reduces training time to 25\\% compared to full semantic GS while improving novel view synthesis quality in the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM) and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs enables faster and higher-quality mobile reconstruction tailored to operational needs.",
    "arxiv_url": "https://arxiv.org/abs/2509.04859v2",
    "pdf_url": "https://arxiv.org/pdf/2509.04859v2",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "semantic",
      "ar",
      "fast",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
    "authors": [
      "Jimin Xu",
      "Bosheng Qin",
      "Tao Jin",
      "Zhou Zhao",
      "Zhenhui Ye",
      "Jun Yu",
      "Fei Wu"
    ],
    "abstract": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.",
    "arxiv_url": "https://arxiv.org/abs/2509.04379v1",
    "pdf_url": "https://arxiv.org/pdf/2509.04379v1",
    "published_date": "2025-09-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction",
    "authors": [
      "Sankeerth Durvasula",
      "Sharanshangar Muhunthan",
      "Zain Moustafa",
      "Richard Chen",
      "Ruofan Liang",
      "Yushi Guan",
      "Nilesh Ahuja",
      "Nilesh Jain",
      "Selvakumar Panneer",
      "Nandita Vijaykumar"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.",
    "arxiv_url": "https://arxiv.org/abs/2509.03775v1",
    "pdf_url": "https://arxiv.org/pdf/2509.03775v1",
    "published_date": "2025-09-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "compact",
      "ar",
      "fast",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals",
    "authors": [
      "Mohit Mendiratta",
      "Mayur Deshmukh",
      "Kartik Teotia",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Christian Theobalt"
    ],
    "abstract": "3D Morphable Models (3DMMs) enable controllable facial geometry and expression editing for reconstruction, animation, and AR/VR, but traditional PCA-based mesh models are limited in resolution, detail, and photorealism. Neural volumetric methods improve realism but remain too slow for interactive use. Recent Gaussian Splatting (3DGS) based facial models achieve fast, high-quality rendering but still depend solely on a mesh-based 3DMM prior for expression control, limiting their ability to capture fine-grained geometry, expressions, and full-head coverage. We introduce GRMM, the first full-head Gaussian 3D morphable model that augments a base 3DMM with residual geometry and appearance components, additive refinements that recover high-frequency details such as wrinkles, fine skin texture, and hairline variations. GRMM provides disentangled control through low-dimensional, interpretable parameters (e.g., identity shape, facial expressions) while separately modelling residuals that capture subject- and expression-specific detail beyond the base model's capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders represent per-Gaussian appearance, and a lightweight CNN refines rasterised images for enhanced realism, all while maintaining 75 FPS real-time rendering. To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first dataset with 60 aligned expressions across 50 identities, enabling robust disentanglement of identity and expression in Gaussian-based 3DMMs. Across monocular 3D face reconstruction, novel-view synthesis, and expression transfer, GRMM surpasses state-of-the-art methods in fidelity and expression accuracy while delivering interactive real-time performance.",
    "arxiv_url": "https://arxiv.org/abs/2509.02141v1",
    "pdf_url": "https://arxiv.org/pdf/2509.02141v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "vr",
      "ar",
      "geometry",
      "fast",
      "real-time rendering",
      "lightweight",
      "gaussian splatting",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
    "authors": [
      "Hongyu Li",
      "Chaofeng Chen",
      "Xiaoming Li",
      "Guangming Lu"
    ],
    "abstract": "Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.",
    "arxiv_url": "https://arxiv.org/abs/2509.01964v1",
    "pdf_url": "https://arxiv.org/pdf/2509.01964v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Richard Bowden"
    ],
    "abstract": "Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.",
    "arxiv_url": "https://arxiv.org/abs/2509.01681v1",
    "pdf_url": "https://arxiv.org/pdf/2509.01681v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "ar",
      "fast",
      "avatar",
      "human",
      "neural rendering",
      "gaussian splatting",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars",
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Malte Prinzler",
      "Giorgio Becherini",
      "Michael J. Black",
      "Justus Thies"
    ],
    "abstract": "We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to https://im2haircut.is.tue.mpg.de.",
    "arxiv_url": "https://arxiv.org/abs/2509.01469v1",
    "pdf_url": "https://arxiv.org/pdf/2509.01469v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "geometry",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting",
    "authors": [
      "Josef GrÃ¼n",
      "Lukas Meyer",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2509.00989v1",
    "pdf_url": "https://arxiv.org/pdf/2509.00989v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "ar",
      "fast",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency",
    "authors": [
      "Joongho Jo",
      "Jongsun Park"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.",
    "arxiv_url": "https://arxiv.org/abs/2509.00911v2",
    "pdf_url": "https://arxiv.org/pdf/2509.00911v2",
    "published_date": "2025-08-31",
    "categories": [
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.",
    "arxiv_url": "https://arxiv.org/abs/2509.00800v1",
    "pdf_url": "https://arxiv.org/pdf/2509.00800v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "semantic",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure",
    "authors": [
      "Xiufeng Huang",
      "Ziyuan Luo",
      "Qi Song",
      "Ruofei Wang",
      "Renjie Wan"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.",
    "arxiv_url": "https://arxiv.org/abs/2509.00757v1",
    "pdf_url": "https://arxiv.org/pdf/2509.00757v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments",
    "authors": [
      "Yi Liu",
      "Keyu Fan",
      "Bin Lan",
      "Houde Liu"
    ],
    "abstract": "Visual SLAM algorithms have been enhanced through the exploration of Gaussian Splatting representations, particularly in generating high-fidelity dense maps. While existing methods perform reliably in static environments, they often encounter camera tracking drift and fuzzy mapping when dealing with the disturbances caused by moving objects. This paper presents DyPho-SLAM, a real-time, resource-efficient visual SLAM system designed to address the challenges of localization and photorealistic mapping in environments with dynamic objects. Specifically, the proposed system integrates prior image information to generate refined masks, effectively minimizing noise from mask misjudgment. Additionally, to enhance constraints for optimization after removing dynamic obstacles, we devise adaptive feature extraction strategies significantly improving the system's resilience. Experiments conducted on publicly dynamic RGB-D datasets demonstrate that the proposed system achieves state-of-the-art performance in camera pose estimation and dense map reconstruction, while operating in real-time in dynamic scenes.",
    "arxiv_url": "https://arxiv.org/abs/2509.00741v1",
    "pdf_url": "https://arxiv.org/pdf/2509.00741v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection",
    "authors": [
      "Houshu He",
      "Naifeng Jing",
      "Li Jiang",
      "Xiaoyao Liang",
      "Zhuoran Song"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a critical task that enables autonomous vehicles to construct maps and localize themselves in unknown environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting (3DGS) to achieve exceptional reconstruction fidelity. However, existing 3DGS-SLAM systems provide insufficient throughput due to the need for multiple training iterations per frame and the vast number of Gaussians.   In this paper, we propose AGS, an algorithm-hardware co-design framework to boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems process frames in a streaming manner, where adjacent frames exhibit high similarity that can be utilized for acceleration. On the software level: 1) We propose a coarse-then-fine-grained pose tracking method with respect to the robot's movement. 2) We avoid redundant computations of Gaussians by sharing their contribution information across frames. On the hardware level, we propose a frame covisibility detection engine to extract intermediate data from the video CODEC. We also implement a pose tracking engine and a mapping engine with workload schedulers to efficiently deploy the AGS algorithm. Our evaluation shows that AGS achieves up to $17.12\\times$, $6.71\\times$, and $5.41\\times$ speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS accelerator, GSCore.",
    "arxiv_url": "https://arxiv.org/abs/2509.00433v1",
    "pdf_url": "https://arxiv.org/pdf/2509.00433v1",
    "published_date": "2025-08-30",
    "categories": [
      "cs.AR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "acceleration",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
    "authors": [
      "Ziwei Liao",
      "Mohamed Sayed",
      "Steven L. Waslander",
      "Sara Vicente",
      "Daniyar Turmukhambetov",
      "Michael Firman"
    ],
    "abstract": "Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.",
    "arxiv_url": "https://arxiv.org/abs/2508.21542v1",
    "pdf_url": "https://arxiv.org/pdf/2508.21542v1",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content",
    "authors": [
      "Jiayu Yang",
      "Weijian Su",
      "Songqian Zhang",
      "Yuqi Han",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \\M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \\M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2508.21444v1",
    "pdf_url": "https://arxiv.org/pdf/2508.21444v1",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "deformation",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARGS: Advanced Regularization on Aligning Gaussians over the Surface",
    "authors": [
      "Jeong Uk Lee",
      "Sung Hee Choi"
    ],
    "abstract": "Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, \"needle-like\" shapes-by favoring more balanced, \"disk-like\" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.",
    "arxiv_url": "https://arxiv.org/abs/2508.21344v2",
    "pdf_url": "https://arxiv.org/pdf/2508.21344v2",
    "published_date": "2025-08-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
    "authors": [
      "Yuxi Hu",
      "Jun Zhang",
      "Kuangyi Chen",
      "Zhe Zhang",
      "Friedrich Fraundorfer"
    ],
    "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.",
    "arxiv_url": "https://arxiv.org/abs/2508.20754v1",
    "pdf_url": "https://arxiv.org/pdf/2508.20754v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "sparse view",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images",
    "authors": [
      "Shiqi Xin",
      "Xiaolin Zhang",
      "Yanbin Liu",
      "Peng Zhang",
      "Caifeng Shan"
    ],
    "abstract": "Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.",
    "arxiv_url": "https://arxiv.org/abs/2508.20623v1",
    "pdf_url": "https://arxiv.org/pdf/2508.20623v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation",
    "authors": [
      "Jiusi Li",
      "Jackson Jiang",
      "Jinyu Miao",
      "Miao Long",
      "Tuopu Wen",
      "Peijin Jia",
      "Shengxiang Liu",
      "Chunlei Yu",
      "Maolin Liu",
      "Yuzhan Cai",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "abstract": "Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.",
    "arxiv_url": "https://arxiv.org/abs/2508.20471v1",
    "pdf_url": "https://arxiv.org/pdf/2508.20471v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seam360GS: Seamless 360Â° Gaussian Splatting from Real-World Omnidirectional Images",
    "authors": [
      "Changha Shin",
      "Woong Oh Cho",
      "Seon Joo Kim"
    ],
    "abstract": "360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.",
    "arxiv_url": "https://arxiv.org/abs/2508.20080v1",
    "pdf_url": "https://arxiv.org/pdf/2508.20080v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction",
    "authors": [
      "Han Jiao",
      "Jiakai Sun",
      "Yexing Xu",
      "Lei Zhao",
      "Wei Xing",
      "Huaizhong Lin"
    ],
    "abstract": "3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to multi-view dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.",
    "arxiv_url": "https://arxiv.org/abs/2508.19786v2",
    "pdf_url": "https://arxiv.org/pdf/2508.19786v2",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers",
    "authors": [
      "Yue Wu",
      "Yufan Wu",
      "Wen Li",
      "Yuxi Lu",
      "Kairui Feng",
      "Xuanhong Chen"
    ],
    "abstract": "Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2508.19754v1",
    "pdf_url": "https://arxiv.org/pdf/2508.19754v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
    "authors": [
      "Yupeng Zhang",
      "Dezhi Zheng",
      "Ping Lu",
      "Han Zhang",
      "Lei Wang",
      "Liping xiang",
      "Cheng Luo",
      "Kaijun Deng",
      "Xiaowen Fu",
      "Linlin Shen",
      "Jinbao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.19699v1",
    "pdf_url": "https://arxiv.org/pdf/2508.19699v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Style4D-Bench: A Benchmark Suite for 4D Stylization",
    "authors": [
      "Beiqi Chen",
      "Shuai Shao",
      "Haitang Feng",
      "Jianhuang Lai",
      "Jianlou Si",
      "Guangcong Wang"
    ],
    "abstract": "We introduce Style4D-Bench, the first benchmark suite specifically designed for 4D stylization, with the goal of standardizing evaluation and facilitating progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive evaluation protocol measuring spatial fidelity, temporal coherence, and multi-view consistency through both perceptual and quantitative metrics, 2) a strong baseline that make an initial attempt for 4D stylization, and 3) a curated collection of high-resolution dynamic 4D scenes with diverse motions and complex backgrounds. To establish a strong baseline, we present Style4D, a novel framework built upon 4D Gaussian Splatting. It consists of three key components: a basic 4DGS scene representation to capture reliable geometry, a Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for temporally and spatially aware appearance control, and a Holistic Geometry-Preserved Style Transfer module designed to enhance spatio-temporal consistency via contrastive coherence learning and structural content preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D achieves state-of-the-art performance in 4D stylization, producing fine-grained stylistic details with stable temporal dynamics and consistent multi-view rendering. We expect Style4D-Bench to become a valuable resource for benchmarking and advancing research in stylized rendering of dynamic 3D scenes. Project page: https://becky-catherine.github.io/Style4D . Code: https://github.com/Becky-catherine/Style4D-Bench .",
    "arxiv_url": "https://arxiv.org/abs/2508.19243v1",
    "pdf_url": "https://arxiv.org/pdf/2508.19243v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps",
    "authors": [
      "Christian LÃ¶wens",
      "Thorben Funke",
      "Jingchao Xie",
      "Alexandru Paul Condurache"
    ],
    "abstract": "Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.",
    "arxiv_url": "https://arxiv.org/abs/2508.18788v1",
    "pdf_url": "https://arxiv.org/pdf/2508.18788v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "mapping",
      "gaussian splatting",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting",
    "authors": [
      "Qun Ji",
      "Peng Li",
      "Mingqiang Wei"
    ],
    "abstract": "High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.",
    "arxiv_url": "https://arxiv.org/abs/2508.18696v1",
    "pdf_url": "https://arxiv.org/pdf/2508.18696v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "deformation",
      "nerf",
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
    "authors": [
      "Jonghyun Kim",
      "Cheng Sun",
      "Michael Stengel",
      "Matthew Chan",
      "Andrew Russell",
      "Jaehyun Jung",
      "Wil Braithwaite",
      "Shalini De Mello",
      "David Luebke"
    ],
    "abstract": "Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.",
    "arxiv_url": "https://arxiv.org/abs/2508.18540v1",
    "pdf_url": "https://arxiv.org/pdf/2508.18540v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses",
    "authors": [
      "Hao Liang",
      "Zhixuan Ge",
      "Soumendu Majee",
      "Ashish Tiwari",
      "G. M. Dilshan Godaliyadda",
      "Ashok Veeraraghavan",
      "Guha Balakrishnan"
    ],
    "abstract": "We present FastAvatar, a fast and robust algorithm for single-image 3D face reconstruction using 3D Gaussian Splatting (3DGS). Given a single input image from an arbitrary pose, FastAvatar recovers a high-quality, full-head 3DGS avatar in approximately 3 seconds on a single NVIDIA A100 GPU. We use a two-stage design: a feed-forward encoder-decoder predicts coarse face geometry by regressing Gaussian structure from a pose-invariant identity embedding, and a lightweight test-time refinement stage then optimizes the appearance parameters for photorealistic rendering. This hybrid strategy combines the speed and stability of direct prediction with the accuracy of optimization, enabling strong identity preservation even under extreme input poses. FastAvatar achieves state-of-the-art reconstruction quality (24.01 dB PSNR, 0.91 SSIM) while running over 600x faster than existing per-subject optimization methods (e.g., FlashAvatar, GaussianAvatars, GASP). Once reconstructed, our avatars support photorealistic novel-view synthesis and FLAME-guided expression animation, enabling controllable reenactment from a single image. By jointly offering high fidelity, robustness to pose, and rapid reconstruction, FastAvatar significantly broadens the applicability of 3DGS-based facial avatars.",
    "arxiv_url": "https://arxiv.org/abs/2508.18389v2",
    "pdf_url": "https://arxiv.org/pdf/2508.18389v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "avatar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations",
    "authors": [
      "Fadi Khatib",
      "Dror Moran",
      "Guy Trostianetsky",
      "Yoni Kasten",
      "Meirav Galun",
      "Ronen Basri"
    ],
    "abstract": "We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.",
    "arxiv_url": "https://arxiv.org/abs/2508.18242v1",
    "pdf_url": "https://arxiv.org/pdf/2508.18242v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Pose Refinement via 3D Gaussian Splatting",
    "authors": [
      "Lulu Hao",
      "Lipu Zhou",
      "Zhenzhong Wei",
      "Xu Wang"
    ],
    "abstract": "Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.",
    "arxiv_url": "https://arxiv.org/abs/2508.17876v1",
    "pdf_url": "https://arxiv.org/pdf/2508.17876v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting",
    "authors": [
      "Hanzhi Chang",
      "Ruijie Zhu",
      "Wenjie Chang",
      "Mulin Yu",
      "Yanzhe Liang",
      "Jiahao Lu",
      "Zhuoyuan Li",
      "Tianzhu Zhang"
    ],
    "abstract": "Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web",
    "arxiv_url": "https://arxiv.org/abs/2508.17811v2",
    "pdf_url": "https://arxiv.org/pdf/2508.17811v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph",
    "authors": [
      "Xiaoyang Huang",
      "Bingbing Ni",
      "Wenjun Zhang"
    ],
    "abstract": "The emergence of 3D artificial intelligence-generated content (3D-AIGC) has enabled rapid synthesis of intricate geometries. However, a fundamental disconnect persists between AI-generated content and human-centric design paradigms, rooted in representational incompatibilities: conventional AI frameworks predominantly manipulate meshes or neural representations (\\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within parametric modeling tools. This disconnection diminishes the practical value of AI for 3D industry, undermining the efficiency of human-AI collaboration. To resolve this disparity, we focus on generating design operation sequences, which are structured modeling histories that comprehensively capture the step-by-step construction process of 3D assets and align with designers' typical workflows in modern 3D software. We first reformulate fundamental modeling operations (\\emph{e.g.}, \\emph{Extrude}, \\emph{Boolean}) into differentiable units, enabling joint optimization of continuous (\\emph{e.g.}, \\emph{Extrude} height) and discrete (\\emph{e.g.}, \\emph{Boolean} type) parameters via gradient-based learning. Based on these differentiable operations, a hierarchical graph with gating mechanism is constructed and optimized end-to-end by minimizing Chamfer Distance to target geometries. Multi-stage sequence length constraint and domain rule penalties enable unsupervised learning of compact design sequences without ground-truth sequence supervision. Extensive validation demonstrates that the generated operation sequences achieve high geometric fidelity, smooth mesh wiring, rational step composition and flexible editing capacity, with full compatibility within design industry.",
    "arxiv_url": "https://arxiv.org/abs/2508.17645v2",
    "pdf_url": "https://arxiv.org/pdf/2508.17645v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "nerf",
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation",
    "authors": [
      "Guanxing Lu",
      "Baoxiong Jia",
      "Puhao Li",
      "Yixin Chen",
      "Ziwei Wang",
      "Yansong Tang",
      "Siyuan Huang"
    ],
    "abstract": "Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.",
    "arxiv_url": "https://arxiv.org/abs/2508.17600v2",
    "pdf_url": "https://arxiv.org/pdf/2508.17600v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data",
    "authors": [
      "Meida Chen",
      "Luis Leal",
      "Yue Hu",
      "Rong Liu",
      "Butian Xiong",
      "Andrew Feng",
      "Jiuyi Xu",
      "Yangming Shi"
    ],
    "abstract": "For simulation and training purposes, military organizations have made substantial investments in developing high-resolution 3D virtual environments through extensive imaging and 3D scanning. However, the dynamic nature of battlefield conditions-where objects may appear or vanish over time-makes frequent full-scale updates both time-consuming and costly. In response, we introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with only a small set of newly acquired images. Our approach starts with camera pose estimation to align new images with the existing 3D model, followed by change detection to pinpoint modifications in the scene. A 3D generative AI model is then used to create high-quality 3D assets of the new elements, which are seamlessly integrated into the existing 3D model. The IDU pipeline incorporates human guidance to ensure high accuracy in object identification and placement, with each update focusing on a single new object at a time. Experimental results confirm that our proposed IDU pipeline significantly reduces update time and labor, offering a cost-effective and targeted solution for maintaining up-to-date 3D models in rapidly evolving military scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2508.17579v1",
    "pdf_url": "https://arxiv.org/pdf/2508.17579v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fiducial Marker Splatting for High-Fidelity Robotics Simulations",
    "authors": [
      "Diram Tabaa",
      "Gianni Di Caro"
    ],
    "abstract": "High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2508.17012v1",
    "pdf_url": "https://arxiv.org/pdf/2508.17012v1",
    "published_date": "2025-08-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "localization",
      "neural rendering",
      "lighting",
      "gaussian splatting",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Arbitrary-Scale 3D Gaussian Super-Resolution",
    "authors": [
      "Huimin Zeng",
      "Yue Bai",
      "Yun Fu"
    ],
    "abstract": "Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).",
    "arxiv_url": "https://arxiv.org/abs/2508.16467v2",
    "pdf_url": "https://arxiv.org/pdf/2508.16467v2",
    "published_date": "2025-08-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation",
    "authors": [
      "Zhaodong Jiang",
      "Ashish Sinha",
      "Tongtong Cao",
      "Yuan Ren",
      "Bingbing Liu",
      "Binbin Xu"
    ],
    "abstract": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.",
    "arxiv_url": "https://arxiv.org/abs/2508.15972v1",
    "pdf_url": "https://arxiv.org/pdf/2508.15972v1",
    "published_date": "2025-08-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework",
    "authors": [
      "Zongqi He",
      "Hanmin Li",
      "Kin-Chung Chan",
      "Yushen Zuo",
      "Hao Xie",
      "Zhe Xiao",
      "Jun Xiao",
      "Kin-Man Lam"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.",
    "arxiv_url": "https://arxiv.org/abs/2508.15457v1",
    "pdf_url": "https://arxiv.org/pdf/2508.15457v1",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians",
    "authors": [
      "Cong Wang",
      "Xianda Guo",
      "Wenbo Xu",
      "Wei Tian",
      "Ruiqi Song",
      "Chenming Zhang",
      "Lingxi Li",
      "Long Chen"
    ],
    "abstract": "In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2508.15376v3",
    "pdf_url": "https://arxiv.org/pdf/2508.15376v3",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Image-Conditioned 3D Gaussian Splat Quantization",
    "authors": [
      "Xinshuang Liu",
      "Runfa Blark Li",
      "Keito Suzuki",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.",
    "arxiv_url": "https://arxiv.org/abs/2508.15372v2",
    "pdf_url": "https://arxiv.org/pdf/2508.15372v2",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "compression",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion",
    "authors": [
      "Xuyang Chen",
      "Zhijun Zhai",
      "Kaixuan Zhou",
      "Zengmao Wang",
      "Jianan He",
      "Dong Wang",
      "Yanfeng Zhang",
      "mingwei Sun",
      "RÃ¼diger Westermann",
      "Konrad Schindler",
      "Liqiu Meng"
    ],
    "abstract": "Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques. project page: https://albertchen98.github.io/mess/",
    "arxiv_url": "https://arxiv.org/abs/2508.15169v3",
    "pdf_url": "https://arxiv.org/pdf/2508.15169v3",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "geometry",
      "relighting",
      "sparse view",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors",
    "authors": [
      "Jeonghyun Noh",
      "Hyun-Jic Oh",
      "Byungju Chae",
      "Won-Ki Jeong"
    ],
    "abstract": "Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.",
    "arxiv_url": "https://arxiv.org/abs/2508.15151v1",
    "pdf_url": "https://arxiv.org/pdf/2508.15151v1",
    "published_date": "2025-08-21",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels",
    "authors": [
      "Long Le",
      "Ryan Lucas",
      "Chen Wang",
      "Chuhao Chen",
      "Dinesh Jayaraman",
      "Eric Eaton",
      "Lingjie Liu"
    ],
    "abstract": "Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2508.17437v2",
    "pdf_url": "https://arxiv.org/pdf/2508.17437v2",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting",
    "authors": [
      "Jiaxin Wei",
      "Stefan Leutenegger",
      "Simon Schaefer"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have significantly enhanced novel view synthesis, yet generating high-quality renderings from extreme novel viewpoints or partially observed regions remains challenging. Meanwhile, diffusion models exhibit strong generative capabilities, but their reliance on text prompts and lack of awareness of specific scene information hinder accurate 3D reconstruction tasks. To address these limitations, we introduce GSFix3D, a novel framework that improves the visual fidelity in under-constrained regions by distilling prior knowledge from diffusion models into 3D representations, while preserving consistency with observed scene details. At its core is GSFixer, a latent diffusion model obtained via our customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to adapt pretrained generative models to a variety of environments and artifact types from different reconstruction methods, enabling robust novel view repair for unseen camera poses. Moreover, we propose a random mask augmentation strategy that empowers GSFixer to plausibly inpaint missing regions. Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer achieve state-of-the-art performance, requiring only minimal scene-specific fine-tuning on captured data. Real-world test further confirms its resilience to potential pose errors. Our code and data will be made publicly available. Project page: https://gsfix3d.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2508.14717v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14717v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeMS: Efficient Gaussian Splatting for Extreme Motion Blur",
    "authors": [
      "Gopi Raju Matta",
      "Trisha Reddypalli",
      "Vemunuri Divya Madhuri",
      "Kaushik Mitra"
    ],
    "abstract": "We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to handle severely motion-blurred images. State-of-the-art deblurring methods for extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches like Deblur-GS, typically assume access to sharp images for camera pose estimation and point cloud generation, an unrealistic assumption. Methods relying on COLMAP initialization, such as BAD-Gaussians, also fail due to unreliable feature correspondences under severe blur. To address these challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep learning-based Structure-from-Motion pipeline that estimates poses and generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which enables robust scene initialization by treating Gaussians as samples from a probability distribution, eliminating heuristic densification and pruning; and (3) joint optimization of camera trajectories and Gaussian parameters for stable reconstruction. While this pipeline produces strong results, inaccuracies may remain when all inputs are severely blurred. To mitigate this, we propose GeMS-E, which integrates a progressive refinement step using events: (4) Event-based Double Integral (EDI) deblurring restores sharper images that are then fed into GeMS, improving pose estimation, point cloud generation, and overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets. To our knowledge, this is the first framework to address extreme motion blur within 3DGS directly from severely blurred inputs.",
    "arxiv_url": "https://arxiv.org/abs/2508.14682v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14682v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels",
    "authors": [
      "Xingyuan Yang",
      "Min Wei"
    ],
    "abstract": "Inverse rendering of glossy objects from RGB imagery remains fundamentally limited by inherent ambiguity. Although NeRF-based methods achieve high-fidelity reconstruction via dense-ray sampling, their computational cost is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction efficiency but exhibits limitations under specular reflections. Multi-view inconsistencies introduce high-frequency surface noise and structural artifacts, while simplified rendering equations obscure material properties, leading to implausible relighting results. To address these issues, we propose GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we establish robust surface reconstruction through physics-based rendering with split-sum approximation, enhanced by geometric priors from foundation models. Second, we perform material decomposition by leveraging Monte Carlo importance sampling of the full rendering equation, modeling indirect illumination via differentiable 2D Gaussian ray tracing and refining high-frequency specular details through spherical mipmap-based directional encoding that captures anisotropic highlights. Extensive experiments demonstrate state-of-the-art performance in geometry reconstruction, material separation, and photorealistic relighting under novel illuminations, outperforming existing inverse rendering approaches.",
    "arxiv_url": "https://arxiv.org/abs/2508.14563v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14563v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ray tracing",
      "nerf",
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Slices to Structures: Unsupervised 3D Reconstruction of Female Pelvic Anatomy from Freehand Transvaginal Ultrasound",
    "authors": [
      "Max KrÃ¤henmann",
      "Sergio Tascon-Morales",
      "Fabian Laumer",
      "Julia E. Vogt",
      "Ece Ozkan"
    ],
    "abstract": "Volumetric ultrasound has the potential to significantly improve diagnostic accuracy and clinical decision-making, yet its widespread adoption remains limited by dependence on specialized hardware and restrictive acquisition protocols. In this work, we present a novel unsupervised framework for reconstructing 3D anatomical structures from freehand 2D transvaginal ultrasound (TVS) sweeps, without requiring external tracking or learned pose estimators. Our method adapts the principles of Gaussian Splatting to the domain of ultrasound, introducing a slice-aware, differentiable rasterizer tailored to the unique physics and geometry of ultrasound imaging. We model anatomy as a collection of anisotropic 3D Gaussians and optimize their parameters directly from image-level supervision, leveraging sensorless probe motion estimation and domain-specific geometric priors. The result is a compact, flexible, and memory-efficient volumetric representation that captures anatomical detail with high spatial fidelity. This work demonstrates that accurate 3D reconstruction from 2D ultrasound images can be achieved through purely computational means, offering a scalable alternative to conventional 3D systems and enabling new opportunities for AI-assisted analysis and diagnosis.",
    "arxiv_url": "https://arxiv.org/abs/2508.14552v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14552v1",
    "published_date": "2025-08-20",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting",
    "authors": [
      "Gyusam Chang",
      "Tuan-Anh Vu",
      "Vivek Alumootil",
      "Harris Song",
      "Deanna Pham",
      "Sangpil Kim",
      "M. Khalid Jawed"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in agriculture remains underexplored. Agricultural scenes present unique challenges for 3D reconstruction methods, particularly due to uneven illumination, occlusions, and a limited field of view. To address these limitations, we introduce \\textbf{NIRPlant}, a novel multimodal dataset encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth, and LiDAR data collected under varied indoor and outdoor lighting conditions. By integrating NIR data, our approach enhances robustness and provides crucial botanical insights that extend beyond the visible spectrum. Additionally, we leverage text-based metadata derived from vegetation indices, such as NDVI, NDWI, and the chlorophyll index, which significantly enriches the contextual understanding of complex agricultural environments. To fully exploit these modalities, we propose \\textbf{NIRSplat}, an effective multimodal Gaussian splatting architecture employing a cross-attention mechanism combined with 3D point-based positional encoding, providing robust geometric priors. Comprehensive experiments demonstrate that \\textbf{NIRSplat} outperforms existing landmark methods, including 3DGS, CoR-GS, and InstantSplat, highlighting its effectiveness in challenging agricultural scenarios. The code and dataset are publicly available at: https://github.com/StructuresComp/3D-Reconstruction-NIR",
    "arxiv_url": "https://arxiv.org/abs/2508.14443v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14443v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "outdoor",
      "illumination",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting",
    "authors": [
      "Elena Alegret",
      "Kunyi Li",
      "Sen Wang",
      "Siyun Liang",
      "Michael Niemeyer",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "3D scene reconstruction and understanding have gained increasing popularity, yet existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images. In this paper, we present GALA, a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. To extend to generalized language feature fields, we introduce the core contribution of GALA, a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning. Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.",
    "arxiv_url": "https://arxiv.org/abs/2508.14278v2",
    "pdf_url": "https://arxiv.org/pdf/2508.14278v2",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "authors": [
      "Chin-Yang Lin",
      "Cheng Sun",
      "Fu-En Yang",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ],
    "abstract": "LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/",
    "arxiv_url": "https://arxiv.org/abs/2508.14041v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14041v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distilled-3DGS:Distilled 3D Gaussian Splatting",
    "authors": [
      "Lintao Xiang",
      "Xinkai Chen",
      "Jianhuang Lai",
      "Guangcong Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS .",
    "arxiv_url": "https://arxiv.org/abs/2508.14037v1",
    "pdf_url": "https://arxiv.org/pdf/2508.14037v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Online 3D Gaussian Splatting Modeling with Novel View Selection",
    "authors": [
      "Byeonggwon Lee",
      "Junkyu Park",
      "Khang Truong Giang",
      "Soohwan Song"
    ],
    "abstract": "This study addresses the challenge of generating online 3D Gaussian Splatting (3DGS) models from RGB-only frames. Previous studies have employed dense SLAM techniques to estimate 3D scenes from keyframes for 3DGS model construction. However, these methods are limited by their reliance solely on keyframes, which are insufficient to capture an entire scene, resulting in incomplete reconstructions. Moreover, building a generalizable model requires incorporating frames from diverse viewpoints to achieve broader scene coverage. However, online processing restricts the use of many frames or extensive training iterations. Therefore, we propose a novel method for high-quality 3DGS modeling that improves model completeness through adaptive view selection. By analyzing reconstruction quality online, our approach selects optimal non-keyframes for additional training. By integrating both keyframes and selected non-keyframes, the method refines incomplete regions from diverse viewpoints, significantly enhancing completeness. We also present a framework that incorporates an online multi-view stereo approach, ensuring consistency in 3D information throughout the 3DGS modeling process. Experimental results demonstrate that our method outperforms state-of-the-art methods, delivering exceptional performance in complex outdoor scenes.",
    "arxiv_url": "https://arxiv.org/abs/2508.14014v2",
    "pdf_url": "https://arxiv.org/pdf/2508.14014v2",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis",
    "authors": [
      "Chunji Lv",
      "Zequn Chen",
      "Donglin Di",
      "Weinan Zhang",
      "Hao Li",
      "Wei Chen",
      "Yinjie Lei",
      "Changsheng Li"
    ],
    "abstract": "Despite advances in physics-based 3D motion synthesis, current methods face key limitations: reliance on pre-reconstructed 3D Gaussian Splatting (3DGS) built from dense multi-view images with time-consuming per-scene optimization; physics integration via either inflexible, hand-specified attributes or unstable, optimization-heavy guidance from video models using Score Distillation Sampling (SDS); and naive concatenation of prebuilt 3DGS with physics modules, which ignores physical information embedded in appearance and yields suboptimal performance. To address these issues, we propose PhysGM, a feed-forward framework that jointly predicts 3D Gaussian representation and physical properties from a single image, enabling immediate simulation and high-fidelity 4D rendering. Unlike slow appearance-agnostic optimization methods, we first pre-train a physics-aware reconstruction model that directly infers both Gaussian and physical parameters. We further refine the model with Direct Preference Optimization (DPO), aligning simulations with the physically plausible reference videos and avoiding the high-cost SDS optimization. To address the absence of a supporting dataset for this task, we propose PhysAssets, a dataset of 50K+ 3D assets annotated with physical properties and corresponding reference videos. Experiments show that PhysGM produces high-fidelity 4D simulations from a single image in one minute, achieving a significant speedup over prior work while delivering realistic renderings.Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2508.13911v2",
    "pdf_url": "https://arxiv.org/pdf/2508.13911v2",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors",
    "authors": [
      "Shikun Zhang",
      "Cunjian Chen",
      "Yiqun Wang",
      "Qiuhong Ke",
      "Yong Li"
    ],
    "abstract": "High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2508.13537v1",
    "pdf_url": "https://arxiv.org/pdf/2508.13537v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "vr",
      "ar",
      "geometry",
      "real-time rendering",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting",
    "authors": [
      "Shuxin Liang",
      "Yihan Xiao",
      "Wenlu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: https://github.com/Shuxin-Liang/InnerGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.13287v1",
    "pdf_url": "https://arxiv.org/pdf/2508.13287v1",
    "published_date": "2025-08-18",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IntelliCap: Intelligent Guidance for Consistent View Sampling",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Dieter Schmalstieg",
      "Shohei Mori"
    ],
    "abstract": "Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.",
    "arxiv_url": "https://arxiv.org/abs/2508.13043v1",
    "pdf_url": "https://arxiv.org/pdf/2508.13043v1",
    "published_date": "2025-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Kangjie Chen",
      "Yingji Zhong",
      "Zhihao Li",
      "Jiaqi Lin",
      "Youyu Chen",
      "Minghan Qin",
      "Haoqian Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.12720v3",
    "pdf_url": "https://arxiv.org/pdf/2508.12720v3",
    "published_date": "2025-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "sparse-view",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TiP4GEN: Text to Immersive Panorama 4D Scene Generation",
    "authors": [
      "Ke Xing",
      "Hanwen Liang",
      "Dejia Xu",
      "Yuyang Yin",
      "Konstantinos N. Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "With the rapid advancement and widespread adoption of VR/AR technologies, there is a growing demand for the creation of high-quality, immersive dynamic scenes. However, existing generation works predominantly concentrate on the creation of static scenes or narrow perspective-view dynamic scenes, falling short of delivering a truly 360-degree immersive experience from any viewpoint. In this paper, we introduce \\textbf{TiP4GEN}, an advanced text-to-dynamic panorama scene generation framework that enables fine-grained content control and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN integrates panorama video generation and dynamic scene reconstruction to create 360-degree immersive virtual environments. For video generation, we introduce a \\textbf{Dual-branch Generation Model} consisting of a panorama branch and a perspective branch, responsible for global and local view generation, respectively. A bidirectional cross-attention mechanism facilitates comprehensive information exchange between the branches. For scene reconstruction, we propose a \\textbf{Geometry-aligned Reconstruction Model} based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using metric depth maps and initializing scene cameras with estimated poses, our method ensures geometric consistency and temporal coherence for the reconstructed scenes. Extensive experiments demonstrate the effectiveness of our proposed designs and the superiority of TiP4GEN in generating visually compelling and motion-coherent dynamic panoramic scenes. Our project page is at https://ke-xing.github.io/TiP4GEN/.",
    "arxiv_url": "https://arxiv.org/abs/2508.12415v2",
    "pdf_url": "https://arxiv.org/pdf/2508.12415v2",
    "published_date": "2025-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "vr",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering",
    "authors": [
      "Xiaobin Deng",
      "Changyu Diao",
      "Min Li",
      "Ruohan Yu",
      "Duanqing Xu"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians.",
    "arxiv_url": "https://arxiv.org/abs/2508.12313v1",
    "pdf_url": "https://arxiv.org/pdf/2508.12313v1",
    "published_date": "2025-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes",
    "authors": [
      "Hongyuan Liu",
      "Haochen Yu",
      "Bochao Zou",
      "Jianfei Jiang",
      "Qiankun Liu",
      "Jiansheng Chen",
      "Huimin Ma"
    ],
    "abstract": "Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page.",
    "arxiv_url": "https://arxiv.org/abs/2508.12015v2",
    "pdf_url": "https://arxiv.org/pdf/2508.12015v2",
    "published_date": "2025-08-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "outdoor",
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haorang Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Wilian Lunardi",
      "Martin Andreoni",
      "Duen Horng Chau"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.",
    "arxiv_url": "https://arxiv.org/abs/2508.11854v2",
    "pdf_url": "https://arxiv.org/pdf/2508.11854v2",
    "published_date": "2025-08-16",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting",
    "authors": [
      "Simona Kocour",
      "Assia Benbihi",
      "Torsten Sattler"
    ],
    "abstract": "Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre/post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com/spatial-intelligence-ai/Remove360.git. Data are available at huggingface.co/datasets/simkoc/Remove360.",
    "arxiv_url": "https://arxiv.org/abs/2508.11431v1",
    "pdf_url": "https://arxiv.org/pdf/2508.11431v1",
    "published_date": "2025-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting",
    "authors": [
      "Zhenghao Chen",
      "Zicong Chen",
      "Lei Liu",
      "Yiming Wu",
      "Dong Xu"
    ],
    "abstract": "Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.",
    "arxiv_url": "https://arxiv.org/abs/2508.11183v1",
    "pdf_url": "https://arxiv.org/pdf/2508.11183v1",
    "published_date": "2025-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "recognition",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhou",
      "Jia-Chen Zhang",
      "Yu-Jie Xiong",
      "Chun-Ming Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).",
    "arxiv_url": "https://arxiv.org/abs/2508.10507v1",
    "pdf_url": "https://arxiv.org/pdf/2508.10507v1",
    "published_date": "2025-08-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting",
    "authors": [
      "Yuning Huang",
      "Jiahao Pang",
      "Fengqing Zhu",
      "Dong Tian"
    ],
    "abstract": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.",
    "arxiv_url": "https://arxiv.org/abs/2508.10227v1",
    "pdf_url": "https://arxiv.org/pdf/2508.10227v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation",
    "authors": [
      "Shuting He",
      "Peilin Ji",
      "Yitong Yang",
      "Changshuo Wang",
      "Jiayi Ji",
      "Yinglin Wang",
      "Henghui Ding"
    ],
    "abstract": "In the context of novel view synthesis, 3D Gaussian Splatting (3DGS) has recently emerged as an efficient and competitive counterpart to Neural Radiance Field (NeRF), enabling high-fidelity photorealistic rendering in real time. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into three foundational tasks: segmentation, editing, and generation, alongside additional functional applications built upon or tightly coupled with these foundational capabilities. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.",
    "arxiv_url": "https://arxiv.org/abs/2508.09977v3",
    "pdf_url": "https://arxiv.org/pdf/2508.09977v3",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "understanding",
      "compact",
      "semantic",
      "nerf",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics",
    "authors": [
      "Weiqi Li",
      "Zehao Zhang",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "abstract": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \\emph{geometric inconsistency} and \\emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \\emph{motion generalization limitations} and \\emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \\textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \\textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \\textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.",
    "arxiv_url": "https://arxiv.org/abs/2508.09858v1",
    "pdf_url": "https://arxiv.org/pdf/2508.09858v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.",
    "arxiv_url": "https://arxiv.org/abs/2508.09855v1",
    "pdf_url": "https://arxiv.org/pdf/2508.09855v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "human",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors",
    "authors": [
      "Xingyilang Yin",
      "Qi Zhang",
      "Jiahao Chang",
      "Ying Feng",
      "Qingnan Fan",
      "Xi Yang",
      "Chi-Man Pun",
      "Huaqi Zhang",
      "Xiaodong Cun"
    ],
    "abstract": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer.",
    "arxiv_url": "https://arxiv.org/abs/2508.09667v1",
    "pdf_url": "https://arxiv.org/pdf/2508.09667v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images",
    "authors": [
      "Xuejun Huang",
      "Xinyi Liu",
      "Yi Wan",
      "Zhi Zheng",
      "Bin Zhang",
      "Mingtao Xiong",
      "Yingying Pei",
      "Yongjun Zhang"
    ],
    "abstract": "Three-dimensional scene reconstruction from sparse-view satellite images is a long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its variants have recently attracted attention for its high efficiency, existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies. To address these limitations, we propose SkySplat, a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results. Compared to per-scene optimization methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2508.09479v1",
    "pdf_url": "https://arxiv.org/pdf/2508.09479v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction",
    "authors": [
      "Cheng Chen",
      "Hao Huang",
      "Saurabh Bagchi"
    ],
    "abstract": "Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.",
    "arxiv_url": "https://arxiv.org/abs/2508.10936v2",
    "pdf_url": "https://arxiv.org/pdf/2508.10936v2",
    "published_date": "2025-08-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A new dataset and comparison for multi-camera frame synthesis",
    "authors": [
      "Conall Daly",
      "Anil Kokaram"
    ],
    "abstract": "Many methods exist for frame synthesis in image sequences but can be broadly categorised into frame interpolation and view synthesis techniques. Fundamentally, both frame interpolation and view synthesis tackle the same task, interpolating a frame given surrounding frames in time or space. However, most frame interpolation datasets focus on temporal aspects with single cameras moving through time and space, while view synthesis datasets are typically biased toward stereoscopic depth estimation use cases. This makes direct comparison between view synthesis and frame interpolation methods challenging. In this paper, we develop a novel multi-camera dataset using a custom-built dense linear camera array to enable fair comparison between these approaches. We evaluate classical and deep learning frame interpolators against a view synthesis method (3D Gaussian Splatting) for the task of view in-betweening. Our results reveal that deep learning methods do not significantly outperform classical methods on real image data, with 3D Gaussian Splatting actually underperforming frame interpolators by as much as 3.5 dB PSNR. However, in synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.",
    "arxiv_url": "https://arxiv.org/abs/2508.09068v2",
    "pdf_url": "https://arxiv.org/pdf/2508.09068v2",
    "published_date": "2025-08-12",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhou",
      "Yu-Jie Xiong",
      "Chun-Ming Xia",
      "Jia-Chen Zhang",
      "Hong-Jian Zhan"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware adaptive density control framework to address these challenges. Our key innovations: the gradient coherence ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50\\% reduced memory consumption through optimized Gaussians utilization.",
    "arxiv_url": "https://arxiv.org/abs/2508.09239v1",
    "pdf_url": "https://arxiv.org/pdf/2508.09239v1",
    "published_date": "2025-08-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization",
    "authors": [
      "Chenxuan Liu",
      "He Li",
      "Zongze Li",
      "Shuai Wang",
      "Wei Xu",
      "Kejiang Ye",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Realizing low-cost communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSMR), which enables the simulator to opportunistically render a photo-realistic view from the robot's pose by calling ``memory'' from a GS model, thus reducing the need for excessive image uploads. However, the GS model may involve discrepancies compared to the actual environments. To this end, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation (i.e., adjusting to content profiles) across different frames by minimizing a newly derived GSMR loss function. The GSCLO problem is addressed by an accelerated penalty optimization (APO) algorithm that reduces computational complexity by over $10$x compared to traditional branch-and-bound and search algorithms. Moreover, variants of GSCLO are presented to achieve robust, low-power, and multi-robot GSMR. Extensive experiments demonstrate that the proposed GSMR paradigm and GSCLO method achieve significant improvements over existing benchmarks on both wheeled and legged robots in terms of diverse metrics in various scenarios. For the first time, it is found that RoboMR can be achieved with ultra-low communication costs, and mixture of data is useful for enhancing GS performance in dynamic scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2508.08624v2",
    "pdf_url": "https://arxiv.org/pdf/2508.08624v2",
    "published_date": "2025-08-12",
    "categories": [
      "cs.RO",
      "cs.IT"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
    "authors": [
      "Shuting He",
      "Guangquan Jie",
      "Changshuo Wang",
      "Yun Zhou",
      "Shuming Hu",
      "Guanbin Li",
      "Henghui Ding"
    ],
    "abstract": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.",
    "arxiv_url": "https://arxiv.org/abs/2508.08252v1",
    "pdf_url": "https://arxiv.org/pdf/2508.08252v1",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGOnline: Segment Any Gaussians Online",
    "authors": [
      "Wentao Sun",
      "Quanyun Wu",
      "Hanqing Xu",
      "Kyle Gao",
      "Zhengsen Xu",
      "Yiping Chen",
      "Dedong Zhang",
      "Lingfei Ma",
      "John S. Zelek",
      "Jonathan Li"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Existing segmentation approaches typically rely on high-dimensional feature lifting, which causes costly optimization, implicit semantics, and task-specific constraints. We present \\textbf{Segment Any Gaussians Online (SAGOnline)}, a unified, zero-shot framework that achieves real-time, cross-view consistent segmentation without scene-specific training. SAGOnline decouples the monolithic segmentation problem into lightweight sub-tasks. By integrating video foundation models (e.g., SAM 2), we first generate temporally consistent 2D masks across rendered views. Crucially, instead of learning continuous feature fields, we introduce a \\textbf{Rasterization-aware Geometric Consensus} mechanism that leverages the traceability of the Gaussian rasterization pipeline. This allows us to deterministically map 2D predictions to explicit, discrete 3D primitive labels in real-time. This discrete representation eliminates the memory and computational burden of feature distillation, enabling instant inference. Extensive evaluations on NVOS and SPIn-NeRF benchmarks demonstrate that SAGOnline achieves state-of-the-art accuracy (92.7\\% and 95.2\\% mIoU) while operating at the fastest speed at 27 ms per frame. By providing a flexible interface for diverse foundation models, our framework supports instant prompt, instance, and semantic segmentation, paving the way for interactive 3D understanding in AR/VR and robotics.",
    "arxiv_url": "https://arxiv.org/abs/2508.08219v2",
    "pdf_url": "https://arxiv.org/pdf/2508.08219v2",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "nerf",
      "vr",
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction",
    "authors": [
      "Tianle Zeng",
      "Junlei Hu",
      "Gerardo Loza Galindo",
      "Sharib Ali",
      "Duygu Sarikaya",
      "Pietro Valdastri",
      "Dominic Jones"
    ],
    "abstract": "Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.",
    "arxiv_url": "https://arxiv.org/abs/2508.07897v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07897v1",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction",
    "authors": [
      "Yuchen Gao",
      "Xiao Xu",
      "Eckehard Steinbach",
      "Daniel E. Lucani",
      "Qi Zhang"
    ],
    "abstract": "This paper presents a multimodal framework that integrates touch signals (contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our approach enhances scene reconstruction, particularly under challenging conditions like low lighting, limited camera viewpoints, and occlusions. Different from the visual-only method, the proposed approach incorporates spatially selective touch measurements to refine both the geometry and appearance of the 3D Gaussian representation. To guide the touch exploration, we introduce a two-stage sampling scheme that initially probes sparse regions and then concentrates on high-uncertainty boundaries identified from the reconstructed mesh. A geometric loss is proposed to ensure surface smoothness, resulting in improved geometry. Experimental results across diverse scenarios show consistent improvements in geometric accuracy. In the most challenging case with severe occlusion, the Chamfer Distance is reduced by over 15x, demonstrating the effectiveness of integrating touch cues into 3D Gaussian Splatting. Furthermore, our approach maintains a fully online pipeline, underscoring its feasibility in visually degraded environments.",
    "arxiv_url": "https://arxiv.org/abs/2508.07717v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07717v1",
    "published_date": "2025-08-11",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction",
    "authors": [
      "Bo Jia",
      "Yanan Guo",
      "Ying Chang",
      "Benkui Zhang",
      "Ying Xie",
      "Kangning Du",
      "Lin Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS. Our code will be made publicly available at (https://github.com/Bistu3DV/MND-GS/).",
    "arxiv_url": "https://arxiv.org/abs/2508.07701v2",
    "pdf_url": "https://arxiv.org/pdf/2508.07701v2",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution",
    "authors": [
      "Pranav Chougule"
    ],
    "abstract": "In this paper, I present a comprehensive study comparing Photogrammetry and Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I created a dataset of images from a real-world scene and constructed 3D models using both methods. To evaluate the performance, I compared the models using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and lp/mm resolution based on the USAF resolution chart. A significant contribution of this work is the development of a modified Gaussian Splatting repository, which I forked and enhanced to enable rendering images from novel camera poses generated in the Blender environment. This innovation allows for the synthesis of high-quality novel views, showcasing the flexibility and potential of Gaussian Splatting. My investigation extends to an augmented dataset that includes both original ground images and novel views synthesized via Gaussian Splatting. This augmented dataset was employed to generate a new photogrammetry model, which was then compared against the original photogrammetry model created using only the original images. The results demonstrate the efficacy of using Gaussian Splatting to generate novel high-quality views and its potential to improve photogrammetry-based 3D reconstructions. The comparative analysis highlights the strengths and limitations of both approaches, providing valuable information for applications in extended reality (XR), photogrammetry, and autonomous vehicle simulations. Code is available at https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.",
    "arxiv_url": "https://arxiv.org/abs/2508.07483v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07483v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
    "authors": [
      "Junyao Gao",
      "Jiaxing Li",
      "Wenran Liu",
      "Yanhong Zeng",
      "Fei Shen",
      "Kai Chen",
      "Yanan Sun",
      "Cairong Zhao"
    ],
    "abstract": "In this paper, we propose \\textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.",
    "arxiv_url": "https://arxiv.org/abs/2508.07409v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07409v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery",
    "authors": [
      "Rajaei Khatib",
      "Raja Giryes"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method, obtaining high-quality reconstruction with real-time rendering runtime performance. The main idea behind 3DGS is to represent the scene as a collection of 3D gaussians, while learning their parameters to fit the given views of the scene. While achieving superior performance in the presence of many views, 3DGS struggles with sparse view reconstruction, where the input views are sparse and do not fully cover the scene and have low overlaps. In this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By using the DIP prior, which utilizes internal structure and patterns, with coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla 3DGS fails, such as sparse view recovery. Note that our approach does not use any pre-trained models such as generative models and depth estimation, but rather relies only on the input frames. Among such methods, DIP-GS obtains state-of-the-art (SOTA) competitive results on various sparse-view reconstruction tasks, demonstrating its capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2508.07372v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07372v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction",
    "authors": [
      "Qilin Zhang",
      "Olaf Wysocki",
      "Boris Jutzi"
    ],
    "abstract": "Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.",
    "arxiv_url": "https://arxiv.org/abs/2508.07355v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07355v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "semantic",
      "ar",
      "geometry",
      "face",
      "gaussian splatting",
      "motion",
      "urban scene",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems",
    "authors": [
      "Qingyuan Zeng",
      "Shu Jiang",
      "Jiajing Lin",
      "Zhenzhong Wang",
      "Kay Chen Tan",
      "Min Jiang"
    ],
    "abstract": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital watermarking techniques, embedding either 1D bitstreams or 2D images, are used for copyright protection. However, the robustness of these watermarking techniques against potential attacks remains underexplored. This paper introduces the first universal black-box attack framework, the Group-based Multi-objective Evolutionary Attack (GMEA), designed to challenge these watermarking systems. We formulate the attack as a large-scale multi-objective optimization problem, balancing watermark removal with visual quality. In a black-box setting, we introduce an indirect objective function that blinds the watermark detector by minimizing the standard deviation of features extracted by a convolutional network, thus rendering the feature maps uninformative. To manage the vast search space of 3DGS models, we employ a group-based optimization strategy to partition the model into multiple, independent sub-optimization problems. Experiments demonstrate that our framework effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking methods while maintaining high visual fidelity. This work reveals critical vulnerabilities in existing 3DGS copyright protection schemes and calls for the development of more robust watermarking systems.",
    "arxiv_url": "https://arxiv.org/abs/2508.07263v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07263v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction",
    "authors": [
      "Xuesong Li",
      "Lars Petersson",
      "Vivien Rolland"
    ],
    "abstract": "This paper addresses the challenge of novel-view synthesis and motion reconstruction of dynamic scenes from monocular video, which is critical for many robotic applications. Although Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering static scenes, extending them to reconstruct dynamic scenes remains challenging. In this work, we introduce a novel approach that combines 3DGS with a motion trajectory field, enabling precise handling of complex object motions and achieving physically plausible motion trajectories. By decoupling dynamic objects from static background, our method compactly optimizes the motion trajectory field. The approach incorporates time-invariant motion coefficients and shared motion trajectory bases to capture intricate motion patterns while minimizing optimization complexity. Extensive experiments demonstrate that our approach achieves state-of-the-art results in both novel-view synthesis and motion trajectory recovery from monocular video, advancing the capabilities of dynamic scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2508.07182v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07182v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit",
    "authors": [
      "Aiden Swann",
      "Alex Qiu",
      "Matthew Strong",
      "Angelina Zhang",
      "Samuel Morstein",
      "Kai Rayle",
      "Monroe Kennedy"
    ],
    "abstract": "DexFruit is a robotic manipulation framework that enables gentle, autonomous handling of fragile fruit and precise evaluation of damage. Many fruits are fragile and prone to bruising, thus requiring humans to manually harvest them with care. In this work, we demonstrate by using optical tactile sensing, autonomous manipulation of fruit with minimal damage can be achieved. We show that our tactile informed diffusion policies outperform baselines in both reduced bruising and pick-and-place success rate across three fruits: strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat, a novel technique to represent and quantify visual damage in high-resolution 3D representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring damage lack quantitative rigor or require expensive equipment. With FruitSplat, we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into the 3DGS representation. Furthermore, this representation is modular and general, compatible with any relevant 2D model. Overall, we demonstrate a 92% grasping policy success rate, up to a 20% reduction in visual bruising, and up to an 31% improvement in grasp success rate on challenging fruit compared to our baselines across our three tested fruits. We rigorously evaluate this result with over 630 trials. Please checkout our website at https://dex-fruit.github.io .",
    "arxiv_url": "https://arxiv.org/abs/2508.07118v3",
    "pdf_url": "https://arxiv.org/pdf/2508.07118v3",
    "published_date": "2025-08-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression",
    "authors": [
      "Yuke Xing",
      "William Gordon",
      "Qi Yang",
      "Kaifa Yang",
      "Jiarui Wang",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high visual fidelity, but its substantial storage requirements hinder practical deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate compression modules. However, these 3DGS generative compression techniques introduce unique distortions lacking systematic quality assessment research. To this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment (VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences generated from 11 scenes across 6 SOTA 3DGS compression algorithms with systematically designed parameter levels. With annotations from 50 participants, we obtained MOS scores with outlier removal and validated dataset reliability. We benchmark 6 3DGS compression algorithms on storage efficiency and visual quality, and evaluate 15 quality assessment metrics across multiple paradigms. Our work enables specialized VQA model training for 3DGS, serving as a catalyst for compression and quality assessment research. The dataset is available at https://github.com/YukeXing/3DGS-VBench.",
    "arxiv_url": "https://arxiv.org/abs/2508.07038v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07038v1",
    "published_date": "2025-08-09",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events",
    "authors": [
      "Siyu Chen",
      "Shenghai Yuan",
      "Thien-Minh Nguyen",
      "Zhuyu Huang",
      "Chenyang Shi",
      "Jin Jing",
      "Lihua Xie"
    ],
    "abstract": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over traditional SLAM methods, enabling photorealistic 3D reconstruction that conventional approaches often struggle to achieve. However, existing GS-SLAM systems perform poorly under persistent and severe motion blur commonly encountered in real-world scenarios, leading to significantly degraded tracking accuracy and compromised 3D reconstruction quality. To address this limitation, we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D inputs to simultaneously reduce motion blur in images and compensate for the sparse and discrete nature of event streams, enabling robust tracking and high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system explicitly models the camera's continuous trajectory during exposure, supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian Splatting scene. Furthermore, we introduce a learnable camera response function to align the dynamic ranges of events and images, along with a no-event loss to suppress ringing artifacts during reconstruction. We validate our approach on a new dataset comprising synthetic and real-world sequences with significant motion blur. Extensive experimental results demonstrate that EGS-SLAM consistently outperforms existing GS-SLAM systems in both trajectory accuracy and photorealistic 3D Gaussian Splatting reconstruction. The source code will be available at https://github.com/Chensiyu00/EGS-SLAM.",
    "arxiv_url": "https://arxiv.org/abs/2508.07003v1",
    "pdf_url": "https://arxiv.org/pdf/2508.07003v1",
    "published_date": "2025-08-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "mapping",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "slam",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View",
    "authors": [
      "Ulas Gunes",
      "Matias Turkulainen",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "abstract": "We present the first evaluation of fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180 degree. Our study covers both indoor and outdoor scenes captured with 200 degree fisheye cameras and analyzes how each method handles extreme distortion in real world settings. We evaluate performance under varying fields of view (200 degree, 160 degree, and 120 degree) to study the tradeoff between peripheral distortion and spatial coverage. Fisheye-GS benefits from field of view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable across all settings and maintains high perceptual quality at the full 200 degree view. To address the limitations of SfM-based initialization, which often fails under strong distortion, we also propose a depth-based strategy using UniK3D predictions from only 2-3 fisheye images per scene. Although UniK3D is not trained on real fisheye data, it produces dense point clouds that enable reconstruction quality on par with SfM, even in difficult scenes with fog, glare, or sky. Our results highlight the practical viability of fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and distortion-heavy image inputs.",
    "arxiv_url": "https://arxiv.org/abs/2508.06968v1",
    "pdf_url": "https://arxiv.org/pdf/2508.06968v1",
    "published_date": "2025-08-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
    "authors": [
      "Giacomo D'Amicantonio",
      "Snehashis Majhi",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "FranÃ§ois Bremond",
      "Egor Bondarev"
    ],
    "abstract": "Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.",
    "arxiv_url": "https://arxiv.org/abs/2508.06318v1",
    "pdf_url": "https://arxiv.org/pdf/2508.06318v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting",
    "authors": [
      "Wenpeng Xing",
      "Jie Chen",
      "Zaifeng Yang",
      "Changting Lin",
      "Jianfeng Dong",
      "Chaochao Chen",
      "Xun Zhou",
      "Meng Han"
    ],
    "abstract": "Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.",
    "arxiv_url": "https://arxiv.org/abs/2508.06169v1",
    "pdf_url": "https://arxiv.org/pdf/2508.06169v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "light transport",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation",
    "authors": [
      "YoungChan Choi",
      "HengFei Wang",
      "YiHua Cheng",
      "Boeun Kim",
      "Hyung Jin Chang",
      "YoungGeun Choi",
      "Sang-Il Choi"
    ],
    "abstract": "We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2508.06136v2",
    "pdf_url": "https://arxiv.org/pdf/2508.06136v2",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors",
    "authors": [
      "Minsu Kim",
      "Subin Jeon",
      "In Cho",
      "Mijin Yoo",
      "Seon Joo Kim"
    ],
    "abstract": "Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io",
    "arxiv_url": "https://arxiv.org/abs/2508.06014v1",
    "pdf_url": "https://arxiv.org/pdf/2508.06014v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image",
    "authors": [
      "Yanxing Liang",
      "Yinghui Wang",
      "Jinlong Yang",
      "Wei Li"
    ],
    "abstract": "The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.",
    "arxiv_url": "https://arxiv.org/abs/2508.05950v1",
    "pdf_url": "https://arxiv.org/pdf/2508.05950v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "light transport",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
    "authors": [
      "Raphael Du Sablon",
      "David Hart"
    ],
    "abstract": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats, allowing for direct stylization on a .ply or .splat file without requiring the original camera views. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This also allows for fast stylization of splats with no additional training, achieving speeds under 2 minutes even on CPU-based consumer hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.",
    "arxiv_url": "https://arxiv.org/abs/2508.05813v2",
    "pdf_url": "https://arxiv.org/pdf/2508.05813v2",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAP: Gaussianize Any Point Clouds with Text Guidance",
    "authors": [
      "Weiqi Zhang",
      "Junsheng Zhou",
      "Haotian Geng",
      "Wenyuan Zhang",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.",
    "arxiv_url": "https://arxiv.org/abs/2508.05631v1",
    "pdf_url": "https://arxiv.org/pdf/2508.05631v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering",
    "authors": [
      "Junyu Zhou",
      "Yuyang Huang",
      "Wenrui Dai",
      "Junni Zou",
      "Ziyang Zheng",
      "Nuowen Kan",
      "Chenglin Li",
      "Hongkai Xiong"
    ],
    "abstract": "Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time rendering while maintaining high-fidelity novel view synthesis. However, 3DGS resorts to the Gaussian function that is low-pass by nature and is restricted in representing high-frequency details in 3D scenes. Moreover, it causes redundant primitives with degraded training and rendering efficiency and excessive memory overhead. To overcome these limitations, we propose 3D Gabor Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with multiple directional 3D frequency responses for radiance field representation supervised by multi-view images. The proposed 3D Gabor-based primitive forms a filter bank incorporating multiple 3D Gabor kernels at different frequencies to enhance flexibility and efficiency in capturing fine 3D details. Furthermore, to achieve novel view rendering, an efficient CUDA-based rasterizer is developed to project the multiple directional 3D frequency components characterized by 3D Gabor-based primitives onto the 2D image plane, and a frequency-adaptive mechanism is presented for adaptive joint optimization of primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless integration into existing 3DGS paradigms to enhance both efficiency and quality of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat outperforms 3DGS and its variants using alternative primitives, and achieves state-of-the-art rendering quality across both real-world and synthetic scenes. Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously reduced number of primitives and memory consumption.",
    "arxiv_url": "https://arxiv.org/abs/2508.05343v1",
    "pdf_url": "https://arxiv.org/pdf/2508.05343v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CF3: Compact and Fast 3D Feature Fields",
    "authors": [
      "Hyunjoon Lee",
      "Joonkyu Min",
      "Jaesik Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.05254v3",
    "pdf_url": "https://arxiv.org/pdf/2508.05254v3",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Refining Gaussian Splatting: A Volumetric Densification Approach",
    "authors": [
      "Mohamed Abdul Gafoor",
      "Marius Preda",
      "Titus Zaharia"
    ],
    "abstract": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.",
    "arxiv_url": "https://arxiv.org/abs/2508.05187v1",
    "pdf_url": "https://arxiv.org/pdf/2508.05187v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Yehor Karpichev",
      "Brandon Haworth",
      "Homayoun Najjaran"
    ],
    "abstract": "Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.",
    "arxiv_url": "https://arxiv.org/abs/2508.05064v2",
    "pdf_url": "https://arxiv.org/pdf/2508.05064v2",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "understanding",
      "semantic",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS",
    "authors": [
      "Zhihao Guo",
      "Peng Wang",
      "Zidong Chen",
      "Xiangyu Kong",
      "Yan Lyu",
      "Guanyu Gao",
      "Liangxiu Han"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\\% PSNR improvements on the MipNeRF 360 dataset.",
    "arxiv_url": "https://arxiv.org/abs/2508.04968v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04968v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction",
    "authors": [
      "Yifan Zhou",
      "Beizhen Zhao",
      "Pengcheng Wu",
      "Hao Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2508.04966v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04966v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting",
    "authors": [
      "Zijian Wang",
      "Beizhen Zhao",
      "Hao Wang"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable capabilities in real-time and photorealistic novel view synthesis. However, traditional 3DGS representations often struggle with large-scale scene management and efficient storage, particularly when dealing with complex environments or limited computational resources. To address these limitations, we introduce a novel perceive-sample-compress framework for 3D Gaussian Splatting. Specifically, we propose a scene perception compensation algorithm that intelligently refines Gaussian parameters at each level. This algorithm intelligently prioritizes visual importance for higher fidelity rendering in critical areas, while optimizing resource usage and improving overall visible quality. Furthermore, we propose a pyramid sampling representation to manage Gaussian primitives across hierarchical levels. Finally, to facilitate efficient storage of proposed hierarchical pyramid representations, we develop a Generalized Gaussian Mixed model compression algorithm to achieve significant compression ratios without sacrificing visual fidelity. The extensive experiments demonstrate that our method significantly improves memory efficiency and high visual quality while maintaining real-time rendering speed.",
    "arxiv_url": "https://arxiv.org/abs/2508.04965v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04965v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Gaussian Radio Fields for Channel Estimation",
    "authors": [
      "Muhammad Umer",
      "Muhammad Ahmed Mohsin",
      "Ahsan Bilal",
      "John M. Cioffi"
    ],
    "abstract": "Accurate channel state information (CSI) remains the most critical bottleneck in modern wireless networks, with pilot overhead consuming up to 11-21% of transmission bandwidth, increasing latency by 20-40% in massive MIMO systems, and reducing potential spectral efficiency by over 53%. Traditional estimation techniques fundamentally fail under mobility, with feedback delays as small as 4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We present neural Gaussian radio fields (nGRF), a novel framework that leverages explicit 3D Gaussian primitives to synthesize complex channel matrices accurately and efficiently. Unlike NeRF-based approaches that rely on slow implicit representations or existing Gaussian splatting methods that use non-physical 2D projections, nGRF performs direct 3D electromagnetic field aggregation, with each Gaussian acting as a localized radio modulator. nGRF demonstrates superior performance across diverse environments: in indoor scenarios, it achieves a 10.9$\\times$ higher prediction SNR than state of the art methods while reducing inference latency from 242 ms to just 1.1 ms (a 220$\\times$ speedup). For large-scale outdoor environments, where existing approaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF requires only 0.011 measurements per cubic foot compared to 0.2-178.1 for existing methods, thereby reducing data collection burden by 18$\\times$. Training time is similarly reduced from hours to minutes (a 180$\\times$ reduction), enabling rapid adaptation to dynamic environments. The code and datasets are available at: https://github.com/anonym-auth/n-grf",
    "arxiv_url": "https://arxiv.org/abs/2508.11668v1",
    "pdf_url": "https://arxiv.org/pdf/2508.11668v1",
    "published_date": "2025-08-06",
    "categories": [
      "eess.SP",
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction",
    "authors": [
      "Suyi Chen",
      "Haibin Ling"
    ],
    "abstract": "As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. In parallel, differentiable rendering techniques such as Gaussian splatting have demonstrated remarkable scalability and efficiency for volumetric representations, suggesting a natural fit for GMM-based cryo-EM reconstruction. However, off-the-shelf Gaussian splatting methods are designed for photorealistic view synthesis and remain incompatible with cryo-EM due to mismatches in the image formation physics, reconstruction objectives, and coordinate systems. Addressing these issues, we propose cryoSplat, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a view-dependent normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. These innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoSplat over representative baselines. The code will be released upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2508.04929v3",
    "pdf_url": "https://arxiv.org/pdf/2508.04929v3",
    "published_date": "2025-08-06",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research",
    "authors": [
      "Ke Li",
      "Mana Masuda",
      "Susanne Schmidt",
      "Shohei Mori"
    ],
    "abstract": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.",
    "arxiv_url": "https://arxiv.org/abs/2508.04326v2",
    "pdf_url": "https://arxiv.org/pdf/2508.04326v2",
    "published_date": "2025-08-06",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "nerf",
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction",
    "authors": [
      "Yaopeng Lou",
      "Liao Shen",
      "Tianqi Liu",
      "Jiaqi Li",
      "Zihao Huang",
      "Huiqiang Sun",
      "Zhiguo Cao"
    ],
    "abstract": "We present Multi-Baseline Gaussian Splatting (MuGS), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuGS achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets. Code is available at https://github.com/EuclidLou/MuGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.04297v2",
    "pdf_url": "https://arxiv.org/pdf/2508.04297v2",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition",
    "authors": [
      "Jiahui Li",
      "Shengeng Tang",
      "Jingxuan He",
      "Gang Huang",
      "Zhangye Wang",
      "Yantao Pan",
      "Lechao Cheng"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \\textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.",
    "arxiv_url": "https://arxiv.org/abs/2508.04224v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04224v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) represents a significant advancement in the field of efficient and high-fidelity novel view synthesis. Despite recent progress, achieving accurate geometric reconstruction under sparse-view conditions remains a fundamental challenge. Existing methods often rely on non-local depth regularization, which fails to capture fine-grained structures and is highly sensitive to depth estimation noise. Furthermore, traditional smoothing methods neglect semantic boundaries and indiscriminately degrade essential edges and textures, consequently limiting the overall quality of reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware regularization framework for 3D Gaussian Splatting. DET-GS introduces a hierarchical geometric depth supervision framework that adaptively enforces multi-level geometric consistency, significantly enhancing structural fidelity and robustness against depth estimation noise. To preserve scene boundaries, we design an edge-aware depth regularization guided by semantic masks derived from Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving Total Variation loss that selectively smooths homogeneous regions while rigorously retaining high-frequency details and textures. Extensive experiments demonstrate that DET-GS achieves substantial improvements in both geometric accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on sparse-view novel view synthesis benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2508.04099v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04099v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework",
    "authors": [
      "Yi-Ting Chen",
      "Ting-Hsuan Liao",
      "Pengsheng Guo",
      "Alexander Schwing",
      "Jia-Bin Huang"
    ],
    "abstract": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions.",
    "arxiv_url": "https://arxiv.org/abs/2508.04090v2",
    "pdf_url": "https://arxiv.org/pdf/2508.04090v2",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting",
    "authors": [
      "Zhan Li",
      "Huangying Zhan",
      "Changyang Li",
      "Qingan Yan",
      "Yi Xu"
    ],
    "abstract": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive and expert-driven process, often resulting in inconsistent reconstructions and suboptimal results. We propose RLGS, a plug-and-play reinforcement learning framework for adaptive hyperparameter tuning in 3DGS through lightweight policy modules, dynamically adjusting critical hyperparameters such as learning rates and densification thresholds. The framework is model-agnostic and seamlessly integrates into existing 3DGS pipelines without architectural modifications. We demonstrate its generalization ability across multiple state-of-the-art 3DGS variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness across diverse datasets. RLGS consistently enhances rendering quality. For example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT) dataset, under a fixed Gaussian budget, and continues to yield gains even when baseline performance saturates. Our results suggest that RLGS provides an effective and general solution for automating hyperparameter tuning in 3DGS training, bridging a gap in applying reinforcement learning to 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2508.04078v1",
    "pdf_url": "https://arxiv.org/pdf/2508.04078v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
    "authors": [
      "Hongyu Shen",
      "Junfeng Ni",
      "Yixin Chen",
      "Weishuo Li",
      "Mingtao Pei",
      "Siyuan Huang"
    ],
    "abstract": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.",
    "arxiv_url": "https://arxiv.org/abs/2508.03227v1",
    "pdf_url": "https://arxiv.org/pdf/2508.03227v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "gaussian splatting",
      "ar",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting",
    "authors": [
      "Weihang Liu",
      "Yuke Li",
      "Yuxuan Li",
      "Jingyi Yu",
      "Xin Lou"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.",
    "arxiv_url": "https://arxiv.org/abs/2508.03180v2",
    "pdf_url": "https://arxiv.org/pdf/2508.03180v2",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction",
    "authors": [
      "Heng Jia",
      "Linchao Zhu",
      "Na Zhao"
    ],
    "abstract": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a fundamental trade-off: explicit methods achieve geometric precision but struggle with ambiguous regions, while implicit methods provide robustness but suffer from slow convergence. We present H3R, a hybrid framework that addresses this limitation by integrating volumetric latent fusion with attention-based feature aggregation. Our framework consists of two complementary components: an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages PlÃ¼cker coordinates for adaptive correspondence refinement. By integrating both paradigms, our approach enhances generalization while converging 2$\\times$ faster than existing methods. Furthermore, we show that spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2), resolving the mismatch between semantic representations and spatial reconstruction requirements. Our method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively. Code is available at https://github.com/JiaHeng-DLUT/H3R.",
    "arxiv_url": "https://arxiv.org/abs/2508.03118v1",
    "pdf_url": "https://arxiv.org/pdf/2508.03118v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions",
    "authors": [
      "Anran Wu",
      "Long Peng",
      "Xin Di",
      "Xueyuan Dai",
      "Chen Wu",
      "Yang Wang",
      "Xueyang Fu",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "abstract": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of optimization-based 3DGS by enabling fast and high-quality reconstruction without the need for per-scene optimization. However, existing feedforward approaches typically assume that input multi-view images are clean and high-quality. In real-world scenarios, images are often captured under challenging conditions such as noise, low light, or rain, resulting in inaccurate geometry and degraded 3D reconstruction. To address these challenges, we propose a general and efficient multi-view feature enhancement module, RobustGS, which substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction. The RobustGS module can be seamlessly integrated into existing pretrained pipelines in a plug-and-play manner to enhance reconstruction robustness. Specifically, we introduce a novel component, Generalized Degradation Learner, designed to extract generic representations and distributions of multiple degradations from multi-view inputs, thereby enhancing degradation-awareness and improving the overall quality of 3D reconstruction. In addition, we propose a novel semantic-aware state-space model. It first leverages the extracted degradation representations to enhance corrupted inputs in the feature space. Then, it employs a semantic-aware strategy to aggregate semantically similar information across different views, enabling the extraction of fine-grained cross-view correspondences and further improving the quality of 3D representations. Extensive experiments demonstrate that our approach, when integrated into existing methods in a plug-and-play manner, consistently achieves state-of-the-art reconstruction quality across various types of degradations.",
    "arxiv_url": "https://arxiv.org/abs/2508.03077v1",
    "pdf_url": "https://arxiv.org/pdf/2508.03077v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting",
    "authors": [
      "Liheng Zhang",
      "Weihao Yu",
      "Zubo Lu",
      "Haozhi Gu",
      "Jin Huang"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.",
    "arxiv_url": "https://arxiv.org/abs/2508.03017v2",
    "pdf_url": "https://arxiv.org/pdf/2508.03017v2",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
    "authors": [
      "MikoÅaj ZieliÅski",
      "Krzysztof Byrski",
      "Tomasz Szczepanik",
      "PrzemysÅaw Spurek"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)",
    "arxiv_url": "https://arxiv.org/abs/2508.02831v1",
    "pdf_url": "https://arxiv.org/pdf/2508.02831v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "fast",
      "real-time rendering",
      "geometry",
      "dynamic",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PMGS: Reconstruction of Projectile Motion Across Large Spatiotemporal Spans via 3D Gaussian Splatting",
    "authors": [
      "Yijun Xu",
      "Jingrui Zhang",
      "Yuhan Chen",
      "Dingwen Wang",
      "Lei Yu",
      "Chu He"
    ],
    "abstract": "Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Furthermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.",
    "arxiv_url": "https://arxiv.org/abs/2508.02660v3",
    "pdf_url": "https://arxiv.org/pdf/2508.02660v3",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "acceleration",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting",
    "authors": [
      "Jianchao Wang",
      "Peng Zhou",
      "Cen Li",
      "Rong Quan",
      "Jie Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. Project Website: https://jcwang-gh.github.io/EFA-GS",
    "arxiv_url": "https://arxiv.org/abs/2508.02493v3",
    "pdf_url": "https://arxiv.org/pdf/2508.02493v3",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRIMU: Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Coverage",
    "authors": [
      "Thomas Gottwald",
      "Edgar Heinert",
      "Peter Stehr",
      "Chamuditha Jayanga Galappaththige",
      "Matthias Rottmann"
    ],
    "abstract": "We introduce Primitive-based Representations of Uncertainty (PRIMU), a post-hoc uncertainty estimation (UE) framework for Gaussian Splatting (GS). Reliable UE is essential for deploying GS in safety-critical domains such as robotics and medicine. Existing approaches typically estimate Gaussian-primitive variances and rely on the rendering process to obtain pixel-wise uncertainties. In contrast, we construct primitive-level representations of error and visibility/coverage from training views, capturing interpretable uncertainty information. These representations are obtained by projecting view-dependent training errors and coverage statistics onto the primitives. Uncertainties for novel views are inferred by rendering these primitive-level representations, producing uncertainty feature maps, which are aggregate through pixel-wise regression on holdout data. We analyze combinations of uncertainty feature maps and regression models to understand how their interactions affect prediction accuracy and generalization. PRIMU also enables an effective active view selection strategy by directly leveraging these uncertainty feature maps. Additionally, we study the effect of separating splatting into foreground and background regions. Our estimates show strong correlations with true errors, outperforming state-of-the-art methods, especially for depth UE and foreground objects. Finally, our regression models show generalization capabilities to unseen scenes, enabling UE without additional holdout data.",
    "arxiv_url": "https://arxiv.org/abs/2508.02443v2",
    "pdf_url": "https://arxiv.org/pdf/2508.02443v2",
    "published_date": "2025-08-04",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction",
    "authors": [
      "Yikuang Yuluo",
      "Yue Ma",
      "Kuan Shen",
      "Tongtong Jin",
      "Wang Liao",
      "Yangpu Ma",
      "Fuquan Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT reconstruction. However, existing methods rely on the average gradient magnitude of points within the view, often leading to severe needle-like artifacts under sparse-view conditions. To address this challenge, we propose GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses needle-like artifacts and improves reconstruction accuracy under sparse-view conditions. Our framework introduces two key innovations: (1) a Denoised Point Cloud Initialization Strategy that reduces initialization errors and accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that refines gradient computation using graph-based density differences, improving splitting accuracy and density representation. Experiments on X-3D and real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These results highlight the applicability of GR-Gaussian for accurate CT reconstruction under challenging sparse-view conditions.",
    "arxiv_url": "https://arxiv.org/abs/2508.02408v2",
    "pdf_url": "https://arxiv.org/pdf/2508.02408v2",
    "published_date": "2025-08-04",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
    "authors": [
      "Lei Yao",
      "Yi Wang",
      "Yi Zhang",
      "Moyun Liu",
      "Lap-Pui Chau"
    ],
    "abstract": "The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
    "arxiv_url": "https://arxiv.org/abs/2508.02172v1",
    "pdf_url": "https://arxiv.org/pdf/2508.02172v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
    "authors": [
      "Seungyeon Kim",
      "Junsu Ha",
      "Young Hun Kim",
      "Yonghyeon Lee",
      "Frank C. Park"
    ],
    "abstract": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model. See the project website at: https://screwsplat.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2508.02146v2",
    "pdf_url": "https://arxiv.org/pdf/2508.02146v2",
    "published_date": "2025-08-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "gaussian splatting",
      "ar",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling",
    "authors": [
      "Yuru Xiao",
      "Zihan Lin",
      "Chao Lu",
      "Deming Zhai",
      "Kui Jiang",
      "Wenbo Zhao",
      "Wei Zhang",
      "Junjun Jiang",
      "Huanran Wang",
      "Xianming Liu"
    ],
    "abstract": "Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.",
    "arxiv_url": "https://arxiv.org/abs/2508.02129v1",
    "pdf_url": "https://arxiv.org/pdf/2508.02129v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "fast",
      "dynamic",
      "face",
      "gaussian splatting",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment",
    "authors": [
      "Petteri Teikari",
      "Mike Jarrell",
      "Irene Bandera Moreno",
      "Harri Pesola"
    ],
    "abstract": "The convergence of autonomous indoor drones with physics-aware sensing technologies promises to transform property assessment from subjective visual inspection to objective, quantitative measurement. This comprehensive review examines the technical foundations enabling this paradigm shift across four critical domains: (1) platform architectures optimized for indoor navigation, where weight constraints drive innovations in heterogeneous computing, collision-tolerant design, and hierarchical control systems; (2) advanced sensing modalities that extend perception beyond human vision, including hyperspectral imaging for material identification, polarimetric sensing for surface characterization, and computational imaging with metaphotonics enabling radical miniaturization; (3) intelligent autonomy through active reconstruction algorithms, where drones equipped with 3D Gaussian Splatting make strategic decisions about viewpoint selection to maximize information gain within battery constraints; and (4) integration pathways with existing property workflows, including Building Information Modeling (BIM) systems and industry standards like Uniform Appraisal Dataset (UAD) 3.6.",
    "arxiv_url": "https://arxiv.org/abs/2508.01965v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01965v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing",
    "authors": [
      "Zhaonan Wang",
      "Manyi Li",
      "Changhe Tu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2508.01740v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01740v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Luqi Cheng",
      "Zhangshuo Qi",
      "Zijie Zhou",
      "Chao Lu",
      "Guangming Xiong"
    ],
    "abstract": "Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at https://github.com/ChengLuqi/LT-gaussian.",
    "arxiv_url": "https://arxiv.org/abs/2508.01704v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01704v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing",
    "authors": [
      "Yufeng Chi",
      "Huimin Ma",
      "Kafeng Wang",
      "Jianmin Li"
    ],
    "abstract": "While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \\textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.",
    "arxiv_url": "https://arxiv.org/abs/2508.01684v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01684v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians",
    "authors": [
      "Quankai Gao",
      "Iliyan Georgiev",
      "Tuanfeng Y. Wang",
      "Krishna Kumar Singh",
      "Ulrich Neumann",
      "Jae Shin Yoon"
    ],
    "abstract": "3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.",
    "arxiv_url": "https://arxiv.org/abs/2508.01464v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01464v1",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS",
    "authors": [
      "Han Ling",
      "Xian Xu",
      "Yinghui Sun",
      "Quansen Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become one of the most promising 3D reconstruction technologies. However, label noise in real-world scenarios-such as moving objects, non-Lambertian surfaces, and shadows-often leads to reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods either fail to separate noise effectively or require scene-specific fine-tuning of hyperparameters, making them difficult to apply in practice. This paper re-examines the problem of anti-noise reconstruction from the perspective of epistemic uncertainty, proposing a novel framework, OCSplats. By combining key technologies such as hybrid noise assessment and observation-based cognitive correction, the accuracy of noise classification in areas with cognitive differences has been significantly improved. Moreover, to address the issue of varying noise proportions in different scenarios, we have designed a label noise classification pipeline based on dynamic anchor points. This pipeline enables OCSplats to be applied simultaneously to scenarios with vastly different noise proportions without adjusting parameters. Extensive experiments demonstrate that OCSplats always achieve leading reconstruction performance and precise label noise classification in scenes of different complexity levels.",
    "arxiv_url": "https://arxiv.org/abs/2508.01239v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01239v1",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.",
    "arxiv_url": "https://arxiv.org/abs/2508.01171v2",
    "pdf_url": "https://arxiv.org/pdf/2508.01171v2",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging",
    "authors": [
      "Tianshuang Qiu",
      "Zehan Ma",
      "Karim El-Refai",
      "Hiya Shah",
      "Chung Min Kim",
      "Justin Kerr",
      "Ken Goldberg"
    ],
    "abstract": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view images. Such \"digital twins\" are useful for simulations, virtual reality, marketing, robot policy fine-tuning, and part inspection. 3D object scanning usually requires multi-camera arrays, precise laser scanners, or robot wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan, a pipeline for producing high-quality 3D Gaussian Splat models using a bi-manual robot that grasps an object with one gripper and rotates the object with respect to a stationary camera. The object is then re-grasped by a second gripper to expose surfaces that were occluded by the first gripper. We present the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as RAFT optical flow models to identify and isolate objects held by a robot gripper while removing the gripper and the background. We then modify the 3DGS training pipeline to support concatenated datasets with gripper occlusion, producing an omni-directional (360 degree view) model of the object. We apply Omni-Scan to part defect inspection, finding that it can identify visual or geometric defects in 12 different industrial and household objects with an average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be found at https://berkeleyautomation.github.io/omni-scan/",
    "arxiv_url": "https://arxiv.org/abs/2508.00354v1",
    "pdf_url": "https://arxiv.org/pdf/2508.00354v1",
    "published_date": "2025-08-01",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting",
    "authors": [
      "Wentao Sun",
      "Hanqing Xu",
      "Quanyun Wu",
      "Dedong Zhang",
      "Yiping Chen",
      "Lingfei Ma",
      "John S. Zelek",
      "Jonathan Li"
    ],
    "abstract": "We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360Â° coverage, and (5) 3D evaluation masks.",
    "arxiv_url": "https://arxiv.org/abs/2508.00259v1",
    "pdf_url": "https://arxiv.org/pdf/2508.00259v1",
    "published_date": "2025-08-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis",
    "authors": [
      "Bowen Zhang",
      "Sicheng Xu",
      "Chuxin Wang",
      "Jiaolong Yang",
      "Feng Zhao",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2507.23785v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23785v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "4d",
      "ar",
      "dynamic",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting",
    "authors": [
      "Di Li",
      "Jie Feng",
      "Jiahao Chen",
      "Weisheng Dong",
      "Guanbin Li",
      "Yuhui Zheng",
      "Mingtao Feng",
      "Guangming Shi"
    ],
    "abstract": "3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.",
    "arxiv_url": "https://arxiv.org/abs/2507.23772v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23772v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction",
    "authors": [
      "Zhenyang Li",
      "Xiaoyang Bai",
      "Tongchen Zhang",
      "Pengfei Shen",
      "Weiwei Xu",
      "Yifan Peng"
    ],
    "abstract": "High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.",
    "arxiv_url": "https://arxiv.org/abs/2507.23704v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23704v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation",
    "authors": [
      "Jialei Chen",
      "Wuhao Xu",
      "Sipeng He",
      "Baoru Huang",
      "Dongchun Ren"
    ],
    "abstract": "Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.",
    "arxiv_url": "https://arxiv.org/abs/2507.23683v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23683v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d reconstruction",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes",
    "authors": [
      "Xiaohan Li",
      "Ziren Gong",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia",
      "Dong Liu",
      "Jun Wu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM applications due to its fast rendering and high-fidelity representation. However, existing 3DGS-SLAM systems have predominantly focused on indoor environments and relied on active depth sensors, leaving a gap for large-scale outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian Splatting SLAM system designed for outdoor scenarios. Our approach uses only RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages depth estimates from pre-trained deep stereo networks to guide 3D Gaussian optimization with a multi-loss strategy enhancing both geometric consistency and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM achieves superior tracking accuracy and mapping performance compared to other 3DGS-based solutions in complex outdoor environments.",
    "arxiv_url": "https://arxiv.org/abs/2507.23677v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23677v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "fast",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "tracking",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization",
    "authors": [
      "Maxime Pietrantoni",
      "Gabriela Csurka",
      "Torsten Sattler"
    ],
    "abstract": "Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.",
    "arxiv_url": "https://arxiv.org/abs/2507.23569v2",
    "pdf_url": "https://arxiv.org/pdf/2507.23569v2",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Takeo Igarashi",
      "Yufeng Wang",
      "ZeSheng Wang",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou"
    ],
    "abstract": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.",
    "arxiv_url": "https://arxiv.org/abs/2507.23374v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23374v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting",
    "authors": [
      "Xingyue Peng",
      "Yuandong Lyu",
      "Lang Zhang",
      "Jian Zhu",
      "Songtao Wang",
      "Jiaxin Deng",
      "Songxin Lu",
      "Weiliang Ma",
      "Dangen She",
      "Peng Jia",
      "XianPeng Lang"
    ],
    "abstract": "Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.",
    "arxiv_url": "https://arxiv.org/abs/2507.23340v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23340v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "mapping",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "face",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iLRM: An Iterative Large 3D Reconstruction Model",
    "authors": [
      "Gyeongjin Kang",
      "Seungtae Nam",
      "Seungkwon Yang",
      "Xiangyu Sun",
      "Sameh Khamis",
      "Abdelrahman Mohamed",
      "Eunbyung Park"
    ],
    "abstract": "Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed.",
    "arxiv_url": "https://arxiv.org/abs/2507.23277v2",
    "pdf_url": "https://arxiv.org/pdf/2507.23277v2",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting",
    "authors": [
      "Jaeseok Park",
      "Chanoh Park",
      "Minsu Kim",
      "Soohwan Kim"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2507.23273v1",
    "pdf_url": "https://arxiv.org/pdf/2507.23273v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "mapping",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views",
    "authors": [
      "Yuki Fujimura",
      "Takahiro Kushida",
      "Kazuya Kitano",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa"
    ],
    "abstract": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views.",
    "arxiv_url": "https://arxiv.org/abs/2507.22342v2",
    "pdf_url": "https://arxiv.org/pdf/2507.22342v2",
    "published_date": "2025-07-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments",
    "authors": [
      "Yufei Jia",
      "Guangyu Wang",
      "Yuhang Dong",
      "Junzhe Wu",
      "Yupei Zeng",
      "Haonan Lin",
      "Zifan Wang",
      "Haizhou Ge",
      "Weibin Gu",
      "Kairui Ding",
      "Zike Yan",
      "Yunjie Cheng",
      "Yue Li",
      "Ziming Wang",
      "Chuxuan Li",
      "Wei Sui",
      "Lu Shi",
      "Guanzhong Tian",
      "Ruqi Huang",
      "Guyue Zhou"
    ],
    "abstract": "We present the first unified, modular, open-source 3DGS-based simulation framework for Real2Sim2Real robot learning. It features a holistic Real2Sim pipeline that synthesizes hyper-realistic geometry and appearance of complex real-world scenarios, paving the way for analyzing and bridging the Sim2Real gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively parallel simulation of multiple sensor modalities and accurate physics, with inclusive supports for existing 3D assets, robot models, and ROS plugins, empowering large-scale robot learning and complex robotic benchmarks. Through extensive experiments on imitation learning, Discoverse demonstrates state-of-the-art zero-shot Sim2Real transfer performance compared to existing simulators. For code and demos: https://air-discoverse.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2507.21981v1",
    "pdf_url": "https://arxiv.org/pdf/2507.21981v1",
    "published_date": "2025-07-29",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors",
    "authors": [
      "Shouyi Lu",
      "Zihan Lin",
      "Chao Lu",
      "Huanran Wang",
      "Guirong Zhuo",
      "Lianqing Zheng"
    ],
    "abstract": "Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.",
    "arxiv_url": "https://arxiv.org/abs/2507.21872v3",
    "pdf_url": "https://arxiv.org/pdf/2507.21872v3",
    "published_date": "2025-07-29",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for Real-time Rendering",
    "authors": [
      "Linye Wei",
      "Jiajun Tang",
      "Fan Fei",
      "Boxin Shi",
      "Runsheng Wang",
      "Meng Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and is getting increasing adoption in domains like autonomous driving and embodied intelligence. However, 3DGS still faces major efficiency challenges when faced with high frame rate requirements and resource-constrained edge deployment. To enable efficient 3DGS, in this paper, we propose LS-Gaussian, an algorithm/hardware co-design framework for lightweight streaming 3D rendering. LS-Gaussian is motivated by the core observation that 3DGS suffers from substantial computation redundancy and stalls. On one hand, in practical scenarios, high-frame-rate 3DGS is often applied in settings where a camera observes and renders the same scene continuously but from slightly different viewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian proposes a viewpoint transformation algorithm that leverages inter-frame continuity for efficient sparse rendering. On the other hand, as different tiles within an image are rendered in parallel but have imbalanced workloads, frequent hardware stalls also slow down the rendering process. LS-Gaussian predicts the workload for each tile based on viewpoint transformation to enable more balanced parallel computation and co-designs a customized 3DGS accelerator to support the workload-aware mapping in real-time. Experimental results demonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline on average and up to 17.3x speedup with the customized accelerator, while incurring only minimal visual quality degradation.",
    "arxiv_url": "https://arxiv.org/abs/2507.21572v2",
    "pdf_url": "https://arxiv.org/pdf/2507.21572v2",
    "published_date": "2025-07-29",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "mapping",
      "real-time rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections",
    "authors": [
      "Haiyang Bai",
      "Jiaqi Zhu",
      "Songru Jiang",
      "Wei Huang",
      "Tao Lu",
      "Yuanqi Li",
      "Jie Guo",
      "Runze Fu",
      "Yanwen Guo",
      "Lijun Chen"
    ],
    "abstract": "We propose a 3D Gaussian splatting-based framework for outdoor relighting that leverages intrinsic image decomposition to precisely integrate sunlight, sky radiance, and indirect lighting from unconstrained photo collections. Unlike prior methods that compress the per-image global illumination into a single latent vector, our approach enables simultaneously diverse shading manipulation and the generation of dynamic shadow effects. This is achieved through three key innovations: (1) a residual-based sun visibility extraction method to accurately separate direct sunlight effects, (2) a region-based supervision framework with a structural consistency loss for physically interpretable and coherent illumination decomposition, and (3) a ray-tracing-based technique for realistic shadow simulation. Extensive experiments demonstrate that our framework synthesizes novel views with competitive fidelity against state-of-the-art relighting solutions and produces more natural and multifaceted illumination and shadow effects.",
    "arxiv_url": "https://arxiv.org/abs/2507.20512v1",
    "pdf_url": "https://arxiv.org/pdf/2507.20512v1",
    "published_date": "2025-07-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "relightable",
      "illumination",
      "relighting",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "global illumination",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features",
    "authors": [
      "Shiyang Liu",
      "Dianyi Yang",
      "Yu Gao",
      "Bohan Ren",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "abstract": "In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation demonstrates significant potential in real-time rendering and training efficiency. However, most existing methods primarily focus on single-map reconstruction, while the registration and fusion of multiple 3D-GS sub-maps remain underexplored. Existing methods typically rely on manual intervention to select a reference sub-map as a template and use point cloud matching for registration. Moreover, hard-threshold filtering of 3D-GS primitives often degrades rendering quality after fusion. In this paper, we present a novel approach for automated 3D-GS sub-map alignment and fusion, eliminating the need for manual intervention while enhancing registration accuracy and fusion quality. First, we extract geometric skeletons across multiple scenes and leverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating robust scene registration. Second, we introduce a multi-factor Gaussian fusion strategy to mitigate the scene element loss caused by rigid thresholding. Experiments on the ScanNet-GSReg and our Coord datasets demonstrate the effectiveness of the proposed method in registration and fusion. For registration, it achieves a 41.9\\% reduction in RRE on complex scenes, ensuring more precise pose estimation. For fusion, it improves PSNR by 10.11 dB, highlighting superior structural preservation. These results confirm its ability to enhance scene alignment and reconstruction fidelity, ensuring more consistent and accurate 3D scene representation for robotic perception and autonomous navigation.",
    "arxiv_url": "https://arxiv.org/abs/2507.20480v1",
    "pdf_url": "https://arxiv.org/pdf/2507.20480v1",
    "published_date": "2025-07-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "real-time rendering",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos",
    "authors": [
      "Chenjian Gao",
      "Lihe Ding",
      "Rui Han",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue"
    ],
    "abstract": "Inserting 3D objects into videos is a longstanding challenge in computer graphics with applications in augmented reality, virtual try-on, and video composition. Achieving both temporal consistency, or realistic lighting remains difficult, particularly in dynamic scenarios with complex object motion, perspective changes, and varying illumination. While 2D diffusion models have shown promise for producing photorealistic edits, they often struggle with maintaining temporal coherence across frames. Conversely, traditional 3D rendering methods excel in spatial and temporal consistency but fall short in achieving photorealistic lighting. In this work, we propose a hybrid object insertion pipeline that combines the strengths of both paradigms. Specifically, we focus on inserting bracelets into dynamic wrist scenes, leveraging the high temporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and refining the results using a 2D diffusion-based enhancement model to ensure realistic lighting interactions. Our method introduces a shading-driven pipeline that separates intrinsic object properties (albedo, shading, reflectance) and refines both shading and sRGB images for photorealism. To maintain temporal coherence, we optimize the 3DGS model with multi-frame weighted adjustments. This is the first approach to synergize 3D rendering and 2D diffusion for video object insertion, offering a robust solution for realistic and consistent video editing. Project Page: https://cjeen.github.io/BraceletPaper/",
    "arxiv_url": "https://arxiv.org/abs/2507.20331v2",
    "pdf_url": "https://arxiv.org/pdf/2507.20331v2",
    "published_date": "2025-07-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhengwu Liu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (GS) has emerged as a powerful representation for high-quality scene reconstruction, offering compelling rendering quality. However, the training process of GS often suffers from slow convergence due to inefficient densification and suboptimal spatial distribution of Gaussian primitives. In this work, we present a comprehensive analysis of the split and clone operations during the densification phase, revealing their distinct roles in balancing detail preservation and computational efficiency. Building upon this analysis, we propose a global-to-local densification strategy, which facilitates more efficient growth of Gaussians across the scene space, promoting both global coverage and local refinement. To cooperate with the proposed densification strategy and promote sufficient diffusion of Gaussian primitives in space, we introduce an energy-guided coarse-to-fine multi-resolution training framework, which gradually increases resolution based on energy density in 2D images. Additionally, we dynamically prune unnecessary Gaussian primitives to speed up the training. Extensive experiments on MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our approach significantly accelerates training,achieving over 2x speedup with fewer Gaussian primitives and superior reconstruction performance.",
    "arxiv_url": "https://arxiv.org/abs/2507.20239v1",
    "pdf_url": "https://arxiv.org/pdf/2507.20239v1",
    "published_date": "2025-07-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Shell Texture Splatting: More Details and Fewer Primitives",
    "authors": [
      "Xin Zhang",
      "Anpei Chen",
      "Jincheng Xiong",
      "Pinxuan Dai",
      "Yujun Shen",
      "Weiwei Xu"
    ],
    "abstract": "Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.",
    "arxiv_url": "https://arxiv.org/abs/2507.20200v1",
    "pdf_url": "https://arxiv.org/pdf/2507.20200v1",
    "published_date": "2025-07-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection",
    "authors": [
      "Xiaokai Bai",
      "Chenxu Zhou",
      "Lianqing Zheng",
      "Si-Yuan Cao",
      "Jianan Liu",
      "Xiaohan Zhang",
      "Yiming Li",
      "Zhengzhuang Zhang",
      "Hui-liang Shen"
    ],
    "abstract": "4D millimeter-wave radar is a promising sensing modality for autonomous driving, yet effective 3D object detection from 4D radar and monocular images remains challenging. Existing fusion approaches either rely on instance proposals lacking global context or dense BEV grids constrained by rigid structures, lacking a flexible and adaptive representation for diverse scenes. To address this, we propose RaGS, the first framework that leverages 3D Gaussian Splatting (GS) to fuse 4D radar and monocular cues for 3D object detection. 3D GS models the scene as a continuous field of Gaussians, enabling dynamic resource allocation to foreground objects while maintaining flexibility and efficiency. Moreover, the velocity dimension of 4D radar provides motion cues that help anchor and refine the spatial distribution of Gaussians. Specifically, RaGS adopts a cascaded pipeline to construct and progressively refine the Gaussian field. It begins with Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse Gaussian centers. Then, Iterative Multimodal Aggregation (IMA) explicitly exploits image semantics and implicitly integrates 4D radar velocity geometry to refine the Gaussians within regions of interest. Finally, Multi-level Gaussian Fusion (MGF) renders the Gaussian field into hierarchical BEV features for 3D object detection. By dynamically focusing on sparse and informative regions, RaGS achieves object-centric precision and comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes demonstrate its robustness and SOTA performance. Code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2507.19856v3",
    "pdf_url": "https://arxiv.org/pdf/2507.19856v3",
    "published_date": "2025-07-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Taking Language Embedded 3D Gaussian Splatting into the Wild",
    "authors": [
      "Yuze Wang",
      "Yue Qi"
    ],
    "abstract": "Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.",
    "arxiv_url": "https://arxiv.org/abs/2507.19830v2",
    "pdf_url": "https://arxiv.org/pdf/2507.19830v2",
    "published_date": "2025-07-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "recognition",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting",
    "authors": [
      "David Bauer",
      "Qi Wu",
      "Hamid Gadirov",
      "Kwan-Liu Ma"
    ],
    "abstract": "Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2507.19718v2",
    "pdf_url": "https://arxiv.org/pdf/2507.19718v2",
    "published_date": "2025-07-25",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "path tracing",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations",
    "authors": [
      "Ziren Gong",
      "Xiaohan Li",
      "Fabio Tosi",
      "Youmin Zhang",
      "Stefano Mattoccia",
      "Jun Wu",
      "Matteo Poggi"
    ],
    "abstract": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2507.19474v1",
    "pdf_url": "https://arxiv.org/pdf/2507.19474v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization",
    "authors": [
      "Pol Francesch Huc",
      "Emily Bates",
      "Simone D'Amico"
    ],
    "abstract": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.",
    "arxiv_url": "https://arxiv.org/abs/2507.19459v1",
    "pdf_url": "https://arxiv.org/pdf/2507.19459v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Jie Chen",
      "Zhangchi Hu",
      "Peixi Wu",
      "Huyue Zhu",
      "Hebei Li",
      "Xiaoyan Sun"
    ],
    "abstract": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing plane-based methods in dynamic Gaussian splatting suffer from an unsuitable low-rank assumption, causing feature overlap and poor rendering quality. Although 4D hash encoding provides an explicit representation without low-rank constraints, directly applying it to the entire dynamic scene leads to substantial hash collisions and redundancy. To address these challenges, we present DASH, a real-time dynamic scene rendering framework that employs 4D hash encoding coupled with self-supervised decomposition. Our approach begins with a self-supervised decomposition mechanism that separates dynamic and static components without manual annotations or precomputed masks. Next, we introduce a multiresolution 4D hash encoder for dynamic elements, providing an explicit representation that avoids the low-rank assumption. Finally, we present a spatio-temporal smoothness regularization strategy to mitigate unstable deformation artifacts. Experiments on real-world datasets demonstrate that DASH achieves state-of-the-art dynamic rendering performance, exhibiting enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU. Code: https://github.com/chenj02/DASH.",
    "arxiv_url": "https://arxiv.org/abs/2507.19141v2",
    "pdf_url": "https://arxiv.org/pdf/2507.19141v2",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital CIM for High Frame Rate Real-Time Edge Rendering",
    "authors": [
      "Wei-Hsing Huang",
      "Cheng-Jhih Shih",
      "Jian-Wei Su",
      "Samuel Wade Wang",
      "Vaidehi Garg",
      "Yuyao Kong",
      "Jen-Chun Tien",
      "Nealson Li",
      "Arijit Raychowdhury",
      "Meng-Fan Chang",
      "Yingyan",
      "Lin",
      "Shimeng Yu"
    ],
    "abstract": "Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic scenes, enabling AR/VR applications with moving objects. However, implementing dynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian parameters from DRAM for frustum culling incurs high energy costs. (2) Increased parameters for dynamic scenes elevate sorting latency and energy consumption. (3) Limited on-chip buffer capacity with higher parameters reduces buffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not readily compatible with digital compute-in-memory (DCIM). These challenges hinder real-time performance and power efficiency on edge devices, leading to reduced battery life or requiring bulky batteries. To tackle these challenges, we propose algorithm-hardware co-design techniques. At the algorithmic level, we introduce three optimizations: (1) DRAM-access reduction frustum culling to lower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip buffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to reduce sorting latency. At the hardware level, we present a DCIM-friendly computation flow that is evaluated using the measured data from a 16nm DCIM prototype chip. Our experimental results on Large-Scale Real-World Static/Dynamic Datasets demonstrate the ability to achieve high frame rate real-time rendering exceeding 200 frame per second (FPS) with minimal power consumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W for dynamic Large-Scale Real-World scenes. This work successfully addresses the significant challenges of implementing static/dynamic 3DGS technology on resource-constrained edge devices.",
    "arxiv_url": "https://arxiv.org/abs/2507.19133v1",
    "pdf_url": "https://arxiv.org/pdf/2507.19133v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization",
    "authors": [
      "Zhentao Huang",
      "Di Wu",
      "Zhenbang He",
      "Minglun Gong"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.",
    "arxiv_url": "https://arxiv.org/abs/2507.18923v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18923v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time",
    "authors": [
      "Yun Chen",
      "Matthew Haines",
      "Jingkang Wang",
      "Krzysztof Baron-Lis",
      "Sivabalan Manivasagam",
      "Ze Yang",
      "Raquel Urtasun"
    ],
    "abstract": "High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/",
    "arxiv_url": "https://arxiv.org/abs/2507.18713v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18713v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "large scene",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping",
    "authors": [
      "Chong Cheng",
      "Zijian Wang",
      "Sicheng Yu",
      "Yu Hu",
      "Nanjie Yao",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D representation. Its effectiveness largely depends on precise camera poses and accurate point cloud initialization, which are often derived from pretrained Multi-View Stereo (MVS) models. However, in unposed reconstruction task from hundreds of outdoor images, existing MVS models may struggle with memory limits and lose accuracy as the number of input images grows. To address this limitation, we propose a novel unposed 3DGS reconstruction framework that integrates pretrained MVS priors with the probabilistic Procrustes mapping strategy. The method partitions input images into subsets, maps submaps into a global space, and jointly optimizes geometry and poses with 3DGS. Technically, we formulate the mapping of tens of millions of point clouds as a probabilistic Procrustes problem and solve a closed-form alignment. By employing probabilistic coupling along with a soft dustbin mechanism to reject uncertain correspondences, our method globally aligns point clouds and poses within minutes across hundreds of images. Moreover, we propose a joint optimization framework for 3DGS and camera poses. It constructs Gaussians from confidence-aware anchor points and integrates 3DGS differentiable rendering with an analytical Jacobian to jointly refine scene and poses, enabling accurate reconstruction and pose estimation. Experiments on Waymo and KITTI datasets show that our method achieves accurate reconstruction from unposed image sequences, setting a new state of the art for unposed 3DGS reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2507.18541v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18541v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting",
    "authors": [
      "Haoran Xu",
      "Saining Zhang",
      "Peishuo Li",
      "Baijun Ye",
      "Xiaoxue Chen",
      "Huan-ang Gao",
      "Jv Zheng",
      "Xiaowei Song",
      "Ziqiao Peng",
      "Run Miao",
      "Jinrang Jia",
      "Yifeng Shi",
      "Guangqi Yi",
      "Hang Zhao",
      "Hao Tang",
      "Hongyang Li",
      "Kaicheng Yu",
      "Hao Zhao"
    ],
    "abstract": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.",
    "arxiv_url": "https://arxiv.org/abs/2507.18473v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18473v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "tracking",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image",
    "authors": [
      "DongFu Yin",
      "Xiaotian Chen",
      "Fei Richard Yu",
      "Xuanchen Li",
      "Xinhao Zhang"
    ],
    "abstract": "Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.",
    "arxiv_url": "https://arxiv.org/abs/2507.18371v2",
    "pdf_url": "https://arxiv.org/pdf/2507.18371v2",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "deformation",
      "4d",
      "vr",
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM",
    "authors": [
      "Gyuhyeon Pak",
      "Hae Min Cho",
      "Euntai Kim"
    ],
    "abstract": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D reconstruction and robust camera pose tracking in real-time by representing each scene element using a Gaussian distribution constrained to the local tangent plane. This effectively models the local surface as a 2D Gaussian disk aligned with the underlying geometry, leading to more consistent depth interpretation across multiple viewpoints compared to conventional 3D ellipsoid-based representations with isotropic uncertainty. To integrate this representation into the SLAM pipeline, we embed the surface-aligned Gaussian disks into a Generalized ICP framework by introducing anisotropic covariance prior without altering the underlying registration formulation. Furthermore we propose a geometry-aware loss that supervises photometric, depth, and normal consistency. Our system achieves real-time operation while preserving both visual and geometric fidelity. Extensive experiments on the Replica and TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems in terms of localization accuracy, reconstruction completeness, while maintaining the rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2507.18344v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18344v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "slam",
      "tracking",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo",
    "authors": [
      "Yixiao Chen",
      "Bin Liang",
      "Hanzhi Guo",
      "Yongqing Cheng",
      "Jiayi Zhao",
      "Dongdong Weng"
    ],
    "abstract": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2507.18231v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18231v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
    "authors": [
      "SeungJun Moon",
      "Hah Min Lew",
      "Seungeun Lee",
      "Ji-Su Kang",
      "Gyeong-Moon Park"
    ],
    "abstract": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2507.18155v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18155v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details",
    "authors": [
      "Jun Zhou",
      "Dinghao Li",
      "Nannan Li",
      "Mingjie Wang"
    ],
    "abstract": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.",
    "arxiv_url": "https://arxiv.org/abs/2507.18023v1",
    "pdf_url": "https://arxiv.org/pdf/2507.18023v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting",
    "authors": [
      "Hyeongmin Lee",
      "Kyungjune Baek"
    ],
    "abstract": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91$\\times$ compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments. The source code is available at https://github.com/HyeongminLEE/RD4DGS.",
    "arxiv_url": "https://arxiv.org/abs/2507.17336v2",
    "pdf_url": "https://arxiv.org/pdf/2507.17336v2",
    "published_date": "2025-07-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "4d",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
    "authors": [
      "Luchuan Song",
      "Yang Zhou",
      "Zhan Xu",
      "Yi Zhou",
      "Deepali Aneja",
      "Chenliang Xu"
    ],
    "abstract": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.",
    "arxiv_url": "https://arxiv.org/abs/2507.17029v1",
    "pdf_url": "https://arxiv.org/pdf/2507.17029v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "relighting",
      "fast",
      "geometry",
      "avatar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion",
    "authors": [
      "Shang Liu",
      "Chenjie Cao",
      "Chaohui Yu",
      "Wen Qian",
      "Jing Wang",
      "Fan Wang"
    ],
    "abstract": "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/",
    "arxiv_url": "https://arxiv.org/abs/2507.16535v2",
    "pdf_url": "https://arxiv.org/pdf/2507.16535v2",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "ar",
      "geometry",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges",
    "authors": [
      "Tanveer Younis",
      "Zhanglin Cheng"
    ],
    "abstract": "Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2507.16406v1",
    "pdf_url": "https://arxiv.org/pdf/2507.16406v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "nerf",
      "vr",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images",
    "authors": [
      "Guichen Huang",
      "Ruoyu Wang",
      "Xiangjun Gao",
      "Che Sun",
      "Yuwei Wu",
      "Shenghua Gao",
      "Yunde Jia"
    ],
    "abstract": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\\% compared to existing per-pixel Gaussian prediction methods.",
    "arxiv_url": "https://arxiv.org/abs/2507.16144v1",
    "pdf_url": "https://arxiv.org/pdf/2507.16144v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.",
    "arxiv_url": "https://arxiv.org/abs/2507.15690v3",
    "pdf_url": "https://arxiv.org/pdf/2507.15690v3",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing",
    "authors": [
      "Boni Hu",
      "Zhenyu Xia",
      "Lin Chen",
      "Pengcheng Han",
      "Shuhui Bu"
    ],
    "abstract": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera pose from query images, is fundamental to remote sensing and UAV applications. Existing methods face inherent trade-offs: image-based retrieval and pose regression approaches lack precision, while structure-based methods that register queries to Structure-from-Motion (SfM) models suffer from computational complexity and limited scalability. These challenges are particularly pronounced in remote sensing scenarios due to large-scale scenes, high altitude variations, and domain gaps of existing visual priors. To overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel scene representation that compactly encodes both 3D geometry and appearance. We introduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting the rich semantic information and geometric constraints inherent in Gaussian primitives. To handle large-scale remote sensing scenarios, we incorporate partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic memory management strategies. Our approach consists of two stages: (1) a sparse stage featuring a Gaussian-specific consistent render-aware sampling strategy and landmark-guided detector for robust and accurate initial pose estimation, and (2) a dense stage that iteratively refines poses through coarse-to-fine dense rasterization matching while incorporating reliability verification. Through comprehensive evaluation on simulation data, public datasets, and real flight experiments, we demonstrate that our method delivers competitive localization accuracy, recall rate, and computational efficiency while effectively filtering unreliable pose estimates. The results confirm the effectiveness of our approach for practical remote sensing applications.",
    "arxiv_url": "https://arxiv.org/abs/2507.15683v1",
    "pdf_url": "https://arxiv.org/pdf/2507.15683v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "semantic",
      "ar",
      "geometry",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "authors": [
      "Zuo-Liang Zhu",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
    "arxiv_url": "https://arxiv.org/abs/2507.15629v1",
    "pdf_url": "https://arxiv.org/pdf/2507.15629v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "relightable",
      "ar",
      "relighting",
      "geometry",
      "face",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting",
    "authors": [
      "Zihui Gao",
      "Jia-Wang Bian",
      "Guosheng Lin",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "abstract": "Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines the strengths of both approaches: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine the details of SDF for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on the DTU and MobileBrick datasets. Code will be released at https://github.com/aim-uofa/SurfaceSplat.",
    "arxiv_url": "https://arxiv.org/abs/2507.15602v2",
    "pdf_url": "https://arxiv.org/pdf/2507.15602v2",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting",
    "authors": [
      "Ruijie Zhu",
      "Mulin Yu",
      "Linning Xu",
      "Lihan Jiang",
      "Yixuan Li",
      "Tianzhu Zhang",
      "Jiangmiao Pang",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
    "arxiv_url": "https://arxiv.org/abs/2507.15454v1",
    "pdf_url": "https://arxiv.org/pdf/2507.15454v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing",
    "authors": [
      "Minnan Pei",
      "Gang Li",
      "Junwen Si",
      "Zeyu Zhu",
      "Zitao Mo",
      "Peisong Wang",
      "Zhuoran Song",
      "Xiaoyao Liang",
      "Jian Cheng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering technique for high-fidelity view synthesis, prompting the development of dedicated 3DGS accelerators for resource-constrained platforms. The conventional decoupled preprocessing-rendering dataflow in existing accelerators has two major limitations: 1) a significant portion of preprocessed Gaussians are not used in rendering, and 2) the same Gaussian gets repeatedly loaded across different tile renderings, resulting in substantial computational and data movement overhead. To address these issues, we propose GCC, a novel accelerator designed for fast and energy-efficient 3DGS inference. GCC introduces a novel dataflow featuring: 1) \\textit{cross-stage conditional processing}, which interleaves preprocessing and rendering to dynamically skip unnecessary Gaussian preprocessing; and 2) \\textit{Gaussian-wise rendering}, ensuring that all rendering operations for a given Gaussian are completed before moving to the next, thereby eliminating duplicated Gaussian loading. We also propose an alpha-based boundary identification method to derive compact and accurate Gaussian regions, thereby reducing rendering costs. We implement our GCC accelerator in 28nm technology. Extensive experiments demonstrate that GCC significantly outperforms the state-of-the-art 3DGS inference accelerator, GSCore, in both performance and energy efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2507.15300v3",
    "pdf_url": "https://arxiv.org/pdf/2507.15300v3",
    "published_date": "2025-07-21",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "fast",
      "dynamic",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction",
    "authors": [
      "Xiufeng Huang",
      "Ka Chun Cheung",
      "Runmin Cong",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced Image-to-3D content creation but requires substantial computational resources and large datasets, posing challenges to training models from scratch. Current methods usually entangle the prediction of 3D Gaussian geometry and appearance, which rely heavily on data-driven priors and result in slow regression speeds. To address this, we propose \\method, a disentangled framework for efficient 3D Gaussian prediction. Our method extracts features from local image pairs using a stereo vision backbone and fuses them via global attention blocks. Dedicated point and Gaussian prediction heads generate multi-view point-maps for geometry and Gaussian features for appearance, combined as GS-maps to represent the 3DGS object. A refinement network enhances these GS-maps for high-quality reconstruction. Unlike existing methods that depend on camera parameters, our approach achieves pose-free 3D reconstruction, improving robustness and practicality. By reducing resource demands while maintaining high-quality outputs, \\method provides an efficient, scalable solution for real-world 3D content generation.",
    "arxiv_url": "https://arxiv.org/abs/2507.14921v2",
    "pdf_url": "https://arxiv.org/pdf/2507.14921v2",
    "published_date": "2025-07-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection",
    "authors": [
      "Jiahao Ma",
      "Tianyu Wang",
      "Miaomiao Liu",
      "David Ahmedt-Aristizabal",
      "Chuong Nguyen"
    ],
    "abstract": "Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.",
    "arxiv_url": "https://arxiv.org/abs/2507.14505v1",
    "pdf_url": "https://arxiv.org/pdf/2507.14505v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "sparse-view",
      "localization",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey",
    "authors": [
      "Jiahui Zhang",
      "Yuelei Li",
      "Anpei Chen",
      "Muyu Xu",
      "Kunhao Liu",
      "Jianyuan Wang",
      "Xiao-Xiao Long",
      "Hanxue Liang",
      "Zexiang Xu",
      "Hao Su",
      "Christian Theobalt",
      "Christian Rupprecht",
      "Andrea Vedaldi",
      "Kaichen Zhou",
      "Hanspeter Pfister",
      "Paul Pu Liang",
      "Shijian Lu",
      "Fangneng Zhan"
    ],
    "abstract": "3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.",
    "arxiv_url": "https://arxiv.org/abs/2507.14501v5",
    "pdf_url": "https://arxiv.org/pdf/2507.14501v5",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "nerf",
      "vr",
      "ar",
      "fast",
      "human",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "slam",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation",
    "authors": [
      "Han Gong",
      "Qiyue Li",
      "Jie Li",
      "Zhi Liu"
    ],
    "abstract": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a research hotspot in both academia and industry, owing to its impressive ability to deliver immersive 3D video experiences. However, research in this area is still in its early stages, and several fundamental challenges, such as tiling, quality assessment, and bitrate adaptation, require further investigation. In this paper, we tackle these challenges by proposing a comprehensive set of solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by saliency analysis, which integrates both spatial and temporal features. Each tile is encoded into versions possessing dedicated deformation fields and multiple quality levels for adaptive selection. We also introduce a novel quality assessment framework for 3DGS video that jointly evaluates spatial-domain degradation in 3DGS representations during streaming and the quality of the resulting 2D rendered images. Additionally, we develop a meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS video streaming, achieving optimal performance across varying network conditions. Extensive experiments demonstrate that our proposed approaches significantly outperform state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2507.14454v1",
    "pdf_url": "https://arxiv.org/pdf/2507.14454v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive 3D Gaussian Splatting Video Streaming",
    "authors": [
      "Han Gong",
      "Qiyue Li",
      "Zhi Liu",
      "Hao Zhou",
      "Peng Yuan Zhou",
      "Zhu Li",
      "Jie Li"
    ],
    "abstract": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.",
    "arxiv_url": "https://arxiv.org/abs/2507.14432v1",
    "pdf_url": "https://arxiv.org/pdf/2507.14432v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading",
    "authors": [
      "Efstratios Geronikolakis",
      "Manos Kamarianakis",
      "Antonis Protopsaltis",
      "George Papagiannakis"
    ],
    "abstract": "This paper presents Neural-GASh, a novel real-time shading pipeline for 3D meshes, that leverages a neural radiance field architecture to perform image-based rendering (IBR) using Conformal Geometric Algebra (CGA)-encoded vertex information as input. Unlike traditional Precomputed Radiance Transfer (PRT) methods, that require expensive offline precomputations, our learned model directly consumes CGA-based representations of vertex positions and normals, enabling dynamic scene shading without precomputation. Integrated seamlessly into the Unity engine, Neural-GASh facilitates accurate shading of animated and deformed 3D meshes - capabilities essential for dynamic, interactive environments. The shading of the scene is implemented within Unity, where rotation of scene lights in terms of Spherical Harmonics is also performed optimally using CGA. This neural field approach is designed to deliver fast and efficient light transport simulation across diverse platforms, including mobile and VR, while preserving high rendering quality. Additionally, we evaluate our method on scenes generated via 3D Gaussian splats, further demonstrating the flexibility and robustness of Neural-GASh in diverse scenarios. Performance is evaluated in comparison to conventional PRT, demonstrating competitive rendering speeds even with complex geometries.",
    "arxiv_url": "https://arxiv.org/abs/2507.13917v1",
    "pdf_url": "https://arxiv.org/pdf/2507.13917v1",
    "published_date": "2025-07-18",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "fast",
      "dynamic",
      "3d gaussian",
      "light transport"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
    "authors": [
      "Yu Wei",
      "Jiahui Zhang",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
    "arxiv_url": "https://arxiv.org/abs/2507.13891v2",
    "pdf_url": "https://arxiv.org/pdf/2507.13891v2",
    "published_date": "2025-07-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting",
    "authors": [
      "Kaiyuan Tang",
      "Kuangshi Ai",
      "Jun Han",
      "Chaoli Wang"
    ],
    "abstract": "Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.",
    "arxiv_url": "https://arxiv.org/abs/2507.13586v1",
    "pdf_url": "https://arxiv.org/pdf/2507.13586v1",
    "published_date": "2025-07-18",
    "categories": [
      "cs.GR",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "real-time rendering",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians",
    "authors": [
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.",
    "arxiv_url": "https://arxiv.org/abs/2507.12667v1",
    "pdf_url": "https://arxiv.org/pdf/2507.12667v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
    "authors": [
      "Davide Di Nucci",
      "Matteo Tomei",
      "Guido Borghi",
      "Luca Ciuffreda",
      "Roberto Vezzani",
      "Rita Cucchiara"
    ],
    "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.",
    "arxiv_url": "https://arxiv.org/abs/2507.12095v1",
    "pdf_url": "https://arxiv.org/pdf/2507.12095v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "gaussian splatting",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation",
    "authors": [
      "Beining Xu",
      "Siting Zhu",
      "Hesheng Wang"
    ],
    "abstract": "We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at https://github.com/IRMVLab/SGLoc.",
    "arxiv_url": "https://arxiv.org/abs/2507.12027v1",
    "pdf_url": "https://arxiv.org/pdf/2507.12027v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark",
    "authors": [
      "Jingqian Wu",
      "Peiqi Duan",
      "Zongqiang Wang",
      "Changwei Wang",
      "Boxin Shi",
      "Edmund Y. Lam"
    ],
    "abstract": "In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.",
    "arxiv_url": "https://arxiv.org/abs/2507.11931v1",
    "pdf_url": "https://arxiv.org/pdf/2507.11931v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Sicheng Yu",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction, which effectively balances rendering quality, efficiency, and speed. However, existing 3DGS approaches usually generate plausible outputs and face significant challenges in complex scene reconstruction, manifesting as incomplete holistic structural outlines and unclear local lighting effects. To address these issues simultaneously, we propose a novel decoupled optimization framework, which integrates wavelet decomposition into 3D Gaussian Splatting and 2D sampling. Technically, through 3D wavelet decomposition, our approach divides point clouds into high-frequency and low-frequency components, enabling targeted optimization for each. The low-frequency component captures global structural outlines and manages the distribution of Gaussians through voxelization. In contrast, the high-frequency component restores intricate geometric and textural details while incorporating a relight module to mitigate lighting artifacts and enhance photorealistic rendering. Additionally, a 2D wavelet decomposition is applied to the training images, simulating radiance variations. This provides critical guidance for high-frequency detail reconstruction, ensuring seamless integration of details with the global structure. Extensive experiments on challenging datasets demonstrate our method achieves state-of-the-art performance across various metrics, surpassing existing approaches and advancing the field of 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2507.12498v2",
    "pdf_url": "https://arxiv.org/pdf/2507.12498v2",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction",
    "authors": [
      "Haoxuan Qu",
      "Yujun Cai",
      "Hossein Rahmani",
      "Ajay Kumar",
      "Junsong Yuan",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
    "arxiv_url": "https://arxiv.org/abs/2507.11321v1",
    "pdf_url": "https://arxiv.org/pdf/2507.11321v1",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "high quality"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update",
    "authors": [
      "Jeongyun Kim",
      "Seunghoon Jeong",
      "Giseop Kim",
      "Myung-Hwan Jeon",
      "Eunji Jun",
      "Ayoung Kim"
    ],
    "abstract": "Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a Î´ < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.",
    "arxiv_url": "https://arxiv.org/abs/2507.11069v3",
    "pdf_url": "https://arxiv.org/pdf/2507.11069v3",
    "published_date": "2025-07-15",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "geometry",
      "sparse view",
      "dynamic",
      "sparse-view",
      "gaussian splatting",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
    "authors": [
      "Hayeon Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ],
    "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.",
    "arxiv_url": "https://arxiv.org/abs/2507.11061v2",
    "pdf_url": "https://arxiv.org/pdf/2507.11061v2",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
    "authors": [
      "Shivangi Aneja",
      "Sebastian Weiss",
      "Irene Baeza",
      "Prashanth Chandran",
      "Gaspard Zoss",
      "Matthias NieÃner",
      "Derek Bradley"
    ],
    "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
    "arxiv_url": "https://arxiv.org/abs/2507.10542v1",
    "pdf_url": "https://arxiv.org/pdf/2507.10542v1",
    "published_date": "2025-07-14",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "avatar",
      "dynamic",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving",
    "authors": [
      "Yixun Zhang",
      "Lizhi Wang",
      "Junjun Zhao",
      "Wending Zhao",
      "Feng Zhou",
      "Yonghao Dang",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. Existing 2D and 3D physical attacks, due to their focus on texture optimization, often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture optimization, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module that filters outliers to preserve geometric fidelity, and a physical augmentation module that simulates complex physical scenarios to enhance attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21\\% to 7.38\\%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks.",
    "arxiv_url": "https://arxiv.org/abs/2507.09993v3",
    "pdf_url": "https://arxiv.org/pdf/2507.09993v3",
    "published_date": "2025-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning human-to-robot handovers through 3D scene reconstruction",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.",
    "arxiv_url": "https://arxiv.org/abs/2507.08726v1",
    "pdf_url": "https://arxiv.org/pdf/2507.08726v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "human",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting",
    "authors": [
      "Ji Hyun Seo",
      "Byounhyun Yoo",
      "Gerard Jounghyun Kim"
    ],
    "abstract": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian Splatting, have emerged as seminal 3D representations for synthesizing realistic novel views. For practical applications, there is ongoing research on flexible scene editing techniques, among which object removal is a representative task. However, removing objects exposes occluded regions, often leading to unnatural appearances. Thus, studies have employed image inpainting techniques to replace such regions with plausible content - a task referred to as 3D scene inpainting. However, image inpainting methods produce one of many plausible completions for each view, leading to inconsistencies between viewpoints. A widely adopted approach leverages perceptual cues to blend inpainted views smoothly. However, it is prone to detail loss and can fail when there are perceptual inconsistencies across views. In this paper, we propose a novel 3D scene inpainting method that reliably produces realistic and perceptually consistent results even for complex scenes by leveraging a reference view. Given the inpainted reference view, we estimate the inpainting similarity of the other views to adjust their contribution in constructing an accurate geometry tailored to the reference. This geometry is then used to warp the reference inpainting to other views as pseudo-ground truth, guiding the optimization to match the reference appearance. Comparative evaluation studies have shown that our approach improves both the geometric fidelity and appearance consistency of inpainted scenes.",
    "arxiv_url": "https://arxiv.org/abs/2507.08434v1",
    "pdf_url": "https://arxiv.org/pdf/2507.08434v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Occlusion-Aware Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction",
    "authors": [
      "Hyungjun Doh",
      "Dong In Lee",
      "Seunggeun Chi",
      "Pin-Hao Huang",
      "Kwonjoon Lee",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "abstract": "We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.",
    "arxiv_url": "https://arxiv.org/abs/2507.08137v3",
    "pdf_url": "https://arxiv.org/pdf/2507.08137v3",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration",
    "authors": [
      "Chong Cheng",
      "Yu Hu",
      "Sicheng Yu",
      "Beizhen Zhao",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.",
    "arxiv_url": "https://arxiv.org/abs/2507.08136v2",
    "pdf_url": "https://arxiv.org/pdf/2507.08136v2",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection",
    "authors": [
      "Yongyang Zhou",
      "Fang-Lue Zhang",
      "Zichen Wang",
      "Lei Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.",
    "arxiv_url": "https://arxiv.org/abs/2507.07733v2",
    "pdf_url": "https://arxiv.org/pdf/2507.07733v2",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "relighting",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation",
    "authors": [
      "Bangning Wei",
      "Joshua Maraval",
      "Meriem Outtas",
      "Kidiyo Kpalma",
      "Nicolas Ramin",
      "Lu Zhang"
    ],
    "abstract": "The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.",
    "arxiv_url": "https://arxiv.org/abs/2507.07519v1",
    "pdf_url": "https://arxiv.org/pdf/2507.07519v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "outdoor",
      "4d",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction",
    "authors": [
      "Wei Yao",
      "Shuzhao Xie",
      "Letian Li",
      "Weixiang Zhang",
      "Zhixin Lai",
      "Shiqi Dai",
      "Ke Zhang",
      "Zhi Wang"
    ],
    "abstract": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\\% reduction in model size and an average of 100\\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2507.07465v1",
    "pdf_url": "https://arxiv.org/pdf/2507.07465v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections",
    "authors": [
      "Yongtang Bao",
      "Chengjie Tang",
      "Yuze Wang",
      "Haojie Li"
    ],
    "abstract": "Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.",
    "arxiv_url": "https://arxiv.org/abs/2507.07395v1",
    "pdf_url": "https://arxiv.org/pdf/2507.07395v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting",
    "authors": [
      "Wijayathunga W. M. R. D. B"
    ],
    "abstract": "We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.",
    "arxiv_url": "https://arxiv.org/abs/2507.07000v1",
    "pdf_url": "https://arxiv.org/pdf/2507.07000v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
    "authors": [
      "MatÃ©o Ducastel",
      "David TschumperlÃ©",
      "Yvain QuÃ©au"
    ],
    "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural networks and operate either through prior learning or inverse rendering optimization. Here, we revisit the problem of calibrated photometric stereo by leveraging recent advances in 3D inverse rendering using the Gaussian Splatting formalism. This allows us to parameterize the 3D scene to be reconstructed and optimize it in a more interpretable manner. Our approach incorporates a simplified model for light representation and demonstrates the potential of the Gaussian Splatting rendering engine for the photometric stereo problem.",
    "arxiv_url": "https://arxiv.org/abs/2507.06684v1",
    "pdf_url": "https://arxiv.org/pdf/2507.06684v1",
    "published_date": "2025-07-09",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting",
    "authors": [
      "Boyuan Tian",
      "Qizhe Gao",
      "Siran Xianyu",
      "Xiaotong Cui",
      "Minjia Zhang"
    ],
    "abstract": "3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian",
    "arxiv_url": "https://arxiv.org/abs/2507.06671v1",
    "pdf_url": "https://arxiv.org/pdf/2507.06671v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data",
    "authors": [
      "Chengkun Li",
      "Yuqi Tong",
      "Kai Chen",
      "Zhenya Yang",
      "Ruiyang Li",
      "Shi Qiu",
      "Jason Ying-Kuen Chan",
      "Pheng-Ann Heng",
      "Qi Dou"
    ],
    "abstract": "The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2507.06647v1",
    "pdf_url": "https://arxiv.org/pdf/2507.06647v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS",
    "authors": [
      "Wanhua Li",
      "Yujie Zhao",
      "Minghan Qin",
      "Yang Liu",
      "Yuanhao Cai",
      "Chuang Gan",
      "Hanspeter Pfister"
    ],
    "abstract": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\\times$ speedup and a 47 $\\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2507.07136v2",
    "pdf_url": "https://arxiv.org/pdf/2507.07136v2",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "semantic",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures",
    "authors": [
      "Seungoh Han",
      "Jaehoon Jang",
      "Hyunsu Kim",
      "Jaeheung Surh",
      "Junhyung Kwak",
      "Hyowon Ha",
      "Kyungdon Joo"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement.",
    "arxiv_url": "https://arxiv.org/abs/2507.06109v1",
    "pdf_url": "https://arxiv.org/pdf/2507.06109v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering",
    "authors": [
      "Jiayi Song",
      "Zihan Ye",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ben Fei",
      "Jingyi Xu",
      "Ying He",
      "Wanli Ouyang"
    ],
    "abstract": "Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2507.06103v1",
    "pdf_url": "https://arxiv.org/pdf/2507.06103v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis",
    "authors": [
      "Alexandre Symeonidis-Herzig",
      "Ãzge MercanoÄlu Sincan",
      "Richard Bowden"
    ],
    "abstract": "Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.",
    "arxiv_url": "https://arxiv.org/abs/2507.06060v2",
    "pdf_url": "https://arxiv.org/pdf/2507.06060v2",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "recognition",
      "ar",
      "human",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos",
    "authors": [
      "Wenkang Zhang",
      "Yan Zhao",
      "Qiang Wang",
      "Zhixin Xu",
      "Li Song",
      "Zhengxue Cheng"
    ],
    "abstract": "Free-Viewpoint Video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representation remains a major challenge. Existing dynamic 3D Gaussian Splatting methods couple reconstruction with optimization-dependent compression and customized motion formats, limiting generalization and standardization. To address this, we propose D-FCGS, a novel Feedforward Compression framework for Dynamic Gaussian Splatting. Key innovations include: (1) a standardized Group-of-Frames (GoF) structure with I-P coding, leveraging sparse control points to extract inter-frame motion tensors; (2) a dual prior-aware entropy model that fuses hyperprior and spatial-temporal priors for accurate rate estimation; (3) a control-point-guided motion compensation mechanism and refinement network to enhance view-consistent fidelity. Trained on Gaussian frames derived from multi-view videos, D-FCGS generalizes across diverse scenes in a zero-shot fashion. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 17 times compression compared to the baseline while preserving visual quality across viewpoints. This work advances feedforward compression of dynamic 3DGS, facilitating scalable FVV transmission and storage for immersive applications.",
    "arxiv_url": "https://arxiv.org/abs/2507.05859v4",
    "pdf_url": "https://arxiv.org/pdf/2507.05859v4",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamArt: Generating Interactable Articulated Objects from a Single Image",
    "authors": [
      "Ruijie Lu",
      "Yu Liu",
      "Jiaxiang Tang",
      "Junfeng Ni",
      "Yuxiang Wang",
      "Diwen Wan",
      "Gang Zeng",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "abstract": "Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.",
    "arxiv_url": "https://arxiv.org/abs/2507.05763v1",
    "pdf_url": "https://arxiv.org/pdf/2507.05763v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "vr",
      "ar",
      "geometry",
      "gaussian splatting",
      "motion",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting",
    "authors": [
      "Haitao Lu",
      "Haijier Chen",
      "Haoze Liu",
      "Shoujian Zhang",
      "Bo Xu",
      "Ziao Liu"
    ],
    "abstract": "In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails.",
    "arxiv_url": "https://arxiv.org/abs/2507.05661v1",
    "pdf_url": "https://arxiv.org/pdf/2507.05661v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields",
    "authors": [
      "Rushil Desai"
    ],
    "abstract": "Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.",
    "arxiv_url": "https://arxiv.org/abs/2507.06269v3",
    "pdf_url": "https://arxiv.org/pdf/2507.06269v3",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors",
    "authors": [
      "Lanqing Guo",
      "Yufei Wang",
      "Hezhen Hu",
      "Yan Zheng",
      "Yeying Jin",
      "Siyu Huang",
      "Zhangyang Wang"
    ],
    "abstract": "Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.",
    "arxiv_url": "https://arxiv.org/abs/2507.05426v1",
    "pdf_url": "https://arxiv.org/pdf/2507.05426v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation",
    "authors": [
      "Jiahao Zhu",
      "Zixuan Chen",
      "Guangcong Wang",
      "Xiaohua Xie",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).",
    "arxiv_url": "https://arxiv.org/abs/2507.05256v2",
    "pdf_url": "https://arxiv.org/pdf/2507.05256v2",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior",
    "authors": [
      "Minghao Wen",
      "Shengjie Wu",
      "Kangkan Wang",
      "Dong Liang"
    ],
    "abstract": "3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a \"one-shot deal\", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.",
    "arxiv_url": "https://arxiv.org/abs/2507.04961v1",
    "pdf_url": "https://arxiv.org/pdf/2507.04961v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality",
    "authors": [
      "Shuo Xin",
      "Haiyu Wang",
      "Sai Qian Zhang"
    ],
    "abstract": "Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\\times$ while maintaining visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2507.04147v1",
    "pdf_url": "https://arxiv.org/pdf/2507.04147v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "human",
      "dynamic",
      "neural rendering",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM",
    "authors": [
      "Xiaolei Lang",
      "Jiajun Lv",
      "Kai Tang",
      "Laijian Li",
      "Jianxin Huang",
      "Lina Liu",
      "Yong Liu",
      "Xingxing Zuo"
    ],
    "abstract": "This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis. Both the dataset and code will be made publicly available on project page https://xingxingzuo.github.io/gaussian_lic2.",
    "arxiv_url": "https://arxiv.org/abs/2507.04004v2",
    "pdf_url": "https://arxiv.org/pdf/2507.04004v2",
    "published_date": "2025-07-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments",
    "authors": [
      "Guile Wu",
      "Dongfeng Bai",
      "Bingbing Liu"
    ],
    "abstract": "This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2507.03886v1",
    "pdf_url": "https://arxiv.org/pdf/2507.03886v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "urban scene",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps",
    "authors": [
      "Chong Cheng",
      "Sicheng Yu",
      "Zijian Wang",
      "Yifan Zhou",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, lack geometric priors in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to scale drift. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.",
    "arxiv_url": "https://arxiv.org/abs/2507.03737v2",
    "pdf_url": "https://arxiv.org/pdf/2507.03737v2",
    "published_date": "2025-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "mapping",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. BÃ¼hler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "arxiv_url": "https://arxiv.org/abs/2507.02803v2",
    "pdf_url": "https://arxiv.org/pdf/2507.02803v2",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "fast",
      "avatar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects",
    "authors": [
      "Qiaojun Yu",
      "Xibin Yuan",
      "Yu jiang",
      "Junting Chen",
      "Dongzhe Zheng",
      "Ce Hao",
      "Yang You",
      "Yixing Chen",
      "Yao Mu",
      "Liu Liu",
      "Cewu Lu"
    ],
    "abstract": "Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: https://sites.google.com/view/artgs/home",
    "arxiv_url": "https://arxiv.org/abs/2507.02600v1",
    "pdf_url": "https://arxiv.org/pdf/2507.02600v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Jianbo Jiao",
      "Jiayu Yang",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jie Liang",
      "Jinbo Yan",
      "Runling Liu",
      "Ronggang Wang"
    ],
    "abstract": "Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
    "arxiv_url": "https://arxiv.org/abs/2507.02363v1",
    "pdf_url": "https://arxiv.org/pdf/2507.02363v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes",
    "authors": [
      "Stephen Pasch",
      "Joel K. Salzman",
      "Changxi Zheng"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "arxiv_url": "https://arxiv.org/abs/2507.02257v1",
    "pdf_url": "https://arxiv.org/pdf/2507.02257v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation",
    "authors": [
      "Tianrui Lou",
      "Xiaojun Jia",
      "Siyuan Liang",
      "Jiawei Liang",
      "Ming Zhang",
      "Yanjun Xiao",
      "Xiaochun Cao"
    ],
    "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.",
    "arxiv_url": "https://arxiv.org/abs/2507.01367v2",
    "pdf_url": "https://arxiv.org/pdf/2507.01367v2",
    "published_date": "2025-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting",
    "authors": [
      "Keiko Nagami",
      "Timothy Chen",
      "Javier Yu",
      "Ola Shorinwa",
      "Maximilian Adang",
      "Carlyn Dougherty",
      "Eric Cristofalo",
      "Mac Schwager"
    ],
    "abstract": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "arxiv_url": "https://arxiv.org/abs/2507.01125v1",
    "pdf_url": "https://arxiv.org/pdf/2507.01125v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
    "authors": [
      "Felix Windisch",
      "Thomas KÃ¶hler",
      "Lukas Radl",
      "Michael Steiner",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
    "arxiv_url": "https://arxiv.org/abs/2507.01110v3",
    "pdf_url": "https://arxiv.org/pdf/2507.01110v3",
    "published_date": "2025-07-01",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Masks make discriminative models great again!",
    "authors": [
      "Tianshi Cao",
      "Marie-Julie Rakotosaona",
      "Ben Poole",
      "Federico Tombari",
      "Michael Niemeyer"
    ],
    "abstract": "We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.",
    "arxiv_url": "https://arxiv.org/abs/2507.00916v1",
    "pdf_url": "https://arxiv.org/pdf/2507.00916v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
    "authors": [
      "Anna-Maria Halacheva",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
    "arxiv_url": "https://arxiv.org/abs/2507.00886v1",
    "pdf_url": "https://arxiv.org/pdf/2507.00886v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "compact",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing",
    "authors": [
      "Zhenya Yang",
      "Bingchen Gong",
      "Kai Chen"
    ],
    "abstract": "Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.",
    "arxiv_url": "https://arxiv.org/abs/2507.00554v3",
    "pdf_url": "https://arxiv.org/pdf/2507.00554v3",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control",
    "authors": [
      "Xingjun Wang",
      "Lianlei Shan"
    ],
    "abstract": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.",
    "arxiv_url": "https://arxiv.org/abs/2507.00363v1",
    "pdf_url": "https://arxiv.org/pdf/2507.00363v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "real-time rendering",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction",
    "authors": [
      "Antoine GuÃ©don",
      "Diego Gomez",
      "Nissim Maruani",
      "Bingchen Gong",
      "George Drettakis",
      "Maks Ovsjanikov"
    ],
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "arxiv_url": "https://arxiv.org/abs/2506.24096v2",
    "pdf_url": "https://arxiv.org/pdf/2506.24096v2",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering",
    "authors": [
      "Zinuo You",
      "Stamatios Georgoulis",
      "Anpei Chen",
      "Siyu Tang",
      "Dengxin Dai"
    ],
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.",
    "arxiv_url": "https://arxiv.org/abs/2506.23957v2",
    "pdf_url": "https://arxiv.org/pdf/2506.23957v2",
    "published_date": "2025-06-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention",
    "authors": [
      "Ziao Liu",
      "Zhenjia Li",
      "Yifeng Shi",
      "Xiangang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2506.23611v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23611v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "face",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "efficient rendering",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Chao Yang",
      "Tao Guan",
      "Lili Ju"
    ],
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "arxiv_url": "https://arxiv.org/abs/2506.23479v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23479v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Kun Yuan",
      "Guankun Wang",
      "Mobarak I. Hoque",
      "Nicolas Padoy",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.",
    "arxiv_url": "https://arxiv.org/abs/2506.23309v2",
    "pdf_url": "https://arxiv.org/pdf/2506.23309v2",
    "published_date": "2025-06-29",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "deformation",
      "ar",
      "lighting",
      "gaussian splatting",
      "tracking",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Yanheng Li",
      "Tong Chen",
      "Jie Wang",
      "Jinlin Wu",
      "Zhen Lei",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "arxiv_url": "https://arxiv.org/abs/2506.23308v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23308v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "illumination",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Lei Feng",
      "Yangbing Ge",
      "Shuaifeng Zhi",
      "Jiaxiong Liu",
      "Dewen Hu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
    "arxiv_url": "https://arxiv.org/abs/2506.23207v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23207v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "illumination",
      "geometry",
      "mapping",
      "dynamic",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene",
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ],
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
    "arxiv_url": "https://arxiv.org/abs/2506.23157v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23157v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "An Le",
      "Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.",
    "arxiv_url": "https://arxiv.org/abs/2506.23042v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23042v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Elaheh Badali Golezani",
      "Shohreh Kasaei"
    ],
    "abstract": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting",
    "arxiv_url": "https://arxiv.org/abs/2506.22973v1",
    "pdf_url": "https://arxiv.org/pdf/2506.22973v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "compression",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors",
    "authors": [
      "Sicong Du",
      "Jiarun Liu",
      "Qifeng Chen",
      "Hao-Xiang Chen",
      "Tai-Jiang Mu",
      "Sheng Yang"
    ],
    "abstract": "A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS.",
    "arxiv_url": "https://arxiv.org/abs/2506.22800v3",
    "pdf_url": "https://arxiv.org/pdf/2506.22800v3",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
    "authors": [
      "Minchao Jiang",
      "Shunyu Jia",
      "Jiaming Gu",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Anqi Dong",
      "Liang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/",
    "arxiv_url": "https://arxiv.org/abs/2506.22799v1",
    "pdf_url": "https://arxiv.org/pdf/2506.22799v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "real-time rendering",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "authors": [
      "Tao Tang",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "xia zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "abstract": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.",
    "arxiv_url": "https://arxiv.org/abs/2506.22756v1",
    "pdf_url": "https://arxiv.org/pdf/2506.22756v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model",
    "authors": [
      "Yuliang Huang",
      "Imraj Singh",
      "Thomas Joyce",
      "Kris Thielemans",
      "Jamie R. McClelland"
    ],
    "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.",
    "arxiv_url": "https://arxiv.org/abs/2506.22280v1",
    "pdf_url": "https://arxiv.org/pdf/2506.22280v1",
    "published_date": "2025-06-27",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BÃ©zierGS: Dynamic Urban Scene Reconstruction with BÃ©zier Curve Gaussian Splatting",
    "authors": [
      "Zipei Ma",
      "Junzhe Jiang",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose BÃ©zier curve Gaussian splatting (BÃ©zierGS), which represents the motion trajectories of dynamic objects using learnable BÃ©zier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that BÃ©zierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2506.22099v3",
    "pdf_url": "https://arxiv.org/pdf/2506.22099v3",
    "published_date": "2025-06-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "urban scene",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MADrive: Memory-Augmented Driving Scene Modeling",
    "authors": [
      "Polina Karpikova",
      "Daniil Selikhanovych",
      "Kirill Struminsky",
      "Ruslan Musaev",
      "Maria Golitsyna",
      "Dmitry Baranchuk"
    ],
    "abstract": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360Â° car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/",
    "arxiv_url": "https://arxiv.org/abs/2506.21520v2",
    "pdf_url": "https://arxiv.org/pdf/2506.21520v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "relighting",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Zhuoxiao Li",
      "Haocheng Zhao",
      "Kang Dang",
      "Jionglong Su",
      "Limin Yu",
      "Haoang Li"
    ],
    "abstract": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes.",
    "arxiv_url": "https://arxiv.org/abs/2506.21420v2",
    "pdf_url": "https://arxiv.org/pdf/2506.21420v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "slam",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image",
    "authors": [
      "Pufan Li",
      "Bi'an Du",
      "Wei Hu"
    ],
    "abstract": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To tackle these issues, we present a novel method that seamlessly integrates geometry and perception information without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we incorporate geometry and perception priors to initialize the Gaussian branches and guide their parameter optimization. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we introduce a stable Score Distillation Sampling for fine-grained prior distillation to ensure effective knowledge transfer. The model is further enhanced by a reprojection-based strategy that enforces depth consistency. Experimental results show that we outperform existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.",
    "arxiv_url": "https://arxiv.org/abs/2506.21152v3",
    "pdf_url": "https://arxiv.org/pdf/2506.21152v3",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization",
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ],
    "abstract": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "arxiv_url": "https://arxiv.org/abs/2506.21117v2",
    "pdf_url": "https://arxiv.org/pdf/2506.21117v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting",
      "robotics",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "User-in-the-Loop View Sampling with Error Peaking Visualization",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Shohei Mori"
    ],
    "abstract": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.",
    "arxiv_url": "https://arxiv.org/abs/2506.21009v1",
    "pdf_url": "https://arxiv.org/pdf/2506.21009v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting",
    "authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Byung-Ju Kim",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.",
    "arxiv_url": "https://arxiv.org/abs/2506.20998v1",
    "pdf_url": "https://arxiv.org/pdf/2506.20998v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGH: 3D Head Generation with Composable Hair and Face",
    "authors": [
      "Chengan He",
      "Junxuan Li",
      "Tobias Kirschstein",
      "Artem Sevastopolsky",
      "Shunsuke Saito",
      "Qingyang Tan",
      "Javier Romero",
      "Chen Cao",
      "Holly Rushmeier",
      "Giljoo Nam"
    ],
    "abstract": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.",
    "arxiv_url": "https://arxiv.org/abs/2506.20875v1",
    "pdf_url": "https://arxiv.org/pdf/2506.20875v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer",
    "authors": [
      "Da Li",
      "Donggang Jia",
      "Yousef Rajeh",
      "Dominik Engel",
      "Ivan Viola"
    ],
    "abstract": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.",
    "arxiv_url": "https://arxiv.org/abs/2506.20202v1",
    "pdf_url": "https://arxiv.org/pdf/2506.20202v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "high-fidelity",
      "ar",
      "real-time rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAR-GS: Gaussian Splatting based SAR Images Rendering and Target Reconstruction",
    "authors": [
      "Aobo Li",
      "Zhengxin Lei",
      "Jiangtao Wei",
      "Feng Xu"
    ],
    "abstract": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.",
    "arxiv_url": "https://arxiv.org/abs/2506.21633v2",
    "pdf_url": "https://arxiv.org/pdf/2506.21633v2",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes",
    "authors": [
      "Chenhao Zhang",
      "Yezhi Shen",
      "Fengqing Zhu"
    ],
    "abstract": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2506.21629v1",
    "pdf_url": "https://arxiv.org/pdf/2506.21629v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "nerf",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "arxiv_url": "https://arxiv.org/abs/2506.19842v1",
    "pdf_url": "https://arxiv.org/pdf/2506.19842v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "body",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtual Memory for 3D Gaussian Splatting",
    "authors": [
      "Jonathan Haberl",
      "Philipp Fleck",
      "Clemens Arth"
    ],
    "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
    "arxiv_url": "https://arxiv.org/abs/2506.19415v1",
    "pdf_url": "https://arxiv.org/pdf/2506.19415v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "authors": [
      "Xiaoyuan Wang",
      "Yizhou Zhao",
      "Botao Ye",
      "Xiaojun Shan",
      "Weijie Lyu",
      "Lu Qi",
      "Kelvin C. K. Chan",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2506.19291v1",
    "pdf_url": "https://arxiv.org/pdf/2506.19291v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "abstract": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.",
    "arxiv_url": "https://arxiv.org/abs/2506.18885v1",
    "pdf_url": "https://arxiv.org/pdf/2506.18885v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "authors": [
      "Michal Nazarczuk",
      "Sibi Catley-Chandar",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Gregory Slabaugh",
      "Eduardo PÃ©rez-Pellitero"
    ],
    "abstract": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io",
    "arxiv_url": "https://arxiv.org/abs/2506.18792v1",
    "pdf_url": "https://arxiv.org/pdf/2506.18792v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
    "authors": [
      "Dylan Ebert"
    ],
    "abstract": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons.   Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource.   Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.",
    "arxiv_url": "https://arxiv.org/abs/2506.18787v1",
    "pdf_url": "https://arxiv.org/pdf/2506.18787v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
    "authors": [
      "Adam Yang",
      "Nadula Kadawedduwa",
      "Tianfu Wang",
      "Sunny Sharma",
      "Emily F. Wisinski",
      "Jhayron S. PÃ©rez-Carrasquilla",
      "Kyle J. C. Hall",
      "Dean Calhoun",
      "Jonathan Starfeldt",
      "Timothy P. Canty",
      "Maria Molina",
      "Christopher Metzler"
    ],
    "abstract": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2506.18677v2",
    "pdf_url": "https://arxiv.org/pdf/2506.18677v2",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene",
    "authors": [
      "Shihan Chen",
      "Zhaojin Li",
      "Zeyu Chen",
      "Qingsong Yan",
      "Gaoyang Shen",
      "Ran Duan"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.",
    "arxiv_url": "https://arxiv.org/abs/2506.17636v1",
    "pdf_url": "https://arxiv.org/pdf/2506.17636v1",
    "published_date": "2025-06-21",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "outdoor",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision",
    "authors": [
      "Weeyoung Kwon",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.",
    "arxiv_url": "https://arxiv.org/abs/2506.16262v2",
    "pdf_url": "https://arxiv.org/pdf/2506.16262v2",
    "published_date": "2025-06-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "high-fidelity",
      "nerf",
      "vr",
      "ar",
      "autonomous driving",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Information-computation trade-offs in non-linear transforms",
    "authors": [
      "Connor Ding",
      "Abhiram Rao Gorle",
      "Jiwon Jeong",
      "Naomi Sagan",
      "Tsachy Weissman"
    ],
    "abstract": "In this work, we explore the interplay between information and computation in non-linear transform-based compression for broad classes of modern information-processing tasks. We first investigate two emerging nonlinear data transformation frameworks for image compression: Implicit Neural Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their representational properties, behavior under lossy compression, and convergence dynamics. Our results highlight key trade-offs between INR's compact, resolution-flexible neural field representations and GS's highly parallelizable, spatially interpretable fitting, providing insights for future hybrid and compression-aware frameworks. Next, we introduce the textual transform that enables efficient compression at ultra-low bitrate regimes and simultaneously enhances human perceptual satisfaction. When combined with the concept of denoising via lossy compression, the textual transform becomes a powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78) \"transform\", a universal method that, when applied to any member of a broad compressor family, produces new compressors that retain the asymptotic universality guarantees of the LZ78 algorithm. Collectively, these three transforms illuminate the fundamental trade-offs between coding efficiency and computational cost. We discuss how these insights extend beyond compression to tasks such as classification, denoising, and generative AI, suggesting new pathways for using non-linear transformations to balance resource constraints and performance.",
    "arxiv_url": "https://arxiv.org/abs/2506.15948v1",
    "pdf_url": "https://arxiv.org/pdf/2506.15948v1",
    "published_date": "2025-06-19",
    "categories": [
      "cs.IT",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "human",
      "compression",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos",
    "authors": [
      "Kaifeng Zhang",
      "Baoyu Li",
      "Kris Hauser",
      "Yunzhu Li"
    ],
    "abstract": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .",
    "arxiv_url": "https://arxiv.org/abs/2506.15680v2",
    "pdf_url": "https://arxiv.org/pdf/2506.15680v2",
    "published_date": "2025-06-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories",
    "authors": [
      "Qingsong Yan",
      "Qiang Wang",
      "Kaiyong Zhao",
      "Jie Chen",
      "Bo Li",
      "Xiaowen Chu",
      "Fei Deng"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks\\&Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories.",
    "arxiv_url": "https://arxiv.org/abs/2506.15242v2",
    "pdf_url": "https://arxiv.org/pdf/2506.15242v2",
    "published_date": "2025-06-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting",
    "authors": [
      "Ziqiao Peng",
      "Wentao Hu",
      "Junyuan Ma",
      "Xiangyu Zhu",
      "Xiaomei Zhang",
      "Hao Zhao",
      "Hui Tian",
      "Jun He",
      "Hongyan Liu",
      "Zhaoxin Fan"
    ],
    "abstract": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.",
    "arxiv_url": "https://arxiv.org/abs/2506.14742v1",
    "pdf_url": "https://arxiv.org/pdf/2506.14742v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "dynamic",
      "gaussian splatting",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting",
    "authors": [
      "Yuke Xing",
      "Jiarui Wang",
      "Peizhi Niu",
      "Wenjie Huang",
      "Guangtao Zhai",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.",
    "arxiv_url": "https://arxiv.org/abs/2506.14642v2",
    "pdf_url": "https://arxiv.org/pdf/2506.14642v2",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "compression",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction",
    "authors": [
      "Zhengquan Zhang",
      "Feng Xu",
      "Mengmi Zhang"
    ],
    "abstract": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.",
    "arxiv_url": "https://arxiv.org/abs/2506.14856v1",
    "pdf_url": "https://arxiv.org/pdf/2506.14856v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "neural rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Juan Zhang",
      "Tongfei Chen",
      "Shuo Yang",
      "Shuwei Shao",
      "Wenhao Dong",
      "Baochang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
    "arxiv_url": "https://arxiv.org/abs/2506.14229v1",
    "pdf_url": "https://arxiv.org/pdf/2506.14229v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAF: Gaussian Action Field as a 4D Representation for Dynamic World Modeling in Robotic Manipulation",
    "authors": [
      "Ying Chai",
      "Litao Deng",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Kangchen Lv",
      "Liangjun Xing",
      "Xiang Li",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Accurate scene perception is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we adopt a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing 4D modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF provides three interrelated outputs: reconstruction of the current scene, prediction of future frames, and estimation of init action via Gaussian motion. Furthermore, we employ an action-vision-aligned denoising framework, conditioned on a unified representation that combines the init action and the Gaussian perception, both generated by the GAF, to further obtain more precise actions. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR, +0.3864 SSIM and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average +7.3% success rate in robotic manipulation tasks over state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.14135v4",
    "pdf_url": "https://arxiv.org/pdf/2506.14135v4",
    "published_date": "2025-06-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics",
    "authors": [
      "Qianzhong Chen",
      "Naixiang Gao",
      "Suning Huang",
      "JunEn Low",
      "Timothy Chen",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.",
    "arxiv_url": "https://arxiv.org/abs/2506.14009v1",
    "pdf_url": "https://arxiv.org/pdf/2506.14009v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images",
    "authors": [
      "Lingteng Qiu",
      "Peihao Li",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Weihao Yuan",
      "Siyu Zhu",
      "Xiaoguang Han",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.",
    "arxiv_url": "https://arxiv.org/abs/2506.13766v1",
    "pdf_url": "https://arxiv.org/pdf/2506.13766v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "avatar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.",
    "arxiv_url": "https://arxiv.org/abs/2506.13516v1",
    "pdf_url": "https://arxiv.org/pdf/2506.13516v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "illumination",
      "gaussian splatting",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate Radiance Fields",
    "authors": [
      "Jungeon Kim",
      "Geonsoo Park",
      "Seungyong Lee"
    ],
    "abstract": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.",
    "arxiv_url": "https://arxiv.org/abs/2506.13508v1",
    "pdf_url": "https://arxiv.org/pdf/2506.13508v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas. Code will be available at https://github.com/maeyounes/TextureSplat",
    "arxiv_url": "https://arxiv.org/abs/2506.13348v2",
    "pdf_url": "https://arxiv.org/pdf/2506.13348v2",
    "published_date": "2025-06-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction",
    "authors": [
      "Jinguang Tong",
      "Xuesong li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ],
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS",
    "arxiv_url": "https://arxiv.org/abs/2506.13110v1",
    "pdf_url": "https://arxiv.org/pdf/2506.13110v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "relighting",
      "real-time rendering",
      "fast",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "authors": [
      "Hyunjin Kim",
      "Haebeom Jung",
      "Jaesik Park"
    ],
    "abstract": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending, show that our approach reduces the number of Gaussians needed, achieving faster convergence while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.",
    "arxiv_url": "https://arxiv.org/abs/2506.12945v2",
    "pdf_url": "https://arxiv.org/pdf/2506.12945v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ],
    "abstract": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "arxiv_url": "https://arxiv.org/abs/2506.12787v2",
    "pdf_url": "https://arxiv.org/pdf/2506.12787v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "deformation",
      "nerf",
      "ar",
      "fast",
      "localization",
      "lightweight",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient multi-view training for 3D Gaussian Splatting",
    "authors": [
      "Minhyuk Choi",
      "Injae Kim",
      "Hyunwoo J. Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize \"single-view\" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's \"multi-view\" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.",
    "arxiv_url": "https://arxiv.org/abs/2506.12727v2",
    "pdf_url": "https://arxiv.org/pdf/2506.12727v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors",
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ],
    "abstract": "We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2506.12716v1",
    "pdf_url": "https://arxiv.org/pdf/2506.12716v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting",
    "authors": [
      "Hongbi Zhou",
      "Zhangkai Ni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS",
    "arxiv_url": "https://arxiv.org/abs/2506.12400v2",
    "pdf_url": "https://arxiv.org/pdf/2506.12400v2",
    "published_date": "2025-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure",
    "authors": [
      "Stanley Lewis",
      "Vishal Chandra",
      "Tom Gao",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Representing articulated objects remains a difficult problem within the field of robotics. Objects such as pliers, clamps, or cabinets require representations that capture not only geometry and color information, but also part seperation, connectivity, and joint parametrization. Furthermore, learning these representations becomes even more difficult with each additional degree of freedom. Complex articulated objects such as robot arms may have seven or more degrees of freedom, and the depth of their kinematic tree may be notably greater than the tools, drawers, and cabinets that are the typical subjects of articulated object research. To address these concerns, we introduce SPLATART - a pipeline for learning Gaussian splat representations of articulated objects from posed images, of which a subset contains image space part segmentations. SPLATART disentangles the part separation task from the articulation estimation task, allowing for post-facto determination of joint estimation and representation of articulated objects with deeper kinematic trees than previously exhibited. In this work, we present data on the SPLATART pipeline as applied to the syntheic Paris dataset objects, and qualitative results on a real-world object under spare segmentation supervision. We additionally present on articulated serial chain manipulators to demonstrate usage on deeper kinematic tree structures.",
    "arxiv_url": "https://arxiv.org/abs/2506.12184v1",
    "pdf_url": "https://arxiv.org/pdf/2506.12184v1",
    "published_date": "2025-06-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "robotics",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction",
    "authors": [
      "Ke Song",
      "Yunhe Wu",
      "Chunchit Siu",
      "Huiyuan Xiong"
    ],
    "abstract": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.",
    "arxiv_url": "https://arxiv.org/abs/2506.14825v2",
    "pdf_url": "https://arxiv.org/pdf/2506.14825v2",
    "published_date": "2025-06-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Anti-Aliased 2D Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an anti-aliased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world-space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object-space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.",
    "arxiv_url": "https://arxiv.org/abs/2506.11252v2",
    "pdf_url": "https://arxiv.org/pdf/2506.11252v2",
    "published_date": "2025-06-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "mapping",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting",
    "authors": [
      "Lintao Xiang",
      "Hongpei Zheng",
      "Yating Huang",
      "Qijun Yang",
      "Hujun Yin"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.10335v1",
    "pdf_url": "https://arxiv.org/pdf/2506.10335v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "few-shot",
      "ar",
      "sparse view",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
    "authors": [
      "Chieh Hubert Lin",
      "Zhaoyang Lv",
      "Songyin Wu",
      "Zhen Xu",
      "Thu Nguyen-Phuoc",
      "Hung-Yu Tseng",
      "Julian Straub",
      "Numair Khan",
      "Lei Xiao",
      "Ming-Hsuan Yang",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong",
      "Zhengqin Li"
    ],
    "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.09997v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09997v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "3d gaussian",
      "motion",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "arxiv_url": "https://arxiv.org/abs/2506.09952v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09952v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction",
    "authors": [
      "Junli Deng",
      "Ping Shi",
      "Qipei Li",
      "Jinyang Guo"
    ],
    "abstract": "Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2506.09836v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09836v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
    "authors": [
      "Tao Wang",
      "Mengyu Li",
      "Geduo Zeng",
      "Cheng Meng",
      "Qiong Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering. The code is publicly available at https://github.com/DrunkenPoet/GHAP",
    "arxiv_url": "https://arxiv.org/abs/2506.09534v2",
    "pdf_url": "https://arxiv.org/pdf/2506.09534v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "geometry",
      "neural rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene",
    "authors": [
      "Jianing Chen",
      "Zehao Li",
      "Yujun Cai",
      "Hao Jiang",
      "Chengxuan Qian",
      "Juyuan Kang",
      "Shuqin Gao",
      "Honglong Zhao",
      "Tianlu Mao",
      "Yucheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppress redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2506.09518v2",
    "pdf_url": "https://arxiv.org/pdf/2506.09518v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation",
    "authors": [
      "Zetian Song",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Xiaohan Lu",
      "Chuanmin Jia",
      "Siwei Ma",
      "Wen Gao"
    ],
    "abstract": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.",
    "arxiv_url": "https://arxiv.org/abs/2506.09479v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09479v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "authors": [
      "Yunxiao Shi",
      "Yinhao Zhu",
      "Shizhong Han",
      "Jisoo Jeong",
      "Amin Ansari",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "abstract": "Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.",
    "arxiv_url": "https://arxiv.org/abs/2506.09417v2",
    "pdf_url": "https://arxiv.org/pdf/2506.09417v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Jingyu Gong",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2506.09378v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09378v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams",
    "authors": [
      "Zike Wu",
      "Qi Yan",
      "Xuanyu Yi",
      "Lele Wang",
      "Renjie Liao"
    ],
    "abstract": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.",
    "arxiv_url": "https://arxiv.org/abs/2506.08862v1",
    "pdf_url": "https://arxiv.org/pdf/2506.08862v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting",
    "authors": [
      "Keyi Liu",
      "Weidong Yang",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.08777v2",
    "pdf_url": "https://arxiv.org/pdf/2506.08777v2",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting",
    "authors": [
      "Mengjiao Ma",
      "Qi Ma",
      "Yue Li",
      "Jiahuan Cheng",
      "Runyi Yang",
      "Bin Ren",
      "Nikola Popovic",
      "Mingqiang Wei",
      "Nicu Sebe",
      "Luc Van Gool",
      "Theo Gevers",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are released at https://scenesplatpp.gaussianworld.ai/.",
    "arxiv_url": "https://arxiv.org/abs/2506.08710v3",
    "pdf_url": "https://arxiv.org/pdf/2506.08710v3",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "outdoor",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering",
    "authors": [
      "Xiaohan Zhang",
      "Sitong Wang",
      "Yushen Yan",
      "Yi Yang",
      "Mingda Xu",
      "Qi Liu"
    ],
    "abstract": "High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.",
    "arxiv_url": "https://arxiv.org/abs/2506.08704v1",
    "pdf_url": "https://arxiv.org/pdf/2506.08704v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex-Valued Holographic Radiance Fields",
    "authors": [
      "Yicheng Zhan",
      "Dong-Ha Shin",
      "Seung-Hwan Baek",
      "Kaan AkÅit"
    ],
    "abstract": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.",
    "arxiv_url": "https://arxiv.org/abs/2506.08350v1",
    "pdf_url": "https://arxiv.org/pdf/2506.08350v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.ET"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpeeDe3DGS: Speedy Deformable 3D Gaussian Splatting with Temporal Pruning and Motion Grouping",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "abstract": "Dynamic extensions of 3D Gaussian Splatting (3DGS) achieve high-quality reconstructions through neural motion fields, but per-Gaussian neural inference makes these models computationally expensive. Building on DeformableGS, we introduce Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), which bridges this efficiency-fidelity gap through three complementary modules: Temporal Sensitivity Pruning (TSP) removes low-impact Gaussians via temporally aggregated sensitivity analysis, Temporal Sensitivity Sampling (TSS) perturbs timestamps to suppress floaters and improve temporal coherence, and GroupFlow distills the learned deformation field into shared SE(3) transformations for efficient groupwise motion. On the 50 dynamic scenes in MonoDyGauBench, integrating TSP and TSS into DeformableGS accelerates rendering by 6.78$\\times$ on average while maintaining neural-field fidelity and using 10$\\times$ fewer primitives. Adding GroupFlow culminates in 13.71$\\times$ faster rendering and 2.53$\\times$ shorter training, surpassing all baselines in speed while preserving superior image quality.",
    "arxiv_url": "https://arxiv.org/abs/2506.07917v3",
    "pdf_url": "https://arxiv.org/pdf/2506.07917v3",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution",
    "authors": [
      "Shuja Khalid",
      "Mohamed Ibrahim",
      "Yang Liu"
    ],
    "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
    "arxiv_url": "https://arxiv.org/abs/2506.07897v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07897v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "dynamic",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation",
    "authors": [
      "William Ljungbergh",
      "Bernardo Taveira",
      "Wenzhao Zheng",
      "Adam Tonderski",
      "Chensheng Peng",
      "Fredrik Kahl",
      "Christoffer Petersson",
      "Michael Felsberg",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.",
    "arxiv_url": "https://arxiv.org/abs/2506.07826v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07826v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "ar",
      "dynamic",
      "neural rendering",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "lightweight",
      "autonomous driving",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting",
    "authors": [
      "Jens Piekenbrinck",
      "Christian Schmidt",
      "Alexander Hermans",
      "Narunas Vaskevicius",
      "Timm Linder",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2506.07697v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07697v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views",
    "authors": [
      "Xiaohan Lu",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Zetian Song",
      "Chuanmin Jia",
      "Siwei Ma"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.07670v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07670v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians",
    "authors": [
      "Zeyu Xiao",
      "Zhenyi Wu",
      "Mingyang Sun",
      "Qipeng Yan",
      "Yufan Guo",
      "Zhuoer Liang",
      "Lihua Zhang"
    ],
    "abstract": "3D Gaussian Splatting has achieved remarkable success in reconstructing both static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian primitives, interactions between objects suffer from inaccurate 3D segmentation, imprecise deformation among different materials, and severe rendering artifacts. To address these challenges, we introduce PIG: Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach that combines 3D object segmentation with the simulation of interacting objects in high precision. Firstly, our method facilitates fast and accurate mapping from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation. Secondly, we assign unique physical properties to correspondingly segmented objects within the scene for multi-material coupled interactions. Finally, we have successfully embedded constraint scales into deformation gradients, specifically clamping the scaling and rotation properties of the Gaussian primitives to eliminate artifacts and achieve geometric fidelity and visual consistency. Experimental results demonstrate that our method not only outperforms the state-of-the-art (SOTA) in terms of visual quality, but also opens up new directions and pipelines for the field of physically realistic scene generation.",
    "arxiv_url": "https://arxiv.org/abs/2506.07657v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07657v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support",
    "authors": [
      "Chenqi Zhang",
      "Yu Feng",
      "Jieru Zhao",
      "Guangda Liu",
      "Wenchao Ding",
      "Chentao Wu",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
    "arxiv_url": "https://arxiv.org/abs/2506.09070v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09070v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation",
    "authors": [
      "Yijie Deng",
      "Shuaihang Yuan",
      "Geeta Chandra Raju Bethala",
      "Anthony Tzes",
      "Yu-Shen Liu",
      "Yi Fang"
    ],
    "abstract": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.",
    "arxiv_url": "https://arxiv.org/abs/2506.07338v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07338v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization",
    "authors": [
      "Zhican Wang",
      "Guanghui He",
      "Dantong Liu",
      "Lingjun Gao",
      "Shell Xu Hu",
      "Chen Zhang",
      "Zhuoran Song",
      "Nicholas Lane",
      "Wayne Luk",
      "Hongxiang Fan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a $Ï$-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy savings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.",
    "arxiv_url": "https://arxiv.org/abs/2506.07069v1",
    "pdf_url": "https://arxiv.org/pdf/2506.07069v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "head",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiao Tang",
      "Jiajun Tang",
      "Jiaqi Lin",
      "Yuxin Cheng",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
    "arxiv_url": "https://arxiv.org/abs/2506.06988v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06988v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Mapping for Evolving Scenes",
    "authors": [
      "Vladimir Yugay",
      "Thies Kersten",
      "Luca Carlone",
      "Theo Gevers",
      "Martin R. Oswald",
      "Lukas Schmid"
    ],
    "abstract": "Mapping systems with novel view synthesis (NVS) capabilities, most notably 3D Gaussian Splatting (3DGS), are widely used in computer vision and across various applications, including augmented reality, robotics, and autonomous driving. However, many current approaches are limited to static scenes. While recent works have begun addressing short-term dynamics (motion within the camera's view), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene-adaptation mechanism that continuously updates 3DGS to reflect the latest changes. Since maintaining consistency remains challenging due to stale observations that disrupt the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We thoroughly evaluate Gaussian Mapping for Evolving Scenes (\\ours) on both synthetic and real-world datasets, achieving a 29.7\\% improvement in PSNR and a 3 times improvement in L1 depth error over the most competitive baseline.",
    "arxiv_url": "https://arxiv.org/abs/2506.06909v2",
    "pdf_url": "https://arxiv.org/pdf/2506.06909v2",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics",
      "mapping",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation",
    "authors": [
      "Sumit Sharma",
      "Gopi Raju Matta",
      "Kaushik Mitra"
    ],
    "abstract": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline.",
    "arxiv_url": "https://arxiv.org/abs/2506.06890v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06890v1",
    "published_date": "2025-06-07",
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "illumination",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles",
    "authors": [
      "Yangkai Lin",
      "Jiabao Lei",
      "Kui jia"
    ],
    "abstract": "In recent years, there has been a growing demand to stylize a given 3D scene to align with the artistic style of reference images for creative purposes. While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method for realistic 3D scene modeling, there remains a challenge in adapting it to stylize 3D GS to match with multiple styles through automatic local style transfer or manual designation, while maintaining memory efficiency for stylization training. In this paper, we introduce a novel 3D GS stylization solution termed Multi-StyleGS to tackle these challenges. In particular, we employ a bipartite matching mechanism to au tomatically identify correspondences between the style images and the local regions of the rendered images. To facilitate local style transfer, we introduce a novel semantic style loss function that employs a segmentation network to apply distinct styles to various objects of the scene and propose a local-global feature matching to enhance the multi-view consistency. Furthermore, this technique can achieve memory efficient training, more texture details and better color match. To better assign a robust semantic label to each Gaussian, we propose several techniques to regularize the segmentation network. As demonstrated by our comprehensive experiments, our approach outperforms existing ones in producing plausible stylization results and offering flexible editing.",
    "arxiv_url": "https://arxiv.org/abs/2506.06846v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06846v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
    "arxiv_url": "https://arxiv.org/abs/2506.06822v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06822v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "localization",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling",
    "authors": [
      "Cheng Peng",
      "Jingxiang Sun",
      "Yushuo Chen",
      "Zhaoqi Su",
      "Zhuo Su",
      "Yebin Liu"
    ],
    "abstract": "Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.",
    "arxiv_url": "https://arxiv.org/abs/2506.06645v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06645v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "geometry",
      "fast",
      "avatar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "Marcus A. Brubaker",
      "David J. Fleet"
    ],
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.09063v1",
    "pdf_url": "https://arxiv.org/pdf/2506.09063v1",
    "published_date": "2025-06-06",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "gaussian splatting",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4: Generalizable Sparse Splatting Semantic SLAM",
    "authors": [
      "Mingqi Jiang",
      "Chanho Kim",
      "Chen Ziwen",
      "Li Fuxin"
    ],
    "abstract": "Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, when significant pose changes are detected, we perform only 1-5 iterations of joint Gaussian-pose optimization to correct drift, remove floaters, and further improve tracking accuracy. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.",
    "arxiv_url": "https://arxiv.org/abs/2506.06517v3",
    "pdf_url": "https://arxiv.org/pdf/2506.06517v3",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "fast",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements",
    "authors": [
      "NicolÃ¡s Violante",
      "Andreas Meuleman",
      "Alban Gauthier",
      "FrÃ©do Durand",
      "Thibault Groueix",
      "George Drettakis"
    ],
    "abstract": "We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2506.06462v1",
    "pdf_url": "https://arxiv.org/pdf/2506.06462v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments",
    "authors": [
      "Mingrui Li",
      "Yiming Zhou",
      "Hongxing Zhou",
      "Xinggang Hu",
      "Florian Roemer",
      "Hongyu Wang",
      "Ahmad Osman"
    ],
    "abstract": "Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.05965v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05965v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction",
    "authors": [
      "Yuchao Zheng",
      "Jianing Zhang",
      "Guochen Ning",
      "Hongen Liao"
    ],
    "abstract": "Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page https://surgsplat.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2506.05935v2",
    "pdf_url": "https://arxiv.org/pdf/2506.05935v2",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy",
    "authors": [
      "Yu Feng",
      "Weikai Lin",
      "Yuge Cheng",
      "Zihan Liu",
      "Jingwen Leng",
      "Minyi Guo",
      "Chen Chen",
      "Shixuan Sun",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural rendering, but it remains computationally demanding on today's mobile SoCs. To address this challenge, we propose Lumina, a hardware-algorithm co-designed system, which integrates two principal optimizations: a novel algorithm, S^2, and a radiance caching mechanism, RC, to improve the efficiency of neural rendering. S2 algorithm exploits temporal coherence in rendering to reduce the computational overhead, while RC leverages the color integration process of 3DGS to decrease the frequency of intensive rasterization computations. Coupled with these techniques, we propose an accelerator architecture, LuminCore, to further accelerate cache lookup and address the fundamental inefficiencies in Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB peak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
    "arxiv_url": "https://arxiv.org/abs/2506.05682v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05682v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction",
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ],
    "abstract": "Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancing performance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at https://zzy816.github.io/VoxelSplat-Demo/.",
    "arxiv_url": "https://arxiv.org/abs/2506.05563v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05563v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images",
    "authors": [
      "Andreas Meuleman",
      "Ishaan Shah",
      "Alexandre Lanvin",
      "Bernhard Kerbl",
      "George Drettakis"
    ],
    "abstract": "Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that our solution can provide on-the-fly processing of all the capture scenarios and scene sizes we target while remaining competitive with other methods that only handle specific capture styles or scene sizes in speed, image quality, or both.",
    "arxiv_url": "https://arxiv.org/abs/2506.05558v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05558v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "large scene",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting",
    "authors": [
      "Daniel Wang",
      "Patrick Rim",
      "Tian Tian",
      "Dong Lao",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "We introduce ODE-GS, a novel approach that integrates 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to enable future extrapolation of dynamic 3D scenes. Unlike existing dynamic scene reconstruction methods, which rely on time-conditioned deformation networks and are limited to interpolation within a fixed time window, ODE-GS eliminates timestamp dependency by modeling Gaussian parameter trajectories as continuous-time latent dynamics. Our approach first learns an interpolation model to generate accurate Gaussian trajectories within the observed window, then trains a Transformer encoder to aggregate past trajectories into a latent state evolved via a neural ODE. Finally, numerical integration produces smooth, physically plausible future Gaussian trajectories, enabling rendering at arbitrary future timestamps. On the D-NeRF, NVFi, and HyperNeRF benchmarks, ODE-GS achieves state-of-the-art extrapolation performance, improving metrics by 19.8% compared to leading baselines, demonstrating its ability to accurately represent and predict 3D scene dynamics.",
    "arxiv_url": "https://arxiv.org/abs/2506.05480v3",
    "pdf_url": "https://arxiv.org/pdf/2506.05480v3",
    "published_date": "2025-06-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Duochao Shi",
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Jia-Wang Bian",
      "Bohan Zhuang",
      "Chunhua Shen"
    ],
    "abstract": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss",
    "arxiv_url": "https://arxiv.org/abs/2506.05327v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05327v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting",
    "authors": [
      "Nan Wang",
      "Yuantao Chen",
      "Lixing Xiao",
      "Weiqing Xiao",
      "Bohan Li",
      "Zhaoxi Chen",
      "Chongjie Ye",
      "Shaocong Xu",
      "Saining Zhang",
      "Ziyang Yan",
      "Pierre Merriaux",
      "Lei Lei",
      "Tianfan Xue",
      "Hao Zhao"
    ],
    "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
    "arxiv_url": "https://arxiv.org/abs/2506.05280v3",
    "pdf_url": "https://arxiv.org/pdf/2506.05280v3",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "dynamic",
      "neural rendering",
      "gaussian splatting",
      "mapping",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D Gaussian Splatting for Vision Training",
    "authors": [
      "Aneesh Deogan",
      "Wout Beks",
      "Peter Teurlings",
      "Koen de Vos",
      "Mark van den Brand",
      "Rene van de Molengraft"
    ],
    "abstract": "Annotated datasets are critical for training neural networks for object detection, yet their manual creation is time- and labour-intensive, subjective to human error, and often limited in diversity. This challenge is particularly pronounced in the domain of robotics, where diverse and dynamic scenarios further complicate the creation of representative datasets. To address this, we propose a novel method for automatically generating annotated synthetic data in Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for rapid synthetic data generation. We demonstrate that synthetic datasets can achieve performance comparable to that of real-world datasets while significantly reducing the time required to generate and annotate data. Additionally, combining real-world and synthetic data significantly increases object detection performance by leveraging the quality of real-world images with the easier scalability of synthetic data. To our knowledge, this is the first application of synthetic data for training object detection algorithms in the highly dynamic and varied environment of robot soccer. Validation experiments reveal that a detector trained on synthetic images performs on par with one trained on manually annotated real-world images when tested on robot soccer match scenarios. Our method offers a scalable and comprehensive alternative to traditional dataset creation, eliminating the labour-intensive error-prone manual annotation process. By generating datasets in a simulator where all elements are intrinsically known, we ensure accurate annotations while significantly reducing manual effort, which makes it particularly valuable for robotics applications requiring diverse and scalable training data.",
    "arxiv_url": "https://arxiv.org/abs/2506.05092v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05092v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Christopher Maxey",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "Despite significant advancements in dynamic neural rendering, existing methods fail to address the unique challenges posed by UAV-captured scenarios, particularly those involving monocular camera setups, top-down perspective, and multiple small, moving humans, which are not adequately represented in existing datasets. In this work, we introduce UAV4D, a framework for enabling photorealistic rendering for dynamic real-world scenes captured by UAVs. Specifically, we address the challenge of reconstructing dynamic scenes with multiple moving pedestrians from monocular video data without the need for additional sensors. We use a combination of a 3D foundation model and a human mesh reconstruction model to reconstruct both the scene background and humans. We propose a novel approach to resolve the scene scale ambiguity and place both humans and the scene in world coordinates by identifying human-scene contact points. Additionally, we exploit the SMPL model and background mesh to initialize Gaussian splats, enabling holistic scene rendering. We evaluated our method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and Okutama-Action, each with distinct characteristics and 10~50 humans. Our results demonstrate the benefits of our approach over existing methods in novel view synthesis, achieving a 1.5 dB PSNR improvement and superior visual sharpness.",
    "arxiv_url": "https://arxiv.org/abs/2506.05011v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05011v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "human",
      "dynamic",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting",
    "authors": [
      "Alfred T. Christiansen",
      "Andreas H. HÃ¸jrup",
      "Morten K. Stephansen",
      "Md Ibtihaj A. Sakib",
      "Taman S. Poojary",
      "Filip Slezak",
      "Morten S. Laursen",
      "Thomas B. Moeslund",
      "Joakim B. Haurum"
    ],
    "abstract": "Training neural networks for tasks such as 3D point cloud semantic segmentation demands extensive datasets, yet obtaining and annotating real-world point clouds is costly and labor-intensive. This work aims to introduce a novel pipeline for generating realistic synthetic data, by leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to generate 3D assets of multiple different agricultural vehicles instead of using generic models. These assets are placed in a simulated environment, where the point clouds are generated using a simulated LiDAR. This is a flexible approach that allows changing the LiDAR specifications without incurring additional costs. We evaluated the impact of synthetic data on segmentation models such as PointNet++, Point Transformer V3, and OACNN, by training and validating the models only on synthetic data. Remarkably, the PTv3 model had an mIoU of 91.35\\%, a noteworthy result given that the model had neither been trained nor validated on any real data. Further studies even suggested that in certain scenarios the models trained only on synthetically generated data performed better than models trained on real-world data. Finally, experiments demonstrated that the models can generalize across semantic classes, enabling accurate predictions on mesh models they were never trained on.",
    "arxiv_url": "https://arxiv.org/abs/2506.05009v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05009v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer",
    "authors": [
      "Filip Slezak",
      "Magnus K. Gjerde",
      "Joakim B. Haurum",
      "Ivan Nikolov",
      "Morten S. Laursen",
      "Thomas B. Moeslund"
    ],
    "abstract": "In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for stereo dataset generation, offering an efficient alternative to Neural Radiance Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore utilizing the reconstructed geometry from the explicit 3D representations as well as depth estimates from the FoundationStereo model in an expert knowledge transfer setup. We find that when fine-tuning stereo models on 3DGS-generated datasets, we demonstrate competitive performance in zero-shot generalization benchmarks. When using the reconstructed geometry directly, we observe that it is often noisy and contains artifacts, which propagate noise to the trained model. In contrast, we find that the disparity estimates from FoundationStereo are cleaner and consequently result in a better performance on the zero-shot generalization benchmarks. Our method highlights the potential for low-cost, high-fidelity dataset creation and fast fine-tuning for deep stereo models. Moreover, we also reveal that while the latest Gaussian Splatting based methods have achieved superior performance on established benchmarks, their robustness falls short in challenging in-the-wild settings warranting further exploration.",
    "arxiv_url": "https://arxiv.org/abs/2506.04908v1",
    "pdf_url": "https://arxiv.org/pdf/2506.04908v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "ar",
      "fast",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
    "authors": [
      "Gaia Di Lorenzo",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.",
    "arxiv_url": "https://arxiv.org/abs/2506.04789v3",
    "pdf_url": "https://arxiv.org/pdf/2506.04789v3",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "semantic",
      "ar",
      "geometry",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photoreal Scene Reconstruction from an Egocentric Device",
    "authors": [
      "Zhaoyang Lv",
      "Maurizio Monge",
      "Ka Chen",
      "Yufeng Zhu",
      "Michael Goesele",
      "Jakob Engel",
      "Zhao Dong",
      "Richard Newcombe"
    ],
    "abstract": "In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at http://www.projectaria.com/photoreal-reconstruction/",
    "arxiv_url": "https://arxiv.org/abs/2506.04444v1",
    "pdf_url": "https://arxiv.org/pdf/2506.04444v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "dynamic",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.",
    "arxiv_url": "https://arxiv.org/abs/2506.04351v1",
    "pdf_url": "https://arxiv.org/pdf/2506.04351v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "mapping",
      "human",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pseudo-Simulation for Autonomous Driving",
    "authors": [
      "Wei Cao",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Daniel Dauner",
      "Xunjiang Gu",
      "Caojun Wang",
      "Yakov Miron",
      "Marco Aiello",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim.",
    "arxiv_url": "https://arxiv.org/abs/2506.04218v2",
    "pdf_url": "https://arxiv.org/pdf/2506.04218v2",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands relatively significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the effectiveness of our approach. Code is available at https://flexgs.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2506.04174v1",
    "pdf_url": "https://arxiv.org/pdf/2506.04174v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data",
    "authors": [
      "Ben Moran",
      "Mauro Comi",
      "Arunkumar Byravan",
      "Steven Bohez",
      "Tom Erez",
      "Zhibin Li",
      "Leonard Hasenclever"
    ],
    "abstract": "Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2506.04120v2",
    "pdf_url": "https://arxiv.org/pdf/2506.04120v2",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting",
    "authors": [
      "Shengjie Lin",
      "Jiading Fang",
      "Muhammad Zubair Irshad",
      "Vitor Campagnolo Guizilini",
      "Rares Andrei Ambrus",
      "Greg Shakhnarovich",
      "Matthew R. Walter"
    ],
    "abstract": "Reconstructing articulated objects prevalent in daily environments is crucial for applications in augmented/virtual reality and robotics. However, existing methods face scalability limitations (requiring 3D supervision or costly annotations), robustness issues (being susceptible to local optima), and rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a self-supervised, category-agnostic framework that leverages 3D Gaussian Splatting (3DGS) to reconstruct articulated objects and infer kinematics from two sets of posed RGB images captured at different articulation states, enabling real-time photorealistic rendering for novel viewpoints and articulations. SplArt augments 3DGS with a differentiable mobility parameter per Gaussian, achieving refined part segmentation. A multi-stage optimization strategy is employed to progressively handle reconstruction, part segmentation, and articulation estimation, significantly enhancing robustness and accuracy. SplArt exploits geometric self-supervision, effectively addressing challenging scenarios without requiring 3D annotations or category-specific priors. Evaluations on established and newly proposed benchmarks, along with applications to real-world scenarios using a handheld RGB camera, demonstrate SplArt's state-of-the-art performance and real-world practicality. Code is publicly available at https://github.com/ripl/splart.",
    "arxiv_url": "https://arxiv.org/abs/2506.03594v1",
    "pdf_url": "https://arxiv.org/pdf/2506.03594v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
    "authors": [
      "Chengqi Li",
      "Zhihao Shi",
      "Yangdi Lu",
      "Wenbo He",
      "Xiangyu Xu"
    ],
    "abstract": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose \\modelname{}, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. See the project website at https://steveli88.github.io/AsymGS.",
    "arxiv_url": "https://arxiv.org/abs/2506.03538v3",
    "pdf_url": "https://arxiv.org/pdf/2506.03538v3",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "neural rendering",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "lightweight",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation",
    "authors": [
      "Lukas Meyer",
      "Josef GrÃ¼n",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS) framework that is able to generate multi-view consistent novel views from images of multiple, independent cameras with different spectral domains. In contrast to previous approaches, our method does not require cross-modal camera calibration and is versatile enough to model a variety of different spectra, including thermal and near-infra red, without any algorithmic changes.   Unlike existing 3DGS-based frameworks that treat each modality separately (by optimizing per-channel spherical harmonics) and therefore fail to exploit the underlying spectral and spatial correlations, our method leverages a novel neural color representation that encodes multi-spectral information into a learned, compact, per-splat feature embedding. A shallow multi-layer perceptron (MLP) then decodes this embedding to obtain spectral color values, enabling joint learning of all bands within a unified representation.   Our experiments show that this simple yet effective strategy is able to improve multi-spectral rendering quality, while also leading to improved per-spectra rendering quality over state-of-the-art methods. We demonstrate the effectiveness of this new technique in agricultural applications to render vegetation indices, such as normalized difference vegetation index (NDVI).",
    "arxiv_url": "https://arxiv.org/abs/2506.03407v1",
    "pdf_url": "https://arxiv.org/pdf/2506.03407v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gen4D: Synthesizing Humans and Scenes in the Wild",
    "authors": [
      "Jerrin Bright",
      "Zhibo Wang",
      "Yuhao Chen",
      "Sirisha Rambhatla",
      "John Zelek",
      "David Clausi"
    ],
    "abstract": "Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.",
    "arxiv_url": "https://arxiv.org/abs/2506.05397v1",
    "pdf_url": "https://arxiv.org/pdf/2506.05397v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "human",
      "avatar",
      "gaussian splatting",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM",
    "authors": [
      "Roman Titkov",
      "Egor Zubkov",
      "Dmitry Yudin",
      "Jaafar Mahmoud",
      "Malik Mohrat",
      "Gennady Sidorov"
    ],
    "abstract": "Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM -- a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera's ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: https://titrom025.github.io/LEG-SLAM/",
    "arxiv_url": "https://arxiv.org/abs/2506.03073v1",
    "pdf_url": "https://arxiv.org/pdf/2506.03073v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Processor Chip Model",
    "authors": [
      "Kaiyan Chang",
      "Mingzhi Chen",
      "Yunji Chen",
      "Zhirong Chen",
      "Dongrui Fan",
      "Junfeng Gong",
      "Nan Guo",
      "Yinhe Han",
      "Qinfen Hao",
      "Shuo Hou",
      "Xuan Huang",
      "Pengwei Jin",
      "Changxin Ke",
      "Cangyuan Li",
      "Guangli Li",
      "Huawei Li",
      "Kuan Li",
      "Naipeng Li",
      "Shengwen Liang",
      "Cheng Liu",
      "Hongwei Liu",
      "Jiahua Liu",
      "Junliang Lv",
      "Jianan Mu",
      "Jin Qin",
      "Bin Sun",
      "Chenxi Wang",
      "Duo Wang",
      "Mingjun Wang",
      "Ying Wang",
      "Chenggang Wu",
      "Peiyang Wu",
      "Teng Wu",
      "Xiao Xiao",
      "Mengyao Xie",
      "Chenwei Xiong",
      "Ruiyuan Xu",
      "Mingyu Yan",
      "Xiaochun Ye",
      "Kuai Yu",
      "Rui Zhang",
      "Shuoming Zhang",
      "Jiacheng Zhao"
    ],
    "abstract": "Computer System Architecture serves as a crucial bridge between software applications and the underlying hardware, encompassing components like compilers, CPUs, coprocessors, and RTL designs. Its development, from early mainframes to modern domain-specific architectures, has been driven by rising computational demands and advancements in semiconductor technology. However, traditional paradigms in computer system architecture design are confronting significant challenges, including a reliance on manual expertise, fragmented optimization across software and hardware layers, and high costs associated with exploring expansive design spaces. While automated methods leveraging optimization algorithms and machine learning have improved efficiency, they remain constrained by a single-stage focus, limited data availability, and a lack of comprehensive human domain knowledge. The emergence of large language models offers transformative opportunities for the design of computer system architecture. By leveraging the capabilities of LLMs in areas such as code generation, data analysis, and performance modeling, the traditional manual design process can be transitioned to a machine-based automated design approach. To harness this potential, we present the Large Processor Chip Model (LPCM), an LLM-driven framework aimed at achieving end-to-end automated computer architecture design. The LPCM is structured into three levels: Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D Gaussian Splatting as a representative workload and employs the concept of software-hardware collaborative design to examine the implementation of the LPCM at Level 1, demonstrating the effectiveness of the proposed approach. Furthermore, this paper provides an in-depth discussion on the pathway to implementing Level 2 and Level 3 of the LPCM, along with an analysis of the existing challenges.",
    "arxiv_url": "https://arxiv.org/abs/2506.02929v1",
    "pdf_url": "https://arxiv.org/pdf/2506.02929v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Voyager: Real-Time Splatting City-Scale Gaussians on Resource-Constrained Devices",
    "authors": [
      "Zheng Liu",
      "He Zhu",
      "Xinyang Li",
      "Yirun Wang",
      "Yujiao Shi",
      "Yiming Gan",
      "Wei Li",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an emerging technique for photorealistic 3D scene rendering. However, rendering city-scale 3DGS scenes on resource-constrained mobile devices in real-time remains a significant challenge due to two compute-intensive stages: level-of-detail (LoD) search and rasterization.   In this paper, we propose Voyager, an effective solution to accelerate city-scale 3DGS rendering on mobile devices. Our key insight is that, under normal user motion, the number of newly visible Gaussians within the view frustum remains roughly constant. Leveraging this temporal correlation, we propose a temporal-aware LoD search to identify the necessary Gaussians for the remaining rendering stages. For the remaining rendering process, we accelerate the bottleneck stage, rasterization, via preemptive $Î±$-filtering. With all optimizations above, our system can deliver low-latency, city-scale 3DGS rendering on mobile devices. Compared to existing solutions, Voyager achieves up to 6.6$\\times$ speedup and 85\\% energy savings with superior rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2506.02774v3",
    "pdf_url": "https://arxiv.org/pdf/2506.02774v3",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
    "authors": [
      "Chuanyu Fu",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Guanying Chen",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.",
    "arxiv_url": "https://arxiv.org/abs/2506.02751v3",
    "pdf_url": "https://arxiv.org/pdf/2506.02751v3",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR",
    "authors": [
      "Zihao Ding",
      "Cheng-Tse Lee",
      "Mufeng Zhu",
      "Tao Guan",
      "Yuan-Chun Sun",
      "Cheng-Hsin Hsu",
      "Yao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an emerging media representation that reconstructs real-world 3D scenes in high fidelity, enabling 6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However, developing and evaluating 3DGS-enabled applications and optimizing their rendering performance, require realistic user navigation data. Such data is currently unavailable for photorealistic 3DGS reconstructions of real-world scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available 6-DoF navigation dataset featuring traces from 46 participants exploring twelve diverse, real-world 3DGS scenes. The dataset was collected at two sites, using the Meta Quest Pro headsets, recording the head pose and eye gaze data for each rendered frame during free world standing 6-DoF navigation. For each of the twelve scenes, we performed careful scene initialization to correct for scene tilt and scale, ensuring a perceptually-comfortable VR experience. We also release our open-source SIBR viewer software fork with record-and-replay functionalities and a suite of utility tools for data processing, conversion, and visualization. The EyeNavGS dataset and its accompanying software tools provide valuable resources for advancing research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.",
    "arxiv_url": "https://arxiv.org/abs/2506.02380v1",
    "pdf_url": "https://arxiv.org/pdf/2506.02380v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCodec Studio: A Modular Framework for Gaussian Splat Compression",
    "authors": [
      "Sicheng Li",
      "Chengzhen Wu",
      "Hao Li",
      "Xiang Gao",
      "Yiyi Liao",
      "Lu Yu"
    ],
    "abstract": "3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available at https://github.com/JasonLSC/GSCodec_Studio , to advance the research on Gaussian Splats compression.",
    "arxiv_url": "https://arxiv.org/abs/2506.01822v1",
    "pdf_url": "https://arxiv.org/pdf/2506.01822v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes",
    "authors": [
      "Manuel-Andreas Schneider",
      "Lukas HÃ¶llein",
      "Matthias NieÃner"
    ],
    "abstract": "Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.",
    "arxiv_url": "https://arxiv.org/abs/2506.01799v2",
    "pdf_url": "https://arxiv.org/pdf/2506.01799v2",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
    "authors": [
      "Tenny Yin",
      "Zhiting Mei",
      "Tao Sun",
      "Lihan Zha",
      "Emily Zhou",
      "Jeremy Bao",
      "Miyu Yamane",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.",
    "arxiv_url": "https://arxiv.org/abs/2506.01600v1",
    "pdf_url": "https://arxiv.org/pdf/2506.01600v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "dynamic",
      "localization",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes",
    "authors": [
      "Pou-Chun Kung",
      "Skanda Harisha",
      "Ram Vasudevan",
      "Aline Eid",
      "Katherine A. Skinner"
    ],
    "abstract": "High-Fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR / 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction. A project page is available at https://umautobots.github.io/radarsplat.",
    "arxiv_url": "https://arxiv.org/abs/2506.01379v1",
    "pdf_url": "https://arxiv.org/pdf/2506.01379v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "autonomous driving",
      "gaussian splatting",
      "3d reconstruction",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian Splatting",
    "authors": [
      "Fengze Li",
      "Yangle Liu",
      "Jieming Ma",
      "Hai-Ning Liang",
      "Yaochun Shen",
      "Huangxiang Li",
      "Zhijing Wu"
    ],
    "abstract": "Accurate 3D fruit counting in orchards is challenging due to heavy occlusion, semantic ambiguity between fruits and surrounding structures, and the high computational cost of volumetric reconstruction. Existing pipelines often rely on multi-view 2D segmentation and dense volumetric sampling, which lead to accumulated fusion errors and slow inference. We introduce FruitLangGS, a language-guided 3D fruit counting framework that reconstructs orchard-scale scenes using an adaptive-density Gaussian Splatting pipeline with radius-aware pruning and tile-based rasterization, enabling scalable 3D representation. During inference, compressed CLIP-aligned semantic vectors embedded in each Gaussian are filtered via a dual-threshold cosine similarity mechanism, retrieving Gaussians relevant to target prompts while suppressing common distractors (e.g., foliage), without requiring retraining or image-space masks. The selected Gaussians are then sampled into dense point clouds and clustered geometrically to estimate fruit instances, remaining robust under severe occlusion and viewpoint variation. Experiments on nine different orchard-scale datasets demonstrate that FruitLangGS consistently outperforms existing pipelines in instance counting recall, avoiding multi-view segmentation fusion errors and achieving up to 99.7% recall on Pfuji-Size_Orch2018 orchard dataset. Ablation studies further confirm that language-conditioned semantic embedding and dual-threshold prompt filtering are essential for suppressing distractors and improving counting accuracy under heavy occlusion. Beyond fruit counting, the same framework enables prompt-driven 3D semantic retrieval without retraining, highlighting the potential of language-guided 3D perception for scalable agricultural scene understanding.",
    "arxiv_url": "https://arxiv.org/abs/2506.01109v3",
    "pdf_url": "https://arxiv.org/pdf/2506.01109v3",
    "published_date": "2025-06-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "lighting",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Globally Consistent RGB-D SLAM with 2D Gaussian Splatting",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Liren Jin",
      "Marija PopoviÄ",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable performance of high-fidelity 3D reconstruction. However, the lack of depth rendering consistency and efficient loop closure limits the quality of its geometric reconstructions and its ability to perform globally consistent mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using 2D Gaussian splatting as the map representation. By leveraging the depth-consistent rendering property of the 2D variant, we propose an accurate camera pose optimization method and achieve geometrically accurate 3D reconstruction. In addition, we implement efficient loop detection and camera relocalization by leveraging MASt3R, a 3D foundation model, and achieve efficient map updates by maintaining a local active map. Experiments show that our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface reconstruction quality, and more consistent global map reconstruction compared to existing rendering-based SLAM methods, while maintaining high-fidelity image rendering and improved computational efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2506.00970v1",
    "pdf_url": "https://arxiv.org/pdf/2506.00970v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "tracking",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splat Vulnerabilities",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haoran Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Willian T. Lunardi",
      "Martin Andreoni",
      "Polo Chau"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.",
    "arxiv_url": "https://arxiv.org/abs/2506.00280v1",
    "pdf_url": "https://arxiv.org/pdf/2506.00280v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Voxelization for Transform coding of 3D Gaussian splatting data",
    "authors": [
      "Chenjunjie Wang",
      "Shashank N. Sridhara",
      "Eduardo Pavez",
      "Antonio Ortega",
      "Cheng Chang"
    ],
    "abstract": "We present a novel compression framework for 3D Gaussian splatting (3DGS) data that leverages transform coding tools originally developed for point clouds. Contrary to existing 3DGS compression methods, our approach can produce compressed 3DGS models at multiple bitrates in a computationally efficient way. Point cloud voxelization is a discretization technique that point cloud codecs use to improve coding efficiency while enabling the use of fast transform coding algorithms. We propose an adaptive voxelization algorithm tailored to 3DGS data, to avoid the inefficiencies introduced by uniform voxelization used in point cloud codecs. We ensure the positions of larger volume Gaussians are represented at high resolution, as these significantly impact rendering quality. Meanwhile, a low-resolution representation is used for dense regions with smaller Gaussians, which have a relatively lower impact on rendering quality. This adaptive voxelization approach significantly reduces the number of Gaussians and the bitrate required to encode the 3DGS data. After voxelization, many Gaussians are moved or eliminated. Thus, we propose to fine-tune/recolor the remaining 3DGS attributes with an initialization that can reduce the amount of retraining required. Experimental results on pre-trained datasets show that our proposed compression framework outperforms existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2506.00271v1",
    "pdf_url": "https://arxiv.org/pdf/2506.00271v1",
    "published_date": "2025-05-30",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Understanding while Exploring: Semantics-driven Active Mapping",
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Hairong Yin",
      "Yi Xu",
      "Philippos Mordohai"
    ],
    "abstract": "Effective robotic autonomy in unknown environments demands proactive exploration and precise understanding of both geometry and semantics. In this paper, we propose ActiveSGM, an active semantic mapping framework designed to predict the informativeness of potential observations before execution. Built upon a 3D Gaussian Splatting (3DGS) mapping backbone, our approach employs semantic and geometric uncertainty quantification, coupled with a sparse semantic representation, to guide exploration. By enabling robots to strategically select the most beneficial viewpoints, ActiveSGM efficiently enhances mapping completeness, accuracy, and robustness to noisy semantic data, ultimately supporting more adaptive scene exploration. Our experiments on the Replica and Matterport3D datasets highlight the effectiveness of ActiveSGM in active semantic mapping tasks.",
    "arxiv_url": "https://arxiv.org/abs/2506.00225v2",
    "pdf_url": "https://arxiv.org/pdf/2506.00225v2",
    "published_date": "2025-05-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion",
    "authors": [
      "Yangyi Huang",
      "Ye Yuan",
      "Xueting Li",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.",
    "arxiv_url": "https://arxiv.org/abs/2505.24877v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24877v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "ar",
      "human",
      "avatar",
      "3d gaussian",
      "motion",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores",
    "authors": [
      "Zimu Liao",
      "Jifeng Ding",
      "Siwei Cui",
      "Ruixuan Gong",
      "Boni Hu",
      "Yi Wang",
      "Hengjie Li",
      "XIngcheng Zhang",
      "Hui Wang",
      "Rong Fu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where conditional alpha-blending dominates the computational cost in the rendering pipeline. This paper proposes TC-GS, an algorithm-independent universal module that expands the applicability of Tensor Core (TCU) for 3DGS, leading to substantial speedups and seamless integration into existing 3DGS optimization frameworks. The key innovation lies in mapping alpha computation to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS implementations. TC-GS provides plug-and-play acceleration for existing top-tier acceleration algorithms and integrates seamlessly with rendering pipeline designs, such as Gaussian compression and redundancy elimination algorithms. Additionally, we introduce a global-to-local coordinate transformation to mitigate rounding errors from quadratic terms of pixel coordinates caused by Tensor Core half-precision computation. Extensive experiments demonstrate that our method maintains rendering quality while providing an additional 2.18x speedup over existing Gaussian acceleration algorithms, thereby achieving a total acceleration of up to 5.6x.",
    "arxiv_url": "https://arxiv.org/abs/2505.24796v2",
    "pdf_url": "https://arxiv.org/pdf/2505.24796v2",
    "published_date": "2025-05-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting",
    "authors": [
      "Jiazhong Cen",
      "Xudong Zhou",
      "Jiemin Fang",
      "Changsong Wen",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints--a phenomenon we term view-dependent semantics. To address this challenge, we propose LaGa (Language Gaussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of +18.7% mIoU over the previous SOTA on the LERF-OVS dataset. Our code is available at: https://github.com/SJTU-DeepVisionLab/LaGa.",
    "arxiv_url": "https://arxiv.org/abs/2505.24746v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24746v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework",
    "authors": [
      "Xin Kang",
      "Zihan Zheng",
      "Lei Chu",
      "Yue Gao",
      "Jiahao Li",
      "Hao Pan",
      "Xuejin Chen",
      "Yan Lu"
    ],
    "abstract": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.",
    "arxiv_url": "https://arxiv.org/abs/2505.24245v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24245v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians",
    "authors": [
      "Zixun Huang",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the quality and efficiency of differentiable rendering. However, its high efficiency stems from an approximation of projecting 3D Gaussians onto the image plane as 2D Gaussians, which inherently limits rendering quality--particularly under large Field-of-View (FoV) camera inputs. While several recent works have extended 3DGS to mitigate these approximation errors, none have successfully achieved both exactness and high efficiency simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient Volumetric Gaussian Rendering method. Starting from first principles, we derive a closed-form expression for the density integral along a ray traversing a 3D Gaussian distribution. This formulation enables precise forward rendering with arbitrary camera models and supports gradient-based optimization of 3D Gaussian parameters. To ensure both exactness and real-time performance, we propose an efficient method for computing a tight Particle Bounding Frustum (PBF) for each 3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also introduce a novel Bipolar Equiangular Projection (BEAP) representation to accelerate ray association under generic camera models. BEAP further provides a more uniform ray sampling strategy to apply supervision, which empirically improves reconstruction quality. Experiments on multiple pinhole and fisheye datasets show that our method consistently outperforms prior methods, establishing a new state-of-the-art in real-time neural rendering.",
    "arxiv_url": "https://arxiv.org/abs/2505.24053v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24053v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "authors": [
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Duochao Shi",
      "Akide Liu",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.",
    "arxiv_url": "https://arxiv.org/abs/2505.23734v4",
    "pdf_url": "https://arxiv.org/pdf/2505.23734v4",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "compression",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mobi-$Ï$: Mobilizing Your Robot Learning Policy",
    "authors": [
      "Jingyun Yang",
      "Isabella Huang",
      "Brandon Vu",
      "Max Bajracharya",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. We propose a novel approach for policy mobilization that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. To understand policy mobilization in more depth, we also introduce the Mobi-$Ï$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, and (3) visualization tools for analysis. In both our developed simulation task suite and the real world, we show that our approach outperforms baselines, demonstrating its effectiveness for policy mobilization.",
    "arxiv_url": "https://arxiv.org/abs/2505.23692v2",
    "pdf_url": "https://arxiv.org/pdf/2505.23692v2",
    "published_date": "2025-05-29",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "authors": [
      "Chuandong Liu",
      "Huijiao Wang",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.",
    "arxiv_url": "https://arxiv.org/abs/2505.23280v1",
    "pdf_url": "https://arxiv.org/pdf/2505.23280v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
    "authors": [
      "Jonas Kulhanek",
      "Marie-Julie Rakotosaona",
      "Fabian Manhardt",
      "Christina Tsalicoglou",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Songyou Peng",
      "Federico Tombari"
    ],
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.",
    "arxiv_url": "https://arxiv.org/abs/2505.23158v2",
    "pdf_url": "https://arxiv.org/pdf/2505.23158v2",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "nerf",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting. Code is avilable at https://github.com/youngju-na/SHARE",
    "arxiv_url": "https://arxiv.org/abs/2505.22978v3",
    "pdf_url": "https://arxiv.org/pdf/2505.22978v3",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Hierarchical Sparse Transform Coding of 3DGS",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) supports fast, high quality, novel view synthesis but has a heavy memory footprint, making the compression of its model crucial. Current state-of-the-art (SOTA) 3DGS compression methods adopt an anchor-based architecture that pairs the Scaffold-GS representation with conditional entropy coding. However, these methods forego the analysis-synthesis transform, a vital mechanism in visual data compression. As a result, redundancy remains intact in the signal and its removal is left to the entropy coder, which computationally overburdens the entropy coding module, increasing coding latency. Even with added complexity thorough redundancy removal is a task unsuited to an entropy coder. To fix this critical omission, we introduce a Sparsity-guided Hierarchical Transform Coding (SHTC) method, the first study on the end-to-end learned neural transform coding of 3DGS. SHTC applies KLT to decorrelate intra-anchor attributes, followed by quantization and entropy coding, and then compresses KLT residuals with a low-complexity, scene-adaptive neural transform. Aided by the sparsity prior and deep unfolding technique, the learned transform uses only a few trainable parameters, reducing the memory usage. Overall, SHTC achieves an appreciably improved R-D performance and at the same time higher decoding speed over SOTA. Its prior-guided, parameter-efficient design may also inspire low-complexity neural image and video codecs. Our code will be released at https://github.com/hxu160/SHTC_for_3DGS_compression.",
    "arxiv_url": "https://arxiv.org/abs/2505.22908v2",
    "pdf_url": "https://arxiv.org/pdf/2505.22908v2",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting",
    "authors": [
      "Kornel Howil",
      "Joanna WaczyÅska",
      "Piotr Borycki",
      "Tadeusz Dziarmaga",
      "Marcin Mazur",
      "PrzemysÅaw Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussian, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. The CLIPGaussian approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving the model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussian as a universal and efficient solution for multimodal style transfer.",
    "arxiv_url": "https://arxiv.org/abs/2505.22854v2",
    "pdf_url": "https://arxiv.org/pdf/2505.22854v2",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Zehao Li",
      "Hao Jiang",
      "Yujun Cai",
      "Jianing Chen",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Yiwei Wang",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ],
    "abstract": "Although dynamic scene reconstruction has long been a fundamental challenge in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a promising direction by enabling high-quality, real-time rendering through explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic reconstruction often suffer from \\textit{spatio-temporal incoherence} during initialization, where canonical Gaussians are constructed by aggregating observations from multiple frames without temporal distinction. This results in spatio-temporally entangled representations, making it difficult to model dynamic motion accurately. To overcome this limitation, we propose \\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a plug-and-play module that learns spatio-temporal probability distributions for each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation field, and a consistency regularization to jointly disentangle spatial and temporal patterns. Extensive experiments demonstrate that incorporating our module into existing 3DGS-based dynamic scene reconstruction frameworks leads to notable improvements in both reconstruction quality and spatio-temporal consistency across synthetic and real-world benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2505.22400v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22400v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments",
    "authors": [
      "Wancai Zheng",
      "Linlin Ou",
      "Jiajie He",
      "Libo Zhou",
      "Xinyi Yu",
      "Yan Wei"
    ],
    "abstract": "Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous Localization and Mapping (SLAM) have significantly progressed in tracking and high-fidelity mapping. However, their sequential optimization framework and sensitivity to dynamic objects limit real-time performance and robustness in real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for dynamic environments that decouples tracking and mapping through a parallelized framework. A probabilistic octree is employed to manage Gaussian primitives adaptively, enabling efficient initialization and pruning without hand-crafted thresholds. To robustly filter dynamic regions during tracking, we propose a training-free uncertainty estimator that fuses multi-modal residuals to estimate per-pixel motion uncertainty, achieving open-set dynamic object handling without reliance on semantic labels. Furthermore, a temporal encoder is designed to enhance rendering quality. Concurrently, low-dimensional features are efficiently transformed via a shallow multilayer perceptron to construct DINO features, which are then employed to enrich the Gaussian field and improve the robustness of uncertainty prediction. Extensive experiments on multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57 dB PSNR), while maintaining real-time performance and producing reusable, artifact-free static maps in dynamic environments.The project: https://aczheng-cai.github.io/up_slam.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2505.22335v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22335v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss",
    "authors": [
      "Wenjun Lu",
      "Haodong Chen",
      "Anqi Yi",
      "Yuk Ying Chung",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Novel view synthesis is a fundamental task in 3D computer vision that aims to reconstruct realistic images from a set of posed input views. However, reconstruction quality degrades significantly under sparse-view conditions due to limited geometric cues. Existing methods, such as Neural Radiance Fields (NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from blurred details and structural artifacts when trained with insufficient views. Recent works have identified the quality of rendered depth as a key factor in mitigating these artifacts, as it directly affects geometric accuracy and view consistency. In this paper, we address these challenges by introducing Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that progressively refines geometry from coarse to fine levels. Central to HDGS is a novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and estimated monocular depths across multiple spatial scales. By enforcing multi-scale depth consistency, our method substantially improves structural fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU benchmarks demonstrate that HDGS achieves state-of-the-art performance under sparse-view settings while maintaining efficient and high-quality rendering",
    "arxiv_url": "https://arxiv.org/abs/2505.22279v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22279v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-Denoised Hyperspectral Gaussian Splatting",
    "authors": [
      "Sunil Kumar Narayanan",
      "Lingjun Zhao",
      "Lu Gan",
      "Yongsheng Chen"
    ],
    "abstract": "Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise quantification of sample nutritional elements. Recently, 3D reconstruction methods, such as Neural Radiance Field (NeRF), have been used to create implicit neural representations of HSI scenes. This capability enables the rendering of hyperspectral channel compositions at every spatial location, thereby helping localize the target object's nutrient composition both spatially and spectrally. However, it faces limitations in training time and rendering speed. In this paper, we propose Diffusion-Denoised Hyperspectral Gaussian Splatting (DD-HGS), which enhances the state-of-the-art 3D Gaussian Splatting (3DGS) method with wavelength-aware spherical harmonics, a Kullback-Leibler divergence-based spectral loss, and a diffusion-based denoiser to enable 3D explicit reconstruction of the hyperspectral scenes for the entire spectral range. We present extensive evaluations on diverse real-world hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our DD-HGS. The results demonstrate that DD-HGS achieves the new state-of-the-art performance compared to all the previously published methods. Project page: https://dragonpg2000.github.io/DDHGS-website/",
    "arxiv_url": "https://arxiv.org/abs/2505.21890v3",
    "pdf_url": "https://arxiv.org/pdf/2505.21890v3",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility",
    "authors": [
      "Yidi Li",
      "Jun Xiao",
      "Zhengda Lu",
      "Yiqun Wang",
      "Haiyong Jiang"
    ],
    "abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling.",
    "arxiv_url": "https://arxiv.org/abs/2505.21377v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21377v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure from Collision",
    "authors": [
      "Takuhiro Kaneko"
    ],
    "abstract": "Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.",
    "arxiv_url": "https://arxiv.org/abs/2505.21335v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21335v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling",
    "authors": [
      "Jieyu Yuan",
      "Yujun Li",
      "Yuanlin Zhang",
      "Chunle Guo",
      "Xiongxin Tang",
      "Ruixing Wang",
      "Chongyi Li"
    ],
    "abstract": "Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at https://bilityniu.github.io/3D-UIR.",
    "arxiv_url": "https://arxiv.org/abs/2505.21238v2",
    "pdf_url": "https://arxiv.org/pdf/2505.21238v2",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians",
    "authors": [
      "Weihang Liu",
      "Yuhui Zhong",
      "Yuke Li",
      "Xi Chen",
      "Jiadi Cui",
      "Honglong Zhang",
      "Lan Xu",
      "Xin Lou",
      "Yujiao Shi",
      "Jingyi Yu",
      "Yingliang Zhang"
    ],
    "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for applications such as AR navigation, UAV based inspection, and smart city digital twins. While aerial imagery offers broad coverage and complements limitations of ground-based data, reconstructing city-scale environments from such views remains challenging due to occlusions, incomplete geometry, and high memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve scalability and visual quality but remain limited by dense primitive usage, long training times, and poor suit ability for edge devices. We propose CityGo, a hybrid framework that combines textured proxy geometry with residual and surrounding 3D Gaussians for lightweight, photorealistic rendering of urban scenes from aerial perspectives. Our approach first extracts compact building proxy meshes from MVS point clouds, then uses zero order SH Gaussians to generate occlusion-free textures via image-based rendering and back-projection. To capture high-frequency details, we introduce residual Gaussians placed based on proxy-photo discrepancies and guided by depth priors. Broader urban context is represented by surrounding Gaussians, with importance-aware downsampling applied to non-critical regions to reduce redundancy. A tailored optimization strategy jointly refines proxy textures and Gaussian parameters, enabling real-time rendering of complex urban scenes on mobile GPUs with significantly reduced training and memory requirements. Extensive experiments on real-world aerial datasets demonstrate that our hybrid representation significantly reduces training time, achieving on average 1.4x speedup, while delivering comparable visual fidelity to pure 3D Gaussian Splatting approaches. Furthermore, CityGo enables real-time rendering of large-scale urban scenes on mobile consumer GPUs, with substantially reduced memory usage and energy consumption.",
    "arxiv_url": "https://arxiv.org/abs/2505.21041v3",
    "pdf_url": "https://arxiv.org/pdf/2505.21041v3",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "geometry",
      "real-time rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Runnan Chen",
      "Mingming Gong",
      "Dong Xu",
      "Tongliang Liu"
    ],
    "abstract": "Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.",
    "arxiv_url": "https://arxiv.org/abs/2505.20729v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20729v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting",
    "authors": [
      "Zechen Li",
      "Lanqing Yang",
      "Yiheng Bian",
      "Hao Pan",
      "Yongjian Fu",
      "Yezhou Wang",
      "Zhuxi Chen",
      "Yi-Chao Chen",
      "Guangtao Xue"
    ],
    "abstract": "Indoor environments typically contain diverse RF signals distributed across multiple frequency bands, including NB-IoT, Wi-Fi, and millimeter-wave. Consequently, wideband RF modeling is essential for practical applications such as joint deployment of heterogeneous RF systems, cross-band communication, and distributed RF sensing. Although 3D Gaussian Splatting (3DGS) techniques effectively reconstruct RF radiance fields at a single frequency, they cannot model fields at arbitrary or unknown frequencies across a wide range. In this paper, we present a novel 3DGS algorithm for unified wideband RF radiance field modeling. RF wave propagation depends on signal frequency and the 3D spatial environment, including geometry and material electromagnetic (EM) properties. To address these factors, we introduce a frequency-embedded EM feature network that utilizes 3D Gaussian spheres at each spatial location to learn the relationship between frequency and transmission characteristics, such as attenuation and radiance intensity. With a dataset containing sparse frequency samples in a specific 3D environment, our model can efficiently reconstruct RF radiance fields at arbitrary and unseen frequencies. To assess our approach, we introduce a large-scale power angular spectrum (PAS) dataset with 50,000 samples spanning 1 to 94 GHz across six indoor environments. Experimental results show that the proposed model trained on multiple frequencies achieves a Structural Similarity Index Measure (SSIM) of 0.922 for PAS reconstruction, surpassing state-of-the-art single-frequency 3DGS models with SSIM of 0.863.",
    "arxiv_url": "https://arxiv.org/abs/2505.20714v2",
    "pdf_url": "https://arxiv.org/pdf/2505.20714v2",
    "published_date": "2025-05-27",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Toshiki Watanabe",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Recent advancements in radiance field rendering, exemplified by Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly progressed 3D modeling and reconstruction. The use of multiple 360-degree omnidirectional images for these tasks is increasingly favored due to advantages in data acquisition and comprehensive scene capture. However, the inherent geometric distortions in common omnidirectional representations, such as equirectangular projection (particularly severe in polar regions and varying with latitude), pose substantial challenges to achieving high-fidelity 3D reconstructions. Current datasets, while valuable, often lack the specific focus, scene composition, and ground truth granularity required to systematically benchmark and drive progress in overcoming these omnidirectional-specific challenges. To address this critical gap, we introduce Omnidirectional Blender 3D (OB3D), a new synthetic dataset curated for advancing 3D reconstruction from multiple omnidirectional images. OB3D features diverse and complex 3D scenes generated from Blender 3D projects, with a deliberate emphasis on challenging scenarios. The dataset provides comprehensive ground truth, including omnidirectional RGB images, precise omnidirectional camera parameters, and pixel-aligned equirectangular maps for depth and normals, alongside evaluation metrics. By offering a controlled yet challenging environment, OB3Daims to facilitate the rigorous evaluation of existing methods and prompt the development of new techniques to enhance the accuracy and reliability of 3D reconstruction from omnidirectional images.",
    "arxiv_url": "https://arxiv.org/abs/2505.20126v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20126v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time",
    "authors": [
      "Chen Sang",
      "Yeqiang Qian",
      "Jiale Zhang",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "For tasks such as urban digital twins, VR/AR/game scene design, or creating synthetic films, the traditional industrial approach often involves manually modeling scenes and using various rendering engines to complete the rendering process. This approach typically requires high labor costs and hardware demands, and can result in poor quality when replicating complex real-world scenes. A more efficient approach is to use data from captured real-world scenes, then apply reconstruction and rendering algorithms to quickly recreate the authentic scene. However, current algorithms are unable to effectively reconstruct and render real-world weather effects. To address this, we propose a framework based on gaussian splatting, that can reconstruct real scenes and render them under synthesized 4D weather effects. Our work can simulate various common weather effects by applying Gaussians modeling and rendering techniques. It supports continuous dynamic weather changes and can easily control the details of the effects. Additionally, our work has low hardware requirements and achieves real-time rendering performance. The result demos can be accessed on our project homepage: weathermagician.github.io",
    "arxiv_url": "https://arxiv.org/abs/2505.19919v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19919v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "vr",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud",
    "authors": [
      "Natsuki Takama",
      "Shintaro Ito",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Gaussian Splatting (GS) has gained attention as a fast and effective method for novel view synthesis. It has also been applied to 3D reconstruction using multi-view images and can achieve fast and accurate 3D reconstruction. However, GS assumes that the input contains a large number of multi-view images, and therefore, the reconstruction accuracy significantly decreases when only a limited number of input images are available. One of the main reasons is the insufficient number of 3D points in the sparse point cloud obtained through Structure from Motion (SfM), which results in a poor initialization for optimizing the Gaussian primitives. We propose a new 3D reconstruction method, called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along with COLMAP MVS to generate highly accurate and dense 3D point clouds, which are then used to initialize 2D Gaussians. Through experiments on the DTU dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of objects using just three images. The project page is available at https://gsisaoki.github.io/SPARSE2DGS/",
    "arxiv_url": "https://arxiv.org/abs/2505.19854v2",
    "pdf_url": "https://arxiv.org/pdf/2505.19854v2",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "sparse-view",
      "gaussian splatting",
      "motion",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers",
    "authors": [
      "Haofan Ren",
      "Zunjie Zhu",
      "Xiang Chen",
      "Ming Lu",
      "Rongfeng Lu",
      "Chenggang Yan"
    ],
    "abstract": "Neural fields are now the central focus of research in 3D vision and computer graphics. Existing methods mainly focus on various scene representations, such as neural points and 3D Gaussians. However, few works have studied the rendering process to enhance the neural fields. In this work, we propose a plug-in method named K-Buffers that leverages multiple buffers to improve the rendering performance. Our method first renders K buffers from scene representations and constructs K pixel-wise feature maps. Then, We introduce a K-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally, we adopt a feature decoder to generate the rendering image. We also introduce an acceleration strategy to improve rendering speed and quality. We apply our method to well-known radiance field baselines, including neural point fields and 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our method effectively enhances the rendering performance of neural point fields and 3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2505.19564v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19564v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images",
    "authors": [
      "Guangan Chen",
      "Anh Minh Truong",
      "Hanhe Lin",
      "Michiel Vlaminck",
      "Wilfried Philips",
      "Hiep Luong"
    ],
    "abstract": "Novel view synthesis in 360$^\\circ$ scenes from extremely sparse input views is essential for applications like virtual reality and augmented reality. This paper presents a novel framework for novel view synthesis in extremely sparse-view cases. As typical structure-from-motion methods are unable to estimate camera poses in extremely sparse-view cases, we apply DUSt3R to estimate camera poses and generate a dense point cloud. Using the poses of estimated cameras, we densely sample additional views from the upper hemisphere space of the scenes, from which we render synthetic images together with the point cloud. Training 3D Gaussian Splatting model on a combination of reference images from sparse views and densely sampled synthetic images allows a larger scene coverage in 3D space, addressing the overfitting challenge due to the limited input in sparse-view cases. Retraining a diffusion-based image enhancement model on our created dataset, we further improve the quality of the point-cloud-rendered images by removing artifacts. We compare our framework with benchmark methods in cases of only four input views, demonstrating significant improvement in novel view synthesis under extremely sparse-view conditions for 360$^\\circ$ scenes.",
    "arxiv_url": "https://arxiv.org/abs/2505.19264v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19264v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Adrien Deliege",
      "Abdullah Hamdi",
      "Silvio Giancola",
      "Anthony Cioppa",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Andrea Tagliasacchi",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for a triangle comeback. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible with standard graphics stacks and GPU hardware, and highly efficient: for the \\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks. The project page is https://trianglesplatting.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2505.19175v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19175v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FHGS: Feature-Homogenized Gaussian Splatting",
    "authors": [
      "Q. G. Duan",
      "Benyun Zhao",
      "Mingqiao Han Yijun Huang",
      "Ben M. Chen"
    ],
    "abstract": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently achieved notable advances. Although 3DGS related methods have efficient rendering capabilities, they fail to address the inherent contradiction between the anisotropic color representation of gaussian primitives and the isotropic requirements of semantic features, leading to insufficient cross-view feature consistency. To overcome the limitation, we proposes $\\textit{FHGS}$ (Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework inspired by physical models, which can achieve high-precision mapping of arbitrary 2D features from pre-trained models to 3D scenes while preserving the real-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$ introduces the following innovations: Firstly, a universal feature fusion architecture is proposed, enabling robust embedding of large-scale pre-trained models' semantic features (e.g., SAM, CLIP) into sparse 3D structures. Secondly, a non-differentiable feature fusion mechanism is introduced, which enables semantic features to exhibit viewpoint independent isotropic distributions. This fundamentally balances the anisotropic rendering of gaussian primitives and the isotropic expression of features; Thirdly, a dual-driven optimization strategy inspired by electric potential fields is proposed, which combines external supervision from semantic feature fields with internal primitive clustering guidance. This mechanism enables synergistic optimization of global semantic alignment and local structural consistency. More interactive results can be accessed on: https://fhgs.cuastro.org/.",
    "arxiv_url": "https://arxiv.org/abs/2505.19154v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19154v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "mapping",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal infrared Novel-view Synthesis",
    "authors": [
      "Myeongseok Nam",
      "Wongi Park",
      "Minsol Kim",
      "Hyejin Hur",
      "Soomok Lee"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR) imaging has gained attention in novel-view synthesis, showing real-time rendering. However, novel-view synthesis with thermal infrared images suffers from transmission effects, emissivity, and low resolution, leading to floaters and blur effects in rendered images. To address these problems, we introduce Veta-GS, which leverages a view-dependent deformation field and a Thermal Feature Extractor (TFE) to precisely capture subtle thermal variations and maintain robustness. Specifically, we design view-dependent deformation field that leverages camera position and viewing direction, which capture thermal variations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and MonoSSIM loss, which consider appearance, edge, and frequency to maintain robustness. Extensive experiments on the TI-NSD benchmark show that our method achieves better performance over existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.19138v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19138v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes",
    "authors": [
      "Tianchen Deng",
      "Wenhua Wu",
      "Junjie He",
      "Yue Pan",
      "Xirui Jiang",
      "Shenghai Yuan",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "3D Gaussian Splatting has recently shown promising results in dense visual SLAM. However, existing 3DGS-based SLAM methods are all constrained to small-room scenarios and struggle with memory explosion in large-scale scenes and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We design a novel voxel-based progressive 3D Gaussian mapping method with multiple submaps for compact and accurate scene representation in large-scale and long-sequence scenes. This allows us to scale up to arbitrary scenes and improves robustness (even under pose drifts). In addition, we propose a 2D-3D fusion camera tracking method to achieve robust and accurate camera tracking in both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D Gaussian loop closure method to eliminate pose drift. We further propose a submap fusion method with online distillation to achieve global consistency in large-scale scenes when detecting a loop. Experiments on various indoor and outdoor datasets demonstrate the superiority and generalizability of the proposed framework. The code will be open source on https://github.com/dtc111111/vpgs-slam.",
    "arxiv_url": "https://arxiv.org/abs/2505.18992v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18992v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "outdoor",
      "ar",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting",
    "authors": [
      "Yitian Yuan",
      "Qianyue He"
    ],
    "abstract": "Recent works demonstrate the advantages of hardware rasterization for 3D Gaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized graphics and fixed memory footprint. However, extending these benefits to backward-pass gradient computation remains challenging due to graphics pipeline constraints. We present a differentiable hardware rasterizer for 3DGS that overcomes the memory and performance limitations of tile-based software rasterization. Our solution employs programmable blending for per-pixel gradient computation combined with a hybrid gradient reduction strategy (quad-level + subgroup) in fragment shaders, achieving over 10x faster backward rasterization versus naive atomic operations and 3x speedup over the canonical tile-based rasterizer. Systematic evaluation reveals 16-bit render targets (float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving higher gradient accuracy among mixed-precision rendering formats with execution speeds second only to unorm8, while float32 texture incurs severe forward pass performance degradation due to suboptimal hardware optimizations. Our method with float16 formats demonstrates 3.07x acceleration in full pipeline execution (forward + backward passes) on RTX4080 GPUs with the MipNeRF 360 dataset, outperforming the baseline tile-based renderer while preserving hardware rasterization's memory efficiency advantages -- incurring merely 2.67% of the memory overhead required for splat sorting operations. This work presents a unified differentiable hardware rasterization method that simultaneously optimizes runtime and memory usage for 3DGS, making it particularly suitable for resource-constrained devices with limited memory capacity.",
    "arxiv_url": "https://arxiv.org/abs/2505.18764v2",
    "pdf_url": "https://arxiv.org/pdf/2505.18764v2",
    "published_date": "2025-05-24",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "acceleration",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting",
    "authors": [
      "Shiyun Xie",
      "Zhiru Wang",
      "Yinghao Zhu",
      "Xu Wang",
      "Chengwei Pan",
      "Xiwang Dong"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis (NVS) with its real-time rendering capabilities and superior quality. However, it encounters challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose SuperGS, an expansion of Scaffold-GS designed with a two-stage coarse-to-fine training framework. In the low-resolution stage, we introduce a latent feature field to represent the low-resolution scene, which serves as both the initialization and foundational information for super-resolution optimization. In the high-resolution stage, we propose a multi-view consistent densification strategy that backprojects high-resolution depth maps based on error maps and employs a multi-view voting mechanism, mitigating ambiguities caused by multi-view inconsistencies in the pseudo labels provided by 2D prior models while avoiding Gaussian redundancy. Furthermore, we model uncertainty through variational feature learning and use it to guide further scene representation refinement and adjust the supervisory effect of pseudo-labels, ensuring consistent and detailed scene reconstruction. Extensive experiments demonstrate that SuperGS outperforms state-of-the-art HRNVS methods on both forward-facing and 360-degree datasets.",
    "arxiv_url": "https://arxiv.org/abs/2505.18649v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18649v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance",
    "authors": [
      "Jack Goffinet",
      "Youngjo Min",
      "Carlo Tomasi",
      "David E. Carlson"
    ],
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose Pose Splatter, a novel framework leveraging shape carving and 3D Gaussian splatting to model the complete pose and appearance of laboratory animals without prior knowledge of animal geometry, per-frame optimization, or manual annotations. We also propose a novel rotation-invariant visual embedding technique for encoding pose and appearance, designed to be a plug-in replacement for 3D keypoint data in downstream behavioral analyses. Experiments on datasets of mice, rats, and zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably, Pose Splatter represents subtle variations in pose, provides better low-dimensional pose embeddings over state-of-the-art as evaluated by humans, and generalizes to unseen data. By eliminating annotation and per-frame optimization bottlenecks, Pose Splatter enables analysis of large-scale, longitudinal behavior needed to map genotype, neural activity, and micro-behavior at unprecedented resolution.",
    "arxiv_url": "https://arxiv.org/abs/2505.18342v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18342v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting",
    "authors": [
      "Karly Hou",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Recently, Gaussian Splatting methods have emerged as a desirable substitute for prior Radiance Field methods for novel-view synthesis of scenes captured with multi-view images or videos. In this work, we propose a novel extension to 4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual learning, we hierarchically decompose the dynamic scene into a \"video-segment-frame\" structure, with segments dynamically adjusted by optical flow. Then, instead of directly predicting the time-dependent signals, we model the signal as the sum of video-constant values, segment-constant values, and frame-specific residuals, as inspired by the success of residual learning. This approach allows more flexible models that adapt to highly variable scenes. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets, with the greatest improvements on complex scenes with large movements, occlusions, and fine details, where current methods degrade most.",
    "arxiv_url": "https://arxiv.org/abs/2505.18306v2",
    "pdf_url": "https://arxiv.org/pdf/2505.18306v2",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes",
    "authors": [
      "Haihong Xiao",
      "Jianan Zou",
      "Yuxin Zhou",
      "Ying He",
      "Wenxiong Kang"
    ],
    "abstract": "We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor scenes. SplatCo builds upon three novel components: 1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features representing fine details. This fusion is achieved through a hierarchical compensation mechanism, ensuring both global spatial awareness and local detail preservation; 2) a cross-view pruning mechanism that removes overfitted or inaccurate Gaussians based on structural consistency, thereby improving storage efficiency and preventing rendering artifacts; 3) a structure view co-learning module that aggregates structural gradients with view gradients,thereby steering the optimization of Gaussian geometric and appearance attributes more robustly. By combining these key components, SplatCo effectively achieves high-fidelity rendering for large-scale scenes. Code and project page are available at https://splatco-tech.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2505.17951v4",
    "pdf_url": "https://arxiv.org/pdf/2505.17951v4",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "high-fidelity",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
    "arxiv_url": "https://arxiv.org/abs/2505.17590v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17590v2",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "ar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Homayoun Najjaran"
    ],
    "abstract": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such as infrastructure monitoring, structural assessment, and environmental surveying. While traditional photogrammetry techniques enable geometric modeling, they lack semantic interpretability, limiting their effectiveness for automated inspection workflows. Recent advances in neural rendering and 3D Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but similarly lack scene-level understanding.   In this work, we present a UAV-based pipeline that extends Feature-3DGS for language-guided 3D segmentation. We leverage LSeg-based feature fields with CLIP embeddings to generate heatmaps in response to language prompts. These are thresholded to produce rough segmentations, and the highest-scoring point is then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view renderings. Our results highlight the strengths and limitations of various feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful structure in large-scale outdoor environments. We demonstrate that this hybrid approach enables flexible, language-driven interaction with photorealistic 3D reconstructions, opening new possibilities for semantic aerial inspection and scene understanding.",
    "arxiv_url": "https://arxiv.org/abs/2505.17402v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17402v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "understanding",
      "semantic",
      "outdoor",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering",
    "authors": [
      "Zhongpai Gao",
      "Meng Zheng",
      "Benjamin Planche",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: https://gaozhongpai.github.io/renderfm/.",
    "arxiv_url": "https://arxiv.org/abs/2505.17338v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17338v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "medical"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion",
    "authors": [
      "Asrar Alruwayqi"
    ],
    "abstract": "We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.   In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.   Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs.",
    "arxiv_url": "https://arxiv.org/abs/2505.16535v1",
    "pdf_url": "https://arxiv.org/pdf/2505.16535v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "4d",
      "ar",
      "dynamic",
      "sparse-view",
      "gaussian splatting",
      "motion",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction",
    "authors": [
      "Jiacong Chen",
      "Qingyu Mao",
      "Youneng Bao",
      "Xiandong Meng",
      "Fanyang Meng",
      "Ronggang Wang",
      "Yongsheng Liang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers rapid responsiveness and immersive experiences. However, existing online methods face challenge in prohibitive storage requirements primarily due to point-wise modeling that fails to exploit the motion properties. To address this limitation, we propose a novel Compact Gaussian Streaming (ComGS) framework, leveraging the locality and consistency of motion in dynamic scene, that models object-consistent Gaussian point motion through keypoint-driven motion representation. By transmitting only the keypoint attributes, this framework provides a more storage-efficient solution. Specifically, we first identify a sparse set of motion-sensitive keypoints localized within motion regions using a viewspace gradient difference strategy. Equipped with these keypoints, we propose an adaptive motion-driven mechanism that predicts a spatial influence field for propagating keypoint motion to neighboring Gaussian points with similar motion. Moreover, ComGS adopts an error-aware correction strategy for key frame reconstruction that selectively refines erroneous regions and mitigates error accumulation without unnecessary overhead. Overall, ComGS achieves a remarkable storage reduction of over 159 X compared to 3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining competitive visual fidelity and rendering speed.",
    "arxiv_url": "https://arxiv.org/abs/2505.16533v2",
    "pdf_url": "https://arxiv.org/pdf/2505.16533v2",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics. The code of RUSplatting is available at https://github.com/theflash987/RUSplatting and the dataset Submerged3D can be downloaded at https://zenodo.org/records/15482420.",
    "arxiv_url": "https://arxiv.org/abs/2505.15737v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15737v2",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting",
    "authors": [
      "Zane K J Hartley",
      "Lewis A G Stuart",
      "Andrew P French",
      "Michael P Pound"
    ],
    "abstract": "Recent years have seen substantial improvements in the ability to generate synthetic 3D objects using AI. However, generating complex 3D objects, such as plants, remains a considerable challenge. Current generative 3D models struggle with plant generation compared to general objects, limiting their usability in plant analysis tools, which require fine detail and accurate geometry. We introduce PlantDreamer, a novel approach to 3D synthetic plant generation, which can achieve greater levels of realism for complex plant geometry and textures than available text-to-3D models. To achieve this, our new generation pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an adaptable Gaussian culling algorithm, which directly improve textural realism and geometric integrity of generated 3D plant models. Additionally, PlantDreamer enables both purely synthetic plant generation, by leveraging L-System-generated meshes, and the enhancement of real-world plant point clouds by converting them into 3D Gaussian Splats. We evaluate our approach by comparing its outputs with state-of-the-art text-to-3D models, demonstrating that PlantDreamer outperforms existing methods in producing high-fidelity synthetic plants. Our results indicate that our approach not only advances synthetic plant generation, but also facilitates the upgrading of legacy point cloud datasets, making it a valuable tool for 3D phenotyping applications.",
    "arxiv_url": "https://arxiv.org/abs/2505.15528v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15528v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation",
    "authors": [
      "Yuchen Li",
      "Chaoran Feng",
      "Zhenyu Tang",
      "Kaiyuan Deng",
      "Wangbo Yu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic event dataset for high-fidelity event vision tasks, captured from real-world sparse multi-view RGB images. Existing event datasets are often synthesized from dense RGB videos, which typically lack viewpoint diversity and geometric consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E overcomes these limitations by first reconstructing photorealistic static scenes using 3D Gaussian Splatting, and subsequently employing a novel, physically-informed event simulation pipeline. This pipeline generally integrates adaptive trajectory interpolation with physically-consistent event contrast threshold modeling. Such an approach yields temporally dense and geometrically consistent event streams under diverse motion and lighting conditions, while ensuring strong alignment with underlying scene structures. Experimental results on event-based 3D reconstruction demonstrate GS2E's superior generalization capabilities and its practical value as a benchmark for advancing event vision research.",
    "arxiv_url": "https://arxiv.org/abs/2505.15287v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15287v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography",
    "authors": [
      "Yifan Liu",
      "Wuyang Li",
      "Weihao Yu",
      "Chenxin Li",
      "Alexandre Alahi",
      "Max Meng",
      "Yixuan Yuan"
    ],
    "abstract": "Computed Tomography serves as an indispensable tool in clinical workflows, providing non-invasive visualization of internal anatomical structures. Existing CT reconstruction works are limited to small-capacity model architecture and inflexible volume representation. In this work, we present X-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for reconstructing 3D CT volumes from sparse-view 2D X-ray projections. X-GRM employs a scalable transformer-based architecture to encode sparse-view X-ray inputs, where tokens from different views are integrated efficiently. Then, these tokens are decoded into a novel volume representation, named Voxel-based Gaussian Splatting (VoxGS), which enables efficient CT volume extraction and differentiable X-ray rendering. This combination of a high-capacity model and flexible volume representation, empowers our model to produce high-quality reconstructions from various testing inputs, including in-domain and out-domain X-ray projections. Our codes are available at: https://github.com/CUHK-AIM-Group/X-GRM.",
    "arxiv_url": "https://arxiv.org/abs/2505.15235v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15235v2",
    "published_date": "2025-05-21",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GT2-GS: Geometry-aware Texture Transfer for Gaussian Splatting",
    "authors": [
      "Wenjie Liu",
      "Zhongliang Liu",
      "Junwei Shu",
      "Changbo Wang",
      "Yang Li"
    ],
    "abstract": "Transferring 2D textures onto complex 3D scenes plays a vital role in enhancing the efficiency and controllability of 3D multimedia content creation. However, existing 3D style transfer methods primarily focus on transferring abstract artistic styles to 3D scenes. These methods often overlook the geometric information of the scene, which makes it challenging to achieve high-quality 3D texture transfer results. In this paper, we present GT2-GS, a geometry-aware texture transfer framework for gaussian splatting. First, we propose a geometry-aware texture transfer loss that enables view-consistent texture transfer by leveraging prior view-dependent feature information and texture features augmented with additional geometric parameters. Moreover, an adaptive fine-grained control module is proposed to address the degradation of scene information caused by low-granularity texture features. Finally, a geometry preservation branch is introduced. This branch refines the geometric parameters using additionally bound Gaussian color priors, thereby decoupling the optimization objectives of appearance and geometry. Extensive experiments demonstrate the effectiveness and controllability of our method. Through geometric awareness, our approach achieves texture transfer results that better align with human visual perception. Our homepage is available at https://vpx-ecnu.github.io/GT2-GS-website.",
    "arxiv_url": "https://arxiv.org/abs/2505.15208v3",
    "pdf_url": "https://arxiv.org/pdf/2505.15208v3",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "human",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ],
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at https://github.com/CUHK-AIM-Group/MonoSplat.",
    "arxiv_url": "https://arxiv.org/abs/2505.15185v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15185v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression",
    "authors": [
      "Kangli Wang",
      "Shihao Li",
      "Qianxi Yi",
      "Wei Gao"
    ],
    "abstract": "Recently, immersive media and autonomous driving applications have significantly advanced through 3D Gaussian Splatting (3DGS), which offers high-fidelity rendering and computational efficiency. Despite these advantages, 3DGS as a display-oriented representation requires substantial storage due to its numerous Gaussian attributes. Current compression methods have shown promising results but typically neglect the compression of Gaussian spatial positions, creating unnecessary bitstream overhead. We conceptualize Gaussian primitives as point clouds and propose leveraging point cloud compression techniques for more effective storage. AI-based point cloud compression demonstrates superior performance and faster inference compared to MPEG Geometry-based Point Cloud Compression (G-PCC). However, direct application of existing models to Gaussian compression may yield suboptimal results, as Gaussian point clouds tend to exhibit globally sparse yet locally dense geometric distributions that differ from conventional point cloud characteristics. To address these challenges, we introduce GausPcgc for Gaussian point cloud geometry compression along with a specialized training dataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud compression into Gaussian compression pipelines, achieving superior compression ratios. The framework complements existing Gaussian compression methods while delivering significant performance improvements. All code, data, and pre-trained models will be publicly released to facilitate further research advances in this field.",
    "arxiv_url": "https://arxiv.org/abs/2505.18197v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18197v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "geometry",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning",
    "authors": [
      "Amine Elhafsi",
      "Daniel Morton",
      "Marco Pavone"
    ],
    "abstract": "Autonomous robots must reason about the physical consequences of their actions to operate effectively in unstructured, real-world environments. We present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D Gaussian Splatting for accurate scene reconstruction, visual foundation models for semantic segmentation, vision-language models for material property inference, and physics simulation for reliable prediction of action outcomes. By integrating these components, SMS enables generalizable physical reasoning and object-centric planning without the need to re-learn foundational physical dynamics. We empirically validate SMS in a billiards-inspired manipulation task and a challenging quadrotor landing scenario, demonstrating robust performance on both simulated domain transfer and real-world experiments. Our results highlight the potential of bridging differentiable rendering for scene reconstruction, foundation models for semantic understanding, and physics-based simulation to achieve physically grounded robot planning across diverse settings.",
    "arxiv_url": "https://arxiv.org/abs/2505.14938v1",
    "pdf_url": "https://arxiv.org/pdf/2505.14938v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image",
    "authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Qingshan Xu",
      "Yuan Zhou",
      "Long Chen",
      "Hanwang Zhang"
    ],
    "abstract": "Personalizing 3D scenes from a single reference image enables intuitive user-guided editing, which requires achieving both multi-view consistency across perspectives and referential consistency with the input image. However, these goals are particularly challenging due to the viewpoint bias caused by the limited perspective provided in a single image. Lacking the mechanisms to effectively expand reference information beyond the original view, existing methods of image-conditioned 3DGS personalization often suffer from this viewpoint bias and struggle to produce consistent results. Therefore, in this paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS), a framework that progressively propagates the single-view reference appearance to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D generation and iterative LoRA fine-tuning to extract and extend the reference appearance, and finally produces faithful multi-view guidance images and the personalized 3DGS outputs through a view-consistent generation process guided by geometric cues. Extensive experiments on real-world scenes show that our CP-GS effectively mitigates the viewpoint bias, achieving high-quality personalization that significantly outperforms existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.14537v3",
    "pdf_url": "https://arxiv.org/pdf/2505.14537v3",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction",
    "authors": [
      "Zhenyu Bao",
      "Qing Li",
      "Guibiao Liao",
      "Zhongyuan Zhao",
      "Kanglin Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention in streamable dynamic novel view synthesis (DNVS) for its photorealistic rendering capability and computational efficiency. Despite much progress in improving rendering quality and optimization strategies, 3DGS-based streamable dynamic scene reconstruction still suffers from flickering artifacts and storage inefficiency, and struggles to model the emerging objects. To tackle this, we introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related 3DGs are implemented according to the motion mask and the clustering-based convex hull algorithm. The rigid deformation is applied to the motion-related 3DGs for modeling the dynamic, and the attention-based optimization on the motion-related 3DGs enables the reconstruction of the emerging objects. As the deformation and optimization are only conducted on the motion-related 3DGs, MGStream avoids flickering artifacts and improves the storage efficiency. Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that MGStream surpasses existing streaming 3DGS-based approaches in terms of rendering quality, training/storage efficiency and temporal consistency. Our code is available at: https://github.com/pcl3dv/MGStream.",
    "arxiv_url": "https://arxiv.org/abs/2505.13839v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13839v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos",
    "authors": [
      "Ruoyu Wang",
      "Yi Ma",
      "Shenghua Gao"
    ],
    "abstract": "Currently almost all state-of-the-art novel view synthesis and reconstruction models rely on calibrated cameras or additional geometric priors for training. These prerequisites significantly limit their applicability to massive uncalibrated data. To alleviate this requirement and unlock the potential for self-supervised training on large-scale uncalibrated videos, we propose a novel two-stage strategy to train a view synthesis model from only raw video frames or multi-view images, without providing camera parameters or other priors. In the first stage, we learn to reconstruct the scene implicitly in a latent space without relying on any explicit 3D representation. Specifically, we predict per-frame latent camera and scene context features, and employ a view synthesis model as a proxy for explicit rendering. This pretraining stage substantially reduces the optimization complexity and encourages the network to learn the underlying 3D consistency in a self-supervised manner. The learned latent camera and implicit scene representation have a large gap compared with the real 3D world. To reduce this gap, we introduce the second stage training by explicitly predicting 3D Gaussian primitives. We additionally apply explicit Gaussian Splatting rendering loss and depth projection loss to align the learned latent representations with physically grounded 3D geometry. In this way, Stage 1 provides a strong initialization and Stage 2 enforces 3D consistency - the two stages are complementary and mutually beneficial. Extensive experiments demonstrate the effectiveness of our approach, achieving high-quality novel view synthesis and accurate camera pose estimation, compared to methods that employ supervision with calibration, pose, or depth information. The code is available at https://github.com/Dwawayu/Pensieve.",
    "arxiv_url": "https://arxiv.org/abs/2505.13440v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13440v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
    "authors": [
      "Seungjun Oh",
      "Younggeun Lee",
      "Hyejin Jeon",
      "Eunbyung Park"
    ],
    "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2505.13215v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13215v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy",
    "authors": [
      "Chenyu Xu",
      "Zhouyu Jin",
      "Chengkang Shen",
      "Hao Zhu",
      "Zhan Ma",
      "Bo Xiong",
      "You Zhou",
      "Xun Cao",
      "Ning Gu"
    ],
    "abstract": "Compared to light-field microscopy (LFM), which enables high-speed volumetric imaging but suffers from non-uniform spatial sampling, Fourier light-field microscopy (FLFM) introduces sub-aperture division at the pupil plane, thereby ensuring spatially invariant sampling and enhancing spatial resolution. Conventional FLFM reconstruction methods, such as Richardson-Lucy (RL) deconvolution, exhibit poor axial resolution and signal degradation due to the ill-posed nature of the inverse problem. While data-driven approaches enhance spatial resolution by leveraging high-quality paired datasets or imposing structural priors, Neural Radiance Fields (NeRF)-based methods employ physics-informed self-supervised learning to overcome these limitations, yet they are hindered by substantial computational costs and memory demands. Therefore, we propose 3D Gaussian Adaptive Tomography (3DGAT) for FLFM, a 3D gaussian splatting based self-supervised learning framework that significantly improves the volumetric reconstruction quality of FLFM while maintaining computational efficiency. Experimental results indicate that our approach achieves higher resolution and improved reconstruction accuracy, highlighting its potential to advance FLFM imaging and broaden its applications in 3D optical microscopy.",
    "arxiv_url": "https://arxiv.org/abs/2505.12875v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12875v1",
    "published_date": "2025-05-19",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy",
    "authors": [
      "Luyao Lei",
      "Shuo Xu",
      "Yifan Bai",
      "Xing Wei"
    ],
    "abstract": "The performance of multi-modal 3D occupancy prediction is limited by ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion strategies and surface detail loss caused by sparse, noisy annotations. The mismatch stems from the heterogeneous scale and distribution of point cloud and image features, leading to biased matching under fixed neighborhood fusion. To address this, we propose a target-scale adaptive, bidirectional symmetric retrieval mechanism. It expands the neighborhood for large targets to enhance context awareness and shrinks it for small ones to improve efficiency and suppress noise, enabling accurate cross-modal feature alignment. This mechanism explicitly establishes spatial correspondences and improves fusion accuracy. For surface detail loss, sparse labels provide limited supervision, resulting in poor predictions for small objects. We introduce an improved volume rendering pipeline based on 3D Gaussian Splatting, which takes fused features as input to render images, applies photometric consistency supervision, and jointly optimizes 2D-3D consistency. This enhances surface detail reconstruction while suppressing noise propagation. In summary, we propose TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy prediction, enhanced by volume rendering supervision. Experiments on the nuScenes and SemanticKITTI benchmarks validate its effectiveness.",
    "arxiv_url": "https://arxiv.org/abs/2505.12693v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12693v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey",
    "authors": [
      "Calvin Galagain",
      "Martyna Poreba",
      "FranÃ§ois Goulette"
    ],
    "abstract": "In embedded systems, robots must perceive and interpret their environment efficiently to operate reliably in real-world conditions. Visual Semantic SLAM (Simultaneous Localization and Mapping) enhances standard SLAM by incorporating semantic information into the map, enabling more informed decision-making. However, implementing such systems on resource-limited hardware involves trade-offs between accuracy, computing efficiency, and power usage.   This paper provides a comparative review of recent Semantic Visual SLAM methods with a focus on their applicability to embedded platforms. We analyze three main types of architectures - Geometric SLAM, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting - and evaluate their performance on constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their accuracy, segmentation quality, memory usage, and energy consumption.   Our results show that methods based on NeRF and Gaussian Splatting achieve high semantic detail but demand substantial computing resources, limiting their use on embedded devices. In contrast, Semantic Geometric SLAM offers a more practical balance between computational cost and accuracy. The review highlights a need for SLAM algorithms that are better adapted to embedded environments, and it discusses key directions for improving their efficiency through algorithm-hardware co-design.",
    "arxiv_url": "https://arxiv.org/abs/2505.12384v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12384v1",
    "published_date": "2025-05-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "semantic",
      "nerf",
      "ar",
      "mapping",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity",
    "authors": [
      "Takuya Ikeda",
      "Sergey Zakharov",
      "Muhammad Zubair Irshad",
      "Istvan Balazs Opra",
      "Shun Iwase",
      "Dian Chen",
      "Mark Tjersland",
      "Robert Lee",
      "Alexandre Dilly",
      "Rares Ambrus",
      "Koichi Nishiwaki"
    ],
    "abstract": "We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.",
    "arxiv_url": "https://arxiv.org/abs/2505.11905v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11905v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos",
    "authors": [
      "Hongyi Zhou",
      "Yulan Guo",
      "Xiaogang Wang",
      "Kai Xu"
    ],
    "abstract": "Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.",
    "arxiv_url": "https://arxiv.org/abs/2505.11868v3",
    "pdf_url": "https://arxiv.org/pdf/2505.11868v3",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "In this work, we argue that Gaussian splatting is a suitable unified representation for autonomous robot navigation in large-scale unstructured outdoor environments. Such environments require representations that can capture complex structures while remaining computationally tractable for real-time navigation. We demonstrate that the dense geometric and photometric information provided by a Gaussian splatting representation is useful for navigation in unstructured environments. Additionally, semantic information can be embedded in the Gaussian map to enable large-scale task-driven navigation. From the lessons learned through our experiments, we highlight several challenges and opportunities arising from the use of such a representation for robot autonomy.",
    "arxiv_url": "https://arxiv.org/abs/2505.11794v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11794v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "semantic",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views",
    "authors": [
      "Abhishek Kashyap",
      "Henrik Andreasson",
      "Todor Stoyanov"
    ],
    "abstract": "Vision based robot manipulation uses cameras to capture one or more images of a scene containing the objects to be manipulated. Taking multiple images can help if any object is occluded from one viewpoint but more visible from another viewpoint. However, the camera has to be moved to a sequence of suitable positions for capturing multiple images, which requires time and may not always be possible, due to reachability constraints. So while additional images can produce more accurate grasp poses due to the extra information available, the time-cost goes up with the number of additional views sampled. Scene representations like Gaussian Splatting are capable of rendering accurate photorealistic virtual images from user-specified novel viewpoints. In this work, we show initial results which indicate that novel view synthesis can provide additional context in generating grasp poses. Our experiments on the Graspnet-1billion dataset show that novel views contributed force-closure grasps in addition to the force-closure grasps obtained from sparsely sampled real views while also improving grasp coverage. In the future we hope this work can be extended to improve grasp extraction from radiance fields constructed with a single input image, using for example diffusion models or generalizable radiance fields.",
    "arxiv_url": "https://arxiv.org/abs/2505.11467v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11467v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Shuangyu Xie",
      "Chung Min Kim",
      "Justin Kerr",
      "Bart M. van Marrewijk",
      "Mieke van Vlaardingen",
      "Tim van Daalen",
      "E. N. van Loo",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Rick van de Zedde",
      "Ken Goldberg"
    ],
    "abstract": "Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/",
    "arxiv_url": "https://arxiv.org/abs/2505.10923v2",
    "pdf_url": "https://arxiv.org/pdf/2505.10923v2",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes",
    "authors": [
      "Jianlin Guo",
      "Haihong Xiao",
      "Wenxiong Kang"
    ],
    "abstract": "Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent performance with its high-quality rendering and real-time speed, especially for objects and small-scale scenes. However, in outdoor scenes, its point-based explicit representation lacks an effective adjustment mechanism, and the millions of Gaussian points required often lead to memory constraints during training. To address these challenges, we propose EA-3DGS, a high-quality real-time rendering method designed for outdoor scenes. First, we introduce a mesh structure to regulate the initialization of Gaussian components by leveraging an adaptive tetrahedral mesh that partitions the grid and initializes Gaussian components on each face, effectively capturing geometric structures in low-texture regions. Second, we propose an efficient Gaussian pruning strategy that evaluates each 3D Gaussian's contribution to the view and prunes accordingly. To retain geometry-critical Gaussian points, we also present a structure-aware densification strategy that densifies Gaussian points in low-curvature regions. Additionally, we employ vector quantization for parameter quantization of Gaussian components, significantly reducing disk space requirements with only a minimal impact on rendering quality. Extensive experiments on 13 scenes, including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \\& Temples, WHU) and five self-collected scenes acquired through UAV photogrammetry measurement from SCUT-CA and plateau regions, further demonstrate the superiority of our method.",
    "arxiv_url": "https://arxiv.org/abs/2505.10787v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10787v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "nerf",
      "ar",
      "geometry",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ControlGS: Consistent Structural Compression Control for Deployment-Aware Gaussian Splatting",
    "authors": [
      "Fengdi Zhang",
      "Yibao Sun",
      "Hongkun Cao",
      "Ruqi Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a highly deployable real-time method for novel view synthesis. In practice, it requires a universal, consistent control mechanism that adjusts the trade-off between rendering quality and model compression without scene-specific tuning, enabling automated deployment across different device performances and communication bandwidths. In this work, we present ControlGS, a control-oriented optimization framework that maps the trade-off between Gaussian count and rendering quality to a continuous, scene-agnostic, and highly responsive control axis. Extensive experiments across a wide range of scene scales and types (from small objects to large outdoor scenes) demonstrate that, by adjusting a globally unified control hyperparameter, ControlGS can flexibly generate models biased toward either structural compactness or high fidelity, regardless of the specific scene scale or complexity, while achieving markedly higher rendering quality with the same or fewer Gaussians compared to potential competing methods. Project page: https://zhang-fengdi.github.io/ControlGS/",
    "arxiv_url": "https://arxiv.org/abs/2505.10473v3",
    "pdf_url": "https://arxiv.org/pdf/2505.10473v3",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "outdoor",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
    "authors": [
      "Xuechang Tu",
      "Lukas Radl",
      "Michael Steiner",
      "Markus Steinberger",
      "Bernhard Kerbl",
      "Fernando de la Torre"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.",
    "arxiv_url": "https://arxiv.org/abs/2505.10144v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10144v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field",
    "authors": [
      "Jinlong Fan",
      "Xuepu Zeng",
      "Jing Zhang",
      "Mingming Gong",
      "Yuxiang Yang",
      "Dacheng Tao"
    ],
    "abstract": "Dynamic scene representation and reconstruction have undergone transformative advances in recent years, catalyzed by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques. While initially developed for static environments, these methodologies have rapidly evolved to address the complexities inherent in 4D dynamic scenes through an expansive body of research. Coupled with innovations in differentiable volumetric rendering, these approaches have significantly enhanced the quality of motion representation and dynamic scene reconstruction, thereby garnering substantial attention from the computer vision and graphics communities. This survey presents a systematic analysis of over 200 papers focused on dynamic scene representation using radiance field, spanning the spectrum from implicit neural representations to explicit Gaussian primitives. We categorize and evaluate these works through multiple critical lenses: motion representation paradigms, reconstruction techniques for varied scene dynamics, auxiliary information integration strategies, and regularization approaches that ensure temporal consistency and physical plausibility. We organize diverse methodological approaches under a unified representational framework, concluding with a critical examination of persistent challenges and promising research directions. By providing this comprehensive overview, we aim to establish a definitive reference for researchers entering this rapidly evolving field while offering experienced practitioners a systematic understanding of both conceptual principles and practical frontiers in dynamic scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2505.10049v2",
    "pdf_url": "https://arxiv.org/pdf/2505.10049v2",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "survey",
      "body",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large-Scale Gaussian Splatting SLAM",
    "authors": [
      "Zhe Xin",
      "Chenyang Wu",
      "Penghui Huang",
      "Yanyong Zhang",
      "Yinian Mao",
      "Guoquan Huang"
    ],
    "abstract": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2505.09915v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09915v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "recognition",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
    "authors": [
      "Justin Yu",
      "Letian Fu",
      "Huang Huang",
      "Karim El-Refai",
      "Rares Andrei Ambrus",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "abstract": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com",
    "arxiv_url": "https://arxiv.org/abs/2505.09601v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09601v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Video Compression using 2D Gaussian Splatting",
    "authors": [
      "Lakshya Gupta",
      "Imran N. Junejo"
    ],
    "abstract": "The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.",
    "arxiv_url": "https://arxiv.org/abs/2505.09324v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09324v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: a vision-based low overhead framework for 3D scene reconstruction",
    "authors": [
      "Yunji Feng",
      "Chengpu Yu",
      "Fengrui Ran",
      "Zhi Yang",
      "Yinni Liu"
    ],
    "abstract": "This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.10578v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10578v1",
    "published_date": "2025-05-14",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting",
    "authors": [
      "Holly Dinkel",
      "Marcel BÃ¼sching",
      "Alberta Longhini",
      "Brian Coltin",
      "Trey Smith",
      "Danica Kragic",
      "MÃ¥rten BjÃ¶rkman",
      "Timothy Bretl"
    ],
    "abstract": "This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.08644v2",
    "pdf_url": "https://arxiv.org/pdf/2505.08644v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FOCI: Trajectory Optimization on Gaussian Splats",
    "authors": [
      "Mario Gomez Andreu",
      "Maximum Wilder-Smith",
      "Victor Klemm",
      "Vaishakh Patil",
      "Jesus Tordesillas",
      "Marco Hutter"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at https://rffr.leggedrobotics.com/works/foci/",
    "arxiv_url": "https://arxiv.org/abs/2505.08510v2",
    "pdf_url": "https://arxiv.org/pdf/2505.08510v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey of 3D Reconstruction with Event Cameras",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Langyi Chen",
      "Haodong Chen",
      "Zeke Zexi Hu",
      "Zhicheng Lu",
      "Ying Zhou",
      "Vera Chung",
      "Qiang Qu",
      "Weidong Cai"
    ],
    "abstract": "Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2505.08438v3",
    "pdf_url": "https://arxiv.org/pdf/2505.08438v3",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "nerf",
      "illumination",
      "geometry",
      "autonomous driving",
      "dynamic",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction",
    "authors": [
      "He Huang",
      "Qi Yang",
      "Mufan Liu",
      "Yiling Xu",
      "Zhu Li"
    ],
    "abstract": "Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.",
    "arxiv_url": "https://arxiv.org/abs/2505.08196v1",
    "pdf_url": "https://arxiv.org/pdf/2505.08196v1",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "compact",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAG: Scalable Language-Augmented Gaussian Splatting",
    "authors": [
      "Laszlo Szilagyi",
      "Francis Engelmann",
      "Jeannette Bohg"
    ],
    "abstract": "Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2505.08124v2",
    "pdf_url": "https://arxiv.org/pdf/2505.08124v2",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "large scene",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ],
    "abstract": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream",
    "arxiv_url": "https://arxiv.org/abs/2505.07539v1",
    "pdf_url": "https://arxiv.org/pdf/2505.07539v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "4d",
      "ar",
      "fast",
      "real-time rendering",
      "compression",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
    "authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias NieÃner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win",
    "arxiv_url": "https://arxiv.org/abs/2505.07396v2",
    "pdf_url": "https://arxiv.org/pdf/2505.07396v2",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "outdoor",
      "nerf",
      "ar",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian",
    "authors": [
      "Shijie Lian",
      "Ziyi Zhang",
      "Laurence Tianruo Yang and",
      "Mengyu Ren",
      "Debin Liu",
      "Hua Li"
    ],
    "abstract": "Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications",
    "arxiv_url": "https://arxiv.org/abs/2505.08811v1",
    "pdf_url": "https://arxiv.org/pdf/2505.08811v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "nerf",
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes",
    "authors": [
      "Xijie Yang",
      "Linning Xu",
      "Lihan Jiang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.",
    "arxiv_url": "https://arxiv.org/abs/2505.06523v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06523v1",
    "published_date": "2025-05-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "real-time rendering",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling",
    "authors": [
      "Gengyan Li",
      "Paulo Gotardo",
      "Timo Bolkart",
      "Stephan Garbin",
      "Kripasindhu Sarkar",
      "Abhimitra Meka",
      "Alexandros Lattas",
      "Thabo Beeler"
    ],
    "abstract": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.",
    "arxiv_url": "https://arxiv.org/abs/2505.05672v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05672v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes",
    "authors": [
      "Mark C. Eid",
      "Ana I. L. Namburete",
      "JoÃ£o F. Henriques"
    ],
    "abstract": "Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2505.05643v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05643v1",
    "published_date": "2025-05-08",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "ar",
      "fast",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization",
    "authors": [
      "Yueh-Cheng Liu",
      "Lukas HÃ¶llein",
      "Matthias NieÃner",
      "Angela Dai"
    ],
    "abstract": "Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.05591v2",
    "pdf_url": "https://arxiv.org/pdf/2505.05591v2",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "robotics",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
    "arxiv_url": "https://arxiv.org/abs/2505.05587v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05587v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation",
    "authors": [
      "Yonwoo Choi"
    ],
    "abstract": "Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.",
    "arxiv_url": "https://arxiv.org/abs/2505.05475v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05475v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "human",
      "real-time rendering",
      "avatar",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ],
    "abstract": "We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf",
    "arxiv_url": "https://arxiv.org/abs/2505.05356v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05356v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting",
    "authors": [
      "Feng Yang",
      "Wenliang Qian",
      "Wangmeng Zuo",
      "Hui Li"
    ],
    "abstract": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2505.04262v2",
    "pdf_url": "https://arxiv.org/pdf/2505.04262v2",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ],
    "abstract": "Neural rendering techniques have made substantial progress in generating photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has achieved high quality novel view synthesis as well as fast rendering speed. However, 3D Gaussians lack proficiency in defining accurate 3D geometric structures despite their explicit primitive representations. This is due to the fact that Gaussian's attributes are primarily tailored and fine-tuned for rendering diverse 2D images by their anisotropic nature. To pave the way for efficient 3D reconstruction, we present Spherical Gaussians, a simple and effective representation for 3D geometric boundaries, from which we can directly reconstruct 3D feature curves from a set of calibrated multi-view images. Spherical Gaussians is optimized from grid initialization with a view-based rendering loss, where a 2D edge map is rendered at a specific view and then compared to the ground-truth edge map extracted from the corresponding image, without the need for any 3D guidance or supervision. Given Spherical Gaussians serve as intermedia for the robust edge representation, we further introduce a novel optimization-based algorithm called SGCR to directly extract accurate parametric curves from aligned Spherical Gaussians. We demonstrate that SGCR outperforms existing state-of-the-art methods in 3D edge reconstruction while enjoying great efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2505.04668v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04668v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "ar",
      "fast",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes",
    "authors": [
      "Feng Xiao",
      "Hongbin Xu",
      "Wanlin Liang",
      "Wenxiong Kang"
    ],
    "abstract": "The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.",
    "arxiv_url": "https://arxiv.org/abs/2505.04659v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04659v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "fast",
      "gaussian splatting",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Data Compression with Mixture of Priors",
    "authors": [
      "Lei Liu",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.",
    "arxiv_url": "https://arxiv.org/abs/2505.03310v2",
    "pdf_url": "https://arxiv.org/pdf/2505.03310v2",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "compression",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
    "authors": [
      "Shubhendu Jena",
      "Amine Ouasfi",
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",
    "arxiv_url": "https://arxiv.org/abs/2505.02178v4",
    "pdf_url": "https://arxiv.org/pdf/2505.02178v4",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "fast",
      "sparse view",
      "gaussian splatting",
      "shape reconstruction",
      "face",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting",
    "authors": [
      "Shubhendu Jena",
      "Shishir Reddy Vutukur",
      "Adnane Boukhayma"
    ],
    "abstract": "Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",
    "arxiv_url": "https://arxiv.org/abs/2505.02175v1",
    "pdf_url": "https://arxiv.org/pdf/2505.02175v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "shape reconstruction",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction",
    "authors": [
      "Zhihao Tang",
      "Shenghao Yang",
      "Hongtao Zhang",
      "Mingbo Zhao"
    ],
    "abstract": "Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.",
    "arxiv_url": "https://arxiv.org/abs/2505.02126v2",
    "pdf_url": "https://arxiv.org/pdf/2505.02126v2",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "real-time rendering",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2505.02108v1",
    "pdf_url": "https://arxiv.org/pdf/2505.02108v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "ar",
      "human",
      "gaussian splatting",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder",
    "authors": [
      "Qi Yang",
      "Le Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "abstract": "Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS.",
    "arxiv_url": "https://arxiv.org/abs/2505.01938v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01938v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
    "authors": [
      "Anushka Agarwal",
      "Muhammad Yusuf Hassan",
      "Talha Chafekar"
    ],
    "abstract": "We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2505.01928v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01928v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual enhancement and 3D representation for underwater scenes: a review",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Brett Seymour",
      "Evan Kovacs",
      "John Ellerbrock",
      "Dave Blackham",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in   computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an   in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and   3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian   Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple   benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.",
    "arxiv_url": "https://arxiv.org/abs/2505.01869v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01869v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "ar",
      "lighting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting",
    "authors": [
      "Junhao Shi",
      "Jisheng Xu",
      "Jianping He",
      "Zhiliang Lin"
    ],
    "abstract": "Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.",
    "arxiv_url": "https://arxiv.org/abs/2505.01799v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01799v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FalconWing: An Ultra-Light Indoor Fixed-Wing UAV Platform for Vision-Based Autonomy",
    "authors": [
      "Yan Miao",
      "Will Shen",
      "Hang Cui",
      "Sayan Mitra"
    ],
    "abstract": "We introduce FalconWing, an ultra-light (150 g) indoor fixed-wing UAV platform for vision-based autonomy. Controlled indoor environment enables year-round repeatable UAV experiment but imposes strict weight and maneuverability limits on the UAV, motivating our ultra-light FalconWing design. FalconWing couples a lightweight hardware stack (137g airframe with a 9g camera) and offboard computation with a software stack featuring a photorealistic 3D Gaussian Splat (GSplat) simulator for developing and evaluating vision-based controllers. We validate FalconWing on two challenging vision-based aerial case studies. In the leader-follower case study, our best vision-based controller, trained via imitation learning on GSplat-rendered data augmented with domain randomization, achieves 100% tracking success across 3 types of leader maneuvers over 30 trials and shows robustness to leader's appearance shifts in simulation. In the autonomous landing case study, our vision-based controller trained purely in simulation transfers zero-shot to real hardware, achieving an 80% success rate over ten landing trials. We will release hardware designs, GSplat scenes, and dynamics models upon publication to make FalconWing an open-source flight kit for engineering students and research labs.",
    "arxiv_url": "https://arxiv.org/abs/2505.01383v3",
    "pdf_url": "https://arxiv.org/pdf/2505.01383v3",
    "published_date": "2025-05-02",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "lightweight",
      "3d gaussian",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos",
    "authors": [
      "Xia Yuan",
      "Hai Yuan",
      "Wenyi Ge",
      "Ying Fu",
      "Xi Wu",
      "Guanyu Xing"
    ],
    "abstract": "High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2505.00421v1",
    "pdf_url": "https://arxiv.org/pdf/2505.00421v1",
    "published_date": "2025-05-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "ar",
      "human",
      "avatar",
      "dynamic",
      "gaussian splatting",
      "animation",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation",
    "authors": [
      "Haiyang Zhou",
      "Wangbo Yu",
      "Jiawen Guan",
      "Xinhua Cheng",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.",
    "arxiv_url": "https://arxiv.org/abs/2504.21650v2",
    "pdf_url": "https://arxiv.org/pdf/2504.21650v2",
    "published_date": "2025-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "vr",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond",
    "authors": [
      "Jiajia Li",
      "Xinda Qi",
      "Seyed Hamidreza Nabaei",
      "Meiqi Liu",
      "Dong Chen",
      "Xin Zhang",
      "Xunyuan Yin",
      "Zhaojian Li"
    ],
    "abstract": "Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.",
    "arxiv_url": "https://arxiv.org/abs/2505.00737v1",
    "pdf_url": "https://arxiv.org/pdf/2505.00737v1",
    "published_date": "2025-04-30",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "survey",
      "outdoor",
      "nerf",
      "ar",
      "geometry",
      "sparse view",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
    "authors": [
      "Jiaxin Hong",
      "Sixu Chen",
      "Shuoyang Sun",
      "Hongyao Yu",
      "Hao Fang",
      "Yuqi Tan",
      "Bin Chen",
      "Shuhan Qi",
      "Jiawei Li"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.",
    "arxiv_url": "https://arxiv.org/abs/2504.20829v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20829v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction",
    "authors": [
      "Yuhan Xie",
      "Yixi Cai",
      "Yinqiang Zhang",
      "Lei Yang",
      "Jia Pan"
    ],
    "abstract": "This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",
    "arxiv_url": "https://arxiv.org/abs/2504.21067v1",
    "pdf_url": "https://arxiv.org/pdf/2504.21067v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian",
    "authors": [
      "Hao Tian",
      "Rui Liu",
      "Wen Shen",
      "Yilong Hu",
      "Zhihao Zheng",
      "Xiaolin Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.",
    "arxiv_url": "https://arxiv.org/abs/2504.20607v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20607v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "ar",
      "fast",
      "human",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ],
    "abstract": "Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.",
    "arxiv_url": "https://arxiv.org/abs/2504.20403v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20403v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "geometry",
      "avatar",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting",
    "authors": [
      "Jongwon Lee",
      "Timothy Bretl"
    ],
    "abstract": "In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55Â° in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5Â° in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.",
    "arxiv_url": "https://arxiv.org/abs/2504.20379v2",
    "pdf_url": "https://arxiv.org/pdf/2504.20379v2",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "fast",
      "localization",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ],
    "abstract": "We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach.",
    "arxiv_url": "https://arxiv.org/abs/2504.20378v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20378v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "fast",
      "sparse view",
      "sparse-view",
      "gaussian splatting",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics",
    "authors": [
      "Yunfei Wan",
      "Jianheng Liu",
      "Chunran Zheng",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "abstract": "In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.",
    "arxiv_url": "https://arxiv.org/abs/2504.19938v3",
    "pdf_url": "https://arxiv.org/pdf/2504.19938v3",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes",
    "authors": [
      "Mohammad Altillawi",
      "Fengyi Shen",
      "Liudi Yang",
      "Sai Manoj Prakhya",
      "Ziyuan Liu"
    ],
    "abstract": "Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability.",
    "arxiv_url": "https://arxiv.org/abs/2504.19557v1",
    "pdf_url": "https://arxiv.org/pdf/2504.19557v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field",
    "authors": [
      "Zuxing Lu",
      "Xin Yuan",
      "Shaowen Yang",
      "Jingyu Liu",
      "Changyin Sun"
    ],
    "abstract": "Semantic-aware 3D scene reconstruction is essential for autonomous robots to perform complex interactions. Semantic SLAM, an online approach, integrates pose tracking, geometric reconstruction, and semantic mapping into a unified framework, shows significant potential. However, existing systems, which rely on 2D ground truth priors for supervision, are often limited by the sparsity and noise of these signals in real-world environments. To address this challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D Gaussian Splatting that leverages feature fields to achieve joint rendering of appearance, geometry, and N-dimensional semantic features. By independently optimizing feature gradients, our method supports semantic reconstruction using various forms of 2D priors, particularly sparse and noisy signals. Experimental results demonstrate that our approach outperforms previous methods in both tracking accuracy and photorealistic rendering quality. When utilizing 2D ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation performance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with only marginal performance degradation.",
    "arxiv_url": "https://arxiv.org/abs/2504.19409v2",
    "pdf_url": "https://arxiv.org/pdf/2504.19409v2",
    "published_date": "2025-04-28",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "geometry",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "segmentation",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting",
    "authors": [
      "Xiaofeng Jin",
      "Yan Fang",
      "Matteo Frosi",
      "Jianfei Ge",
      "Jiangjian Xiao",
      "Matteo Matteucci"
    ],
    "abstract": "Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360Â° views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.",
    "arxiv_url": "https://arxiv.org/abs/2504.19261v1",
    "pdf_url": "https://arxiv.org/pdf/2504.19261v1",
    "published_date": "2025-04-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression",
    "authors": [
      "Zicong Chen",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Lei Liu",
      "Dong Xu"
    ],
    "abstract": "Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints. Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively. Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding. Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook. By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.",
    "arxiv_url": "https://arxiv.org/abs/2504.18925v2",
    "pdf_url": "https://arxiv.org/pdf/2504.18925v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians",
    "authors": [
      "Letian Huang",
      "Dongwei Ye",
      "Jialin Dan",
      "Chengzhi Tao",
      "Huiwen Liu",
      "Kun Zhou",
      "Bo Ren",
      "Yuanqi Li",
      "Yanwen Guo",
      "Jie Guo"
    ],
    "abstract": "The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",
    "arxiv_url": "https://arxiv.org/abs/2504.18768v2",
    "pdf_url": "https://arxiv.org/pdf/2504.18768v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGS-DR: Deferred Reflections and Residual Shading in 2D Gaussian Splatting",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "In this work, we address specular appearance in inverse rendering using 2D Gaussian splatting with deferred shading and argue for a refinement stage to improve specular detail, thereby bridging the gap with reconstruction-only methods. Our pipeline estimates editable material properties and environment illumination while employing a directional residual pass that captures leftover view-dependent effects for further refining novel view synthesis. In contrast to per-Gaussian shading with shortest-axis normals and normal residuals, which tends to result in more noisy geometry and specular appearance, a pixel-deferred surfel formulation with specular residuals yields sharper highlights, cleaner materials, and improved editability. We evaluate our approach on rendering and reconstruction quality on three popular datasets featuring glossy objects, and also demonstrate high-quality relighting and material editing.",
    "arxiv_url": "https://arxiv.org/abs/2504.18468v6",
    "pdf_url": "https://arxiv.org/pdf/2504.18468v6",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "relighting",
      "geometry",
      "ar",
      "lighting",
      "gaussian splatting",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models",
    "authors": [
      "Michel Gokan Khan",
      "Renan Guarese",
      "Fabian Johnson",
      "Xi Vincent Wang",
      "Anders Bergman",
      "Benjamin Edvinsson",
      "Mario Romero",
      "JÃ©rÃ©my Vachier",
      "Jan Kronqvist"
    ],
    "abstract": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.",
    "arxiv_url": "https://arxiv.org/abs/2504.18165v1",
    "pdf_url": "https://arxiv.org/pdf/2504.18165v1",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting",
    "authors": [
      "Kaiyuan Tang",
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "In volume visualization, users can interactively explore the three-dimensional data by specifying color and opacity mappings in the transfer function (TF) or adjusting lighting parameters, facilitating meaningful interpretation of the underlying structure. However, rendering large-scale volumes demands powerful GPUs and high-speed memory access for real-time performance. While existing novel view synthesis (NVS) methods offer faster rendering speeds with lower hardware requirements, the visible parts of a reconstructed scene are fixed and constrained by preset TF settings, significantly limiting user exploration. This paper introduces inverse volume rendering via Gaussian splatting (iVR-GS), an innovative NVS method that reduces the rendering cost while enabling scene editing for interactive volume exploration. Specifically, we compose multiple iVR-GS models associated with basic TFs covering disjoint visible parts to make the entire volumetric scene visible. Each basic model contains a collection of 3D editable Gaussians, where each Gaussian is a 3D spatial point that supports real-time scene rendering and editing. We demonstrate the superior reconstruction quality and composability of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on various volume datasets. The code is available at https://github.com/TouKaienn/iVR-GS.",
    "arxiv_url": "https://arxiv.org/abs/2504.17954v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17954v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "vr",
      "ar",
      "fast",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Casual3DHDR: Deblurring High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos",
    "authors": [
      "Shucheng Gong",
      "Lingzhe Zhao",
      "Wenpu Li",
      "Hong Xie",
      "Yin Zhang",
      "Shiyu Zhao",
      "Peidong Liu"
    ],
    "abstract": "Photo-realistic novel view synthesis from multi-view images, such as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), has gained significant attention for its superior performance. However, most existing methods rely on low dynamic range (LDR) images, limiting their ability to capture detailed scenes in high-contrast environments. While some prior works address high dynamic range (HDR) scene reconstruction, they typically require multi-view sharp images with varying exposure times captured at fixed camera positions, which is time-consuming and impractical. To make data acquisition more flexible, we propose \\textbf{Casual3DHDR}, a robust one-stage method that reconstructs 3D HDR scenes from casually-captured auto-exposure (AE) videos, even under severe motion blur and unknown, varying exposure times. Our approach integrates a continuous-time camera trajectory into a unified physical imaging model, jointly optimizing exposure times, camera trajectory, and the camera response function (CRF). Extensive experiments on synthetic and real-world datasets demonstrate that \\textbf{Casual3DHDR} outperforms existing methods in robustness and rendering quality. Our source code and dataset will be available at https://lingzhezhao.github.io/CasualHDRSplat/",
    "arxiv_url": "https://arxiv.org/abs/2504.17728v3",
    "pdf_url": "https://arxiv.org/pdf/2504.17728v3",
    "published_date": "2025-04-24",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting is an Effective Data Generator for 3D Object Detection",
    "authors": [
      "Farhad G. Zanjani",
      "Davide Abati",
      "Auke Wiggers",
      "Dimitris Kalatzis",
      "Jens Petersen",
      "Hong Cai",
      "Amirhossein Habibian"
    ],
    "abstract": "We investigate data augmentation for 3D object detection in autonomous driving. We utilize recent advancements in 3D reconstruction based on Gaussian Splatting for 3D object placement in driving scenes. Unlike existing diffusion-based methods that synthesize images conditioned on BEV layouts, our approach places 3D objects directly in the reconstructed 3D space with explicitly imposed geometric transformations. This ensures both the physical plausibility of object placement and highly accurate 3D pose and position annotations.   Our experiments demonstrate that even by integrating a limited number of external 3D objects into real scenes, the augmented data significantly enhances 3D object detection performance and outperforms existing diffusion-based 3D augmentation for object detection. Extensive testing on the nuScenes dataset reveals that imposing high geometric diversity in object placement has a greater impact compared to the appearance diversity of objects. Additionally, we show that generating hard examples, either by maximizing detection loss or imposing high visual occlusion in camera images, does not lead to more efficient 3D data augmentation for camera-based 3D object detection in autonomous driving.",
    "arxiv_url": "https://arxiv.org/abs/2504.16740v1",
    "pdf_url": "https://arxiv.org/pdf/2504.16740v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "autonomous driving",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation",
    "authors": [
      "Wenxuan Li",
      "Hang Zhao",
      "Zhiyuan Yu",
      "Yu Du",
      "Qin Zou",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "abstract": "While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.",
    "arxiv_url": "https://arxiv.org/abs/2504.16693v2",
    "pdf_url": "https://arxiv.org/pdf/2504.16693v2",
    "published_date": "2025-04-23",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "few-shot",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration",
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Martin R. Oswald",
      "Stefano Mattoccia"
    ],
    "abstract": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.",
    "arxiv_url": "https://arxiv.org/abs/2504.16545v1",
    "pdf_url": "https://arxiv.org/pdf/2504.16545v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "geometry",
      "mapping",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning",
    "authors": [
      "Mingxuan Cui",
      "Qing Guo",
      "Yuyi Wang",
      "Hongkai Yu",
      "Di Lin",
      "Qin Zou",
      "Ming-Ming Cheng",
      "Xi Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.",
    "arxiv_url": "https://arxiv.org/abs/2504.17815v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17815v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "nerf",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos",
    "authors": [
      "Yuxin Yao",
      "Yan Zhang",
      "Zhening Huang",
      "Joan Lasenby"
    ],
    "abstract": "Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS",
    "arxiv_url": "https://arxiv.org/abs/2504.17810v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17810v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians",
    "authors": [
      "Cailin Zhuang",
      "Yaoqi Hu",
      "Xuanyang Zhang",
      "Wei Cheng",
      "Jiacheng Bao",
      "Shengqi Liu",
      "Yiying Yang",
      "Xianfang Zeng",
      "Gang Yu",
      "Ming Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.",
    "arxiv_url": "https://arxiv.org/abs/2504.15281v1",
    "pdf_url": "https://arxiv.org/pdf/2504.15281v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "nerf",
      "ar",
      "human",
      "real-time rendering",
      "dynamic",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "quality enhancement"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Immersive Teleoperation Framework for Locomanipulation Tasks",
    "authors": [
      "Takuya Boehringer",
      "Jonathan Embley-Riches",
      "Karim Hammoud",
      "Valerio Modugno",
      "Dimitrios Kanoulas"
    ],
    "abstract": "Recent advancements in robotic loco-manipulation have leveraged Virtual Reality (VR) to enhance the precision and immersiveness of teleoperation systems, significantly outperforming traditional methods reliant on 2D camera feeds and joystick controls. Despite these advancements, challenges remain, particularly concerning user experience across different setups. This paper introduces a novel VR-based teleoperation framework designed for a robotic manipulator integrated onto a mobile platform. Central to our approach is the application of Gaussian splatting, a technique that abstracts the manipulable scene into a VR environment, thereby enabling more intuitive and immersive interactions. Users can navigate and manipulate within the virtual scene as if interacting with a real robot, enhancing both the engagement and efficacy of teleoperation tasks. An extensive user study validates our approach, demonstrating significant usability and efficiency improvements. Two-thirds (66%) of participants completed tasks faster, achieving an average time reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface overall, with unanimous (100%) recommendations for future use, highlighting improvements in precision, responsiveness, and situational awareness. Finally, we demonstrate the effectiveness of our framework through real-world experiments in two distinct application scenarios, showcasing the practical capabilities and versatility of the Splat-based VR interface.",
    "arxiv_url": "https://arxiv.org/abs/2504.15229v1",
    "pdf_url": "https://arxiv.org/pdf/2504.15229v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "fast",
      "lighting",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video",
    "authors": [
      "Minh-Quan Viet Bui",
      "Jongmin Park",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "We present MoBGS, a novel motion deblurring 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method using a proposed Blur-adaptive Neural Ordinary Differential Equation (ODE) solver for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both a global camera and local object motions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent methods, achieving state-of-the-art performance for dynamic NVS under motion blur.",
    "arxiv_url": "https://arxiv.org/abs/2504.15122v4",
    "pdf_url": "https://arxiv.org/pdf/2504.15122v4",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays",
    "authors": [
      "Sascha Jecklin",
      "Aidana Massalimova",
      "Ruyi Zha",
      "Lilian Calvet",
      "Christoph J. Laux",
      "Mazda Farshad",
      "Philipp FÃ¼rnstahl"
    ],
    "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems. Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems. However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions. Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements. While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored. This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions. We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality. Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies. We evaluated our approach using an ex-vivo dataset. Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs. This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation.",
    "arxiv_url": "https://arxiv.org/abs/2504.14699v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14699v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation",
    "authors": [
      "Junyuan Fang",
      "Zihan Wang",
      "Yejun Zhang",
      "Shuzhe Wang",
      "Iaroslav Melekhov",
      "Juho Kannala"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive zero-shot transfer capabilities in image-level visual perception tasks. However, they fall short in 3D instance-level segmentation tasks that require accurate localization and recognition of individual objects. To bridge this gap, we introduce a novel 3D Gaussian Splatting based hard visual prompting approach that leverages camera interpolation to generate diverse viewpoints around target objects without any 2D-3D optimization or fine-tuning. Our method simulates realistic 3D perspectives, effectively augmenting existing hard visual prompts by enforcing geometric consistency across viewpoints. This training-free strategy seamlessly integrates with prior hard visual prompts, enriching object-descriptive features and enabling VLMs to achieve more robust and accurate 3D instance segmentation in diverse 3D scenes.",
    "arxiv_url": "https://arxiv.org/abs/2504.14638v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14638v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "ar",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
    "authors": [
      "Lifeng Lin",
      "Rongfeng Lu",
      "Quan Chen",
      "Haofan Ren",
      "Ming Lu",
      "Yaoqi Sun",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in practical 3D reconstruction applications. Recently, many methods based on the 3D Gaussian Splatting (3DGS) framework have been proposed to address sparse-view 3D reconstruction. Although these methods have made considerable advancements, they still show significant issues with overfitting. To reduce the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number Control (VGNC) approach based on generative novel view synthesis (NVS) models. To the best of our knowledge, this is the first attempt to alleviate the overfitting issue of sparse-view 3DGS with generative validation images. Specifically, we first introduce a validation image generation method based on a generative NVS model. We then propose a Gaussian number control strategy that utilizes generated validation images to determine the optimal Gaussian numbers, thereby reducing the issue of overfitting. We conducted detailed experiments on various sparse-view 3DGS baselines and datasets to evaluate the effectiveness of VGNC. Extensive experiments show that our approach not only reduces overfitting but also improves rendering quality on the test set while decreasing the number of Gaussian points. This reduction lowers storage demands and accelerates both training and rendering. The code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2504.14548v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14548v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding",
    "authors": [
      "Junyan Su",
      "Baozhu Zhao",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point features with anchor embeddings has significantly enhanced the performance of newer 3DGS variants. While significant advances have been made, it is still challenging to boost rendering performance. Feature embeddings have difficulty accurately representing colors from different perspectives under varying lighting conditions, which leads to a washed-out appearance. Another reason is the lack of a proper densification strategy that prevents Gaussian point growth in thinly initialized areas, resulting in blurriness and needle-shaped artifacts. To address them, we propose Metamon-GS, from innovative viewpoints of variance-guided densification strategy and multi-level hash grid. The densification strategy guided by variance specifically targets Gaussians with high gradient variance in pixels and compensates for the importance of regions with extra Gaussians to improve reconstruction. The latter studies implicit global lighting conditions and accurately interprets color from different perspectives and feature embeddings. Our thorough experiments on publicly available datasets show that Metamon-GS surpasses its baseline model and previous versions, delivering superior quality in rendering novel views.",
    "arxiv_url": "https://arxiv.org/abs/2504.14460v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14460v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image",
    "authors": [
      "Chen Guo",
      "Zhuo Su",
      "Jian Wang",
      "Shuang Li",
      "Xu Chang",
      "Zhaohu Li",
      "Yang Zhao",
      "Guidong Wang",
      "Ruqi Huang"
    ],
    "abstract": "Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.",
    "arxiv_url": "https://arxiv.org/abs/2504.14373v2",
    "pdf_url": "https://arxiv.org/pdf/2504.14373v2",
    "published_date": "2025-04-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "human",
      "avatar",
      "dynamic",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM",
    "authors": [
      "Samuel Cerezo",
      "Gaetano Meli",
      "TomÃ¡s Berriel Martins",
      "Kirill Safronov",
      "Javier Civera"
    ],
    "abstract": "Models and methods originally developed for Novel View Synthesis and Scene Rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as sequential operations and, in many settings, multi-modality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. Additionally, the data are often collected using sensors which are handheld or mounted on drones or mobile robots, which complicates the accurate reproduction of sensor motions. To bridge these gaps, we introduce SLAM&Render, a novel dataset designed to benchmark methods in the intersection between SLAM, Novel View Rendering and Gaussian Splatting. Recorded with a robot manipulator, it uniquely includes 40 sequences with time-synchronized RGB-D images, IMU readings, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of recent integrations of SLAM paradigms within robotic applications. The dataset features five setups with consumer and industrial objects under four controlled lighting conditions, each with separate training and test trajectories. All sequences are static with different levels of object rearrangements and occlusions. Our experimental results, obtained with several baselines from the literature, validate SLAM&Render as a relevant benchmark for this emerging research area.",
    "arxiv_url": "https://arxiv.org/abs/2504.13713v4",
    "pdf_url": "https://arxiv.org/pdf/2504.13713v4",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "illumination",
      "mapping",
      "ar",
      "localization",
      "neural rendering",
      "lighting",
      "gaussian splatting",
      "motion",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Green Robotic Mixed Reality with Gaussian Splatting",
    "authors": [
      "Chenxuan Liu",
      "He Li",
      "Zongze Li",
      "Shuai Wang",
      "Wei Xu",
      "Kejiang Ye",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Realizing green communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images at high frequencies through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS model which enables the simulator to opportunistically render a photo-realistic view from the robot's pose, thereby reducing the need for excessive image uploads. Since the GS model may involve discrepancies compared to the actual environments, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation across different frames. The GSCLO problem is solved by an accelerated penalty optimization (APO) algorithm. Experiments demonstrate that the proposed GSRMR reduces the communication energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with APO outperforms extensive baseline schemes, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM).",
    "arxiv_url": "https://arxiv.org/abs/2504.13697v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13697v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framework EG-Gaussian, which utilizes epipolar geometry and graph networks for 3D scene reconstruction. Initially, we integrate epipolar geometry into the 3DGS initialization phase to enhance initial 3DGS point construction. Then, we specifically design a graph learning module to refine 3DGS spatial features, in which we incorporate both spatial coordinates and angular relationships among neighboring points. Experiments on indoor and outdoor benchmark datasets demonstrate that our approach significantly improves reconstruction accuracy compared to 3DGS-based methods.",
    "arxiv_url": "https://arxiv.org/abs/2504.13540v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13540v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "ar",
      "geometry",
      "sparse-view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for Volume Rendering",
    "authors": [
      "Landon Dyken",
      "Andres Sewell",
      "Will Usher",
      "Steve Petruzza",
      "Sidharth Kumar"
    ],
    "abstract": "While HPC resources are increasingly being used to produce adaptively refined or unstructured volume datasets, current research in applying machine learning-based representation to visualization has largely ignored this type of data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D Gaussian-based representation for scientific volume visualization focused on unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that store view-dependent color and opacity for each Gaussian, VEG decouple the visual appearance from the data representation by encoding only scalar values, enabling transfer-function-agnostic rendering of 3DGS models for interactive scientific visualization. VEG are directly initialized from volume datasets, eliminating the need for structure-from-motion pipelines like COLMAP. To ensure complete scalar field coverage, we introduce an opacity-guided training strategy, using differentiable rendering with multiple transfer functions to optimize our data representation. This allows VEG to preserve fine features across the full scalar range of a dataset while remaining independent of any specific transfer function. Each Gaussian is scaled and rotated to adapt to local geometry, allowing for efficient representation of unstructured meshes without storing mesh connectivity and while using far fewer primitives. Across a diverse set of data, VEG achieve high reconstruction quality, compress large volume datasets by up to 3600x, and support lightning-fast rendering on commodity GPUs, enabling interactive visualization of large-scale structured and unstructured volumes.",
    "arxiv_url": "https://arxiv.org/abs/2504.13339v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13339v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation",
    "authors": [
      "Sizhe Yang",
      "Wenye Yu",
      "Jia Zeng",
      "Jun Lv",
      "Kerui Ren",
      "Cewu Lu",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world.",
    "arxiv_url": "https://arxiv.org/abs/2504.13175v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13175v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos",
    "authors": [
      "Zetong Zhang",
      "Manuel Kaufmann",
      "Lixin Xue",
      "Jie Song",
      "Martin R. Oswald"
    ],
    "abstract": "Creating a photorealistic scene and human reconstruction from a single monocular in-the-wild video figures prominently in the perception of a human-centric 3D world. Recent neural rendering advances have enabled holistic human-scene reconstruction but require pre-calibrated camera and human poses, and days of training time. In this work, we introduce a novel unified framework that simultaneously performs camera tracking, human pose estimation and human-scene reconstruction in an online fashion. 3D Gaussian Splatting is utilized to learn Gaussian primitives for humans and scenes efficiently, and reconstruction-based camera tracking and human pose estimation modules are designed to enable holistic understanding and effective disentanglement of pose and appearance. Specifically, we design a human deformation module to reconstruct the details and enhance generalizability to out-of-distribution poses faithfully. Aiming to learn the spatial correlation between human and scene accurately, we introduce occlusion-aware human silhouette rendering and monocular geometric priors, which further improve reconstruction quality. Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par performance with existing methods in camera tracking, human pose estimation, novel view synthesis and runtime. Our project page is at https://eth-ait.github.io/ODHSR.",
    "arxiv_url": "https://arxiv.org/abs/2504.13167v2",
    "pdf_url": "https://arxiv.org/pdf/2504.13167v2",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "deformation",
      "ar",
      "human",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital Twin Generation from Visual Data: A Survey",
    "authors": [
      "Andrew Melnik",
      "Benjamin Alt",
      "Giang Nguyen",
      "Artur Wilkowski",
      "Maciej StefaÅczyk",
      "Qirui Wu",
      "Sinan Harms",
      "Helge Rhodin",
      "Manolis Savva",
      "Michael Beetz"
    ],
    "abstract": "This survey explores recent developments in generating digital twins from videos. Such digital twins can be used for robotics application, media content creation, or design and construction works. We analyze various approaches, including 3D Gaussian Splatting, generative in-painting, semantic segmentation, and foundation models highlighting their advantages and limitations. Additionally, we discuss challenges such as occlusions, lighting variations, and scalability, as well as potential future research directions. This survey aims to provide a comprehensive overview of state-of-the-art methodologies and their implications for real-world applications. Awesome list: https://github.com/ndrwmlnk/awesome-digital-twins",
    "arxiv_url": "https://arxiv.org/abs/2504.13159v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13159v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "semantic",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting with Superpoint Graphs",
    "authors": [
      "Shaohui Dai",
      "Yansong Qu",
      "Zheyan Li",
      "Xinyang Li",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "abstract": "Bridging natural language and 3D geometry is a crucial step toward flexible, language-driven scene understanding. While recent advances in 3D Gaussian Splatting (3DGS) have enabled fast and high-quality scene reconstruction, research has also explored incorporating open-vocabulary understanding into 3DGS. However, most existing methods require iterative optimization over per-view 2D semantic feature maps, which not only results in inefficiencies but also leads to inconsistent 3D semantics across views. To address these limitations, we introduce a training-free framework that constructs a superpoint graph directly from Gaussian primitives. The superpoint graph partitions the scene into spatially compact and semantically coherent regions, forming view-consistent 3D entities and providing a structured foundation for open-vocabulary understanding. Based on the graph structure, we design an efficient reprojection strategy that lifts 2D semantic features onto the superpoints, avoiding costly multi-view iterative training. The resulting representation ensures strong 3D semantic coherence and naturally supports hierarchical understanding, enabling both coarse- and fine-grained open-vocabulary perception within a unified semantic field. Extensive experiments demonstrate that our method achieves state-of-the-art open-vocabulary segmentation performance, with semantic field reconstruction completed over $30\\times$ faster. Our code will be available at https://github.com/Atrovast/THGS.",
    "arxiv_url": "https://arxiv.org/abs/2504.13153v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13153v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "compact",
      "semantic",
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation",
    "authors": [
      "Xiangrui Liu",
      "Xinju Wu",
      "Shiqi Wang",
      "Zhu Li",
      "Sam Kwong"
    ],
    "abstract": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers from substantial data volume due to inherent primitive redundancy. To enable future photorealistic 3D immersive visual communication applications, significant compression is essential for transmission over the existing Internet infrastructure. Hence, we propose Compressed Gaussian Splatting (CompGS++), a novel framework that leverages compact Gaussian primitives to achieve accurate 3D modeling with substantial size reduction for both static and dynamic scenes. Our design is based on the principle of eliminating redundancy both between and within primitives. Specifically, we develop a comprehensive prediction paradigm to address inter-primitive redundancy through spatial and temporal primitive prediction modules. The spatial primitive prediction module establishes predictive relationships for scene primitives and enables most primitives to be encoded as compact residuals, substantially reducing the spatial redundancy. We further devise a temporal primitive prediction module to handle dynamic scenes, which exploits primitive correlations across timestamps to effectively reduce temporal redundancy. Moreover, we devise a rate-constrained optimization module that jointly minimizes reconstruction error and rate consumption. This module effectively eliminates parameter redundancy within primitives and enhances the overall compactness of scene representations. Comprehensive evaluations across multiple benchmark datasets demonstrate that CompGS++ significantly outperforms existing methods, achieving superior compression performance while preserving accurate scene modeling. Our implementation will be made publicly available on GitHub to facilitate further research.",
    "arxiv_url": "https://arxiv.org/abs/2504.13022v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13022v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration",
    "authors": [
      "Rendong Zhang",
      "Alexandra Watkins",
      "Nilanjan Sarkar"
    ],
    "abstract": "Photorealistic avatars have become essential for immersive applications in virtual reality (VR) and augmented reality (AR), enabling lifelike interactions in areas such as training simulations, telemedicine, and virtual collaboration. These avatars bridge the gap between the physical and digital worlds, improving the user experience through realistic human representation. However, existing avatar creation techniques face significant challenges, including high costs, long creation times, and limited utility in virtual applications. Manual methods, such as MetaHuman, require extensive time and expertise, while automatic approaches, such as NeRF-based pipelines often lack efficiency, detailed facial expression fidelity, and are unable to be rendered at a speed sufficent for real-time applications. By involving several cutting-edge modern techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar creation pipeline that leverages monocular video input to create a scalable and efficient photorealistic avatar directly compatible with the Unity game engine. Our pipeline incorporates a novel Gaussian splatting technique with customized preprocessing that enables the user of \"in the wild\" monocular video capture, detailed facial expression reconstruction and embedding within a fully rigged avatar model. Additionally, we present a Unity-integrated Gaussian Splatting Avatar Editor, offering a user-friendly environment for VR/AR application development. Experimental results validate the effectiveness of our preprocessing pipeline in standardizing custom data for 3DGS training and demonstrate the versatility of Gaussian avatars in Unity, highlighting the scalability and practicality of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2504.12999v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12999v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "vr",
      "ar",
      "human",
      "avatar",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Matrix-free Second-order Optimization of Gaussian Splats with Residual Sampling",
    "authors": [
      "Hamza Pehlivan",
      "Andrea Boscolo Camiletto",
      "Lin Geng Foo",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to its high rendering quality and fast inference time. However, 3DGS predominantly relies on first-order optimizers such as Adam, which leads to long training times. To address this limitation, we propose a novel second-order optimization strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which we specifically tailor towards Gaussian Splatting. Our key insight is that the Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only a limited number of pixels. We exploit this sparsity by proposing a matrix-free and GPU-parallelized LM optimization. To further improve its efficiency, we propose sampling strategies for both the camera views and loss function and, consequently, the normal equation, significantly reducing the computational complexity. In addition, we increase the convergence rate of the second-order approximation by introducing an effective heuristic to determine the learning rate that avoids the expensive computation cost of line search methods. As a result, our method achieves a $3\\times$ speedup over standard LM and outperforms Adam by $~6\\times$ when the Gaussian count is low while remaining competitive for moderate counts. Project Page: https://vcai.mpi-inf.mpg.de/projects/LM-IS",
    "arxiv_url": "https://arxiv.org/abs/2504.12905v2",
    "pdf_url": "https://arxiv.org/pdf/2504.12905v2",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering",
    "authors": [
      "Michael Steiner",
      "Thomas KÃ¶hler",
      "Lukas Radl",
      "Felix Windisch",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering.",
    "arxiv_url": "https://arxiv.org/abs/2504.12811v2",
    "pdf_url": "https://arxiv.org/pdf/2504.12811v2",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation",
    "authors": [
      "Yifei Tong",
      "Runze Tian",
      "Xiao Han",
      "Dingyao Liu",
      "Fenggen Yu",
      "Yan Zhang"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of real scenes, enabling user-friendly deformation to create novel scenes while preserving fine details from the original 3DGS has attracted significant research attention. We introduce CAGE-GS, a cage-based 3DGS deformation method that seamlessly aligns a source 3DGS scene with a user-defined target shape. Our approach learns a deformation cage from the target, which guides the geometric transformation of the source scene. While the cages effectively control structural alignment, preserving the textural appearance of 3DGS remains challenging due to the complexity of covariance parameters. To address this, we employ a Jacobian matrix-based strategy to update the covariance parameters of each Gaussian, ensuring texture fidelity post-deformation. Our method is highly flexible, accommodating various target shape representations, including texts, images, point clouds, meshes and 3DGS models. Extensive experiments and ablation studies on both public datasets and newly proposed scenes demonstrate that our method significantly outperforms existing techniques in both efficiency and deformation quality.",
    "arxiv_url": "https://arxiv.org/abs/2504.12800v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12800v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors",
    "authors": [
      "Mingwei Li",
      "Pu Pang",
      "Hehe Fan",
      "Hua Huang",
      "Yi Yang"
    ],
    "abstract": "Reconstructing transparent surfaces is essential for tasks such as robotic manipulation in labs, yet it poses a significant challenge for 3D reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods often encounter a transparency-depth dilemma, where the pursuit of photorealistic rendering through standard $Î±$-blending undermines geometric precision, resulting in considerable depth estimation errors for transparent materials. To address this issue, we introduce Transparent Surface Gaussian Splatting (TSGS), a new framework that separates geometry learning from appearance refinement. In the geometry learning stage, TSGS focuses on geometry by using specular-suppressed inputs to accurately represent surfaces. In the second stage, TSGS improves visual fidelity through anisotropic specular modeling, crucially maintaining the established opacity to ensure geometric accuracy. To enhance depth inference, TSGS employs a first-surface depth extraction method. This technique uses a sliding window over $Î±$-blending weights to pinpoint the most likely surface location and calculates a robust weighted average depth. To evaluate the transparent surface reconstruction task under realistic conditions, we collect a TransLab dataset that includes complex transparent laboratory glassware. Extensive experiments on TransLab show that TSGS achieves accurate geometric reconstruction and realistic rendering of transparent objects simultaneously within the efficient 3DGS framework. Specifically, TSGS significantly surpasses current leading methods, achieving a 37.3% reduction in chamfer distance and an 8.0% improvement in F1 score compared to the top baseline. The code and dataset are available at https://longxiang-ai.github.io/TSGS/.",
    "arxiv_url": "https://arxiv.org/abs/2504.12799v2",
    "pdf_url": "https://arxiv.org/pdf/2504.12799v2",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior",
    "authors": [
      "Xiao Han",
      "Runze Tian",
      "Yifei Tong",
      "Fenggen Yu",
      "Dingyao Liu",
      "Yan Zhang"
    ],
    "abstract": "Drag-driven editing has become popular among designers for its ability to modify complex geometric structures through simple and intuitive manipulation, allowing users to adjust and reshape content with minimal technical skill. This drag operation has been incorporated into numerous methods to facilitate the editing of 2D images and 3D meshes in design. However, few studies have explored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS) representation, as deforming 3DGS while preserving shape coherence and visual continuity remains challenging. In this paper, we introduce ARAP-GS, a drag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP) deformation. Unlike previous 3DGS editing methods, we are the first to apply ARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven geometric transformations. To preserve scene appearance after deformation, we incorporate an advanced diffusion prior for image super-resolution within our iterative optimization process. This approach enhances visual quality while maintaining multi-view consistency in the edited results. Experiments show that ARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its effectiveness and superiority for drag-driven 3DGS editing. Additionally, our method is highly efficient, requiring only 10 to 20 minutes to edit a scene on a single RTX 3090 GPU.",
    "arxiv_url": "https://arxiv.org/abs/2504.12788v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12788v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting",
    "authors": [
      "Wei Sun",
      "Yanzhao Zhou",
      "Jianbin Jiao",
      "Yuan Li"
    ],
    "abstract": "Open-vocabulary 3D scene understanding is crucial for applications requiring natural language-driven spatial interpretation, such as robotics and augmented reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation for scene reconstruction, integrating it with open-vocabulary frameworks reveals a key challenge: cross-view granularity inconsistency. This issue, stemming from 2D segmentation methods like SAM, results in inconsistent object segmentations across views (e.g., a \"coffee set\" segmented as a single entity in one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based methods often rely on isolated per-Gaussian feature learning, neglecting the spatial context needed for cohesive object reasoning, leading to fragmented representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel framework that addresses this challenge by incorporating spatial context into 3DGS. CAGS constructs local graphs to propagate contextual features across Gaussians, reducing noise from inconsistent granularity, employs mask-centric contrastive learning to smooth SAM-derived features across views, and leverages a precomputation strategy to reduce computational cost by precomputing neighborhood relationships, enabling efficient training in large-scale scenes. By integrating spatial context, CAGS significantly improves 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet, enabling robust language-guided 3D scene understanding.",
    "arxiv_url": "https://arxiv.org/abs/2504.11893v1",
    "pdf_url": "https://arxiv.org/pdf/2504.11893v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "robotics",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction",
    "authors": [
      "Wenhua Wu",
      "Tong Zhao",
      "Chensheng Peng",
      "Lei Yang",
      "Yintao Wei",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typically rely on multi-view image inputs and require prolonged optimization times. In this paper, we propose BEV-GS, a real-time single-frame road surface reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists of a prediction module and a rendering module. The prediction module introduces separate geometry and texture networks following Bird's-Eye-View paradigm. Geometric and texture parameters are directly estimated from a single frame, avoiding per-scene optimization. In the rendering module, we utilize grid Gaussian for road surface representation and novel view synthesis, which better aligns with road surface characteristics. Our method achieves state-of-the-art performance on the real-world dataset RSRD. The road elevation error reduces to 1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and real-time applications. The code will be available at: \\href{https://github.com/cat-wwh/BEV-GS}{\\texttt{https://github.com/cat-wwh/BEV-GS}}",
    "arxiv_url": "https://arxiv.org/abs/2504.13207v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13207v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDGS: Eliminating Densification for Efficient Convergence of 3DGS",
    "authors": [
      "Dmytro Kotovenko",
      "Olga Grebenkova",
      "BjÃ¶rn Ommer"
    ],
    "abstract": "3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a lengthy optimization path. Moreover, this incremental approach often leads to suboptimal renderings, particularly in high-frequency regions where detail is critical.   We propose a fundamentally different approach: we eliminate densification process with a one-step approximation of scene geometry using triangulated pixels from dense image correspondences. This dense initialization allows us to estimate rough geometry of the scene while preserving rich details from input RGB images, providing each Gaussian with well-informed colors, scales, and positions. As a result, we dramatically shorten the optimization path and remove the need for densification. Unlike traditional methods that rely on sparse keypoints, our dense initialization ensures uniform detail across the scene, even in high-frequency regions where 3DGS and other methods struggle. Moreover, since all splats are initialized in parallel at the start of optimization, we eliminate the need to wait for densification to adjust new Gaussians.   Our method not only outperforms speed-optimized models in training efficiency but also achieves higher rendering quality than state-of-the-art approaches, all while using only half the splats of standard 3DGS. It is fully compatible with other 3DGS acceleration techniques, making it a versatile and efficient solution that can be integrated with existing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2504.13204v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13204v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "acceleration",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians",
    "authors": [
      "Zeming Wei",
      "Junyi Lin",
      "Yang Liu",
      "Weixing Chen",
      "Jingzhou Luo",
      "Guanbin Li",
      "Liang Lin"
    ],
    "abstract": "3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2504.11218v2",
    "pdf_url": "https://arxiv.org/pdf/2504.11218v2",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "recognition",
      "ar",
      "human",
      "real-time rendering",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation",
    "authors": [
      "Andrea Simonelli",
      "Norman MÃ¼ller",
      "Peter Kontschieder"
    ],
    "abstract": "The increasing availability of digital 3D environments, whether through image-based 3D reconstruction, generation, or scans obtained by robots, is driving innovation across various applications. These come with a significant demand for 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and performing well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a 3D interactive segmentation method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as the ones obtained by Gaussian Splatting. The project web-page is available at https://simonelli-andrea.github.io/easy3d.",
    "arxiv_url": "https://arxiv.org/abs/2504.11024v1",
    "pdf_url": "https://arxiv.org/pdf/2504.11024v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "lightweight",
      "gaussian splatting",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gabor Splatting: Reconstruction of High-frequency Surface Texture using Gabor Noise",
    "authors": [
      "Haato Watanabe",
      "Kenji Tojo",
      "Nobuyuki Umetani"
    ],
    "abstract": "3D Gaussian splatting has experienced explosive popularity in the past few years in the field of novel view synthesis. The lightweight and differentiable representation of the radiance field using the Gaussian enables rapid and high-quality reconstruction and fast rendering. However, reconstructing objects with high-frequency surface textures (e.g., fine stripes) requires many skinny Gaussian kernels because each Gaussian represents only one color if viewed from one direction. Thus, reconstructing the stripes pattern, for example, requires Gaussians for at least the number of stripes. We present 3D Gabor splatting, which augments the Gaussian kernel to represent spatially high-frequency signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term and spatially fluctuating wave functions, making it suitable for representing spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can reconstruct various high-frequency textures on the objects.",
    "arxiv_url": "https://arxiv.org/abs/2504.11003v1",
    "pdf_url": "https://arxiv.org/pdf/2504.11003v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR",
    "authors": [
      "Christophe Bolduc",
      "Yannick Hold-Geoffroy",
      "Zhixin Shu",
      "Jean-FranÃ§ois Lalonde"
    ],
    "abstract": "We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: https://lvsn.github.io/gaslight/",
    "arxiv_url": "https://arxiv.org/abs/2504.10809v4",
    "pdf_url": "https://arxiv.org/pdf/2504.10809v4",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting",
    "authors": [
      "Zeren Jiang",
      "Shaofei Wang",
      "Siyu Tang"
    ],
    "abstract": "Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.",
    "arxiv_url": "https://arxiv.org/abs/2504.10486v2",
    "pdf_url": "https://arxiv.org/pdf/2504.10486v2",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "relightable",
      "ar",
      "relighting",
      "fast",
      "geometry",
      "avatar",
      "human",
      "lighting",
      "gaussian splatting",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis",
    "authors": [
      "Hao Sun",
      "Fenggen Yu",
      "Huiyao Xu",
      "Tao Zhang",
      "Changqing Zou"
    ],
    "abstract": "Novel view synthesis (NVS) in low-light scenes remains a significant challenge due to degraded inputs characterized by severe noise, low dynamic range (LDR) and unreliable initialization. While recent NeRF-based approaches have shown promising results, most suffer from high computational costs, and some rely on carefully captured or pre-processed data--such as RAW sensor inputs or multi-exposure sequences--which severely limits their practicality. In contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with competitive visual fidelity; however, existing 3DGS-based methods struggle with low-light sRGB inputs, resulting in unstable Gaussian initialization and ineffective noise suppression. To address these challenges, we propose LL-Gaussian, a novel framework for 3D reconstruction and enhancement from low-light sRGB images, enabling pseudo normal-light novel view synthesis. Our method introduces three key innovations: 1) an end-to-end Low-Light Gaussian Initialization Module (LLGIM) that leverages dense priors from learning-based MVS approach to generate high-quality initial point clouds; 2) a dual-branch Gaussian decomposition model that disentangles intrinsic scene properties (reflectance and illumination) from transient interference, enabling stable and interpretable optimization; 3) an unsupervised optimization strategy guided by both physical constrains and diffusion prior to jointly steer decomposition and enhancement. Additionally, we contribute a challenging dataset collected in extreme low-light environments and demonstrate the effectiveness of LL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian achieves up to 2,000 times faster inference and reduces training time to just 2%, while delivering superior reconstruction and rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2504.10331v3",
    "pdf_url": "https://arxiv.org/pdf/2504.10331v3",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "illumination",
      "fast",
      "real-time rendering",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting",
    "authors": [
      "Yufei Deng",
      "Yuanjian Wang",
      "Rong Xiao",
      "Chenwei Tang",
      "Jizhe Zhou",
      "Jiahao Fan",
      "Deng Xiong",
      "Jiancheng Lv",
      "Huajin Tang"
    ],
    "abstract": "While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view synthesis, its performance degrades with motion blur. In scenarios with rapid motion or low-light conditions, existing RGB-based deblurring methods struggle to model camera pose and radiance changes during exposure, reducing reconstruction accuracy. Event cameras, capturing continuous brightness changes during exposure, can effectively assist in modeling motion blur and improving reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D Gaussians from event streams and severely blurred images. This method jointly learns the parameters of these Gaussians while recovering camera motion trajectories during exposure time. Specifically, we first construct a blur loss function by synthesizing multiple latent sharp images during the exposure time, minimizing the difference between real and synthesized blurred images. Then we use event stream to supervise the light intensity changes between latent sharp images at any time within the exposure period, supplementing the light intensity dynamic changes lost in RGB images. Furthermore, we optimize the latent sharp images at intermediate exposure times based on the event-based double integral (EDI) prior, applying consistency constraints to enhance the details and texture information of the reconstructed images. Extensive experiments on synthetic and real-world datasets show that EBAD-Gaussian can achieve high-quality 3D scene reconstruction under the condition of blurred images and event stream inputs.",
    "arxiv_url": "https://arxiv.org/abs/2504.10012v2",
    "pdf_url": "https://arxiv.org/pdf/2504.10012v2",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting",
    "authors": [
      "Junlin Hao",
      "Peiheng Wang",
      "Haoyang Wang",
      "Xinggong Zhang",
      "Zongming Guo"
    ],
    "abstract": "Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.",
    "arxiv_url": "https://arxiv.org/abs/2504.10001v3",
    "pdf_url": "https://arxiv.org/pdf/2504.10001v3",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "fast",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling",
    "authors": [
      "Yunpeng Tan",
      "Junlin Hao",
      "Jiangkai Wu",
      "Liming Liu",
      "Qingyang Li",
      "Xinggong Zhang"
    ],
    "abstract": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view synthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting, requires minutes for training, far from the real-time performance required by multimedia scenarios like telemedicine. One of the obstacles is its inefficient sampling, which is only partially addressed by existing works. Existing point-sampling algorithms uniformly sample simple-texture regions (easy to fit) and complex-texture regions (hard to fit), while existing ray-sampling algorithms sample these regions all in the finest granularity (i.e. the pixel level), both wasting GPU training resources. Actually, regions with different texture intensities require different sampling granularities. To this end, we propose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which employs Monte Carlo Tree Search (MCTS) to partition each training image into pixel blocks with different sizes for active block-wise training. Specifically, the trees are initialized according to the texture of training images to boost the initialization speed, and an expansion/pruning module dynamically optimizes the block partition. MCBlock is implemented in Nerfstudio, an open-source toolset, and achieves a training acceleration of up to 2.33x, surpassing other ray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF model and contribute to the multimedia community.",
    "arxiv_url": "https://arxiv.org/abs/2504.09878v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09878v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "nerf",
      "acceleration",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting",
    "authors": [
      "Zhicong Wu",
      "Hongbin Xu",
      "Gang Xu",
      "Ping Nie",
      "Zhixin Yan",
      "Jinkai Zheng",
      "Liangqiong Qu",
      "Ming Li",
      "Liqiang Nie"
    ],
    "abstract": "Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework. The code will be publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2504.09588v2",
    "pdf_url": "https://arxiv.org/pdf/2504.09588v2",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "segmentation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering",
    "authors": [
      "Yexing Xu",
      "Longguang Wang",
      "Minglin Chen",
      "Sheng Ao",
      "Li Li",
      "Yulan Guo"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in novel view synthesis, its performance degrades dramatically with sparse inputs and generates undesirable artifacts. As the number of training views decreases, the novel view synthesis task degrades to a highly under-determined problem such that existing methods suffer from the notorious overfitting issue. Interestingly, we observe that models with fewer Gaussian primitives exhibit less overfitting under sparse inputs. Inspired by this observation, we propose a Random Dropout Regularization (RDR) to exploit the advantages of low-complexity models to alleviate overfitting. In addition, to remedy the lack of high-frequency details for these models, an Edge-guided Splitting Strategy (ESS) is developed. With these two techniques, our method (termed DropoutGS) provides a simple yet effective plug-in approach to improve the generalization performance of existing 3DGS methods. Extensive experiments show that our DropoutGS produces state-of-the-art performance under sparse views on benchmark datasets including Blender, LLFF, and DTU. The project page is at: https://xuyx55.github.io/DropoutGS/.",
    "arxiv_url": "https://arxiv.org/abs/2504.09491v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09491v1",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds",
    "authors": [
      "Jizong Peng",
      "Tze Ho Elden Tse",
      "Kai Xu",
      "Wenchao Gao",
      "Angela Yao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it needs to be initialized from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2504.09129v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09129v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Junuk Cha",
      "Soohyun Hwang",
      "Hyein Hwang",
      "Seungryul Baek"
    ],
    "abstract": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem that can find numerous applications. Despite recent advances, there is no comprehensive pipeline yet for bimanual class-agnostic interaction reconstruction from a monocular RGB video, where two hands and an unknown object are interacting with each other. Previous works tackled the limited hand-object interaction case, where object templates are pre-known or only one hand is involved in the interaction. The bimanual interaction reconstruction exhibits severe occlusions introduced by complex interactions between two hands and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an unknown object from a monocular video. To robustly obtain object Gaussians avoiding severe occlusions, we leverage prior knowledge of pre-trained diffusion model with score distillation sampling (SDS) loss, to reconstruct unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model (i.e., MANO) and share a single Gaussian for two hands to effectively accumulate hand 3D information, given limited views. To further consider the 3D alignment between hands and objects, we include the interacting-subjects optimization step during Gaussian optimization. Our method achieves the state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering quality (PSNR, SSIM, LPIPS), respectively.",
    "arxiv_url": "https://arxiv.org/abs/2504.09097v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09097v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting",
    "authors": [
      "Zhijie Shen",
      "Chunyu Lin",
      "Shujuan Huang",
      "Lang Nie",
      "Kang Liao",
      "Yao Zhao"
    ],
    "abstract": "Recently, reconstructing scenes from a single panoramic image using advanced 3D Gaussian Splatting (3DGS) techniques has attracted growing interest. Panoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the entire scene in a single shot. However, panoramic images introduce severe distortion, making it challenging to render 3D Gaussians into 2D distorted equirectangular space directly. Converting equirectangular images to cubemap projections partially alleviates this problem but introduces new challenges, such as projection distortion and discontinuities across cube-face boundaries. To address these limitations, we present a novel framework, named TPGS, to bridge continuous panoramic 3D scene reconstruction with perspective Gaussian splatting. Firstly, we introduce a Transition Plane between adjacent cube faces to enable smoother transitions in splatting directions and mitigate optimization ambiguity in the boundary region. Moreover, an intra-to-inter face optimization strategy is proposed to enhance local details and restore visual consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians within individual cube faces and then fine-tune them in the stitched panoramic space. Additionally, we introduce a spherical sampling technique to eliminate visible stitching seams. Extensive experiments on indoor and outdoor, egocentric, and roaming benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods. Code and models will be available at https://github.com/zhijieshen-bjtu/TPGS.",
    "arxiv_url": "https://arxiv.org/abs/2504.09062v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09062v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting",
    "authors": [
      "Yongchang Wu",
      "Zipeng Qi",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ],
    "abstract": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian",
    "arxiv_url": "https://arxiv.org/abs/2504.09048v2",
    "pdf_url": "https://arxiv.org/pdf/2504.09048v2",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents",
    "authors": [
      "Xin Tan",
      "Yuzhou Ji",
      "He Zhu",
      "Yuan Xie"
    ],
    "abstract": "The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.",
    "arxiv_url": "https://arxiv.org/abs/2504.08581v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08581v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "semantic",
      "ar",
      "fast",
      "gaussian splatting",
      "3d gaussian",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation",
    "authors": [
      "Bram Vanherle",
      "Brent Zoomers",
      "Jeroen Put",
      "Frank Van Reeth",
      "Nick Michiels"
    ],
    "abstract": "Generating synthetic images is a useful method for cheaply obtaining labeled data for training computer vision models. However, obtaining accurate 3D models of relevant objects is necessary, and the resulting images often have a gap in realism due to challenges in simulating lighting effects and camera artifacts. We propose using the novel view synthesis method called Gaussian Splatting to address these challenges. We have developed a synthetic data pipeline for generating high-quality context-aware instance segmentation training data for specific objects. This process is fully automated, requiring only a video of the target object. We train a Gaussian Splatting model of the target object and automatically extract the object from the video. Leveraging Gaussian Splatting, we then render the object on a random background image, and monocular depth estimation is employed to place the object in a believable pose. We introduce a novel dataset to validate our approach and show superior performance over other data generation approaches, such as Cut-and-Paste and Diffusion model-based generation.",
    "arxiv_url": "https://arxiv.org/abs/2504.08473v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08473v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "lighting",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "authors": [
      "Sauradip Nag",
      "Daniel Cohen-Or",
      "Hao Zhang",
      "Ali Mahdavi-Amiri"
    ],
    "abstract": "We pose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening to interpolate two single-view images. In contrast to video/4D generation from only text or a single image, our interpolative task can leverage more precise motion control to better constrain the generation. Given two monocular RGB images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D, without making assumptions on the object category, motion type, length, or complexity. To handle such arbitrary and diverse motions, we utilize a foundational video interpolation model for motion prediction. However, large frame-to-frame motion gaps can lead to ambiguous interpretations. To this end, we employ a hierarchical approach to identify keyframes that are visually close to the input states while exhibiting significant motions, then generate smooth fragments between them. For each fragment, we construct a 3D representation of the keyframe using Gaussian Splatting (3DGS). The temporal frames within the fragment guide the motion, enabling their transformation into dynamic 3DGS through a deformation field. To improve temporal consistency and refine the 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitive experiments as well as a user study, we demonstrate the effectiveness of our method and design choices.",
    "arxiv_url": "https://arxiv.org/abs/2504.08366v3",
    "pdf_url": "https://arxiv.org/pdf/2504.08366v3",
    "published_date": "2025-04-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians",
    "authors": [
      "Kefan Chen",
      "Sergiu Oprea",
      "Justin Theiss",
      "Sreyas Mohan",
      "Srinath Sridhar",
      "Aayush Prakash"
    ],
    "abstract": "With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses.",
    "arxiv_url": "https://arxiv.org/abs/2504.07949v1",
    "pdf_url": "https://arxiv.org/pdf/2504.07949v1",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "vr",
      "ar",
      "geometry",
      "avatar",
      "dynamic",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "face",
      "shadow"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting",
    "authors": [
      "Chenyu Han",
      "Corentin Dumery"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene reconstruction for its high visual accuracy. However, uncertainty estimation of 3DGS scenes remains underexplored and is crucial to downstream tasks such as asset extraction and scene completion. Since the appearance of 3D gaussians is view-dependent, the color of a gaussian can thus be certain from an angle and uncertain from another. We thus propose to model uncertainty in 3DGS as an additional view-dependent per-gaussian feature that can be modeled with spherical harmonics. This simple yet effective modeling is easily interpretable and can be integrated into the traditional 3DGS pipeline. It is also significantly faster than ensemble methods while maintaining high accuracy, as demonstrated in our experiments.",
    "arxiv_url": "https://arxiv.org/abs/2504.07370v1",
    "pdf_url": "https://arxiv.org/pdf/2504.07370v1",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting",
    "authors": [
      "Daiwei Zhang",
      "Joaquin Gajardo",
      "Tomislav Medic",
      "Isinsu Katircioglu",
      "Mike Boss",
      "Norbert Kirchgessner",
      "Achim Walter",
      "Lukas Roth"
    ],
    "abstract": "Automated extraction of plant morphological traits is crucial for supporting crop breeding and agricultural management through high-throughput field phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due to their scalability and affordability, enabling volumetric measurements that 2D approaches cannot directly capture. While advanced methods like Neural Radiance Fields (NeRFs) have shown promise, their application has been limited to counting or extracting traits from only a few plants or organs. Furthermore, accurately measuring complex structures like individual wheat heads-essential for studying crop yields-remains particularly challenging due to occlusions and the dense arrangement of crop canopies in field conditions. The recent development of 3D Gaussian Splatting (3DGS) offers a promising alternative for HTFP due to its high-quality reconstructions and explicit point-based representation. In this paper, we present Wheat3DGS, a novel approach that leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance segmentation and morphological measurement of hundreds of wheat heads automatically, representing the first application of 3DGS to HTFP. We validate the accuracy of wheat head extraction against high-resolution laser scan data, obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and 40.2% for length, width, and volume. We provide additional comparisons to NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating superior results. Our approach enables rapid, non-destructive measurements of key yield-related traits at scale, with significant implications for accelerating crop breeding and improving our understanding of wheat development.",
    "arxiv_url": "https://arxiv.org/abs/2504.06978v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06978v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "understanding",
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "head",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "abstract": "This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.",
    "arxiv_url": "https://arxiv.org/abs/2504.06827v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06827v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Hanxiao Sun",
      "YuPeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameters and normal, alongside the absence of physical constraints for indirect lighting. In this paper, we present a novel framework called Spatially-vayring Gaussian Inverse Rendering (SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we propose a new representation-Spatially-varying Gaussian (SVG)-that allows per-Gaussian spatially varying parameters. This enhanced representation is complemented by a SVG splatting scheme akin to vertex/fragment shading in traditional graphics pipelines. Furthermore, we integrate a physically-based indirect lighting model, enabling more realistic relighting. The proposed SVG-IR framework significantly improves rendering quality, outperforming state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio (PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in relighting tasks, all while maintaining a real-time rendering speed.",
    "arxiv_url": "https://arxiv.org/abs/2504.06815v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06815v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "illumination",
      "relighting",
      "real-time rendering",
      "ar",
      "lighting",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction",
    "authors": [
      "Anil Armagan",
      "Albert SaÃ -Garriga",
      "Bruno Manganelli",
      "Kyuwon Kim",
      "M. Kerim Yucel"
    ],
    "abstract": "Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly due to its ability to converge reasonably fast, faithfully represent the scene and render (novel) views in a fast fashion. However, it suffers from large storage and memory requirements, and its training speed still lags behind the hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it especially difficult to deploy them in robotics scenarios, where 3D reconstruction is crucial for accurate operation. In this paper, we propose GSta that dynamically identifies Gaussians that have converged well during training, based on their positional and color gradient norms. By forcing such Gaussians into a siesta and stopping their updates (freezing) during training, we improve training speed with competitive accuracy compared to state of the art. We also propose an early stopping mechanism based on the PSNR values computed on a subset of training images. Combined with other improvements, such as integrating a learning rate scheduler, GSta achieves an improved Pareto front in convergence speed, memory and storage requirements, while preserving quality. We also show that GSta can improve other methods and complement orthogonal approaches in efficiency improvement; once combined with Trick-GS, GSta achieves up to 5x faster training, 16x smaller disk size compared to vanilla GS, while having comparable accuracy and consuming only half the peak memory. More visualisations are available at https://anilarmagan.github.io/SRUK-GSta.",
    "arxiv_url": "https://arxiv.org/abs/2504.06716v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06716v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "robotics",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Collision avoidance from monocular vision trained with novel view synthesis",
    "authors": [
      "Valentin Tordjman--Levavasseur",
      "StÃ©phane Caron"
    ],
    "abstract": "Collision avoidance can be checked in explicit environment models such as elevation maps or occupancy grids, yet integrating such models with a locomotion policy requires accurate state estimation. In this work, we consider the question of collision avoidance from an implicit environment model. We use monocular RGB images as inputs and train a collisionavoidance policy from photorealistic images generated by 2D Gaussian splatting. We evaluate the resulting pipeline in realworld experiments under velocity commands that bring the robot on an intercept course with obstacles. Our results suggest that RGB images can be enough to make collision-avoidance decisions, both in the room where training data was collected and in out-of-distribution environments.",
    "arxiv_url": "https://arxiv.org/abs/2504.06651v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06651v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stochastic Ray Tracing of Transparent 3D Gaussians",
    "authors": [
      "Xin Sun",
      "Iliyan Georgiev",
      "Yun Fei",
      "MiloÅ¡ HaÅ¡an"
    ],
    "abstract": "3D Gaussian splatting has been widely adopted as a 3D representation for novel-view synthesis, relighting, and 3D generation tasks. It delivers realistic and detailed results through a collection of explicit 3D Gaussian primitives, each carrying opacity and view-dependent color. However, efficient rendering of many transparent primitives remains a significant challenge. Existing approaches either rasterize the Gaussians with approximate per-view sorting or rely on high-end RTX GPUs. This paper proposes a stochastic ray-tracing method to render 3D clouds of transparent primitives. Instead of processing all ray-Gaussian intersections in sequential order, each ray traverses the acceleration structure only once, randomly accepting and shading a single intersection (or $N$ intersections, using a simple extension). This approach minimizes shading time and avoids primitive sorting along the ray, thereby minimizing register usage and maximizing parallelism even on low-end GPUs. The cost of rays through the Gaussian asset is comparable to that of standard mesh-intersection rays. The shading is unbiased and has low variance, as our stochastic acceptance achieves importance sampling based on accumulated weight. The alignment with Monte Carlo philosophy simplifies implementation and integration into a conventional path-tracing framework.",
    "arxiv_url": "https://arxiv.org/abs/2504.06598v3",
    "pdf_url": "https://arxiv.org/pdf/2504.06598v3",
    "published_date": "2025-04-09",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "acceleration",
      "ar",
      "relighting",
      "lighting",
      "gaussian splatting",
      "3d gaussian",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-splatting: Multistage Isotropy-informed Covariance Regularization Optimization for High-Fidelity 3D Gaussian Splatting",
    "authors": [
      "Jee Won Lee",
      "Hansol Lim",
      "Sooyeun Yang",
      "Jongseong Brad Choi"
    ],
    "abstract": "High-fidelity 3D Gaussian Splatting methods excel at capturing fine textures but often overlook model compactness, resulting in massive splat counts, bloated memory, long training, and complex post-processing. We present Micro-Splatting: Two-Stage Adaptive Growth and Refinement, a unified, in-training pipeline that preserves visual detail while drastically reducing model complexity without any post-processing or auxiliary neural modules. In Stage I (Growth), we introduce a trace-based covariance regularization to maintain near-isotropic Gaussians, mitigating low-pass filtering in high-frequency regions and improving spherical-harmonic color fitting. We then apply gradient-guided adaptive densification that subdivides splats only in visually complex regions, leaving smooth areas sparse. In Stage II (Refinement), we prune low-impact splats using a simple opacity-scale importance score and merge redundant neighbors via lightweight spatial and feature thresholds, producing a lean yet detail-rich model. On four object-centric benchmarks, Micro-Splatting reduces splat count and model size by up to 60% and shortens training by 20%, while matching or surpassing state-of-the-art PSNR, SSIM, and LPIPS in real-time rendering. These results demonstrate that Micro-Splatting delivers both compactness and high fidelity in a single, efficient, end-to-end framework.",
    "arxiv_url": "https://arxiv.org/abs/2504.05740v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05740v2",
    "published_date": "2025-04-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "compact",
      "ar",
      "real-time rendering",
      "lightweight",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Deformation Fields for 2D Editing of 3D Models",
    "authors": [
      "Martin El Mqirmi",
      "Noam Aigerman"
    ],
    "abstract": "We propose a method for authoring non-realistic 3D objects (represented as either 3D Gaussian Splats or meshes), that comply with 2D edits from specific viewpoints. Namely, given a 3D object, a user chooses different viewpoints and interactively deforms the object in the 2D image plane of each view. The method then produces a \"deformation field\" - an interpolation between those 2D deformations in a smooth manner as the viewpoint changes. Our core observation is that the 2D deformations do not need to be tied to an underlying object, nor share the same deformation space. We use this observation to devise a method for authoring view-dependent deformations, holding several technical contributions: first, a novel way to compositionality-blend between the 2D deformations after lifting them to 3D - this enables the user to \"stack\" the deformations similarly to layers in an editing software, each deformation operating on the results of the previous; second, a novel method to apply the 3D deformation to 3D Gaussian Splats; third, an approach to author the 2D deformations, by deforming a 2D mesh encapsulating a rendered image of the object. We show the versatility and efficacy of our method by adding cartoonish effects to objects, providing means to modify human characters, fitting 3D models to given 2D sketches and caricatures, resolving occlusions, and recreating classic non-realistic paintings as 3D models.",
    "arxiv_url": "https://arxiv.org/abs/2504.05544v1",
    "pdf_url": "https://arxiv.org/pdf/2504.05544v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "human",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery",
    "authors": [
      "Yi-Zhen Tsai",
      "Xuechen Zhang",
      "Zheng Li",
      "Jiasi Chen"
    ],
    "abstract": "Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations.",
    "arxiv_url": "https://arxiv.org/abs/2504.05517v1",
    "pdf_url": "https://arxiv.org/pdf/2504.05517v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high quality",
      "ar",
      "3d gaussian",
      "head",
      "efficient rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Let it Snow! Animating 3D Gaussian Scenes with Dynamic Weather Effects via Physics-Guided Score Distillation",
    "authors": [
      "Gal Fiebelman",
      "Hadar Averbuch-Elor",
      "Sagie Benaim"
    ],
    "abstract": "3D Gaussian Splatting has recently enabled fast and photorealistic reconstruction of static 3D scenes. However, dynamic editing of such scenes remains a significant challenge. We introduce a novel framework, Physics-Guided Score Distillation, to address a fundamental conflict: physics simulation provides a strong motion prior that is insufficient for photorealism , while video-based Score Distillation Sampling (SDS) alone cannot generate coherent motion for complex, multi-particle scenarios. We resolve this through a unified optimization framework where physics simulation guides Score Distillation to jointly refine the motion prior for photorealism while simultaneously optimizing appearance. Specifically, we learn a neural dynamics model that predicts particle motion and appearance, optimized end-to-end via a combined loss integrating Video-SDS for photorealism with our physics-guidance prior. This allows for photorealistic refinements while ensuring the dynamics remain plausible. Our framework enables scene-wide dynamic weather effects, including snowfall, rainfall, fog, and sandstorms, with physically plausible motion. Experiments demonstrate our physics-guided approach significantly outperforms baselines, with ablations confirming this joint refinement is essential for generating coherent, high-fidelity dynamics.",
    "arxiv_url": "https://arxiv.org/abs/2504.05296v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05296v2",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "fast",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanoDreamer: Consistent Text to 360-Degree Scene Generation",
    "authors": [
      "Zhexiao Xiong",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu",
      "Nathan Jacobs"
    ],
    "abstract": "Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand/refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes.",
    "arxiv_url": "https://arxiv.org/abs/2504.05152v1",
    "pdf_url": "https://arxiv.org/pdf/2504.05152v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Particle Approximation of VDB Datasets: A Study for Scientific Visualization",
    "authors": [
      "Isha Sharma",
      "Dieter Schmalstieg"
    ],
    "abstract": "The complexity and scale of Volumetric and Simulation datasets for Scientific Visualization(SciVis) continue to grow. And the approaches and advantages of memory-efficient data formats and storage techniques for such datasets vary. OpenVDB library and its VDB data format excels in memory efficiency through its hierarchical and dynamic tree structure, with active and inactive sub-trees for data storage. It is heavily used in current production renderers for both animation and rendering stages in VFX pipelines and photorealistic rendering of volumes and fluids. However, it still remains to be fully leveraged in SciVis where domains dealing with sparse scalar fields like porous media, time varying volumes such as tornado and weather simulation or high resolution simulation of Computational Fluid Dynamics present ample number of large challenging data sets. The goal of this paper hence is not only to explore the use of OpenVDB in SciVis but also to explore a level of detail(LOD) technique using 3D Gaussian particles approximating voxel regions. For rendering, we utilize NVIDIA OptiX library for ray marching through the Gaussians particles. Data modeling using 3D Gaussians has been very popular lately due to success in stereoscopic image to 3D scene conversion using Gaussian Splatting and Gaussian approximation and mixture models aren't entirely new in SciVis as well. Our work explores the integration with rendering software libraries like OpenVDB and OptiX to take advantage of their built-in memory compaction and hardware acceleration features, while also leveraging the performance capabilities of modern GPUs. Thus, we present a SciVis rendering approach that uses 3D Gaussians at varying LOD in a lossy scheme derived from VDB datasets, rather than focusing on photorealistic volume rendering.",
    "arxiv_url": "https://arxiv.org/abs/2504.04857v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04857v2",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "compact",
      "acceleration",
      "ar",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM",
    "authors": [
      "Zhicong Sun",
      "Jacqueline Lo",
      "Jinxing Hu"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) technology has recently achieved photorealistic mapping capabilities thanks to the real-time, high-fidelity rendering enabled by 3D Gaussian Splatting (3DGS). However, due to the static representation of scenes, current 3DGS-based SLAM encounters issues with pose drift and failure to reconstruct accurate maps in dynamic environments. To address this problem, we present D4DGS-SLAM, the first SLAM method based on 4DGS map representation for dynamic environments. By incorporating the temporal dimension into scene representation, D4DGS-SLAM enables high-quality reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we can obtain the dynamics, visibility, and reliability of scene points, and filter out unstable dynamic points for tracking accordingly. When optimizing Gaussian points, we apply different isotropic regularization terms to Gaussians with varying dynamic characteristics. Experimental results on real-world dynamic scene datasets demonstrate that our method outperforms state-of-the-art approaches in both camera pose tracking and map quality.",
    "arxiv_url": "https://arxiv.org/abs/2504.04844v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04844v2",
    "published_date": "2025-04-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "tracking",
      "slam"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal",
    "authors": [
      "Wanzhou Liu",
      "Zhexiao Xiong",
      "Xinyu Li",
      "Nathan Jacobs"
    ],
    "abstract": "Recent novel view synthesis (NVS) techniques, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene reconstruction with high-quality rendering and realistic detail recovery. Effectively removing occlusions while preserving scene details can further enhance the robustness and applicability of these techniques. However, existing approaches for object and occlusion removal predominantly rely on generative priors, which, despite filling the resulting holes, introduce new artifacts and blurriness. Moreover, existing benchmark datasets for evaluating occlusion removal methods lack realistic complexity and viewpoint variations. To address these issues, we introduce DeclutterSet, a novel dataset featuring diverse scenes with pronounced occlusions distributed across foreground, midground, and background, exhibiting substantial relative motion across viewpoints. We further introduce DeclutterNeRF, an occlusion removal method free from generative priors. DeclutterNeRF introduces joint multi-view optimization of learnable camera parameters, occlusion annealing regularization, and employs an explainable stochastic structural similarity loss, ensuring high-quality, artifact-free reconstructions from incomplete images. Experiments demonstrate that DeclutterNeRF significantly outperforms state-of-the-art methods on our proposed DeclutterSet, establishing a strong baseline for future research.",
    "arxiv_url": "https://arxiv.org/abs/2504.04679v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04679v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use",
    "authors": [
      "Haonan Chen",
      "Cheng Zhu",
      "Shuijing Liu",
      "Yunzhu Li",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "Tool use is essential for enabling robots to perform complex real-world tasks, but learning such skills requires extensive datasets. While teleoperation is widely used, it is slow, delay-sensitive, and poorly suited for dynamic tasks. In contrast, human videos provide a natural way for data collection without specialized hardware, though they pose challenges on robot learning due to viewpoint variations and embodiment gaps. To address these challenges, we propose a framework that transfers tool-use knowledge from humans to robots. To improve the policy's robustness to viewpoint variations, we use two RGB cameras to reconstruct 3D scenes and apply Gaussian splatting for novel view synthesis. We reduce the embodiment gap using segmented observations and tool-centric, task-space actions to achieve embodiment-invariant visuomotor policy learning. We demonstrate our framework's effectiveness across a diverse suite of tool-use tasks, where our learned policy shows strong generalization and robustness to human perturbations, camera motion, and robot base movement. Our method achieves a 71\\% improvement in task success over teleoperation-based diffusion policies and dramatically reduces data collection time by 77\\% and 41\\% compared to teleoperation and the state-of-the-art interface, respectively.",
    "arxiv_url": "https://arxiv.org/abs/2504.04612v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04612v2",
    "published_date": "2025-04-06",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "dynamic",
      "gaussian splatting",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models",
    "authors": [
      "Etienne Chassaing",
      "Florent Forest",
      "Olga Fink",
      "Malcolm Mielle"
    ],
    "abstract": "In the European Union, buildings account for 42% of energy use and 35% of greenhouse gas emissions. Since most existing buildings will still be in use by 2050, retrofitting is crucial for emissions reduction. However, current building assessment methods rely mainly on qualitative thermal imaging, which limits data-driven decisions for energy savings. On the other hand, quantitative assessments using finite element analysis (FEA) offer precise insights but require manual CAD design, which is tedious and error-prone. Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, enable precise 3D modeling from sparse images but lack clearly defined volumes and the interfaces between them needed for FEA. We propose Thermoxels, a novel voxel-based method able to generate FEA-compatible models, including both geometry and temperature, from a sparse set of RGB and thermal images. Using pairs of RGB and thermal images as input, Thermoxels represents a scene's geometry as a set of voxels comprising color and temperature information. After optimization, a simple process is used to transform Thermoxels' models into tetrahedral meshes compatible with FEA. We demonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes, surpassing other state-of-the-art methods. To showcase the practical applications of Thermoxels' models, we conduct a simple heat conduction simulation using FEA, achieving convergence from an initial state defined by Thermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image synthesis abilities with current state-of-the-art methods, showing competitive results, and discuss the limitations of existing metrics in assessing mesh quality.",
    "arxiv_url": "https://arxiv.org/abs/2504.04448v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04448v1",
    "published_date": "2025-04-06",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "ar",
      "geometry",
      "gaussian splatting",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS",
    "authors": [
      "Zhisheng Huang",
      "Peng Wang",
      "Jingdong Zhang",
      "Yuan Liu",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its efficiency and quality, but like many novel view synthesis methods, it heavily depends on accurate camera poses from Structure-from-Motion (SfM) systems. Although recent SfM pipelines have made impressive progress, questions remain about how to further improve both their robust performance in challenging conditions (e.g., textureless scenes) and the precision of camera parameter estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework that bridges this gap by jointly optimizing 3D Gaussians and camera parameters from large reconstruction priors MASt3R-SfM. We note that naively performing joint 3D Gaussian and camera optimization faces two challenges: the sensitivity to the quality of SfM initialization, and its limited capacity for global optimization, leading to suboptimal reconstruction results. Our 3R-GS, overcomes these issues by incorporating optimized practices, enabling robust scene reconstruction even with imperfect camera registration. Extensive experiments demonstrate that 3R-GS delivers high-quality novel view synthesis and precise camera pose estimation while remaining computationally efficient. Project page: https://zsh523.github.io/3R-GS/",
    "arxiv_url": "https://arxiv.org/abs/2504.04294v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04294v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "neural rendering",
      "gaussian splatting",
      "3d gaussian",
      "motion",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning",
    "authors": [
      "Yuyang Zhang",
      "Baao Xie",
      "Hu Zhu",
      "Qi Wang",
      "Huanting Guo",
      "Xin Jin",
      "Wenjun Zeng"
    ],
    "abstract": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D reconstruction, delivering both rapid rendering and high-quality results. However, existing 3DGS methods pose challenges in understanding underlying 3D semantics, which hinders model controllability and interpretability. To address it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to discover both coarse- and fine-grained 3D semantics via hierarchical disentangled representation learning (DRL). Specifically, the model employs a dual-branch architecture, consisting of a point cloud initialization branch and a triplane-Gaussian generation branch, to achieve coarse-grained disentanglement by separating 3D geometry and visual appearance features. Subsequently, fine-grained semantic representations within each modality are further discovered through DRL-based encoder-adapters. To our knowledge, this is the first work to achieve unsupervised interpretable 3DGS. Evaluations indicate that our model achieves 3D disentanglement while preserving high-quality and rapid reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2504.04190v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04190v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "semantic",
      "ar",
      "geometry",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration",
    "authors": [
      "Boyuan Wang",
      "Runqi Ouyang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Guosheng Zhao",
      "Chaojun Ni",
      "Xiaopei Zhang",
      "Guan Huang",
      "Yijie Ren",
      "Lihong Liu",
      "Xingang Wang"
    ],
    "abstract": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \\textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.",
    "arxiv_url": "https://arxiv.org/abs/2504.03536v2",
    "pdf_url": "https://arxiv.org/pdf/2504.03536v2",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "geometry",
      "avatar",
      "human",
      "gaussian splatting",
      "3d gaussian",
      "animation",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.",
    "arxiv_url": "https://arxiv.org/abs/2504.03059v2",
    "pdf_url": "https://arxiv.org/pdf/2504.03059v2",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM",
    "authors": [
      "Renwu Li",
      "Wenjing Ke",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "We present MonoGS++, a novel fast and accurate Simultaneous Localization and Mapping (SLAM) method that leverages 3D Gaussian representations and operates solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods largely depended on depth sensors, our approach reduces the hardware dependency and only requires RGB input, leveraging online visual odometry (VO) to generate sparse point clouds in real-time. To reduce redundancy and enhance the quality of 3D scene reconstruction, we implemented a series of methodological enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian insertion to avoid adding redundant Gaussians in previously well-reconstructed areas. Secondly, we introduced clarity-enhancing Gaussian densification module and planar regularization to handle texture-less areas and flat surfaces better. We achieved precise camera tracking results both on the synthetic Replica and real-world TUM-RGBD datasets, comparable to those of the state-of-the-art. Additionally, our method realized a significant 5.57x improvement in frames per second (fps) over the previous state-of-the-art, MonoGS.",
    "arxiv_url": "https://arxiv.org/abs/2504.02437v1",
    "pdf_url": "https://arxiv.org/pdf/2504.02437v1",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "mapping",
      "dynamic",
      "localization",
      "gaussian splatting",
      "3d gaussian",
      "slam",
      "tracking",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]