[
  {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D\n  Reconstruction",
    "authors": [
      "Fengzhi Guo",
      "Chih-Chuan Hsu",
      "Sihao Ding",
      "Cheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "arxiv_url": "http://arxiv.org/abs/2510.12768v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12768v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "authors": [
      "An Zhao",
      "Piaopiao Yu",
      "Zhe Zhu",
      "Mingqiang Wei"
    ],
    "abstract": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction.However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur.To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions.To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages.Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.",
    "arxiv_url": "http://arxiv.org/abs/2510.12493v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12493v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
    "authors": [
      "Mohamed Omran",
      "Farhad Zanjani",
      "Davide Abati",
      "Jens Petersen",
      "Amirhossein Habibian"
    ],
    "abstract": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
    "arxiv_url": "http://arxiv.org/abs/2510.12308v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12308v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "authors": [
      "Ying A",
      "Wenzhang Sun",
      "Chang Zeng",
      "Chunfeng Wang",
      "Hao Li",
      "Jianxun Cui"
    ],
    "abstract": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "arxiv_url": "http://arxiv.org/abs/2510.12282v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12282v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic",
      "3d reconstruction",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal\n  Rendering",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jianhao Jiao",
      "Dimitrios Kanoulas",
      "Jun Ma"
    ],
    "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2510.12174v1",
    "pdf_url": "http://arxiv.org/pdf/2510.12174v1",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in\n  Virtual Reality",
    "authors": [
      "Anastasiya Pechko",
      "Piotr Borycki",
      "Joanna Waczyńska",
      "Daniel Barczyk",
      "Agata Szymańska",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce \\our{} (\\textbf{G}aussian \\textbf{S}platting for \\textbf{V}irtual \\textbf{E}nvironment \\textbf{R}endering and \\textbf{S}cene \\textbf{E}diting), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, \\our{} facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.11878v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11878v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "face",
      "gaussian splatting",
      "efficient",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event\n  Streams",
    "authors": [
      "Takuya Nakabayashi",
      "Navami Kairanda",
      "Hideo Saito",
      "Vladislav Golyanik"
    ],
    "abstract": "Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "arxiv_url": "http://arxiv.org/abs/2510.11717v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11717v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation",
    "authors": [
      "Maggie Wang",
      "Stephen Tian",
      "Aiden Swann",
      "Ola Shorinwa",
      "Jiajun Wu",
      "Mac Schwager"
    ],
    "abstract": "Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ .",
    "arxiv_url": "http://arxiv.org/abs/2510.11689v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11689v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via\n  View Alignment",
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "arxiv_url": "http://arxiv.org/abs/2510.11473v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11473v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "lighting",
      "illumination",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent\n  Material Inference",
    "authors": [
      "Wenyuan Zhang",
      "Jimin Tang",
      "Weiqi Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2510.11387v1",
    "pdf_url": "http://arxiv.org/pdf/2510.11387v1",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "ar",
      "reflection",
      "illumination",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular\n  Videos",
    "authors": [
      "Xuankai Zhang",
      "Junjin Xiao",
      "Qing Zhang"
    ],
    "abstract": "This paper presents a unified framework that allows high-quality dynamic Gaussian Splatting from both defocused and motion-blurred monocular videos. Due to the significant difference between the formation processes of defocus blur and motion blur, existing methods are tailored for either one of them, lacking the ability to simultaneously deal with both of them. Although the two can be jointly modeled as blur kernel-based convolution, the inherent difficulty in estimating accurate blur kernels greatly limits the progress in this direction. In this work, we go a step further towards this direction. Particularly, we propose to estimate per-pixel reliable blur kernels using a blur prediction network that exploits blur-related scene and camera information and is subject to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian densification strategy to mitigate the lack of Gaussians for incomplete regions, and boost the performance of novel view synthesis by incorporating unseen view information to constrain scene optimization. Extensive experiments show that our method outperforms the state-of-the-art methods in generating photorealistic novel view synthesis from defocused and motion-blurred monocular videos. Our code and trained model will be made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2510.10691v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10691v1",
    "published_date": "2025-10-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
    "authors": [
      "Haoyu Zhao",
      "Cheng Zeng",
      "Linghao Zhuang",
      "Yaxi Zhao",
      "Shengke Xue",
      "Hao Wang",
      "Xingyue Zhao",
      "Zhongyu Li",
      "Kehan Li",
      "Siteng Huang",
      "Mingxiu Chen",
      "Xin Li",
      "Deli Zhao",
      "Hua Zou"
    ],
    "abstract": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
    "arxiv_url": "http://arxiv.org/abs/2510.10637v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10637v1",
    "published_date": "2025-10-12",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "high-fidelity",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided\n  Framework",
    "authors": [
      "Shanzhi Yin",
      "Bolin Chen",
      "Xinju Wu",
      "Ru-Ling Liao",
      "Jie Chen",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "abstract": "This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.10492v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10492v1",
    "published_date": "2025-10-12",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM",
      "I.4; I.5"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "compact",
      "human",
      "ar",
      "body",
      "compression",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Opacity-Gradient Driven Density Control for Compact and Efficient\n  Few-Shot 3D Gaussian Splatting",
    "authors": [
      "Abdelrhman Elrawy",
      "Emad A. Mohammed"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions. While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count. This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency. We replace the standard positional gradient heuristic with a novel densification trigger that uses the opacity gradient as a lightweight proxy for rendering error. We find this aggressive densification is only effective when paired with a more conservative pruning schedule, which prevents destructive optimization cycles. Combined with a standard depth-correlation loss for geometric guidance, our framework demonstrates a fundamental improvement in efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a reduction of approximately 70%. This dramatic gain in compactness is achieved with a modest trade-off in reconstruction metrics, establishing a new state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2510.10257v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10257v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "compact",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient",
      "few-shot"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Color3D: Controllable and Consistent 3D Colorization with Personalized\n  Colorizer",
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Renlong Wu",
      "Wangmeng Zuo"
    ],
    "abstract": "In this work, we present Color3D, a highly adaptable framework for colorizing both static and dynamic 3D scenes from monochromatic inputs, delivering visually diverse and chromatically vibrant reconstructions with flexible user-guided control. In contrast to existing methods that focus solely on static scenarios and enforce multi-view consistency by averaging color variations which inevitably sacrifice both chromatic richness and controllability, our approach is able to preserve color diversity and steerability while ensuring cross-view and cross-time consistency. In particular, the core insight of our method is to colorize only a single key view and then fine-tune a personalized colorizer to propagate its color to novel views and time steps. Through personalization, the colorizer learns a scene-specific deterministic color mapping underlying the reference view, enabling it to consistently project corresponding colors to the content in novel views and video frames via its inherent inductive bias. Once trained, the personalized colorizer can be applied to infer consistent chrominance for all other images, enabling direct reconstruction of colorful 3D scenes with a dedicated Lab color space Gaussian splatting representation. The proposed framework ingeniously recasts complicated 3D colorization as a more tractable single image paradigm, allowing seamless integration of arbitrary image colorization models with enhanced flexibility and controllability. Extensive experiments across diverse static and dynamic 3D colorization benchmarks substantiate that our method can deliver more consistent and chromatically rich renderings with precise user control. Project Page https://yecongwan.github.io/Color3D/.",
    "arxiv_url": "http://arxiv.org/abs/2510.10152v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10152v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian\n  Splatting",
    "authors": [
      "Jiahui Lu",
      "Haihong Xiao",
      "Xueyan Zhao",
      "Wenxiong Kang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage. These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient. To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images. Unlike prior works that rely on COLMAP for sparse point cloud initialization, we leverage the VGGT foundation model to obtain more reliable initial poses and dense point clouds. Our approach integrates several key innovations: 1) a hybrid Gaussian representation with dual position-shape optimization enhanced by inter-view matching consistency; 2) a graph-guided attribute refinement module to enhance scene details; and 3) flow-based depth regularization that improves depth estimation accuracy for more effective supervision. Comprehensive quantitative and qualitative experiments demonstrate that our approach achieves more robust performance on both forward-facing and large-scale complex datasets compared to other pose-free methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.10097v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10097v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "P-4DGS: Predictive 4D Gaussian Splatting with 90$\\times$ Compression",
    "authors": [
      "Henan Wang",
      "Hanxin Zhu",
      "Xinliang Gong",
      "Tianyu He",
      "Xin Li",
      "Zhibo Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has garnered significant attention due to its superior scene representation fidelity and real-time rendering performance, especially for dynamic 3D scene reconstruction (\\textit{i.e.}, 4D reconstruction). However, despite achieving promising results, most existing algorithms overlook the substantial temporal and spatial redundancies inherent in dynamic scenes, leading to prohibitive memory consumption. To address this, we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene modeling. Inspired by intra- and inter-frame prediction techniques commonly used in video compression, we first design a 3D anchor point-based spatial-temporal prediction module to fully exploit the spatial-temporal correlations across different 3D Gaussian primitives. Subsequently, we employ an adaptive quantization strategy combined with context-based entropy coding to further reduce the size of the 3D anchor points, thereby achieving enhanced compression efficiency. To evaluate the rate-distortion performance of our proposed P-4DGS in comparison with other dynamic 3DGS representations, we conduct extensive experiments on both synthetic and real-world datasets. Experimental results demonstrate that our approach achieves state-of-the-art reconstruction quality and the fastest rendering speed, with a remarkably low storage footprint (around \\textbf{1MB} on average), achieving up to \\textbf{40$\\times$} and \\textbf{90$\\times$} compression on synthetic and real-world scenes, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2510.10030v1",
    "pdf_url": "http://arxiv.org/pdf/2510.10030v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "4d",
      "fast",
      "ar",
      "compression",
      "gaussian splatting",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting",
    "authors": [
      "Zhigang Cheng",
      "Mingchao Sun",
      "Yu Liu",
      "Zengye Ge",
      "Luyang Tang",
      "Mu Xu",
      "Yangyan Li",
      "Peng Pan"
    ],
    "abstract": "Level of Detail (LoD) is a fundamental technique in real-time computer graphics for managing the rendering costs of complex scenes while preserving visual fidelity. Traditionally, LoD is implemented using discrete levels (DLoD), where multiple, distinct versions of a model are swapped out at different distances. This long-standing paradigm, however, suffers from two major drawbacks: it requires significant storage for multiple model copies and causes jarring visual ``popping\" artifacts during transitions, degrading the user experience. We argue that the explicit, primitive-based nature of the emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm: Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality scaling within a single, unified model, thereby circumventing the core problems of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a continuous LoD mechanism directly into a 3DGS representation. Our method introduces a learnable, distance-dependent decay parameter for each Gaussian primitive, which dynamically adjusts its opacity based on viewpoint proximity. This allows for the progressive and smooth filtering of less significant primitives, effectively creating a continuous spectrum of detail within one model. To train this model to be robust across all distances, we introduce a virtual distance scaling mechanism and a novel coarse-to-fine training strategy with rendered point count regularization. Our approach not only eliminates the storage overhead and visual artifacts of discrete methods but also reduces the primitive count and memory footprint of the final model. Extensive experiments demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a single model, delivering high-fidelity results across a wide range of performance targets.",
    "arxiv_url": "http://arxiv.org/abs/2510.09997v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09997v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene\n  Mapping",
    "authors": [
      "Yicheng He",
      "Jingwen Yu",
      "Guangcheng Chen",
      "Hong Zhang"
    ],
    "abstract": "Maintaining an up-to-date map that accurately reflects recent changes in the environment is crucial, especially for robots that repeatedly traverse the same space. Failing to promptly update the changed regions can degrade map quality, resulting in poor localization, inefficient operations, and even lost robots. 3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online map reconstruction due to its dense, differentiable, and photorealistic properties, yet accurately and efficiently updating the regions of change remains a challenge. In this paper, we propose VG-Mapping, a novel online 3DGS-based mapping system tailored for such semi-static scenes. Our approach introduces a hybrid representation that augments 3DGS with a TSDF-based voxel map to efficiently identify changed regions in a scene, along with a variation-aware density control strategy that inserts or deletes Gaussian primitives in regions undergoing change. Furthermore, to address the absence of public benchmarks for this task, we construct a RGB-D dataset comprising both synthetic and real-world semi-static environments. Experimental results demonstrate that our method substantially improves the rendering quality and map update efficiency in semi-static scenes. The code and dataset are available at https://github.com/heyicheng-never/VG-Mapping.",
    "arxiv_url": "http://arxiv.org/abs/2510.09962v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09962v1",
    "published_date": "2025-10-11",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "efficient",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates",
    "authors": [
      "Minkwan Kim",
      "Seungmin Lee",
      "Junho Kim",
      "Young Min Kim"
    ],
    "abstract": "Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.",
    "arxiv_url": "http://arxiv.org/abs/2510.09881v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09881v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient",
      "face",
      "sparse view",
      "few-shot"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision Language Models: A Survey of 26K Papers",
    "authors": [
      "Fengming Lin"
    ],
    "abstract": "We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.",
    "arxiv_url": "http://arxiv.org/abs/2510.09586v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09586v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "human",
      "understanding",
      "ar",
      "nerf",
      "survey",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
    "authors": [
      "Arthur Bizzi",
      "Matias Grynberg",
      "Vitor Matias",
      "Daniel Perazzo",
      "João Paulo Lima",
      "Luiz Velho",
      "Nuno Gonçalves",
      "João Pereira",
      "Guilherme Schardong",
      "Tiago Novello"
    ],
    "abstract": "Morphing is a long-standing problem in vision and computer graphics, requiring a time-dependent warping for feature alignment and a blending for smooth interpolation. Recently, multilayer perceptrons (MLPs) have been explored as implicit neural representations (INRs) for modeling such deformations, due to their meshlessness and differentiability; however, extracting coherent and accurate morphings from standard MLPs typically relies on costly regularizations, which often lead to unstable training and prevent effective feature alignment. To overcome these limitations, we propose FLOWING (FLOW morphING), a framework that recasts warping as the construction of a differential vector flow, naturally ensuring continuity, invertibility, and temporal coherence by encoding structural flow properties directly into the network architectures. This flow-centric approach yields principled and stable transformations, enabling accurate and structure-preserving morphing of both 2D images and 3D shapes. Extensive experiments across a range of applications - including face and image morphing, as well as Gaussian Splatting morphing - show that FLOWING achieves state-of-the-art morphing quality with faster convergence. Code and pretrained models are available at http://schardong.github.io/flowing.",
    "arxiv_url": "http://arxiv.org/abs/2510.09537v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09537v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV",
      "I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "face",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Two-Stage Gaussian Splatting Optimization for Outdoor Scene\n  Reconstruction",
    "authors": [
      "Deborah Pintani",
      "Ariel Caputo",
      "Noah Lewis",
      "Marc Stamminger",
      "Fabio Pellacini",
      "Andrea Giachetti"
    ],
    "abstract": "Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis. In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes. In stage two, foreground Gaussians are initialized from a Structure-from-Motion reconstruction, added and refined using the standard rendering loss, while the background set remains fixed but contributes to the final image formation. Experiments on diverse outdoor datasets show that our method reduces background artifacts and improves perceptual quality compared to state-of-the-art baselines. Moreover, the explicit background separation enables automatic, object-free environment map estimation, opening new possibilities for photorealistic outdoor rendering and mixed-reality applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.09489v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09489v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "illumination",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic\n  Urban Scenes",
    "authors": [
      "Yikang Zhang",
      "Rui Fan"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive performance in synthesizing high-fidelity novel views. Nonetheless, its effectiveness critically depends on the quality of the initialized point cloud. Specifically, achieving uniform and complete point coverage over the underlying scene structure requires overlapping observation frustums, an assumption that is often violated in unbounded, dynamic urban environments. Training Gaussian models with partially initialized point clouds often leads to distortions and artifacts, as camera rays may fail to intersect valid surfaces, resulting in incorrect gradient propagation to Gaussian primitives associated with occluded or invisible geometry. Additionally, existing densification strategies simply clone and split Gaussian primitives from existing ones, incapable of reconstructing missing structures. To address these limitations, we propose VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban scenes. Our method identifies unreliable geometry structures via voxel-based visibility reasoning, selects informative supporting views through diversity-aware view selection, and recovers missing structures via patch matching-based multi-view stereo reconstruction. This design enables the generation of new Gaussian primitives guided by reliable geometric priors, even in regions lacking initial points. Extensive experiments on the Waymo and nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS approaches and significantly improves the quality of reconstructed geometry for both static and dynamic objects. Source code will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2510.09364v1",
    "pdf_url": "http://arxiv.org/pdf/2510.09364v1",
    "published_date": "2025-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "dynamic",
      "geometry",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "authors": [
      "Haofei Xu",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08575v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08575v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and\n  Accurate Sparse-View Reconstruction",
    "authors": [
      "Meixi Song",
      "Xin Lin",
      "Dizhe Zhang",
      "Haodong Li",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08566v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08566v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "authors": [
      "Xilong Zhou",
      "Bao-Huy Nguyen",
      "Loïc Magne",
      "Vladislav Golyanik",
      "Thomas Leimkühler",
      "Christian Theobalt"
    ],
    "abstract": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "arxiv_url": "http://arxiv.org/abs/2510.08491v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08491v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "geometry",
      "ray marching"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D\n  Gaussian Splatting",
    "authors": [
      "Ankit Gahlawat",
      "Anirban Mukherjee",
      "Dinesh Babu Jayagopi"
    ],
    "abstract": "Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.",
    "arxiv_url": "http://arxiv.org/abs/2510.08096v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08096v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "segmentation",
      "human",
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal\n  Reconstruction Model for Autonomous Driving",
    "authors": [
      "Tianrui Zhang",
      "Yichen Liu",
      "Zilin Guo",
      "Yuxin Guo",
      "Jingcheng Ni",
      "Chenjing Ding",
      "Dan Xu",
      "Lewei Lu",
      "Zehuan Wu"
    ],
    "abstract": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2510.07944v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07944v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "understanding",
      "4d",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale\n  3D Gaussian Splatting",
    "authors": [
      "Houqiang Zhong",
      "Zhenglong Wu",
      "Sihua Fu",
      "Zihan Zheng",
      "Xin Jin",
      "Xiaoyun Zhang",
      "Li Song",
      "Qiang Hu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.",
    "arxiv_url": "http://arxiv.org/abs/2510.07830v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07830v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event\n  Stream",
    "authors": [
      "Junhao He",
      "Jiaxu Wang",
      "Jia Li",
      "Mingyuan Sun",
      "Qiang Zhang",
      "Jiahang Cao",
      "Ziyi Zhang",
      "Yi Gu",
      "Jingkai Sun",
      "Renjing Xu"
    ],
    "abstract": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "arxiv_url": "http://arxiv.org/abs/2510.07752v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07752v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "deformation",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral\n  Probes",
    "authors": [
      "Jian Gao",
      "Mengqi Yuan",
      "Yifei Zeng",
      "Chang Zeng",
      "Zhihao Li",
      "Zhenyu Chen",
      "Weichao Qiu",
      "Xiao-Xiao Long",
      "Hao Zhu",
      "Xun Cao",
      "Yao Yao"
    ],
    "abstract": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.",
    "arxiv_url": "http://arxiv.org/abs/2510.07729v1",
    "pdf_url": "http://arxiv.org/pdf/2510.07729v1",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "ray tracing",
      "light transport",
      "ar",
      "lighting",
      "shadow",
      "gaussian splatting",
      "efficient",
      "face",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
    "authors": [
      "Huanning Dong",
      "Fan Li",
      "Ping Kuang",
      "Jianwen Min"
    ],
    "abstract": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.",
    "arxiv_url": "http://arxiv.org/abs/2510.06967v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06967v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "geometry",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with\n  Gaussian Splatting in Unity",
    "authors": [
      "Islomjon Shukhratov",
      "Sergey Gorinsky"
    ],
    "abstract": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.",
    "arxiv_url": "http://arxiv.org/abs/2510.06802v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06802v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D\n  Novel View Synthesis",
    "authors": [
      "Jipeng Lyu",
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ],
    "abstract": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "arxiv_url": "http://arxiv.org/abs/2510.06694v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06694v1",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "4d",
      "ar",
      "tracking",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy\n  Reduction",
    "authors": [
      "Leshu Li",
      "Jiayin Qin",
      "Jie Peng",
      "Zishen Wan",
      "Huaizhi Qu",
      "Ye Han",
      "Pingqing Zheng",
      "Hongsen Zhang",
      "Yu Cao",
      "Tianlong Chen",
      "Yang Katie Zhao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping (SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering efficiency and accuracy, but have not yet been adopted in resource-constrained edge devices due to insufficient speed. Addressing this, we identify notable redundancies across the SLAM pipeline for acceleration. While conceptually straightforward, practical approaches are required to minimize the overhead associated with identifying and eliminating these redundancies. In response, we propose RTGS, an algorithm-hardware co-design framework that comprehensively reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline. On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to remove the redundant Gaussians by reusing gradients computed during backpropagation; and (2) a dynamic downsampling technique that directly reuses the keyframe identification and alpha computing steps to eliminate redundant pixels. On the hardware side, we propose (1) a subtile-level streaming strategy and a pixel-level pairwise scheduling strategy that mitigates workload imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates the rendering backpropagation by reusing intermediate data computed during rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory accesses caused by atomic operations while enabling pipelined aggregation. Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on four datasets and three algorithms, with up to 82.5x energy efficiency over the baseline and negligible quality loss. Code is available at https://github.com/UMN-ZhaoLab/RTGS.",
    "arxiv_url": "http://arxiv.org/abs/2510.06644v2",
    "pdf_url": "http://arxiv.org/pdf/2510.06644v2",
    "published_date": "2025-10-08",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "slam",
      "mapping",
      "acceleration",
      "gaussian splatting",
      "dynamic",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning",
    "authors": [
      "Amirhossein Mollaei Khass",
      "Guangyi Liu",
      "Vivek Pandey",
      "Wen Jiang",
      "Boshu Lei",
      "Kostas Daniilidis",
      "Nader Motee"
    ],
    "abstract": "Safe navigation in uncertain environments requires planning methods that integrate risk aversion with active perception. In this work, we present a unified framework that refines a coarse reference path by constructing tail-sensitive risk maps from Average Value-at-Risk statistics on an online-updated 3D Gaussian-splat Radiance Field. These maps enable the generation of locally safe and feasible trajectories. In parallel, we formulate Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose manifold, where Riemannian gradient descent maximizes an expected information gain objective to reduce uncertainty most critical for imminent motion. Our approach advances the state-of-the-art by coupling risk-averse path refinement with NBV planning, while introducing scalable gradient decompositions that support efficient online updates in complex environments. We demonstrate the effectiveness of the proposed framework through extensive computational studies.",
    "arxiv_url": "http://arxiv.org/abs/2510.06481v1",
    "pdf_url": "http://arxiv.org/pdf/2510.06481v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "motion",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head\n  Avatars",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.",
    "arxiv_url": "http://arxiv.org/abs/2510.05488v1",
    "pdf_url": "http://arxiv.org/pdf/2510.05488v1",
    "published_date": "2025-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized Minimal 4D Gaussian Splatting",
    "authors": [
      "Minseo Lee",
      "Byeonghyeon Lee",
      "Lucas Yunkyu Lee",
      "Eunsoo Lee",
      "Sangmin Kim",
      "Seunghyeon Song",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Jaesik Park",
      "Eunbyung Park"
    ],
    "abstract": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.",
    "arxiv_url": "http://arxiv.org/abs/2510.03857v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03857v1",
    "published_date": "2025-10-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "head",
      "compact",
      "4d",
      "ar",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "face",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches",
    "authors": [
      "Sixten Norelius",
      "Aaron O. Feldman",
      "Mac Schwager"
    ],
    "abstract": "We propose SketchPlan, a diffusion-based planner that interprets 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation. SketchPlan comprises two components: a SketchAdapter that learns to map the human sketches to projected 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and a first person view depth image. Our model achieves zero-shot sim-to-real transfer, generating accurate and safe flight paths in previously unseen real-world environments. To train the model, we build a synthetic dataset of 32k flight paths using a diverse set of photorealistic 3D Gaussian Splatting scenes. We automatically label the data by computing 2D projections of the 3D flight paths onto the camera plane, and use this to train the DiffPath diffusion model. However, since real human 2D sketches differ significantly from ideal 2D projections, we additionally label 872 of the 3D flight paths with real human sketches and use this to train the SketchAdapter to infer the 2D projection from the human sketch. We demonstrate SketchPlan's effectiveness in both simulated and real-world experiments, and show through ablations that training on a mix of human labeled and auto-labeled data together with a modular design significantly boosts its capabilities to correctly interpret human intent and infer 3D paths. In real-world drone tests, SketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen high-clutter environments, outperforming key ablations by 20-60\\% in task completion.",
    "arxiv_url": "http://arxiv.org/abs/2510.03545v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03545v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "human",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled\n  Fields",
    "authors": [
      "Zhiting Mei",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.",
    "arxiv_url": "http://arxiv.org/abs/2510.03104v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03104v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "geometry",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient\n  Gaussian Deformation",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.",
    "arxiv_url": "http://arxiv.org/abs/2510.08587v1",
    "pdf_url": "http://arxiv.org/pdf/2510.08587v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "animation",
      "gaussian splatting",
      "efficient",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian\n  Splatting",
    "authors": [
      "Xinran Zhang",
      "Hanqi Zhu",
      "Yifan Duan",
      "Yanyong Zhang"
    ],
    "abstract": "Constructing and sharing 3D maps is essential for many applications, including autonomous driving and augmented reality. Recently, 3D Gaussian splatting has emerged as a promising approach for accurate 3D reconstruction. However, a practical map-sharing system that features high-fidelity, continuous updates, and network efficiency remains elusive. To address these challenges, we introduce GS-Share, a photorealistic map-sharing system with a compact representation. The core of GS-Share includes anchor-based global map construction, virtual-image-based map enhancement, and incremental map update. We evaluate GS-Share against state-of-the-art methods, demonstrating that our system achieves higher fidelity, particularly for extrapolated views, with improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively. Furthermore, GS-Share is significantly more compact, reducing map transmission overhead by 36%.",
    "arxiv_url": "http://arxiv.org/abs/2510.02884v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02884v1",
    "published_date": "2025-10-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FSFSplatter: Build Surface and Novel Views with Sparse-Views within 2min",
    "authors": [
      "Yibin Zhao",
      "Yihan Pan",
      "Jun Nan",
      "Liwei Chen",
      "Jianjun Yi"
    ],
    "abstract": "Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU, Replica, and BlendedMVS datasets.",
    "arxiv_url": "http://arxiv.org/abs/2510.02691v2",
    "pdf_url": "http://arxiv.org/pdf/2510.02691v2",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "head",
      "ar",
      "fast",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D\n  Gaussian Splatting",
    "authors": [
      "Sung-Yeon Park",
      "Adam Lee",
      "Juanwu Lu",
      "Can Cui",
      "Luyang Jiang",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ahmadreza Moradipari",
      "Ziran Wang"
    ],
    "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
    "arxiv_url": "http://arxiv.org/abs/2510.02469v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02469v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ],
    "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
    "arxiv_url": "http://arxiv.org/abs/2510.02314v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02314v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Performance-Guided Refinement for Visual Aerial Navigation using\n  Editable Gaussian Splatting in FalconGym 2.0",
    "authors": [
      "Yan Miao",
      "Ege Yuceel",
      "Georgios Fainekos",
      "Bardh Hoxha",
      "Hideki Okamoto",
      "Sayan Mitra"
    ],
    "abstract": "Visual policy design is crucial for aerial navigation. However, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes. We develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds. Leveraging FalconGym 2.0's editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy's training on challenging tracks while iteratively improving its performance. Across two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations. Finally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
    "arxiv_url": "http://arxiv.org/abs/2510.02248v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02248v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "geometry",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy\n  Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.02069v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02069v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "neural rendering",
      "ar",
      "lighting",
      "illumination",
      "reflection",
      "gaussian splatting",
      "face",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object\n  Morphing",
    "authors": [
      "Mengtian Li",
      "Yunshu Bai",
      "Yimin Chu",
      "Yijun Shen",
      "Zhongmei Li",
      "Weifeng Ge",
      "Zhifeng Xie",
      "Chaofeng Chen"
    ],
    "abstract": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2510.02034v1",
    "pdf_url": "http://arxiv.org/pdf/2510.02034v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "mapping",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
    "authors": [
      "Lei Liu",
      "Can Wang",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2510.01991v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01991v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "4d",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting",
    "authors": [
      "Quoc-Anh Bui",
      "Gilles Rougeron",
      "Géraldine Morin",
      "Simone Gasparini"
    ],
    "abstract": "We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\\approx 17\\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2510.01978v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01978v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV",
      "68U05, 68T45 (Primary) 68T07, 68-04 (Secondary)",
      "I.2.10; I.3.3; I.3.5; I.3.7; I.4.5; I.4.6; I.4.8; I.4.10"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for\n  Mobile Robotics",
    "authors": [
      "Diram Tabaa",
      "Gianni Di Caro"
    ],
    "abstract": "Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.",
    "arxiv_url": "http://arxiv.org/abs/2510.01848v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01848v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "localization",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for\n  Large-Scale Scene Reconstruction",
    "authors": [
      "Sheng-Hsiang Hung",
      "Ting-Yu Yen",
      "Wei-Fang Sun",
      "Simon See",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2510.01767v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01767v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "head",
      "ar",
      "fast",
      "efficient",
      "high-fidelity",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust\n  Physics-Based Dynamics",
    "authors": [
      "Changmin Lee",
      "Jihyun Lee",
      "Tae-Kyun Kim"
    ],
    "abstract": "While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/",
    "arxiv_url": "http://arxiv.org/abs/2510.01619v1",
    "pdf_url": "http://arxiv.org/pdf/2510.01619v1",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "human",
      "ar",
      "animation",
      "shadow",
      "body",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Beta Splatting",
    "authors": [
      "Rong Liu",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meida Chen",
      "Van Nguyen Nguyen",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Yue Wang",
      "Andrew Feng",
      "Ziyan Wu"
    ],
    "abstract": "We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at https://rongliu-leo.github.io/universal-beta-splatting/.",
    "arxiv_url": "http://arxiv.org/abs/2510.03312v1",
    "pdf_url": "http://arxiv.org/pdf/2510.03312v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HART: Human Aligned Reconstruction Transformer",
    "authors": [
      "Xiyi Chen",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Taewon Kang",
      "Sergey Prokudin",
      "Ming Lin"
    ],
    "abstract": "We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2509.26621v1",
    "pdf_url": "http://arxiv.org/pdf/2509.26621v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "body",
      "human",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts",
    "authors": [
      "Zhenyu Shu",
      "Junlong Yu",
      "Kai Chao",
      "Shiqing Xin",
      "Ligang Liu"
    ],
    "abstract": "This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.26055v1",
    "pdf_url": "http://arxiv.org/pdf/2509.26055v1",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting\n  Kernels",
    "authors": [
      "Yi Hu",
      "Huiyang Zhou"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.",
    "arxiv_url": "http://arxiv.org/abs/2509.25626v2",
    "pdf_url": "http://arxiv.org/pdf/2509.25626v2",
    "published_date": "2025-09-30",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianLens: Localized High-Resolution Reconstruction via On-Demand\n  Gaussian Densification",
    "authors": [
      "Yijia Weng",
      "Zhicheng Wang",
      "Songyou Peng",
      "Saining Xie",
      "Howard Zhou",
      "Leonidas J. Guibas"
    ],
    "abstract": "We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\\times1024$ resolution.",
    "arxiv_url": "http://arxiv.org/abs/2509.25603v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25603v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "fast",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
    "arxiv_url": "http://arxiv.org/abs/2509.25122v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25122v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "head",
      "ar",
      "fast",
      "nerf",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM\n  Reconstruction",
    "authors": [
      "Huaizhi Qu",
      "Xiao Wang",
      "Gengwei Zhang",
      "Jie Peng",
      "Tianlong Chen"
    ],
    "abstract": "Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.",
    "arxiv_url": "http://arxiv.org/abs/2509.25075v2",
    "pdf_url": "http://arxiv.org/pdf/2509.25075v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "fast",
      "nerf",
      "gaussian splatting",
      "efficient",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
    "authors": [
      "Tooba Imtiaz",
      "Lucy Chai",
      "Kathryn Heal",
      "Xuan Luo",
      "Jungyeon Park",
      "Jennifer Dy",
      "John Flynn"
    ],
    "abstract": "Large transformer models are proving to be a powerful tool for 3D vision and novel view synthesis. However, the standard Transformer's well-known quadratic complexity makes it difficult to scale these methods to large scenes. To address this challenge, we propose the Local View Transformer (LVT), a large-scale scene reconstruction and novel view synthesis architecture that circumvents the need for the quadratic attention operation. Motivated by the insight that spatially nearby views provide more useful signal about the local scene composition than distant views, our model processes all information in a local neighborhood around each view. To attend to tokens in nearby views, we leverage a novel positional encoding that conditions on the relative geometric transformation between the query and nearby views. We decode the output of our model into a 3D Gaussian Splat scene representation that includes both color and opacity view-dependence. Taken together, the Local View Transformer enables reconstruction of arbitrarily large, high-resolution scenes in a single forward pass. See our project page for results and interactive demos https://toobaimt.github.io/lvt/.",
    "arxiv_url": "http://arxiv.org/abs/2509.25001v1",
    "pdf_url": "http://arxiv.org/pdf/2509.25001v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "large scene",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping",
    "authors": [
      "Yu Ma",
      "Guoliang Wei",
      "Haihong Xiao",
      "Yue Cheng"
    ],
    "abstract": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge in 3D reconstruction, where limited multi-view constraints lead to severe overfitting, geometric distortion, and fragmented scenes. While 3D Gaussian Splatting (3DGS) delivers real-time, high-fidelity rendering, its performance drastically deteriorates under sparse inputs, plagued by floating artifacts and structural failures. To address these challenges, we introduce HBSplat, a unified framework that elevates 3DGS by seamlessly integrating robust structural cues, virtual view constraints, and occluded region completion. Our core contributions are threefold: a Hybrid-Loss Depth Estimation module that ensures multi-view consistency by leveraging dense matching priors and integrating reprojection, point propagation, and smoothness constraints; a Bidirectional Warping Virtual View Synthesis method that enforces substantially stronger constraints by creating high-fidelity virtual views through bidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware Reconstruction component that recovers occluded areas using a depth-difference mask and a learning-based inpainting model. Extensive evaluations on LLFF, Blender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time inference. Code is available at: https://github.com/eternalland/HBSplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.24893v3",
    "pdf_url": "http://arxiv.org/pdf/2509.24893v3",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "sparse view",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
    "authors": [
      "Jiaqi Chen",
      "Xinhao Ji",
      "Yuanyuan Gao",
      "Hao Li",
      "Yuning Gong",
      "Yifei Liu",
      "Dan Xu",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun"
    ],
    "abstract": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce ExGS, a novel feed-forward framework that unifies Universal Gaussian Compression (UGC) with GaussPainter for Extreme 3DGS compression. UGC performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas GaussPainter leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over 100X compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at: https://github.com/chenttt2001/ExGS",
    "arxiv_url": "http://arxiv.org/abs/2509.24758v4",
    "pdf_url": "http://arxiv.org/pdf/2509.24758v4",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "neural rendering",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh",
    "authors": [
      "Yuanyuan Gao",
      "Yuning Gong",
      "Yifei Liu",
      "Li Jingfeng",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Yanci Zhang",
      "Dan Xu",
      "Xiao Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2509.24421v2",
    "pdf_url": "http://arxiv.org/pdf/2509.24421v2",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for\n  Robust Scene-Level Surface Reconstruction",
    "authors": [
      "Yuhang Cao",
      "Haojun Yan",
      "Danya Yao"
    ],
    "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2509.24308v1",
    "pdf_url": "http://arxiv.org/pdf/2509.24308v1",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting",
    "authors": [
      "Dragoş-Andrei Chileban",
      "Andrei-Ştefan Bulzan",
      "Cosmin Cernǎzanu-Glǎvan"
    ],
    "abstract": "Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments. However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage. Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views. In this work we introduce an automatic car damage detection pipeline that performs 3D damage segmentation by up-lifting 2D masks. Additionally, we propose a simple yet effective learning-free approach for single-view 3D-GS segmentation. Specifically, Gaussians are projected onto the image plane using camera parameters obtained via Structure from Motion (SfM). They are then filtered through an algorithm that utilizes Z-buffering along with a normal distribution model of depth and opacities. Through experiments we found that this method is particularly effective for challenging scenarios like car damage detection, where target objects (e.g., scratches, small dents) may only be clearly visible in a single view, making multi-view consistency approaches impractical or impossible. The code is publicly available at: https://github.com/DragosChileban/CrashSplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.23947v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23947v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "segmentation",
      "ar",
      "fast",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene\n  Representations",
    "authors": [
      "Javed Ahmad",
      "Penggang Gao",
      "Donatien Delehelle",
      "Mennuti Canio",
      "Nikhil Deshpande",
      "Jesús Ortiz",
      "Darwin G. Caldwell",
      "Yonas Teodros Tefera"
    ],
    "abstract": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted. NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding. This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation. Despite their differences, these domains share common goals: photorealistic rendering, meaningful 3D structure, and accurate downstream tasks. We organize the review around unified research questions that explain why 3DGS is increasingly displacing NeRF-based approaches: What technical advantages drive its adoption? How does it adapt to different input modalities and domain-specific constraints? What limitations remain? By systematically comparing domain-specific pipelines, we show that 3DGS balances photorealism, geometric fidelity, and computational efficiency. The survey offers a roadmap for leveraging neural rendering not only for image synthesis but also for perception, interaction, and content creation across real and virtual environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.23555v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23555v1",
    "published_date": "2025-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "understanding",
      "fast",
      "nerf",
      "slam",
      "ar",
      "survey",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual\n  Videos",
    "authors": [
      "Junyi Wu",
      "Jiachen Tao",
      "Haoxuan Wang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ],
    "abstract": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2509.23492v1",
    "pdf_url": "http://arxiv.org/pdf/2509.23492v1",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting",
      "deformation",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
    "authors": [
      "Atakan Topaloglu",
      "Kunyi Li",
      "Michael Niemeyer",
      "Nassir Navab",
      "A. Murat Tekalp",
      "Federico Tombari"
    ],
    "abstract": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
    "arxiv_url": "http://arxiv.org/abs/2509.23258v2",
    "pdf_url": "http://arxiv.org/pdf/2509.23258v2",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Unified Representation of 3D Gaussian Splatting",
    "authors": [
      "Yuelin Xin",
      "Yuheng Liu",
      "Xiaohui Xie",
      "Xinke Li"
    ],
    "abstract": "A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2509.22917v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22917v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "I.4.10"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "efficient",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-Language Alignment from Compressed Image Representations using 2D\n  Gaussian Splatting",
    "authors": [
      "Yasmine Omri",
      "Connor Ding",
      "Tsachy Weissman",
      "Thierry Tambe"
    ],
    "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.",
    "arxiv_url": "http://arxiv.org/abs/2509.22615v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22615v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "ar",
      "fast",
      "efficient",
      "gaussian splatting",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material\n  Decomposition",
    "authors": [
      "Dinh Minh Nguyen",
      "Malte Avenhaus",
      "Thomas Lindemeier"
    ],
    "abstract": "We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.22276v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22276v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting",
    "authors": [
      "Jiayu Ding",
      "Xinpeng Liu",
      "Zhiyi Pan",
      "Shiqiang Long",
      "Ge Li"
    ],
    "abstract": "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.",
    "arxiv_url": "http://arxiv.org/abs/2509.22225v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22225v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Material Gaussian Model for Relightable 3D Generation",
    "authors": [
      "Jingrui Ye",
      "Lingting Zhu",
      "Runze Zhang",
      "Zeyu Hu",
      "Yingda Yin",
      "Lanjiong Li",
      "Lequan Yu",
      "Qingmin Liao"
    ],
    "abstract": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
    "arxiv_url": "http://arxiv.org/abs/2509.22112v1",
    "pdf_url": "http://arxiv.org/pdf/2509.22112v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drag4D: Align Your Motion with Text-Driven 3D Scene Generation",
    "authors": [
      "Minjun Kang",
      "Inkyu Shin",
      "Taeyeop Lee",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ],
    "abstract": "We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.",
    "arxiv_url": "http://arxiv.org/abs/2509.21888v1",
    "pdf_url": "http://arxiv.org/pdf/2509.21888v1",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "4d",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Novel View Synthesis in High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Zhipeng Xiong",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2509.21853v2",
    "pdf_url": "http://arxiv.org/pdf/2509.21853v2",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "mapping",
      "lighting",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in\n  Power-Constrained XR Systems",
    "authors": [
      "Weikai Lin",
      "Sushant Kondguli",
      "Carl Marshall",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.",
    "arxiv_url": "http://arxiv.org/abs/2509.21702v1",
    "pdf_url": "http://arxiv.org/pdf/2509.21702v1",
    "published_date": "2025-09-25",
    "categories": [
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian splatting holography",
    "authors": [
      "Shuhe Zhang",
      "Liangcai Cao"
    ],
    "abstract": "In-line holography offers high space-bandwidth product imaging with a simplified lens-free optical system. However, in-line holographic reconstruction is troubled by twin images arising from the Hermitian symmetry of complex fields. Twin images disrupt the reconstruction in solving the ill-posed phase retrieval problem. The known parameters are less than the unknown parameters, causing phase ambiguities. State-of-the-art deep-learning or non-learning methods face challenges in balancing data fidelity with twin-image disturbance. We propose the Gaussian splatting holography (GSH) for twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting for optical field representation and compresses the number of unknown parameters by a maximum of 15 folds, transforming the original ill-posed phase retrieval into a well-posed one with reduced phase ambiguities. Additionally, the Gaussian splatting tends to form sharp patterns rather than those with noisy twin-image backgrounds as each Gaussian has a spatially slow-varying profile. Experiments show that GSH achieves constraint-free recovery for in-line holography with accuracy comparable to state-of-the-art constraint-based methods, with an average peak signal-to-noise ratio equal to 26 dB, and structure similarity equal to 0.8. Combined with total variation, GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB, and a high compression ability of up to 15 folds.",
    "arxiv_url": "http://arxiv.org/abs/2509.20774v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20774v1",
    "published_date": "2025-09-25",
    "categories": [
      "physics.optics",
      "math.OC",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Driving Scene Generation With Stereo Forcing",
    "authors": [
      "Hao Lu",
      "Zhuang Ma",
      "Guangfeng Jiang",
      "Wenhang Ge",
      "Bohan Li",
      "Yuzhan Cai",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Yingcong Chen"
    ],
    "abstract": "Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
    "arxiv_url": "http://arxiv.org/abs/2509.20251v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20251v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for\n  Driving Scenes",
    "authors": [
      "Guo Chen",
      "Jiarun Liu",
      "Sicong Du",
      "Chenming Wu",
      "Deqi Li",
      "Shi-Sheng Huang",
      "Guofeng Zhang",
      "Sheng Yang"
    ],
    "abstract": "This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: https://shanzhaguoo.github.io/GS-RoadPatching/",
    "arxiv_url": "http://arxiv.org/abs/2509.19937v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19937v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based\n  Intermediate View Rendering",
    "authors": [
      "Jiangxue Yu",
      "Hui Wang",
      "San Jiang",
      "Xing Zhang",
      "Dejin Zhang",
      "Qingquan Li"
    ],
    "abstract": "The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences. The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes. First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage. Second, 3D Gaussian Splatting is then adopted for scene rendering by taking as inputs sparse points and oriented images. For accurate view rendering, a render viewpoint determination algorithm is designed by using the oriented camera poses of aerial images, which is used to generate high-quality intermediate images that can bridge the gap between aerial and ground images. Third, with the aid of intermediate images, reliable feature matching is conducted for match pairs from render-aerial and render-ground images, and final matches can be generated by transmitting correspondences through intermediate views. By using real aerial and ground datasets, the validation of the proposed solution has been verified in terms of feature matching and scene rendering and compared comprehensively with widely used methods. The experimental results demonstrate that the proposed solution can provide reliable feature matches for aerial and ground images with an obvious increase in the number of initial and refined matches, and it can provide enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based scene rendering.",
    "arxiv_url": "http://arxiv.org/abs/2509.19898v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19898v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth\n  Estimation via 3D Gaussian Splatting",
    "authors": [
      "Yixun Zhang",
      "Feng Zhou",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.19793v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19793v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "autonomous driving",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface\n  Reconstruction",
    "authors": [
      "Yufei Han",
      "Bowen Tie",
      "Heng Guo",
      "Youwei Lyu",
      "Si Li",
      "Boxin Shi",
      "Yunpeng Jia",
      "Zhanyu Ma"
    ],
    "abstract": "Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2509.19726v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19726v1",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "shape reconstruction",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian\n  Bracketing",
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ],
    "abstract": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.",
    "arxiv_url": "http://arxiv.org/abs/2509.20400v1",
    "pdf_url": "http://arxiv.org/pdf/2509.20400v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction",
    "authors": [
      "Weijie Wang",
      "Yeqing Chen",
      "Zeyu Zhang",
      "Hengyu Liu",
      "Haoxiao Wang",
      "Zhiyuan Feng",
      "Wenkang Qin",
      "Zheng Zhu",
      "Donny Y. Chen",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
    "arxiv_url": "http://arxiv.org/abs/2509.19297v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19297v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
    "authors": [
      "Sherwin Bahmani",
      "Tianchang Shen",
      "Jiawei Ren",
      "Jiahui Huang",
      "Yifeng Jiang",
      "Haithem Turki",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Zan Gojcic",
      "Sanja Fidler",
      "Huan Ling",
      "Jun Gao",
      "Xuanchi Ren"
    ],
    "abstract": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2509.19296v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19296v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting",
      "dynamic",
      "3d reconstruction",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian\n  Object Reconstruction",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
    "arxiv_url": "http://arxiv.org/abs/2509.19073v1",
    "pdf_url": "http://arxiv.org/pdf/2509.19073v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "sparse-view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in\n  Mirror-Containing Environments with Gaussian Splatting",
    "authors": [
      "Zijing Guo",
      "Yunyang Zhao",
      "Lin Wang"
    ],
    "abstract": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.18956v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18956v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "mapping",
      "reflection",
      "gaussian splatting",
      "face",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust\n  Deblurring",
    "authors": [
      "Pengteng Li",
      "Yunfan Lu",
      "Pinhao Song",
      "Weiyu Guo",
      "Huizai Yao",
      "F. Richard Yu",
      "Hui Xiong"
    ],
    "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.",
    "arxiv_url": "http://arxiv.org/abs/2509.18898v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18898v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score\n  Distillation",
    "authors": [
      "Zhaorui Wang",
      "Yi Gu",
      "Deming Zhou",
      "Renjing Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.",
    "arxiv_url": "http://arxiv.org/abs/2509.18759v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18759v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for\n  Drones",
    "authors": [
      "Maximilian Adang",
      "JunEn Low",
      "Ola Shorinwa",
      "Mac Schwager"
    ],
    "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions.",
    "arxiv_url": "http://arxiv.org/abs/2509.18610v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18610v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "semantic",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene\n  Reconstruction",
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Kailun Yang",
      "Jiajun Zhai",
      "Shangwei Guo",
      "Lin Wang",
      "Kaiwei Wang"
    ],
    "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
    "arxiv_url": "http://arxiv.org/abs/2509.18566v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18566v1",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "fast",
      "animation",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Differentiable Light Transport with Gaussian Surfels via Adapted\n  Radiosity for Efficient Relighting and Geometry Reconstruction",
    "authors": [
      "Kaiwen Jiang",
      "Jia-Mu Sun",
      "Zilu Li",
      "Dan Wang",
      "Tzu-Mao Li",
      "Ravi Ramamoorthi"
    ],
    "abstract": "Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.",
    "arxiv_url": "http://arxiv.org/abs/2509.18497v2",
    "pdf_url": "http://arxiv.org/pdf/2509.18497v2",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "global illumination",
      "relighting",
      "ar",
      "lighting",
      "illumination",
      "reflection",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface\n  Reconstruction",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Youmin Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Xiaohan Yu",
      "Lin Gu"
    ],
    "abstract": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
    "arxiv_url": "http://arxiv.org/abs/2509.18090v1",
    "pdf_url": "http://arxiv.org/pdf/2509.18090v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "vr",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring\n  the Pareto frontier in multi-criteria optimization",
    "authors": [
      "Phuong Mai Dinh",
      "Van-Nam Huynh"
    ],
    "abstract": "Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.",
    "arxiv_url": "http://arxiv.org/abs/2509.17889v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17889v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for\n  Underwater Scenes",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Zipeng Qi",
      "Wenjun Lu",
      "David Bull",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2509.17789v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17789v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "illumination",
      "gaussian splatting",
      "lightweight",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian\n  Splats from a Mobile Device",
    "authors": [
      "Gunjan Chhablani",
      "Xiaomeng Ye",
      "Muhammad Zubair Irshad",
      "Zsolt Kira"
    ],
    "abstract": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.",
    "arxiv_url": "http://arxiv.org/abs/2509.17430v2",
    "pdf_url": "http://arxiv.org/pdf/2509.17430v2",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS\n  Models to LiDAR",
    "authors": [
      "Junzhe Wu",
      "Yufei Jia",
      "Yiyi Yan",
      "Zhixing Chen",
      "Tiao Tan",
      "Zifan Wang",
      "Guangyu Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic rendering, its vast ecosystem of assets remains incompatible with high-performance LiDAR simulation, a critical tool for robotics and autonomous driving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with a truly plug-and-play approach. Our method converts \\textit{any} pretrained 3DGS model into a high-fidelity, watertight mesh without requiring LiDAR-specific supervision or architectural alterations. This conversion is achieved through a general pipeline of volumetric discretization and Truncated Signed Distance Field (TSDF) extraction. We pair this with a highly optimized, GPU-accelerated ray-casting module that simulates LiDAR returns at over 500 FPS. We validate our approach on indoor and outdoor scenes, demonstrating exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for geometrically accurate depth sensing, our framework extends their utility beyond visualization and unlocks new capabilities for scalable, multimodal simulation. Our open-source implementation is available at https://github.com/TATP-233/FGGS-LiDAR.",
    "arxiv_url": "http://arxiv.org/abs/2509.17390v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17390v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.RO",
      "68T40, 68U05",
      "I.6.8"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "fast",
      "robotics",
      "high-fidelity",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene\n  Reconstruction",
    "authors": [
      "Neham Jain",
      "Andrew Jong",
      "Sebastian Scherer",
      "Ioannis Gkioulekas"
    ],
    "abstract": "Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.",
    "arxiv_url": "http://arxiv.org/abs/2509.17329v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17329v1",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting\n  from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.",
    "arxiv_url": "http://arxiv.org/abs/2509.17246v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17246v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel\n  View Synthesis",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
    "arxiv_url": "http://arxiv.org/abs/2509.17083v2",
    "pdf_url": "http://arxiv.org/pdf/2509.17083v2",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic\n  Views",
    "authors": [
      "Zhenya Yang"
    ],
    "abstract": "Surgical simulation is essential for medical training, enabling practitioners to develop crucial skills in a risk-free environment while improving patient safety and surgical outcomes. However, conventional methods for building simulation environments are cumbersome, time-consuming, and difficult to scale, often resulting in poor details and unrealistic simulations. In this paper, we propose a Gaussian Splatting-based framework to directly reconstruct interactive surgical scenes from endoscopic data while ensuring efficiency, rendering quality, and realism. A key challenge in this data-driven simulation paradigm is the restricted movement of endoscopic cameras, which limits viewpoint diversity. As a result, the Gaussian Splatting representation overfits specific perspectives, leading to reduced geometric accuracy. To address this issue, we introduce a novel virtual camera-based regularization method that adaptively samples virtual viewpoints around the scene and incorporates them into the optimization process to mitigate overfitting. An effective depth-based regularization is applied to both real and virtual views to further refine the scene geometry. To enable fast deformation simulation, we propose a sparse control node-based Material Point Method, which integrates physical properties into the reconstructed scene while significantly reducing computational costs. Experimental results on representative surgical data demonstrate that our method can efficiently reconstruct and simulate surgical scenes from sparse endoscopic views. Notably, our method takes only a few minutes to reconstruct the surgical scene and is able to produce physically plausible deformations in real-time with user-defined interactions.",
    "arxiv_url": "http://arxiv.org/abs/2509.17027v1",
    "pdf_url": "http://arxiv.org/pdf/2509.17027v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "medical",
      "gaussian splatting",
      "efficient",
      "deformation",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D\n  Gaussian Splatting with Pixel-Aware Density Control",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.",
    "arxiv_url": "http://arxiv.org/abs/2509.16922v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16922v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "lightweight",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D\n  Gaussian Splatting SLAM",
    "authors": [
      "Amanuel T. Dufera",
      "Yuan-Li Cai"
    ],
    "abstract": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction. Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism. This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision. The resulting proxy depth guides the optimization of a deformable 3DGS map, which efficiently adapts online to maintain global consistency following pose updates from a DROID-SLAM-inspired frontend and backend optimizations (loop closure, global bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD, ScanNet) and diverse custom mobile datasets demonstrates significant improvements in reconstruction accuracy (L1 depth error) and novel view synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in challenging conditions. ConfidentSplat underscores the efficacy of principled, confidence-aware sensor fusion for advancing state-of-the-art dense visual SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2509.16863v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16863v1",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV",
      "68T20, 68U20"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging",
    "authors": [
      "Kacper Marzol",
      "Ignacy Kolton",
      "Weronika Smolak-Dyżewska",
      "Joanna Kaleta",
      "Marcin Mazur",
      "Przemysław Spurek"
    ],
    "abstract": "Multi-modal three-dimensional (3D) medical imaging data, derived from ultrasound, magnetic resonance imaging (MRI), and potentially computed tomography (CT), provide a widely adopted approach for non-invasive anatomical visualization. Accurate modeling, registration, and visualization in this setting depend on surface reconstruction and frame-to-frame interpolation. Traditional methods often face limitations due to image noise and incomplete information between frames. To address these challenges, we present MedGS, a semi-supervised neural implicit surface reconstruction framework that employs a Gaussian Splatting (GS)-based interpolation mechanism. In this framework, medical imaging data are represented as consecutive two-dimensional (2D) frames embedded in 3D space and modeled using Gaussian-based distributions. This representation enables robust frame interpolation and high-fidelity surface reconstruction across imaging modalities. As a result, MedGS offers more efficient training than traditional neural implicit methods. Its explicit GS-based representation enhances noise robustness, allows flexible editing, and supports precise modeling of complex anatomical structures with fewer artifacts. These features make MedGS highly suitable for scalable and practical applications in medical imaging.",
    "arxiv_url": "http://arxiv.org/abs/2509.16806v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16806v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "medical",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with\n  Spatial-Temporal Gaussian Splatting",
    "authors": [
      "Xiaoyang Yan",
      "Muleilan Pei",
      "Shaojie Shen"
    ],
    "abstract": "3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2509.16552v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16552v1",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "head",
      "understanding",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars",
    "authors": [
      "Weiyi Xiong",
      "Bing Zhu",
      "Tao Huang",
      "Zewei Zheng"
    ],
    "abstract": "4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2509.16119v1",
    "pdf_url": "http://arxiv.org/pdf/2509.16119v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "4d",
      "fast",
      "ar",
      "lighting",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval",
    "authors": [
      "Liwei Liao",
      "Xufeng Li",
      "Xiaoyun Zheng",
      "Boning Liu",
      "Feng Gao",
      "Ronggang Wang"
    ],
    "abstract": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \\underline{G}rounding via \\underline{V}iew \\underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.",
    "arxiv_url": "http://arxiv.org/abs/2509.15871v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15871v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Splatting for Continuous View Optimization",
    "authors": [
      "Gahye Lee",
      "Hyomin Kim",
      "Gwangjin Ju",
      "Jooeun Son",
      "Hyejeong Yoon",
      "Seungyong Lee"
    ],
    "abstract": "We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.",
    "arxiv_url": "http://arxiv.org/abs/2509.15677v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15677v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "reflection",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation\n  based on 3D Gaussian Splatting",
    "authors": [
      "Yuwei Jia",
      "Yutang Lu",
      "Zhe Cui",
      "Fei Su"
    ],
    "abstract": "Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.",
    "arxiv_url": "http://arxiv.org/abs/2509.15648v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15648v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host\n  Offloading",
    "authors": [
      "Donghyun Lee",
      "Dawoon Jeong",
      "Jae W. Lee",
      "Hongil Yoon"
    ],
    "abstract": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
    "arxiv_url": "http://arxiv.org/abs/2509.15645v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15645v1",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "high quality",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild",
    "authors": [
      "Deming Li",
      "Kaiwen Jiang",
      "Yutao Tang",
      "Ravi Ramamoorthi",
      "Rama Chellappa",
      "Cheng Peng"
    ],
    "abstract": "In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision at virtual views in a fine-grained and coarse scheme to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions and outperforms existing approaches significantly across different datasets.",
    "arxiv_url": "http://arxiv.org/abs/2509.15548v3",
    "pdf_url": "http://arxiv.org/pdf/2509.15548v3",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "sparse-view",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model\n  Priors for 3D Monocular Avatar Reconstruction",
    "authors": [
      "Jinlong Fan",
      "Bingyu Hu",
      "Xingguang Li",
      "Yuxiang Yang",
      "Jing Zhang"
    ],
    "abstract": "Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.",
    "arxiv_url": "http://arxiv.org/abs/2509.14739v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14739v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "avatar",
      "human",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform\n  for Embodied AI",
    "authors": [
      "Cong Tai",
      "Zhaoyu Zheng",
      "Haixu Long",
      "Hansheng Wu",
      "Haodong Xiang",
      "Zhengbin Long",
      "Jun Xiong",
      "Rong Shi",
      "Shizhuang Zhang",
      "Gang Qiu",
      "He Wang",
      "Ruifeng Li",
      "Jun Huang",
      "Bin Chang",
      "Shuai Feng",
      "Tao Shen"
    ],
    "abstract": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io",
    "arxiv_url": "http://arxiv.org/abs/2509.14687v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14687v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Causal Reasoning Elicits Controllable 3D Scene Generation",
    "authors": [
      "Shen Chen",
      "Ruiyu Zhao",
      "Jiale Zhou",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "abstract": "Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.",
    "arxiv_url": "http://arxiv.org/abs/2509.15249v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15249v1",
    "published_date": "2025-09-18",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perception-Integrated Safety Critical Control via Analytic Collision\n  Cone Barrier Functions on 3D Gaussian Splatting",
    "authors": [
      "Dario Tscholl",
      "Yashwanth Nakka",
      "Brian Gunter"
    ],
    "abstract": "We present a perception-driven safety filter that converts each 3D Gaussian Splat (3DGS) into a closed-form forward collision cone, which in turn yields a first-order control barrier function (CBF) embedded within a quadratic program (QP). By exploiting the analytic geometry of splats, our formulation provides a continuous, closed-form representation of collision constraints that is both simple and computationally efficient. Unlike distance-based CBFs, which tend to activate reactively only when an obstacle is already close, our collision-cone CBF activates proactively, allowing the robot to adjust earlier and thereby produce smoother and safer avoidance maneuvers at lower computational cost. We validate the method on a large synthetic scene with approximately 170k splats, where our filter reduces planning time by a factor of 3 and significantly decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while maintaining the same level of safety. The approach is entirely analytic, requires no high-order CBF extensions (HOCBFs), and generalizes naturally to robots with physical extent through a principled Minkowski-sum inflation of the splats. These properties make the method broadly applicable to real-time navigation in cluttered, perception-derived extreme environments, including space robotics and satellite systems.",
    "arxiv_url": "http://arxiv.org/abs/2509.14421v1",
    "pdf_url": "http://arxiv.org/pdf/2509.14421v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for\n  High-Fidelity Mapping",
    "authors": [
      "Zhihao Cao",
      "Hanyu Wu",
      "Li Wa Tang",
      "Zizhou Luo",
      "Zihan Zhu",
      "Wei Zhang",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2509.14191v2",
    "pdf_url": "http://arxiv.org/pdf/2509.14191v2",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "slam",
      "robotics",
      "mapping",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Plug-and-Play PDE Optimization for 3D Gaussian Splatting: Toward\n  High-Quality Rendering and Reconstruction",
    "authors": [
      "Yifan Mo",
      "Youcheng Cai",
      "Ligang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction by achieving high-quality novel view synthesis with fast rendering speed, introducing 3D Gaussian primitives to represent the scene. However, 3DGS encounters blurring and floaters when applied to complex scenes, caused by the reconstruction of redundant and ambiguous geometric structures. We attribute this issue to the unstable optimization of the Gaussians. To address this limitation, we present a plug-and-play PDE-based optimization method that overcomes the optimization constraints of 3DGS-based approaches in various tasks, such as novel view synthesis and surface reconstruction. Firstly, we theoretically derive that the 3DGS optimization procedure can be modeled as a PDE, and introduce a viscous term to ensure stable optimization. Secondly, we use the Material Point Method (MPM) to obtain a stable numerical solution of the PDE, which enhances both global and local constraints. Additionally, an effective Gaussian densification strategy and particle constraints are introduced to ensure fine-grained details. Extensive qualitative and quantitative experiments confirm that our method achieves state-of-the-art rendering and reconstruction quality.",
    "arxiv_url": "http://arxiv.org/abs/2509.13938v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13938v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray\n  Laminography Reconstruction",
    "authors": [
      "Chu Chen",
      "Ander Biguri",
      "Jean-Michel Morel",
      "Raymond H. Chan",
      "Carola-Bibiane Schönlieb",
      "Jizhou Li"
    ],
    "abstract": "X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$\\%$ of full views to achieve superior performance over the iterative method optimized on a full dataset.",
    "arxiv_url": "http://arxiv.org/abs/2509.13863v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13863v1",
    "published_date": "2025-09-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "sparse-view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM",
    "authors": [
      "Yinlong Bai",
      "Hongxin Zhang",
      "Sheng Zhong",
      "Junkai Niu",
      "Hai Li",
      "Yijia He",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.",
    "arxiv_url": "http://arxiv.org/abs/2509.13536v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13536v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice\n  Vector Quantization",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its photorealistic rendering quality and real-time performance, but it generates massive amounts of data. Hence compressing 3DGS data is necessary for the cost effectiveness of 3DGS models. Recently, several anchor-based neural compression methods have been proposed, achieving good 3DGS compression performance. However, they all rely on uniform scalar quantization (USQ) due to its simplicity. A tantalizing question is whether more sophisticated quantizers can improve the current 3DGS compression methods with very little extra overhead and minimal change to the system. The answer is yes by replacing USQ with lattice vector quantization (LVQ). To better capture scene-specific characteristics, we optimize the lattice basis for each scene, improving LVQ's adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a balance between the R-D efficiency of vector quantization and the low complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS compression architectures, enhancing their R-D performance with minimal modifications and computational overhead. Moreover, by scaling the lattice basis vectors, SALVQ can dynamically adjust lattice density, enabling a single model to accommodate multiple bit rate targets. This flexibility eliminates the need to train separate models for different compression levels, significantly reducing training time and memory consumption.",
    "arxiv_url": "http://arxiv.org/abs/2509.13482v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13482v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "compression",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single\n  Image",
    "authors": [
      "Gaofeng Liu",
      "Hengsen Li",
      "Ruoyu Gao",
      "Xuetong Li",
      "Zhiyuan Ma",
      "Tao Fang"
    ],
    "abstract": "With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.",
    "arxiv_url": "http://arxiv.org/abs/2509.13013v1",
    "pdf_url": "http://arxiv.org/pdf/2509.13013v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "avatar",
      "ar",
      "animation",
      "body",
      "high-fidelity",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian\n  Splatting and Bag of Embeddings",
    "authors": [
      "Abdalla Arafa",
      "Didier Stricker"
    ],
    "abstract": "Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.",
    "arxiv_url": "http://arxiv.org/abs/2509.12938v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12938v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "robotics",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
    "authors": [
      "Jiateng Liu",
      "Hao Gao",
      "Jiu-Cheng Xie",
      "Chi-Man Pun",
      "Jian Xiong",
      "Haolun Li",
      "Feng Xu"
    ],
    "abstract": "This paper proposes an effective Gaussian management approach for high-fidelity object reconstruction. Departing from recent Gaussian Splatting (GS) methods that employ indiscriminate attribute assignment, our approach introduces a novel densification strategy that dynamically activates spherical harmonics (SHs) or normals under the supervision of a surface reconstruction module, which effectively mitigates the gradient conflicts caused by dual supervision and achieves superior reconstruction results. To further improve representation efficiency, we develop a lightweight Gaussian representation that adaptively adjusts the SH orders of each Gaussian based on gradient magnitudes and performs task-decoupled pruning to remove Gaussian with minimal impact on a reconstruction task without sacrificing others, which balances the representational capacity with parameter quantity. Notably, our management approach is model-agnostic and can be seamlessly integrated into other frameworks, enhancing performance while reducing model size. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art approaches in both reconstruction quality and efficiency, achieving superior performance with significantly fewer parameters.",
    "arxiv_url": "http://arxiv.org/abs/2509.12742v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12742v1",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "lightweight",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface\n  Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique for real-time, photorealistic rendering by optimizing anisotropic Gaussian primitives from view-dependent images. While 3D-GS has been extended to scientific visualization, prior work remains limited to single-GPU settings, restricting scalability for large datasets on high-performance computing (HPC) systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. To eliminate artifacts, we add ghost cells at partition boundaries and apply background masks to remove irrelevant pixels. Benchmarks on the Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup across 8 nodes on Polaris while preserving image quality. These results demonstrate that distributed 3D-GS enables scalable visualization of large-scale scientific data and provide a foundation for future in situ applications.",
    "arxiv_url": "http://arxiv.org/abs/2509.12138v1",
    "pdf_url": "http://arxiv.org/pdf/2509.12138v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
    "authors": [
      "Yi-Hsin Li",
      "Thomas Sikora",
      "Sebastian Knorr",
      "Måarten Sjöström"
    ],
    "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.11853v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11853v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "3d gaussian",
      "segmentation",
      "ar",
      "fast",
      "gaussian splatting",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
    "authors": [
      "Wending Liu",
      "Siyun Liang",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "abstract": "We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
    "arxiv_url": "http://arxiv.org/abs/2509.11624v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11624v1",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Skinning of Gaussian Avatars",
    "authors": [
      "Nikolaos Zioulis",
      "Nikolaos Kotarelas",
      "Georgios Albanis",
      "Spyridon Thermos",
      "Anargyros Chatzitofis"
    ],
    "abstract": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.",
    "arxiv_url": "http://arxiv.org/abs/2509.11411v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11411v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "human",
      "ar",
      "fast",
      "mapping",
      "gaussian splatting",
      "efficient",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting",
    "authors": [
      "Lianjun Liao",
      "Chunhui Zhang",
      "Tong Wu",
      "Henglei Lv",
      "Bailin Deng",
      "Lin Gao"
    ],
    "abstract": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2509.11275v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11275v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "I.2.10; I.3"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "nerf",
      "efficient",
      "efficient rendering",
      "lighting",
      "illumination",
      "gaussian splatting",
      "outdoor",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D\n  Gaussian Splatting",
    "authors": [
      "Ashkan Taghipour",
      "Vahid Naghshin",
      "Benjamin Southwell",
      "Farid Boussaid",
      "Hamid Laga",
      "Mohammed Bennamoun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
    "arxiv_url": "http://arxiv.org/abs/2509.11116v1",
    "pdf_url": "http://arxiv.org/pdf/2509.11116v1",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "ar",
      "fast",
      "nerf",
      "robotics",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting",
    "authors": [
      "Gurutva Patle",
      "Nilay Girgaonkar",
      "Nagabhushan Somraj",
      "Rajiv Soundararajan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods. The source code for our model can be found on our project page: https://gurutvapatle.github.io/publications/2025/ADGS.html .",
    "arxiv_url": "http://arxiv.org/abs/2509.11003v2",
    "pdf_url": "http://arxiv.org/pdf/2509.11003v2",
    "published_date": "2025-09-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing\n  for Physics-based Camera Effect Data Generation",
    "authors": [
      "Yi-Ruei Liu",
      "You-Zhe Xie",
      "Yu-Hsiang Hsu",
      "I-Sheng Fang",
      "Yu-Lun Liu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.",
    "arxiv_url": "http://arxiv.org/abs/2509.10759v1",
    "pdf_url": "http://arxiv.org/pdf/2509.10759v1",
    "published_date": "2025-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "4d",
      "fast",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "T2Bs: Text-to-Character Blendshapes via Video Generation",
    "authors": [
      "Jiahao Luo",
      "Chaoyang Wang",
      "Michael Vasilkovsky",
      "Vladislav Shakhrai",
      "Di Liu",
      "Peiye Zhuang",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "James Davis",
      "Jian Wang"
    ],
    "abstract": "We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.",
    "arxiv_url": "http://arxiv.org/abs/2509.10678v2",
    "pdf_url": "http://arxiv.org/pdf/2509.10678v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "head",
      "4d",
      "ar",
      "gaussian splatting",
      "deformation",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Geometric Accuracy of Implicit and Primitive-based\n  Representations Derived from View Rendering Constraints",
    "authors": [
      "Elias De Smijter",
      "Renaud Detry",
      "Christophe De Vleeschouwer"
    ],
    "abstract": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2509.10241v2",
    "pdf_url": "http://arxiv.org/pdf/2509.10241v2",
    "published_date": "2025-09-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "robotics",
      "lighting",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting",
    "authors": [
      "Mahtab Dahaghin",
      "Milind G. Padalkar",
      "Matteo Toso",
      "Alessio Del Bue"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D scene representations from sets of multi-view images. However, inpainting missing regions, whether due to occlusion or scene editing, remains a challenging task, often leading to blurry details, artifacts, and inconsistent geometry. In this work, we introduce SplatFill, a novel depth-guided approach for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and improved efficiency. Our method combines two key ideas: (1) joint depth-based and object-based supervision to ensure inpainted Gaussians are accurately placed in 3D space and aligned with surrounding geometry, and (2) we propose a consistency-aware refinement scheme that selectively identifies and corrects inconsistent regions without disrupting the rest of the scene. Evaluations on the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing NeRF-based and 3DGS-based inpainting methods in visual fidelity but also reduces training time by 24.5%. Qualitative results show our method delivers sharper details, fewer artifacts, and greater coherence across challenging viewpoints.",
    "arxiv_url": "http://arxiv.org/abs/2509.07809v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07809v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting",
    "authors": [
      "Yimin Pan",
      "Matthias Nießner",
      "Tobias Kirschstein"
    ],
    "abstract": "Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.   While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.   The project page can be found at: https://yimin-pan.github.io/hair-gs/",
    "arxiv_url": "http://arxiv.org/abs/2509.07774v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07774v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accurate and Complete Surface Reconstruction from 3D Gaussians via\n  Direct SDF Learning",
    "authors": [
      "Wenzhi Guo",
      "Bing Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2509.07493v2",
    "pdf_url": "http://arxiv.org/pdf/2509.07493v2",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset\n  Generation",
    "authors": [
      "Ze-Xin Yin",
      "Jiaxiong Qiu",
      "Liu Liu",
      "Xinjie Wang",
      "Wei Sui",
      "Zhizhong Su",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.",
    "arxiv_url": "http://arxiv.org/abs/2509.07435v1",
    "pdf_url": "http://arxiv.org/pdf/2509.07435v1",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "ar",
      "efficient",
      "gaussian splatting",
      "lightweight",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level\n  Guidance in Large Scenes",
    "authors": [
      "Shengkai Zhang",
      "Yuhe Liu",
      "Guanjun Wu",
      "Jianhua He",
      "Xinggang Wang",
      "Mozi Chen",
      "Kezhong Liu"
    ],
    "abstract": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.",
    "arxiv_url": "http://arxiv.org/abs/2509.06685v3",
    "pdf_url": "http://arxiv.org/pdf/2509.06685v3",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "motion",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot\n  Teleoperation",
    "authors": [
      "Ian Page",
      "Pierre Susbielle",
      "Olivier Aycard",
      "Pierre-Brice Wieber"
    ],
    "abstract": "Achieving efficient remote teleoperation is particularly challenging in unknown environments, as the teleoperator must rapidly build an understanding of the site's layout. Online 3D mapping is a proven strategy to tackle this challenge, as it enables the teleoperator to progressively explore the site from multiple perspectives. However, traditional online map-based teleoperation systems struggle to generate visually accurate 3D maps in real-time due to the high computational cost involved, leading to poor teleoperation performances. In this work, we propose a solution to improve teleoperation efficiency in unknown environments. Our approach proposes a novel, modular and efficient GPU-based integration between recent advancement in gaussian splatting SLAM and existing online map-based teleoperation systems. We compare the proposed solution against state-of-the-art teleoperation systems and validate its performances through real-world experiments using an aerial vehicle. The results show significant improvements in decision-making speed and more accurate interaction with the environment, leading to greater teleoperation efficiency. In doing so, our system enhances remote teleoperation by seamlessly integrating photorealistic mapping generation with real-time performances, enabling effective teleoperation in unfamiliar environments.",
    "arxiv_url": "http://arxiv.org/abs/2509.06433v2",
    "pdf_url": "http://arxiv.org/pdf/2509.06433v2",
    "published_date": "2025-09-08",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "slam",
      "mapping",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited\n  Degrees of Freedom",
    "authors": [
      "Matthieu Gendrin",
      "Stéphane Pateux",
      "Théo Ladune"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene reconstruction. With a number of views of a given object or scene, the algorithm trains a model composed of 3D gaussians, which enables the production of novel views from arbitrary points of view. This freedom of movement is referred to as 6DoF for 6 degrees of freedom: a view is produced for any position (3 degrees), orientation of camera (3 other degrees). On large scenes, though, the input views are acquired from a limited zone in space, and the reconstruction is valuable for novel views from the same zone, even if the scene itself is almost unlimited in size. We refer to this particular case as 3DoF+, meaning that the 3 degrees of freedom of camera position are limited to small offsets around the central position. Considering the problem of coordinate quantization, the impact of position error on the projection error in pixels is studied. It is shown that the projection error is proportional to the squared inverse distance of the point being projected. Consequently, a new quantization scheme based on spherical coordinates is proposed. Rate-distortion performance of the proposed method are illustrated on the well-known Garden scene.",
    "arxiv_url": "http://arxiv.org/abs/2509.06400v1",
    "pdf_url": "http://arxiv.org/pdf/2509.06400v1",
    "published_date": "2025-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "large scene",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians\n  and Unified Pruning",
    "authors": [
      "Jiarui Chen",
      "Yikeng Chen",
      "Yingshuang Zou",
      "Ye Huang",
      "Peng Wang",
      "Yuan Liu",
      "Yujing Sun",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: https://megs-2.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2509.07021v2",
    "pdf_url": "http://arxiv.org/pdf/2509.07021v2",
    "published_date": "2025-09-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "vr",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface\n  Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS) pipeline for scientific visualization. Building on previous work that demonstrated high-fidelity isosurface reconstruction using Gaussian primitives, we incorporate a multi-GPU training backend adapted from Grendel-GS to enable scalable processing of large datasets. By distributing optimization across GPUs, our method improves training throughput and supports high-resolution reconstructions that exceed single-GPU capacity. In our experiments, the system achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs compared to a single-GPU baseline, and successfully trains the Miranda dataset (18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays the groundwork for integrating 3D-GS into HPC-based scientific workflows, enabling real-time post hoc and in situ visualization of complex simulations.",
    "arxiv_url": "http://arxiv.org/abs/2509.05216v1",
    "pdf_url": "http://arxiv.org/pdf/2509.05216v1",
    "published_date": "2025-09-05",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
    "authors": [
      "Yangming Li",
      "Chaoyu Liu",
      "Lihao Liu",
      "Simon Masnou",
      "Carola-Bibiane Schönlieb"
    ],
    "abstract": "A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they might also be unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework, GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.",
    "arxiv_url": "http://arxiv.org/abs/2509.05075v3",
    "pdf_url": "http://arxiv.org/pdf/2509.05075v3",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
    "authors": [
      "Hannah Schieber",
      "Dominik Frischmann",
      "Victor Schaack",
      "Simon Boche",
      "Angela Schoellig",
      "Stefan Leutenegger",
      "Daniel Roth"
    ],
    "abstract": "Mobile reconstruction has the potential to support time-critical tasks such as tele-guidance and disaster response, where operators must quickly gain an accurate understanding of the environment. Full high-fidelity scene reconstruction is computationally expensive and often unnecessary when only specific points of interest (POIs) matter for timely decision making. We address this challenge with CoRe-GS, a semantic POI-focused extension of Gaussian Splatting (GS). Instead of optimizing every scene element uniformly, CoRe-GS first produces a fast segmentation-ready GS representation and then selectively refines splats belonging to semantically relevant POIs detected during data acquisition. This targeted refinement reduces training time to 25\\% compared to full semantic GS while improving novel view synthesis quality in the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM) and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs enables faster and higher-quality mobile reconstruction tailored to operational needs.",
    "arxiv_url": "http://arxiv.org/abs/2509.04859v2",
    "pdf_url": "http://arxiv.org/pdf/2509.04859v2",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "understanding",
      "fast",
      "ar",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
    "authors": [
      "Jimin Xu",
      "Bosheng Qin",
      "Tao Jin",
      "Zhou Zhao",
      "Zhenhui Ye",
      "Jun Yu",
      "Fei Wu"
    ],
    "abstract": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.",
    "arxiv_url": "http://arxiv.org/abs/2509.04379v1",
    "pdf_url": "http://arxiv.org/pdf/2509.04379v1",
    "published_date": "2025-09-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "semantic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,\n  Memory-Efficient Reconstruction",
    "authors": [
      "Sankeerth Durvasula",
      "Sharanshangar Muhunthan",
      "Zain Moustafa",
      "Richard Chen",
      "Ruofan Liang",
      "Yushi Guan",
      "Nilesh Ahuja",
      "Nilesh Jain",
      "Selvakumar Panneer",
      "Nandita Vijaykumar"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.",
    "arxiv_url": "http://arxiv.org/abs/2509.03775v1",
    "pdf_url": "http://arxiv.org/pdf/2509.03775v1",
    "published_date": "2025-09-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "fast",
      "high quality",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned\n  Residuals",
    "authors": [
      "Mohit Mendiratta",
      "Mayur Deshmukh",
      "Kartik Teotia",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Christian Theobalt"
    ],
    "abstract": "3D Morphable Models (3DMMs) enable controllable facial geometry and expression editing for reconstruction, animation, and AR/VR, but traditional PCA-based mesh models are limited in resolution, detail, and photorealism. Neural volumetric methods improve realism but remain too slow for interactive use. Recent Gaussian Splatting (3DGS) based facial models achieve fast, high-quality rendering but still depend solely on a mesh-based 3DMM prior for expression control, limiting their ability to capture fine-grained geometry, expressions, and full-head coverage. We introduce GRMM, the first full-head Gaussian 3D morphable model that augments a base 3DMM with residual geometry and appearance components, additive refinements that recover high-frequency details such as wrinkles, fine skin texture, and hairline variations. GRMM provides disentangled control through low-dimensional, interpretable parameters (e.g., identity shape, facial expressions) while separately modelling residuals that capture subject- and expression-specific detail beyond the base model's capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders represent per-Gaussian appearance, and a lightweight CNN refines rasterised images for enhanced realism, all while maintaining 75 FPS real-time rendering. To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first dataset with 60 aligned expressions across 50 identities, enabling robust disentanglement of identity and expression in Gaussian-based 3DMMs. Across monocular 3D face reconstruction, novel-view synthesis, and expression transfer, GRMM surpasses state-of-the-art methods in fidelity and expression accuracy while delivering interactive real-time performance.",
    "arxiv_url": "http://arxiv.org/abs/2509.02141v1",
    "pdf_url": "http://arxiv.org/pdf/2509.02141v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "head",
      "ar",
      "fast",
      "animation",
      "face",
      "high-fidelity",
      "gaussian splatting",
      "lightweight",
      "deformation",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
    "authors": [
      "Hongyu Li",
      "Chaofeng Chen",
      "Xiaoming Li",
      "Guangming Lu"
    ],
    "abstract": "Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.",
    "arxiv_url": "http://arxiv.org/abs/2509.01964v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01964v1",
    "published_date": "2025-09-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "head",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Richard Bowden"
    ],
    "abstract": "Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.",
    "arxiv_url": "http://arxiv.org/abs/2509.01681v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01681v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "neural rendering",
      "avatar",
      "human",
      "segmentation",
      "4d",
      "fast",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Im2Haircut: Single-view Strand-based Hair Reconstruction for Human\n  Avatars",
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Malte Prinzler",
      "Giorgio Becherini",
      "Michael J. Black",
      "Justus Thies"
    ],
    "abstract": "We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to https://im2haircut.is.tue.mpg.de.",
    "arxiv_url": "http://arxiv.org/abs/2509.01469v1",
    "pdf_url": "http://arxiv.org/pdf/2509.01469v1",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "human",
      "ar",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Integrating Multi-Spectral Imaging with Gaussian Splatting",
    "authors": [
      "Josef Grün",
      "Lukas Meyer",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2509.00989v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00989v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing\n  Redundant Sorting while Preserving Rasterization Efficiency",
    "authors": [
      "Joongho Jo",
      "Jongsun Park"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.",
    "arxiv_url": "http://arxiv.org/abs/2509.00911v2",
    "pdf_url": "http://arxiv.org/pdf/2509.00911v2",
    "published_date": "2025-08-31",
    "categories": [
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.",
    "arxiv_url": "http://arxiv.org/abs/2509.00800v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00800v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "understanding",
      "ar",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model\n  via Splatter Image Structure",
    "authors": [
      "Xiufeng Huang",
      "Ziyuan Luo",
      "Qi Song",
      "Ruofei Wang",
      "Renjie Wan"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.",
    "arxiv_url": "http://arxiv.org/abs/2509.00757v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00757v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments",
    "authors": [
      "Yi Liu",
      "Keyu Fan",
      "Bin Lan",
      "Houde Liu"
    ],
    "abstract": "Visual SLAM algorithms have been enhanced through the exploration of Gaussian Splatting representations, particularly in generating high-fidelity dense maps. While existing methods perform reliably in static environments, they often encounter camera tracking drift and fuzzy mapping when dealing with the disturbances caused by moving objects. This paper presents DyPho-SLAM, a real-time, resource-efficient visual SLAM system designed to address the challenges of localization and photorealistic mapping in environments with dynamic objects. Specifically, the proposed system integrates prior image information to generate refined masks, effectively minimizing noise from mask misjudgment. Additionally, to enhance constraints for optimization after removing dynamic obstacles, we devise adaptive feature extraction strategies significantly improving the system's resilience. Experiments conducted on publicly dynamic RGB-D datasets demonstrate that the proposed system achieves state-of-the-art performance in camera pose estimation and dense map reconstruction, while operating in real-time in dynamic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2509.00741v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00741v1",
    "published_date": "2025-08-31",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "slam",
      "mapping",
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame\n  Covisibility Detection",
    "authors": [
      "Houshu He",
      "Naifeng Jing",
      "Li Jiang",
      "Xiaoyao Liang",
      "Zhuoran Song"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a critical task that enables autonomous vehicles to construct maps and localize themselves in unknown environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting (3DGS) to achieve exceptional reconstruction fidelity. However, existing 3DGS-SLAM systems provide insufficient throughput due to the need for multiple training iterations per frame and the vast number of Gaussians.   In this paper, we propose AGS, an algorithm-hardware co-design framework to boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems process frames in a streaming manner, where adjacent frames exhibit high similarity that can be utilized for acceleration. On the software level: 1) We propose a coarse-then-fine-grained pose tracking method with respect to the robot's movement. 2) We avoid redundant computations of Gaussians by sharing their contribution information across frames. On the hardware level, we propose a frame covisibility detection engine to extract intermediate data from the video CODEC. We also implement a pose tracking engine and a mapping engine with workload schedulers to efficiently deploy the AGS algorithm. Our evaluation shows that AGS achieves up to $17.12\\times$, $6.71\\times$, and $5.41\\times$ speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS accelerator, GSCore.",
    "arxiv_url": "http://arxiv.org/abs/2509.00433v1",
    "pdf_url": "http://arxiv.org/pdf/2509.00433v1",
    "published_date": "2025-08-30",
    "categories": [
      "cs.AR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "mapping",
      "acceleration",
      "gaussian splatting",
      "efficient",
      "tracking",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models",
    "authors": [
      "Ziwei Liao",
      "Mohamed Sayed",
      "Steven L. Waslander",
      "Sara Vicente",
      "Daniyar Turmukhambetov",
      "Michael Firman"
    ],
    "abstract": "Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.",
    "arxiv_url": "http://arxiv.org/abs/2508.21542v1",
    "pdf_url": "http://arxiv.org/pdf/2508.21542v1",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering\n  Training on Streaming Content",
    "authors": [
      "Jiayu Yang",
      "Weijian Su",
      "Songqian Zhang",
      "Yuqi Han",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \\M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \\M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2508.21444v1",
    "pdf_url": "http://arxiv.org/pdf/2508.21444v1",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "head",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARGS: Advanced Regularization on Aligning Gaussians over the Surface",
    "authors": [
      "Jeong Uk Lee",
      "Sung Hee Choi"
    ],
    "abstract": "Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, \"needle-like\" shapes-by favoring more balanced, \"disk-like\" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.",
    "arxiv_url": "http://arxiv.org/abs/2508.21344v2",
    "pdf_url": "http://arxiv.org/pdf/2508.21344v2",
    "published_date": "2025-08-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale\n  Feature for Generalizable Gaussian Splatting",
    "authors": [
      "Yuxi Hu",
      "Jun Zhang",
      "Kuangyi Chen",
      "Zhe Zhang",
      "Friedrich Fraundorfer"
    ],
    "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.",
    "arxiv_url": "http://arxiv.org/abs/2508.20754v1",
    "pdf_url": "http://arxiv.org/pdf/2508.20754v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse view",
      "gaussian splatting",
      "lightweight",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View\n  Images",
    "authors": [
      "Shiqi Xin",
      "Xiaolin Zhang",
      "Yanbin Liu",
      "Peng Zhang",
      "Caifeng Shan"
    ],
    "abstract": "Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.",
    "arxiv_url": "http://arxiv.org/abs/2508.20623v1",
    "pdf_url": "http://arxiv.org/pdf/2508.20623v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving\n  Video Generation",
    "authors": [
      "Jiusi Li",
      "Jackson Jiang",
      "Jinyu Miao",
      "Miao Long",
      "Tuopu Wen",
      "Peijin Jia",
      "Shengxiang Liu",
      "Chunlei Yu",
      "Maolin Liu",
      "Yuzhan Cai",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "abstract": "Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.",
    "arxiv_url": "http://arxiv.org/abs/2508.20471v1",
    "pdf_url": "http://arxiv.org/pdf/2508.20471v1",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "autonomous driving",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seam360GS: Seamless 360° Gaussian Splatting from Real-World\n  Omnidirectional Images",
    "authors": [
      "Changha Shin",
      "Woong Oh Cho",
      "Seon Joo Kim"
    ],
    "abstract": "360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.",
    "arxiv_url": "http://arxiv.org/abs/2508.20080v1",
    "pdf_url": "http://arxiv.org/pdf/2508.20080v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "robotics",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for\n  High-Fidelity Dynamic Scene Reconstruction",
    "authors": [
      "Han Jiao",
      "Jiakai Sun",
      "Yexing Xu",
      "Lei Zhao",
      "Wei Xing",
      "Huaizhong Lin"
    ],
    "abstract": "3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.",
    "arxiv_url": "http://arxiv.org/abs/2508.19786v1",
    "pdf_url": "http://arxiv.org/pdf/2508.19786v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction\n  with Large Gaussian Reconstruction Transformers",
    "authors": [
      "Yue Wu",
      "Yufan Wu",
      "Wen Li",
      "Yuxi Lu",
      "Kairui Feng",
      "Xuanhong Chen"
    ],
    "abstract": "Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2508.19754v1",
    "pdf_url": "http://arxiv.org/pdf/2508.19754v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "fast",
      "animation",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
    "authors": [
      "Yupeng Zhang",
      "Dezhi Zheng",
      "Ping Lu",
      "Han Zhang",
      "Lei Wang",
      "Liping xiang",
      "Cheng Luo",
      "Kaijun Deng",
      "Xiaowen Fu",
      "Linlin Shen",
      "Jinbao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.",
    "arxiv_url": "http://arxiv.org/abs/2508.19699v1",
    "pdf_url": "http://arxiv.org/pdf/2508.19699v1",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "efficient rendering",
      "high-fidelity",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Style4D-Bench: A Benchmark Suite for 4D Stylization",
    "authors": [
      "Beiqi Chen",
      "Shuai Shao",
      "Haitang Feng",
      "Jianhuang Lai",
      "Jianlou Si",
      "Guangcong Wang"
    ],
    "abstract": "We introduce Style4D-Bench, the first benchmark suite specifically designed for 4D stylization, with the goal of standardizing evaluation and facilitating progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive evaluation protocol measuring spatial fidelity, temporal coherence, and multi-view consistency through both perceptual and quantitative metrics, 2) a strong baseline that make an initial attempt for 4D stylization, and 3) a curated collection of high-resolution dynamic 4D scenes with diverse motions and complex backgrounds. To establish a strong baseline, we present Style4D, a novel framework built upon 4D Gaussian Splatting. It consists of three key components: a basic 4DGS scene representation to capture reliable geometry, a Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for temporally and spatially aware appearance control, and a Holistic Geometry-Preserved Style Transfer module designed to enhance spatio-temporal consistency via contrastive coherence learning and structural content preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D achieves state-of-the-art performance in 4D stylization, producing fine-grained stylistic details with stable temporal dynamics and consistent multi-view rendering. We expect Style4D-Bench to become a valuable resource for benchmarking and advancing research in stylized rendering of dynamic 3D scenes. Project page: https://becky-catherine.github.io/Style4D . Code: https://github.com/Becky-catherine/Style4D-Bench .",
    "arxiv_url": "http://arxiv.org/abs/2508.19243v1",
    "pdf_url": "http://arxiv.org/pdf/2508.19243v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "lightweight",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps",
    "authors": [
      "Christian Löwens",
      "Thorben Funke",
      "Jingchao Xie",
      "Alexandru Paul Condurache"
    ],
    "abstract": "Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.",
    "arxiv_url": "http://arxiv.org/abs/2508.18788v1",
    "pdf_url": "http://arxiv.org/pdf/2508.18788v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "ar",
      "mapping",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored\n  Gaussian Splatting",
    "authors": [
      "Qun Ji",
      "Peng Li",
      "Mingqiang Wei"
    ],
    "abstract": "High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.",
    "arxiv_url": "http://arxiv.org/abs/2508.18696v1",
    "pdf_url": "http://arxiv.org/pdf/2508.18696v1",
    "published_date": "2025-08-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "lighting",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
    "authors": [
      "Jonghyun Kim",
      "Cheng Sun",
      "Michael Stengel",
      "Matthew Chan",
      "Andrew Russell",
      "Jaehyun Jung",
      "Wil Braithwaite",
      "Shalini De Mello",
      "David Luebke"
    ],
    "abstract": "Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.",
    "arxiv_url": "http://arxiv.org/abs/2508.18540v1",
    "pdf_url": "http://arxiv.org/pdf/2508.18540v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single\n  Unconstrained Poses",
    "authors": [
      "Hao Liang",
      "Zhixuan Ge",
      "Ashish Tiwari",
      "Soumendu Majee",
      "G. M. Dilshan Godaliyadda",
      "Ashok Veeraraghavan",
      "Guha Balakrishnan"
    ],
    "abstract": "We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.",
    "arxiv_url": "http://arxiv.org/abs/2508.18389v1",
    "pdf_url": "http://arxiv.org/pdf/2508.18389v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "ar",
      "fast",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene\n  Representations",
    "authors": [
      "Fadi Khatib",
      "Dror Moran",
      "Guy Trostianetsky",
      "Yoni Kasten",
      "Meirav Galun",
      "Ronen Basri"
    ],
    "abstract": "We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.",
    "arxiv_url": "http://arxiv.org/abs/2508.18242v1",
    "pdf_url": "http://arxiv.org/pdf/2508.18242v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "outdoor",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Pose Refinement via 3D Gaussian Splatting",
    "authors": [
      "Lulu Hao",
      "Lipu Zhou",
      "Zhenzhong Wei",
      "Xu Wang"
    ],
    "abstract": "Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.",
    "arxiv_url": "http://arxiv.org/abs/2508.17876v1",
    "pdf_url": "http://arxiv.org/pdf/2508.17876v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "lightweight",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian\n  Splatting",
    "authors": [
      "Hanzhi Chang",
      "Ruijie Zhu",
      "Wenjie Chang",
      "Mulin Yu",
      "Yanzhe Liang",
      "Jiahao Lu",
      "Zhuoyuan Li",
      "Tianzhu Zhang"
    ],
    "abstract": "Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web",
    "arxiv_url": "http://arxiv.org/abs/2508.17811v1",
    "pdf_url": "http://arxiv.org/pdf/2508.17811v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Human-AI Collaborative Design Sequence for 3D Assets via\n  Differentiable Operation Graph",
    "authors": [
      "Xiaoyang Huang",
      "Bingbing Ni",
      "Wenjun Zhang"
    ],
    "abstract": "The emergence of 3D artificial intelligence-generated content (3D-AIGC) has enabled rapid synthesis of intricate geometries. However, a fundamental disconnect persists between AI-generated content and human-centric design paradigms, rooted in representational incompatibilities: conventional AI frameworks predominantly manipulate meshes or neural representations (\\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within parametric modeling tools. This disconnection diminishes the practical value of AI for 3D industry, undermining the efficiency of human-AI collaboration. To resolve this disparity, we focus on generating design operation sequences, which are structured modeling histories that comprehensively capture the step-by-step construction process of 3D assets and align with designers' typical workflows in modern 3D software. We first reformulate fundamental modeling operations (\\emph{e.g.}, \\emph{Extrude}, \\emph{Boolean}) into differentiable units, enabling joint optimization of continuous (\\emph{e.g.}, \\emph{Extrude} height) and discrete (\\emph{e.g.}, \\emph{Boolean} type) parameters via gradient-based learning. Based on these differentiable operations, a hierarchical graph with gating mechanism is constructed and optimized end-to-end by minimizing Chamfer Distance to target geometries. Multi-stage sequence length constraint and domain rule penalties enable unsupervised learning of compact design sequences without ground-truth sequence supervision. Extensive validation demonstrates that the generated operation sequences achieve high geometric fidelity, smooth mesh wiring, rational step composition and flexible editing capacity, with full compatibility within design industry.",
    "arxiv_url": "http://arxiv.org/abs/2508.17645v2",
    "pdf_url": "http://arxiv.org/pdf/2508.17645v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation",
    "authors": [
      "Guanxing Lu",
      "Baoxiong Jia",
      "Puhao Li",
      "Yixin Chen",
      "Ziwei Wang",
      "Yansong Tang",
      "Siyuan Huang"
    ],
    "abstract": "Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.",
    "arxiv_url": "http://arxiv.org/abs/2508.17600v2",
    "pdf_url": "http://arxiv.org/pdf/2508.17600v2",
    "published_date": "2025-08-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "understanding",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with\n  New Imagery Data",
    "authors": [
      "Meida Chen",
      "Luis Leal",
      "Yue Hu",
      "Rong Liu",
      "Butian Xiong",
      "Andrew Feng",
      "Jiuyi Xu",
      "Yangming Shi"
    ],
    "abstract": "For simulation and training purposes, military organizations have made substantial investments in developing high-resolution 3D virtual environments through extensive imaging and 3D scanning. However, the dynamic nature of battlefield conditions-where objects may appear or vanish over time-makes frequent full-scale updates both time-consuming and costly. In response, we introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with only a small set of newly acquired images. Our approach starts with camera pose estimation to align new images with the existing 3D model, followed by change detection to pinpoint modifications in the scene. A 3D generative AI model is then used to create high-quality 3D assets of the new elements, which are seamlessly integrated into the existing 3D model. The IDU pipeline incorporates human guidance to ensure high accuracy in object identification and placement, with each update focusing on a single new object at a time. Experimental results confirm that our proposed IDU pipeline significantly reduces update time and labor, offering a cost-effective and targeted solution for maintaining up-to-date 3D models in rapidly evolving military scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2508.17579v1",
    "pdf_url": "http://arxiv.org/pdf/2508.17579v1",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fiducial Marker Splatting for High-Fidelity Robotics Simulations",
    "authors": [
      "Diram Tabaa",
      "Gianni Di Caro"
    ],
    "abstract": "High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2508.17012v1",
    "pdf_url": "http://arxiv.org/pdf/2508.17012v1",
    "published_date": "2025-08-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "robotics",
      "lighting",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Arbitrary-Scale 3D Gaussian Super-Resolution",
    "authors": [
      "Huimin Zeng",
      "Yue Bai",
      "Yun Fu"
    ],
    "abstract": "Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).",
    "arxiv_url": "http://arxiv.org/abs/2508.16467v1",
    "pdf_url": "http://arxiv.org/pdf/2508.16467v1",
    "published_date": "2025-08-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose\n  Estimation",
    "authors": [
      "Zhaodong Jiang",
      "Ashish Sinha",
      "Tongtong Cao",
      "Yuan Ren",
      "Bingbing Liu",
      "Binbin Xu"
    ],
    "abstract": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2508.15972v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15972v1",
    "published_date": "2025-08-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Novel View Synthesis from extremely sparse views with SfM-free\n  3D Gaussian Splatting Framework",
    "authors": [
      "Zongqi He",
      "Hanmin Li",
      "Kin-Chung Chan",
      "Yushen Zuo",
      "Hao Xie",
      "Zhe Xiao",
      "Jun Xiao",
      "Kin-Man Lam"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.",
    "arxiv_url": "http://arxiv.org/abs/2508.15457v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15457v1",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "sparse-view",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DriveSplat: Decoupled Driving Scene Reconstruction with\n  Geometry-enhanced Partitioned Neural Gaussians",
    "authors": [
      "Cong Wang",
      "Xianda Guo",
      "Wenbo Xu",
      "Wei Tian",
      "Ruiqi Song",
      "Chenming Zhang",
      "Lingxi Li",
      "Long Chen"
    ],
    "abstract": "In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2508.15376v3",
    "pdf_url": "http://arxiv.org/pdf/2508.15376v3",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "deformation",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Image-Conditioned 3D Gaussian Splat Quantization",
    "authors": [
      "Xinshuang Liu",
      "Runfa Blark Li",
      "Keito Suzuki",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2508.15372v2",
    "pdf_url": "http://arxiv.org/pdf/2508.15372v2",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View\n  Consistent Diffusion",
    "authors": [
      "Xuyang Chen",
      "Zhijun Zhai",
      "Kaixuan Zhou",
      "Zengmao Wang",
      "Jianan He",
      "Dong Wang",
      "Yanfeng Zhang",
      "mingwei Sun",
      "Rüdiger Westermann",
      "Konrad Schindler",
      "Liqiu Meng"
    ],
    "abstract": "Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.",
    "arxiv_url": "http://arxiv.org/abs/2508.15169v2",
    "pdf_url": "http://arxiv.org/pdf/2508.15169v2",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "relighting",
      "3d gaussian",
      "ar",
      "lighting",
      "sparse view",
      "gaussian splatting",
      "outdoor",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting\n  with Upsampled 2D X-ray Projection Priors",
    "authors": [
      "Jeonghyun Noh",
      "Hyun-Jic Oh",
      "Byungju Chae",
      "Won-Ki Jeong"
    ],
    "abstract": "Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.",
    "arxiv_url": "http://arxiv.org/abs/2508.15151v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15151v1",
    "published_date": "2025-08-21",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
    "authors": [
      "Long Le",
      "Ryan Lucas",
      "Chen Wang",
      "Chuhao Chen",
      "Dinesh Jayaraman",
      "Eric Eaton",
      "Lingjie Liu"
    ],
    "abstract": "Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2508.17437v2",
    "pdf_url": "http://arxiv.org/pdf/2508.17437v2",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "human",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting",
    "authors": [
      "Jiaxin Wei",
      "Stefan Leutenegger",
      "Simon Schaefer"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have significantly enhanced novel view synthesis, yet generating high-quality renderings from extreme novel viewpoints or partially observed regions remains challenging. Meanwhile, diffusion models exhibit strong generative capabilities, but their reliance on text prompts and lack of awareness of specific scene information hinder accurate 3D reconstruction tasks. To address these limitations, we introduce GSFix3D, a novel framework that improves the visual fidelity in under-constrained regions by distilling prior knowledge from diffusion models into 3D representations, while preserving consistency with observed scene details. At its core is GSFixer, a latent diffusion model obtained via our customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to adapt pretrained generative models to a variety of environments and artifact types from different reconstruction methods, enabling robust novel view repair for unseen camera poses. Moreover, we propose a random mask augmentation strategy that empowers GSFixer to plausibly inpaint missing regions. Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer achieve state-of-the-art performance, requiring only minimal scene-specific fine-tuning on captured data. Real-world test further confirms its resilience to potential pose errors. Our code and data will be made publicly available. Project page: https://gsfix3d.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2508.14717v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14717v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeMS: Efficient Gaussian Splatting for Extreme Motion Blur",
    "authors": [
      "Gopi Raju Matta",
      "Trisha Reddypalli",
      "Vemunuri Divya Madhuri",
      "Kaushik Mitra"
    ],
    "abstract": "We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to handle severely motion-blurred images. State-of-the-art deblurring methods for extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches like Deblur-GS, typically assume access to sharp images for camera pose estimation and point cloud generation, an unrealistic assumption. Methods relying on COLMAP initialization, such as BAD-Gaussians, also fail due to unreliable feature correspondences under severe blur. To address these challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep learning-based Structure-from-Motion pipeline that estimates poses and generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which enables robust scene initialization by treating Gaussians as samples from a probability distribution, eliminating heuristic densification and pruning; and (3) joint optimization of camera trajectories and Gaussian parameters for stable reconstruction. While this pipeline produces strong results, inaccuracies may remain when all inputs are severely blurred. To mitigate this, we propose GeMS-E, which integrates a progressive refinement step using events: (4) Event-based Double Integral (EDI) deblurring restores sharper images that are then fed into GeMS, improving pose estimation, point cloud generation, and overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets. To our knowledge, this is the first framework to address extreme motion blur within 3DGS directly from severely blurred inputs.",
    "arxiv_url": "http://arxiv.org/abs/2508.14682v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14682v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via\n  Gaussian Surfels",
    "authors": [
      "Xingyuan Yang",
      "Min Wei"
    ],
    "abstract": "Inverse rendering of glossy objects from RGB imagery remains fundamentally limited by inherent ambiguity. Although NeRF-based methods achieve high-fidelity reconstruction via dense-ray sampling, their computational cost is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction efficiency but exhibits limitations under specular reflections. Multi-view inconsistencies introduce high-frequency surface noise and structural artifacts, while simplified rendering equations obscure material properties, leading to implausible relighting results. To address these issues, we propose GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we establish robust surface reconstruction through physics-based rendering with split-sum approximation, enhanced by geometric priors from foundation models. Second, we perform material decomposition by leveraging Monte Carlo importance sampling of the full rendering equation, modeling indirect illumination via differentiable 2D Gaussian ray tracing and refining high-frequency specular details through spherical mipmap-based directional encoding that captures anisotropic highlights. Extensive experiments demonstrate state-of-the-art performance in geometry reconstruction, material separation, and photorealistic relighting under novel illuminations, outperforming existing inverse rendering approaches.",
    "arxiv_url": "http://arxiv.org/abs/2508.14563v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14563v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "relighting",
      "3d gaussian",
      "ar",
      "nerf",
      "lighting",
      "high-fidelity",
      "illumination",
      "gaussian splatting",
      "reflection",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Slices to Structures: Unsupervised 3D Reconstruction of Female\n  Pelvic Anatomy from Freehand Transvaginal Ultrasound",
    "authors": [
      "Max Krähenmann",
      "Sergio Tascon-Morales",
      "Fabian Laumer",
      "Julia E. Vogt",
      "Ece Ozkan"
    ],
    "abstract": "Volumetric ultrasound has the potential to significantly improve diagnostic accuracy and clinical decision-making, yet its widespread adoption remains limited by dependence on specialized hardware and restrictive acquisition protocols. In this work, we present a novel unsupervised framework for reconstructing 3D anatomical structures from freehand 2D transvaginal ultrasound (TVS) sweeps, without requiring external tracking or learned pose estimators. Our method adapts the principles of Gaussian Splatting to the domain of ultrasound, introducing a slice-aware, differentiable rasterizer tailored to the unique physics and geometry of ultrasound imaging. We model anatomy as a collection of anisotropic 3D Gaussians and optimize their parameters directly from image-level supervision, leveraging sensorless probe motion estimation and domain-specific geometric priors. The result is a compact, flexible, and memory-efficient volumetric representation that captures anatomical detail with high spatial fidelity. This work demonstrates that accurate 3D reconstruction from 2D ultrasound images can be achieved through purely computational means, offering a scalable alternative to conventional 3D systems and enabling new opportunities for AI-assisted analysis and diagnosis.",
    "arxiv_url": "http://arxiv.org/abs/2508.14552v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14552v1",
    "published_date": "2025-08-20",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient",
      "tracking",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstruction Using the Invisible: Intuition from NIR and Metadata for\n  Enhanced 3D Gaussian Splatting",
    "authors": [
      "Gyusam Chang",
      "Tuan-Anh Vu",
      "Vivek Alumootil",
      "Harris Song",
      "Deanna Pham",
      "Sangpil Kim",
      "M. Khalid Jawed"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in agriculture remains underexplored. Agricultural scenes present unique challenges for 3D reconstruction methods, particularly due to uneven illumination, occlusions, and a limited field of view. To address these limitations, we introduce \\textbf{NIRPlant}, a novel multimodal dataset encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth, and LiDAR data collected under varied indoor and outdoor lighting conditions. By integrating NIR data, our approach enhances robustness and provides crucial botanical insights that extend beyond the visible spectrum. Additionally, we leverage text-based metadata derived from vegetation indices, such as NDVI, NDWI, and the chlorophyll index, which significantly enriches the contextual understanding of complex agricultural environments. To fully exploit these modalities, we propose \\textbf{NIRSplat}, an effective multimodal Gaussian splatting architecture employing a cross-attention mechanism combined with 3D point-based positional encoding, providing robust geometric priors. Comprehensive experiments demonstrate that \\textbf{NIRSplat} outperforms existing landmark methods, including 3DGS, CoR-GS, and InstantSplat, highlighting its effectiveness in challenging agricultural scenarios. The code and dataset are publicly available at: https://github.com/StructuresComp/3D-Reconstruction-NIR",
    "arxiv_url": "http://arxiv.org/abs/2508.14443v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14443v1",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "understanding",
      "ar",
      "lighting",
      "illumination",
      "gaussian splatting",
      "outdoor",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GALA: Guided Attention with Language Alignment for Open Vocabulary\n  Gaussian Splatting",
    "authors": [
      "Elena Alegret",
      "Kunyi Li",
      "Sen Wang",
      "Siyun Liang",
      "Michael Niemeyer",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "3D scene reconstruction and understanding have gained increasing popularity, yet existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images. In this paper, we present GALA, a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. To extend to generalized language feature fields, we introduce the core contribution of GALA, a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning. Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.",
    "arxiv_url": "http://arxiv.org/abs/2508.14278v2",
    "pdf_url": "http://arxiv.org/pdf/2508.14278v2",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "authors": [
      "Chin-Yang Lin",
      "Cheng Sun",
      "Fu-En Yang",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ],
    "abstract": "LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/",
    "arxiv_url": "http://arxiv.org/abs/2508.14041v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14041v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distilled-3DGS:Distilled 3D Gaussian Splatting",
    "authors": [
      "Lintao Xiang",
      "Xinkai Chen",
      "Jianhuang Lai",
      "Guangcong Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS .",
    "arxiv_url": "http://arxiv.org/abs/2508.14037v1",
    "pdf_url": "http://arxiv.org/pdf/2508.14037v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Online 3D Gaussian Splatting Modeling with Novel View Selection",
    "authors": [
      "Byeonggwon Lee",
      "Junkyu Park",
      "Khang Truong Giang",
      "Soohwan Song"
    ],
    "abstract": "This study addresses the challenge of generating online 3D Gaussian Splatting (3DGS) models from RGB-only frames. Previous studies have employed dense SLAM techniques to estimate 3D scenes from keyframes for 3DGS model construction. However, these methods are limited by their reliance solely on keyframes, which are insufficient to capture an entire scene, resulting in incomplete reconstructions. Moreover, building a generalizable model requires incorporating frames from diverse viewpoints to achieve broader scene coverage. However, online processing restricts the use of many frames or extensive training iterations. Therefore, we propose a novel method for high-quality 3DGS modeling that improves model completeness through adaptive view selection. By analyzing reconstruction quality online, our approach selects optimal non-keyframes for additional training. By integrating both keyframes and selected non-keyframes, the method refines incomplete regions from diverse viewpoints, significantly enhancing completeness. We also present a framework that incorporates an online multi-view stereo approach, ensuring consistency in 3D information throughout the 3DGS modeling process. Experimental results demonstrate that our method outperforms state-of-the-art methods, delivering exceptional performance in complex outdoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2508.14014v2",
    "pdf_url": "http://arxiv.org/pdf/2508.14014v2",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis",
    "authors": [
      "Chunji Lv",
      "Zequn Chen",
      "Donglin Di",
      "Weinan Zhang",
      "Hao Li",
      "Wei Chen",
      "Changsheng Li"
    ],
    "abstract": "While physics-grounded 3D motion synthesis has seen significant progress, current methods face critical limitations. They typically rely on pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics integration depends on either inflexible, manually defined physical attributes or unstable, optimization-heavy guidance from video models. To overcome these challenges, we introduce PhysGM, a feed-forward framework that jointly predicts a 3D Gaussian representation and its physical properties from a single image, enabling immediate, physical simulation and high-fidelity 4D rendering. We first establish a base model by jointly optimizing for Gaussian reconstruction and probabilistic physics prediction. The model is then refined with physically plausible reference videos to enhance both rendering fidelity and physics prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align its simulations with reference videos, circumventing Score Distillation Sampling (SDS) optimization which needs back-propagating gradients through the complex differentiable simulation and rasterization. To facilitate the training, we introduce a new dataset PhysAssets of over 24,000 3D assets, annotated with physical properties and corresponding guiding videos. Experimental results demonstrate that our method effectively generates high-fidelity 4D simulations from a single image in one minute. This represents a significant speedup over prior works while delivering realistic rendering results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2508.13911v1",
    "pdf_url": "http://arxiv.org/pdf/2508.13911v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EAvatar: Expression-Aware Head Avatar Reconstruction with Generative\n  Geometry Priors",
    "authors": [
      "Shikun Zhang",
      "Cunjian Chen",
      "Yiqun Wang",
      "Qiuhong Ke",
      "Yong Li"
    ],
    "abstract": "High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2508.13537v1",
    "pdf_url": "http://arxiv.org/pdf/2508.13537v1",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "face",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting",
    "authors": [
      "Shuxin Liang",
      "Yihan Xiao",
      "Wenlu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: https://github.com/Shuxin-Liang/InnerGS.",
    "arxiv_url": "http://arxiv.org/abs/2508.13287v1",
    "pdf_url": "http://arxiv.org/pdf/2508.13287v1",
    "published_date": "2025-08-18",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "understanding",
      "ar",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IntelliCap: Intelligent Guidance for Consistent View Sampling",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Dieter Schmalstieg",
      "Shohei Mori"
    ],
    "abstract": "Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.",
    "arxiv_url": "http://arxiv.org/abs/2508.13043v1",
    "pdf_url": "http://arxiv.org/pdf/2508.13043v1",
    "published_date": "2025-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "human",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian\n  Splatting",
    "authors": [
      "Kangjie Chen",
      "Yingji Zhong",
      "Zhihao Li",
      "Jiaqi Lin",
      "Youyu Chen",
      "Minghan Qin",
      "Haoqian Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2508.12720v3",
    "pdf_url": "http://arxiv.org/pdf/2508.12720v3",
    "published_date": "2025-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "understanding",
      "ar",
      "gaussian splatting",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TiP4GEN: Text to Immersive Panorama 4D Scene Generation",
    "authors": [
      "Ke Xing",
      "Hanwen Liang",
      "Dejia Xu",
      "Yuyang Yin",
      "Konstantinos N. Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "With the rapid advancement and widespread adoption of VR/AR technologies, there is a growing demand for the creation of high-quality, immersive dynamic scenes. However, existing generation works predominantly concentrate on the creation of static scenes or narrow perspective-view dynamic scenes, falling short of delivering a truly 360-degree immersive experience from any viewpoint. In this paper, we introduce \\textbf{TiP4GEN}, an advanced text-to-dynamic panorama scene generation framework that enables fine-grained content control and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN integrates panorama video generation and dynamic scene reconstruction to create 360-degree immersive virtual environments. For video generation, we introduce a \\textbf{Dual-branch Generation Model} consisting of a panorama branch and a perspective branch, responsible for global and local view generation, respectively. A bidirectional cross-attention mechanism facilitates comprehensive information exchange between the branches. For scene reconstruction, we propose a \\textbf{Geometry-aligned Reconstruction Model} based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using metric depth maps and initializing scene cameras with estimated poses, our method ensures geometric consistency and temporal coherence for the reconstructed scenes. Extensive experiments demonstrate the effectiveness of our proposed designs and the superiority of TiP4GEN in generating visually compelling and motion-coherent dynamic panoramic scenes. Our project page is at https://ke-xing.github.io/TiP4GEN/.",
    "arxiv_url": "http://arxiv.org/abs/2508.12415v2",
    "pdf_url": "http://arxiv.org/pdf/2508.12415v2",
    "published_date": "2025-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Densification in 3D Gaussian Splatting for High-Fidelity\n  Rendering",
    "authors": [
      "Xiaobin Deng",
      "Changyu Diao",
      "Min Li",
      "Ruohan Yu",
      "Duanqing Xu"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2508.12313v1",
    "pdf_url": "http://arxiv.org/pdf/2508.12313v1",
    "published_date": "2025-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes",
    "authors": [
      "Hongyuan Liu",
      "Haochen Yu",
      "Jianfei Jiang",
      "Qiankun Liu",
      "Jiansheng Chen",
      "Huimin Ma"
    ],
    "abstract": "Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page.",
    "arxiv_url": "http://arxiv.org/abs/2508.12015v1",
    "pdf_url": "http://arxiv.org/pdf/2508.12015v1",
    "published_date": "2025-08-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "autonomous driving",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting",
      "outdoor",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by\n  3D Gaussian Splat Camouflages",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haorang Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Wilian Lunardi",
      "Martin Andreoni",
      "Duen Horng Chau"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems.",
    "arxiv_url": "http://arxiv.org/abs/2508.11854v2",
    "pdf_url": "http://arxiv.org/pdf/2508.11854v2",
    "published_date": "2025-08-16",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian\n  Splatting",
    "authors": [
      "Simona Kocour",
      "Assia Benbihi",
      "Torsten Sattler"
    ],
    "abstract": "Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre/post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com/spatial-intelligence-ai/Remove360.git. Data are available at huggingface.co/datasets/simkoc/Remove360.",
    "arxiv_url": "http://arxiv.org/abs/2508.11431v1",
    "pdf_url": "http://arxiv.org/pdf/2508.11431v1",
    "published_date": "2025-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "understanding",
      "ar",
      "gaussian splatting",
      "outdoor",
      "face",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting",
    "authors": [
      "Zhenghao Chen",
      "Zicong Chen",
      "Lei Liu",
      "Yiming Wu",
      "Dong Xu"
    ],
    "abstract": "Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.",
    "arxiv_url": "http://arxiv.org/abs/2508.11183v1",
    "pdf_url": "http://arxiv.org/pdf/2508.11183v1",
    "published_date": "2025-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "compression",
      "gaussian splatting",
      "dynamic",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian\n  Splatting",
    "authors": [
      "Zheng Zhou",
      "Jia-Chen Zhang",
      "Yu-Jie Xiong",
      "Chun-Ming Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).",
    "arxiv_url": "http://arxiv.org/abs/2508.10507v1",
    "pdf_url": "http://arxiv.org/pdf/2508.10507v1",
    "published_date": "2025-08-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting",
    "authors": [
      "Yuning Huang",
      "Jiahao Pang",
      "Fengqing Zhu",
      "Dong Tian"
    ],
    "abstract": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.",
    "arxiv_url": "http://arxiv.org/abs/2508.10227v1",
    "pdf_url": "http://arxiv.org/pdf/2508.10227v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "compression",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,\n  and Generation",
    "authors": [
      "Shuting He",
      "Peilin Ji",
      "Yitong Yang",
      "Changshuo Wang",
      "Jiayi Ji",
      "Yinglin Wang",
      "Henghui Ding"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.",
    "arxiv_url": "http://arxiv.org/abs/2508.09977v2",
    "pdf_url": "http://arxiv.org/pdf/2508.09977v2",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "compact",
      "understanding",
      "ar",
      "nerf",
      "lighting",
      "survey",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for\n  Synthetic Human Dynamics",
    "authors": [
      "Weiqi Li",
      "Zehao Zhang",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "abstract": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \\emph{geometric inconsistency} and \\emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \\emph{motion generalization limitations} and \\emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \\textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \\textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \\textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.",
    "arxiv_url": "http://arxiv.org/abs/2508.09858v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09858v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "4d",
      "ar",
      "face",
      "reflection",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.",
    "arxiv_url": "http://arxiv.org/abs/2508.09855v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09855v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "human",
      "sparse-view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video\n  Diffusion Priors",
    "authors": [
      "Xingyilang Yin",
      "Qi Zhang",
      "Jiahao Chang",
      "Ying Feng",
      "Qingnan Fan",
      "Xi Yang",
      "Chi-Man Pun",
      "Huaqi Zhang",
      "Xiaodong Cun"
    ],
    "abstract": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer.",
    "arxiv_url": "http://arxiv.org/abs/2508.09667v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09667v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "sparse-view",
      "3d gaussian",
      "ar",
      "sparse view",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse\n  Satellite Images",
    "authors": [
      "Xuejun Huang",
      "Xinyi Liu",
      "Yi Wan",
      "Zhi Zheng",
      "Bin Zhang",
      "Mingtao Xiong",
      "Yingying Pei",
      "Yongjun Zhang"
    ],
    "abstract": "Three-dimensional scene reconstruction from sparse-view satellite images is a long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its variants have recently attracted attention for its high efficiency, existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies. To address these limitations, we propose SkySplat, a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results. Compared to per-scene optimization methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.",
    "arxiv_url": "http://arxiv.org/abs/2508.09479v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09479v1",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy\n  Prediction",
    "authors": [
      "Cheng Chen",
      "Hao Huang",
      "Saurabh Bagchi"
    ],
    "abstract": "Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.",
    "arxiv_url": "http://arxiv.org/abs/2508.10936v1",
    "pdf_url": "http://arxiv.org/pdf/2508.10936v1",
    "published_date": "2025-08-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "lighting",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A new dataset and comparison for multi-camera frame synthesis",
    "authors": [
      "Conall Daly",
      "Anil Kokaram"
    ],
    "abstract": "Many methods exist for frame synthesis in image sequences but can be broadly categorised into frame interpolation and view synthesis techniques. Fundamentally, both frame interpolation and view synthesis tackle the same task, interpolating a frame given surrounding frames in time or space. However, most frame interpolation datasets focus on temporal aspects with single cameras moving through time and space, while view synthesis datasets are typically biased toward stereoscopic depth estimation use cases. This makes direct comparison between view synthesis and frame interpolation methods challenging. In this paper, we develop a novel multi-camera dataset using a custom-built dense linear camera array to enable fair comparison between these approaches. We evaluate classical and deep learning frame interpolators against a view synthesis method (3D Gaussian Splatting) for the task of view in-betweening. Our results reveal that deep learning methods do not significantly outperform classical methods on real image data, with 3D Gaussian Splatting actually underperforming frame interpolators by as much as 3.5 dB PSNR. However, in synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.",
    "arxiv_url": "http://arxiv.org/abs/2508.09068v2",
    "pdf_url": "http://arxiv.org/pdf/2508.09068v2",
    "published_date": "2025-08-12",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhou",
      "Yu-Jie Xiong",
      "Chun-Ming Xia",
      "Jia-Chen Zhang",
      "Hong-Jian Zhan"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware adaptive density control framework to address these challenges. Our key innovations: the gradient coherence ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50\\% reduced memory consumption through optimized Gaussians utilization.",
    "arxiv_url": "http://arxiv.org/abs/2508.09239v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09239v1",
    "published_date": "2025-08-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Communication Efficient Robotic Mixed Reality with Gaussian Splatting\n  Cross-Layer Optimization",
    "authors": [
      "Chenxuan Liu",
      "He Li",
      "Zongze Li",
      "Shuai Wang",
      "Wei Xu",
      "Kejiang Ye",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Realizing low-cost communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSMR), which enables the simulator to opportunistically render a photo-realistic view from the robot's pose by calling ``memory'' from a GS model, thus reducing the need for excessive image uploads. However, the GS model may involve discrepancies compared to the actual environments. To this end, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation (i.e., adjusting to content profiles) across different frames by minimizing a newly derived GSMR loss function. The GSCLO problem is addressed by an accelerated penalty optimization (APO) algorithm that reduces computational complexity by over $10$x compared to traditional branch-and-bound and search algorithms. Moreover, variants of GSCLO are presented to achieve robust, low-power, and multi-robot GSMR. Extensive experiments demonstrate that the proposed GSMR paradigm and GSCLO method achieve significant improvements over existing benchmarks on both wheeled and legged robots in terms of diverse metrics in various scenarios. For the first time, it is found that RoboMR can be achieved with ultra-low communication costs, and mixture of data is useful for enhancing GS performance in dynamic scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2508.08624v2",
    "pdf_url": "http://arxiv.org/pdf/2508.08624v2",
    "published_date": "2025-08-12",
    "categories": [
      "cs.RO",
      "cs.IT",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
    "authors": [
      "Shuting He",
      "Guangquan Jie",
      "Changshuo Wang",
      "Yun Zhou",
      "Shuming Hu",
      "Guanbin Li",
      "Henghui Ding"
    ],
    "abstract": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.",
    "arxiv_url": "http://arxiv.org/abs/2508.08252v1",
    "pdf_url": "http://arxiv.org/pdf/2508.08252v1",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGOnline: Segment Any Gaussians Online",
    "authors": [
      "Wentao Sun",
      "Quanyun Wu",
      "Hanqing Xu",
      "Kyle Gao",
      "Zhengsen Xu",
      "Yiping Chen",
      "Dedong Zhang",
      "Lingfei Ma",
      "John S. Zelek",
      "Jonathan Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.",
    "arxiv_url": "http://arxiv.org/abs/2508.08219v1",
    "pdf_url": "http://arxiv.org/pdf/2508.08219v1",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "vr",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient",
      "tracking",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and\n  Deformable 3D Gaussian Reconstruction",
    "authors": [
      "Tianle Zeng",
      "Junlei Hu",
      "Gerardo Loza Galindo",
      "Sharib Ali",
      "Duygu Sarikaya",
      "Pietro Valdastri",
      "Dominic Jones"
    ],
    "abstract": "Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.",
    "arxiv_url": "http://arxiv.org/abs/2508.07897v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07897v1",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.3.3"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "medical",
      "tracking",
      "gaussian splatting",
      "deformation",
      "dynamic",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction",
    "authors": [
      "Yuchen Gao",
      "Xiao Xu",
      "Eckehard Steinbach",
      "Daniel E. Lucani",
      "Qi Zhang"
    ],
    "abstract": "This paper presents a multimodal framework that integrates touch signals (contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our approach enhances scene reconstruction, particularly under challenging conditions like low lighting, limited camera viewpoints, and occlusions. Different from the visual-only method, the proposed approach incorporates spatially selective touch measurements to refine both the geometry and appearance of the 3D Gaussian representation. To guide the touch exploration, we introduce a two-stage sampling scheme that initially probes sparse regions and then concentrates on high-uncertainty boundaries identified from the reconstructed mesh. A geometric loss is proposed to ensure surface smoothness, resulting in improved geometry. Experimental results across diverse scenarios show consistent improvements in geometric accuracy. In the most challenging case with severe occlusion, the Chamfer Distance is reduced by over 15x, demonstrating the effectiveness of integrating touch cues into 3D Gaussian Splatting. Furthermore, our approach maintains a fully online pipeline, underscoring its feasibility in visually degraded environments.",
    "arxiv_url": "http://arxiv.org/abs/2508.07717v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07717v1",
    "published_date": "2025-08-11",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface\n  Reconstruction",
    "authors": [
      "Bo Jia",
      "Yanan Guo",
      "Ying Chang",
      "Benkui Zhang",
      "Ying Xie",
      "Kangning Du",
      "Lin Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS. Our code will be made publicly available at (https://github.com/Bistu3DV/MND-GS/).",
    "arxiv_url": "http://arxiv.org/abs/2508.07701v2",
    "pdf_url": "http://arxiv.org/pdf/2508.07701v2",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "outdoor",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry\n  Model Accuracy and Resolution",
    "authors": [
      "Pranav Chougule"
    ],
    "abstract": "In this paper, I present a comprehensive study comparing Photogrammetry and Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I created a dataset of images from a real-world scene and constructed 3D models using both methods. To evaluate the performance, I compared the models using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and lp/mm resolution based on the USAF resolution chart. A significant contribution of this work is the development of a modified Gaussian Splatting repository, which I forked and enhanced to enable rendering images from novel camera poses generated in the Blender environment. This innovation allows for the synthesis of high-quality novel views, showcasing the flexibility and potential of Gaussian Splatting. My investigation extends to an augmented dataset that includes both original ground images and novel views synthesized via Gaussian Splatting. This augmented dataset was employed to generate a new photogrammetry model, which was then compared against the original photogrammetry model created using only the original images. The results demonstrate the efficacy of using Gaussian Splatting to generate novel high-quality views and its potential to improve photogrammetry-based 3D reconstructions. The comparative analysis highlights the strengths and limitations of both approaches, providing valuable information for applications in extended reality (XR), photogrammetry, and autonomous vehicle simulations. Code is available at https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.",
    "arxiv_url": "http://arxiv.org/abs/2508.07483v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07483v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
    "authors": [
      "Junyao Gao",
      "Jiaxing Li",
      "Wenran Liu",
      "Yanhong Zeng",
      "Fei Shen",
      "Kai Chen",
      "Yanan Sun",
      "Cairong Zhao"
    ],
    "abstract": "In this paper, we propose \\textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.",
    "arxiv_url": "http://arxiv.org/abs/2508.07409v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07409v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "animation",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery",
    "authors": [
      "Rajaei Khatib",
      "Raja Giryes"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method, obtaining high-quality reconstruction with real-time rendering runtime performance. The main idea behind 3DGS is to represent the scene as a collection of 3D gaussians, while learning their parameters to fit the given views of the scene. While achieving superior performance in the presence of many views, 3DGS struggles with sparse view reconstruction, where the input views are sparse and do not fully cover the scene and have low overlaps. In this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By using the DIP prior, which utilizes internal structure and patterns, with coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla 3DGS fails, such as sparse view recovery. Note that our approach does not use any pre-trained models such as generative models and depth estimation, but rather relies only on the input frames. Among such methods, DIP-GS obtains state-of-the-art (SOTA) competitive results on various sparse-view reconstruction tasks, demonstrating its capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2508.07372v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07372v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "sparse view",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building\n  Reconstruction",
    "authors": [
      "Qilin Zhang",
      "Olaf Wysocki",
      "Boris Jutzi"
    ],
    "abstract": "Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.",
    "arxiv_url": "http://arxiv.org/abs/2508.07355v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07355v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry",
      "3d reconstruction",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS\n  Watermarking Systems",
    "authors": [
      "Qingyuan Zeng",
      "Shu Jiang",
      "Jiajing Lin",
      "Zhenzhong Wang",
      "Kay Chen Tan",
      "Min Jiang"
    ],
    "abstract": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital watermarking techniques, embedding either 1D bitstreams or 2D images, are used for copyright protection. However, the robustness of these watermarking techniques against potential attacks remains underexplored. This paper introduces the first universal black-box attack framework, the Group-based Multi-objective Evolutionary Attack (GMEA), designed to challenge these watermarking systems. We formulate the attack as a large-scale multi-objective optimization problem, balancing watermark removal with visual quality. In a black-box setting, we introduce an indirect objective function that blinds the watermark detector by minimizing the standard deviation of features extracted by a convolutional network, thus rendering the feature maps uninformative. To manage the vast search space of 3DGS models, we employ a group-based optimization strategy to partition the model into multiple, independent sub-optimization problems. Experiments demonstrate that our framework effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking methods while maintaining high visual fidelity. This work reveals critical vulnerabilities in existing 3DGS copyright protection schemes and calls for the development of more robust watermarking systems.",
    "arxiv_url": "http://arxiv.org/abs/2508.07263v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07263v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic\n  Scene Reconstruction",
    "authors": [
      "Xuesong Li",
      "Lars Petersson",
      "Vivien Rolland"
    ],
    "abstract": "This paper addresses the challenge of novel-view synthesis and motion reconstruction of dynamic scenes from monocular video, which is critical for many robotic applications. Although Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering static scenes, extending them to reconstruct dynamic scenes remains challenging. In this work, we introduce a novel approach that combines 3DGS with a motion trajectory field, enabling precise handling of complex object motions and achieving physically plausible motion trajectories. By decoupling dynamic objects from static background, our method compactly optimizes the motion trajectory field. The approach incorporates time-invariant motion coefficients and shared motion trajectory bases to capture intricate motion patterns while minimizing optimization complexity. Extensive experiments demonstrate that our approach achieves state-of-the-art results in both novel-view synthesis and motion trajectory recovery from monocular video, advancing the capabilities of dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2508.07182v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07182v1",
    "published_date": "2025-08-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "compact",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of\n  Fruit",
    "authors": [
      "Aiden Swann",
      "Alex Qiu",
      "Matthew Strong",
      "Angelina Zhang",
      "Samuel Morstein",
      "Kai Rayle",
      "Monroe Kennedy III"
    ],
    "abstract": "DexFruit is a robotic manipulation framework that enables gentle, autonomous handling of fragile fruit and precise evaluation of damage. Many fruits are fragile and prone to bruising, thus requiring humans to manually harvest them with care. In this work, we demonstrate by using optical tactile sensing, autonomous manipulation of fruit with minimal damage can be achieved. We show that our tactile informed diffusion policies outperform baselines in both reduced bruising and pick-and-place success rate across three fruits: strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat, a novel technique to represent and quantify visual damage in high-resolution 3D representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring damage lack quantitative rigor or require expensive equipment. With FruitSplat, we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into the 3DGS representation. Furthermore, this representation is modular and general, compatible with any relevant 2D model. Overall, we demonstrate a 92% grasping policy success rate, up to a 20% reduction in visual bruising, and up to an 31% improvement in grasp success rate on challenging fruit compared to our baselines across our three tested fruits. We rigorously evaluate this result with over 630 trials. Please checkout our website at https://dex-fruit.github.io .",
    "arxiv_url": "http://arxiv.org/abs/2508.07118v2",
    "pdf_url": "http://arxiv.org/pdf/2508.07118v2",
    "published_date": "2025-08-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS\n  Compression",
    "authors": [
      "Yuke Xing",
      "William Gordon",
      "Qi Yang",
      "Kaifa Yang",
      "Jiarui Wang",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high visual fidelity, but its substantial storage requirements hinder practical deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate compression modules. However, these 3DGS generative compression techniques introduce unique distortions lacking systematic quality assessment research. To this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment (VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences generated from 11 scenes across 6 SOTA 3DGS compression algorithms with systematically designed parameter levels. With annotations from 50 participants, we obtained MOS scores with outlier removal and validated dataset reliability. We benchmark 6 3DGS compression algorithms on storage efficiency and visual quality, and evaluate 15 quality assessment metrics across multiple paradigms. Our work enables specialized VQA model training for 3DGS, serving as a catalyst for compression and quality assessment research. The dataset is available at https://github.com/YukeXing/3DGS-VBench.",
    "arxiv_url": "http://arxiv.org/abs/2508.07038v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07038v1",
    "published_date": "2025-08-09",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "compression",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events",
    "authors": [
      "Siyu Chen",
      "Shenghai Yuan",
      "Thien-Minh Nguyen",
      "Zhuyu Huang",
      "Chenyang Shi",
      "Jin Jing",
      "Lihua Xie"
    ],
    "abstract": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over traditional SLAM methods, enabling photorealistic 3D reconstruction that conventional approaches often struggle to achieve. However, existing GS-SLAM systems perform poorly under persistent and severe motion blur commonly encountered in real-world scenarios, leading to significantly degraded tracking accuracy and compromised 3D reconstruction quality. To address this limitation, we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D inputs to simultaneously reduce motion blur in images and compensate for the sparse and discrete nature of event streams, enabling robust tracking and high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system explicitly models the camera's continuous trajectory during exposure, supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian Splatting scene. Furthermore, we introduce a learnable camera response function to align the dynamic ranges of events and images, along with a no-event loss to suppress ringing artifacts during reconstruction. We validate our approach on a new dataset comprising synthetic and real-world sequences with significant motion blur. Extensive experimental results demonstrate that EGS-SLAM consistently outperforms existing GS-SLAM systems in both trajectory accuracy and photorealistic 3D Gaussian Splatting reconstruction. The source code will be available at https://github.com/Chensiyu00/EGS-SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2508.07003v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07003v1",
    "published_date": "2025-08-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "slam",
      "mapping",
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real\n  Images Beyond 180 Degree Field of View",
    "authors": [
      "Ulas Gunes",
      "Matias Turkulainen",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "abstract": "We present the first evaluation of fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180 degree. Our study covers both indoor and outdoor scenes captured with 200 degree fisheye cameras and analyzes how each method handles extreme distortion in real world settings. We evaluate performance under varying fields of view (200 degree, 160 degree, and 120 degree) to study the tradeoff between peripheral distortion and spatial coverage. Fisheye-GS benefits from field of view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable across all settings and maintains high perceptual quality at the full 200 degree view. To address the limitations of SfM-based initialization, which often fails under strong distortion, we also propose a depth-based strategy using UniK3D predictions from only 2-3 fisheye images per scene. Although UniK3D is not trained on real fisheye data, it produces dense point clouds that enable reconstruction quality on par with SfM, even in difficult scenes with fog, glare, or sky. Our results highlight the practical viability of fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and distortion-heavy image inputs.",
    "arxiv_url": "http://arxiv.org/abs/2508.06968v1",
    "pdf_url": "http://arxiv.org/pdf/2508.06968v1",
    "published_date": "2025-08-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "outdoor",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach\n  to Weakly-Supervised Video Anomaly Detection",
    "authors": [
      "Giacomo D'Amicantonio",
      "Snehashis Majhi",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "François Bremond",
      "Egor Bondarev"
    ],
    "abstract": "Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.",
    "arxiv_url": "http://arxiv.org/abs/2508.06318v1",
    "pdf_url": "http://arxiv.org/pdf/2508.06318v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian\n  Splatting",
    "authors": [
      "Wenpeng Xing",
      "Jie Chen",
      "Zaifeng Yang",
      "Changting Lin",
      "Jianfeng Dong",
      "Chaochao Chen",
      "Xun Zhou",
      "Meng Han"
    ],
    "abstract": "Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.",
    "arxiv_url": "http://arxiv.org/abs/2508.06169v1",
    "pdf_url": "http://arxiv.org/pdf/2508.06169v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "face",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation",
    "authors": [
      "YoungChan Choi",
      "HengFei Wang",
      "YiHua Cheng",
      "Boeun Kim",
      "Hyung Jin Chang",
      "YoungGeun Choi",
      "Sang-Il Choi"
    ],
    "abstract": "We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2508.06136v2",
    "pdf_url": "http://arxiv.org/pdf/2508.06136v2",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera\n  Samplings and Diffusion Priors",
    "authors": [
      "Minsu Kim",
      "Subin Jeon",
      "In Cho",
      "Mijin Yoo",
      "Seon Joo Kim"
    ],
    "abstract": "Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io",
    "arxiv_url": "http://arxiv.org/abs/2508.06014v1",
    "pdf_url": "http://arxiv.org/pdf/2508.06014v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a\n  Single Image",
    "authors": [
      "Yanxing Liang",
      "Yinghui Wang",
      "Jinlong Yang",
      "Wei Li"
    ],
    "abstract": "The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.",
    "arxiv_url": "http://arxiv.org/abs/2508.05950v1",
    "pdf_url": "http://arxiv.org/pdf/2508.05950v1",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
    "authors": [
      "Raphael Du Sablon",
      "David Hart"
    ],
    "abstract": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.",
    "arxiv_url": "http://arxiv.org/abs/2508.05813v1",
    "pdf_url": "http://arxiv.org/pdf/2508.05813v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "fast",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAP: Gaussianize Any Point Clouds with Text Guidance",
    "authors": [
      "Weiqi Zhang",
      "Junsheng Zhou",
      "Haotian Geng",
      "Wenyuan Zhang",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.",
    "arxiv_url": "http://arxiv.org/abs/2508.05631v1",
    "pdf_url": "http://arxiv.org/pdf/2508.05631v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field\n  Rendering",
    "authors": [
      "Junyu Zhou",
      "Yuyang Huang",
      "Wenrui Dai",
      "Junni Zou",
      "Ziyang Zheng",
      "Nuowen Kan",
      "Chenglin Li",
      "Hongkai Xiong"
    ],
    "abstract": "Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time rendering while maintaining high-fidelity novel view synthesis. However, 3DGS resorts to the Gaussian function that is low-pass by nature and is restricted in representing high-frequency details in 3D scenes. Moreover, it causes redundant primitives with degraded training and rendering efficiency and excessive memory overhead. To overcome these limitations, we propose 3D Gabor Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with multiple directional 3D frequency responses for radiance field representation supervised by multi-view images. The proposed 3D Gabor-based primitive forms a filter bank incorporating multiple 3D Gabor kernels at different frequencies to enhance flexibility and efficiency in capturing fine 3D details. Furthermore, to achieve novel view rendering, an efficient CUDA-based rasterizer is developed to project the multiple directional 3D frequency components characterized by 3D Gabor-based primitives onto the 2D image plane, and a frequency-adaptive mechanism is presented for adaptive joint optimization of primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless integration into existing 3DGS paradigms to enhance both efficiency and quality of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat outperforms 3DGS and its variants using alternative primitives, and achieves state-of-the-art rendering quality across both real-world and synthetic scenes. Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously reduced number of primitives and memory consumption.",
    "arxiv_url": "http://arxiv.org/abs/2508.05343v1",
    "pdf_url": "http://arxiv.org/pdf/2508.05343v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CF3: Compact and Fast 3D Feature Fields",
    "authors": [
      "Hyunjoon Lee",
      "Joonkyu Min",
      "Jaesik Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2508.05254v3",
    "pdf_url": "http://arxiv.org/pdf/2508.05254v3",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Refining Gaussian Splatting: A Volumetric Densification Approach",
    "authors": [
      "Mohamed Abdul Gafoor",
      "Marius Preda",
      "Titus Zaharia"
    ],
    "abstract": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.",
    "arxiv_url": "http://arxiv.org/abs/2508.05187v1",
    "pdf_url": "http://arxiv.org/pdf/2508.05187v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "motion",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Study of the Framework and Real-World Applications of Language\n  Embedding for 3D Scene Understanding",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Yehor Karpichev",
      "Brandon Haworth",
      "Homayoun Najjaran"
    ],
    "abstract": "Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2508.05064v2",
    "pdf_url": "http://arxiv.org/pdf/2508.05064v2",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "understanding",
      "ar",
      "nerf",
      "robotics",
      "survey",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for\n  Enhanced Sparse-View 3DGS",
    "authors": [
      "Zhihao Guo",
      "Peng Wang",
      "Zidong Chen",
      "Xiangyu Kong",
      "Yan Lyu",
      "Guanyu Gao",
      "Liangxiu Han"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\\% PSNR improvements on the MipNeRF 360 dataset.",
    "arxiv_url": "http://arxiv.org/abs/2508.04968v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04968v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.8; I.2.10; I.5.1"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D\n  Reconstruction",
    "authors": [
      "Yifan Zhou",
      "Beizhen Zhao",
      "Pengcheng Wu",
      "Hao Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2508.04966v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04966v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting",
    "authors": [
      "Zijian Wang",
      "Beizhen Zhao",
      "Hao Wang"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable capabilities in real-time and photorealistic novel view synthesis. However, traditional 3DGS representations often struggle with large-scale scene management and efficient storage, particularly when dealing with complex environments or limited computational resources. To address these limitations, we introduce a novel perceive-sample-compress framework for 3D Gaussian Splatting. Specifically, we propose a scene perception compensation algorithm that intelligently refines Gaussian parameters at each level. This algorithm intelligently prioritizes visual importance for higher fidelity rendering in critical areas, while optimizing resource usage and improving overall visible quality. Furthermore, we propose a pyramid sampling representation to manage Gaussian primitives across hierarchical levels. Finally, to facilitate efficient storage of proposed hierarchical pyramid representations, we develop a Generalized Gaussian Mixed model compression algorithm to achieve significant compression ratios without sacrificing visual fidelity. The extensive experiments demonstrate that our method significantly improves memory efficiency and high visual quality while maintaining real-time rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2508.04965v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04965v1",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Gaussian Radio Fields for Channel Estimation",
    "authors": [
      "Muhammad Umer",
      "Muhammad Ahmed Mohsin",
      "Ahsan Bilal",
      "John M. Cioffi"
    ],
    "abstract": "Accurate channel state information (CSI) remains the most critical bottleneck in modern wireless networks, with pilot overhead consuming up to 11-21% of transmission bandwidth, increasing latency by 20-40% in massive MIMO systems, and reducing potential spectral efficiency by over 53%. Traditional estimation techniques fundamentally fail under mobility, with feedback delays as small as 4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We present neural Gaussian radio fields (nGRF), a novel framework that leverages explicit 3D Gaussian primitives to synthesize complex channel matrices accurately and efficiently. Unlike NeRF-based approaches that rely on slow implicit representations or existing Gaussian splatting methods that use non-physical 2D projections, nGRF performs direct 3D electromagnetic field aggregation, with each Gaussian acting as a localized radio modulator. nGRF demonstrates superior performance across diverse environments: in indoor scenarios, it achieves a 10.9$\\times$ higher prediction SNR than state of the art methods while reducing inference latency from 242 ms to just 1.1 ms (a 220$\\times$ speedup). For large-scale outdoor environments, where existing approaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF requires only 0.011 measurements per cubic foot compared to 0.2-178.1 for existing methods, thereby reducing data collection burden by 18$\\times$. Training time is similarly reduced from hours to minutes (a 180$\\times$ reduction), enabling rapid adaptation to dynamic environments. The code and datasets are available at: https://github.com/anonym-auth/n-grf",
    "arxiv_url": "http://arxiv.org/abs/2508.11668v1",
    "pdf_url": "http://arxiv.org/pdf/2508.11668v1",
    "published_date": "2025-08-06",
    "categories": [
      "eess.SP",
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "nerf",
      "efficient",
      "gaussian splatting",
      "outdoor",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction",
    "authors": [
      "Suyi Chen",
      "Haibin Ling"
    ],
    "abstract": "As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. In parallel, differentiable rendering techniques such as Gaussian splatting have demonstrated remarkable scalability and efficiency for volumetric representations, suggesting a natural fit for GMM-based cryo-EM reconstruction. However, off-the-shelf Gaussian splatting methods are designed for photorealistic view synthesis and remain incompatible with cryo-EM due to mismatches in the image formation physics, reconstruction objectives, and coordinate systems. Addressing these issues, we propose cryoSplat, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a view-dependent normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. These innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoSplat over representative baselines. The code will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2508.04929v3",
    "pdf_url": "http://arxiv.org/pdf/2508.04929v3",
    "published_date": "2025-08-06",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "compact",
      "ar",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
    "authors": [
      "Ke Li",
      "Mana Masuda",
      "Susanne Schmidt",
      "Shohei Mori"
    ],
    "abstract": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.",
    "arxiv_url": "http://arxiv.org/abs/2508.04326v2",
    "pdf_url": "http://arxiv.org/pdf/2508.04326v2",
    "published_date": "2025-08-06",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "robotics",
      "survey",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction",
    "authors": [
      "Yaopeng Lou",
      "Liao Shen",
      "Tianqi Liu",
      "Jiaqi Li",
      "Zihao Huang",
      "Huiqiang Sun",
      "Zhiguo Cao"
    ],
    "abstract": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuRF achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets.",
    "arxiv_url": "http://arxiv.org/abs/2508.04297v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04297v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "outdoor",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry\n  Decomposition",
    "authors": [
      "Jiahui Li",
      "Shengeng Tang",
      "Jingxuan He",
      "Gang Huang",
      "Zhangye Wang",
      "Yantao Pan",
      "Lechao Cheng"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \\textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.",
    "arxiv_url": "http://arxiv.org/abs/2508.04224v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04224v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D\n  Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) represents a significant advancement in the field of efficient and high-fidelity novel view synthesis. Despite recent progress, achieving accurate geometric reconstruction under sparse-view conditions remains a fundamental challenge. Existing methods often rely on non-local depth regularization, which fails to capture fine-grained structures and is highly sensitive to depth estimation noise. Furthermore, traditional smoothing methods neglect semantic boundaries and indiscriminately degrade essential edges and textures, consequently limiting the overall quality of reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware regularization framework for 3D Gaussian Splatting. DET-GS introduces a hierarchical geometric depth supervision framework that adaptively enforces multi-level geometric consistency, significantly enhancing structural fidelity and robustness against depth estimation noise. To preserve scene boundaries, we design an edge-aware depth regularization guided by semantic masks derived from Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving Total Variation loss that selectively smooths homogeneous regions while rigorously retaining high-frequency details and textures. Extensive experiments demonstrate that DET-GS achieves substantial improvements in both geometric accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on sparse-view novel view synthesis benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2508.04099v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04099v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "sparse-view",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent\n  Super-Resolution Framework",
    "authors": [
      "Yi-Ting Chen",
      "Ting-Hsuan Liao",
      "Pengsheng Guo",
      "Alexander Schwing",
      "Jia-Bin Huang"
    ],
    "abstract": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions. Code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2508.04090v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04090v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for\n  Gaussian Splatting",
    "authors": [
      "Zhan Li",
      "Huangying Zhan",
      "Changyang Li",
      "Qingan Yan",
      "Yi Xu"
    ],
    "abstract": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive and expert-driven process, often resulting in inconsistent reconstructions and suboptimal results. We propose RLGS, a plug-and-play reinforcement learning framework for adaptive hyperparameter tuning in 3DGS through lightweight policy modules, dynamically adjusting critical hyperparameters such as learning rates and densification thresholds. The framework is model-agnostic and seamlessly integrates into existing 3DGS pipelines without architectural modifications. We demonstrate its generalization ability across multiple state-of-the-art 3DGS variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness across diverse datasets. RLGS consistently enhances rendering quality. For example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT) dataset, under a fixed Gaussian budget, and continues to yield gains even when baseline performance saturates. Our results suggest that RLGS provides an effective and general solution for automating hyperparameter tuning in 3DGS training, bridging a gap in applying reinforcement learning to 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2508.04078v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04078v1",
    "published_date": "2025-08-06",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "lightweight",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
    "authors": [
      "Hongyu Shen",
      "Junfeng Ni",
      "Yixin Chen",
      "Weishuo Li",
      "Mingtao Pei",
      "Siyuan Huang"
    ],
    "abstract": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.",
    "arxiv_url": "http://arxiv.org/abs/2508.03227v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03227v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "semantic",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time\n  Order-Independent Gaussian Splatting",
    "authors": [
      "Weihang Liu",
      "Yuke Li",
      "Yuxuan Li",
      "Jingyi Yu",
      "Xin Lou"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.",
    "arxiv_url": "http://arxiv.org/abs/2508.03180v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03180v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "acceleration",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D\n  Reconstruction",
    "authors": [
      "Heng Jia",
      "Linchao Zhu",
      "Na Zhao"
    ],
    "abstract": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a fundamental trade-off: explicit methods achieve geometric precision but struggle with ambiguous regions, while implicit methods provide robustness but suffer from slow convergence. We present H3R, a hybrid framework that addresses this limitation by integrating volumetric latent fusion with attention-based feature aggregation. Our framework consists of two complementary components: an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages Pl\\\"ucker coordinates for adaptive correspondence refinement. By integrating both paradigms, our approach enhances generalization while converging 2$\\times$ faster than existing methods. Furthermore, we show that spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2), resolving the mismatch between semantic representations and spatial reconstruction requirements. Our method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively. Code is available at https://github.com/JiaHeng-DLUT/H3R.",
    "arxiv_url": "http://arxiv.org/abs/2508.03118v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03118v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under\n  Low-Quality Conditions",
    "authors": [
      "Anran Wu",
      "Long Peng",
      "Xin Di",
      "Xueyuan Dai",
      "Chen Wu",
      "Yang Wang",
      "Xueyang Fu",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "abstract": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of optimization-based 3DGS by enabling fast and high-quality reconstruction without the need for per-scene optimization. However, existing feedforward approaches typically assume that input multi-view images are clean and high-quality. In real-world scenarios, images are often captured under challenging conditions such as noise, low light, or rain, resulting in inaccurate geometry and degraded 3D reconstruction. To address these challenges, we propose a general and efficient multi-view feature enhancement module, RobustGS, which substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction. The RobustGS module can be seamlessly integrated into existing pretrained pipelines in a plug-and-play manner to enhance reconstruction robustness. Specifically, we introduce a novel component, Generalized Degradation Learner, designed to extract generic representations and distributions of multiple degradations from multi-view inputs, thereby enhancing degradation-awareness and improving the overall quality of 3D reconstruction. In addition, we propose a novel semantic-aware state-space model. It first leverages the extracted degradation representations to enhance corrupted inputs in the feature space. Then, it employs a semantic-aware strategy to aggregate semantically similar information across different views, enabling the extraction of fine-grained cross-view correspondences and further improving the quality of 3D representations. Extensive experiments demonstrate that our approach, when integrated into existing methods in a plug-and-play manner, consistently achieves state-of-the-art reconstruction quality across various types of degradations.",
    "arxiv_url": "http://arxiv.org/abs/2508.03077v1",
    "pdf_url": "http://arxiv.org/pdf/2508.03077v1",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting",
      "efficient",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting",
    "authors": [
      "Liheng Zhang",
      "Weihao Yu",
      "Zubo Lu",
      "Haozhi Gu",
      "Jin Huang"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.",
    "arxiv_url": "http://arxiv.org/abs/2508.03017v2",
    "pdf_url": "http://arxiv.org/pdf/2508.03017v2",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
    "authors": [
      "Mikołaj Zieliński",
      "Krzysztof Byrski",
      "Tomasz Szczepanik",
      "Przemysław Spurek"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)",
    "arxiv_url": "http://arxiv.org/abs/2508.02831v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02831v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "fast",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal\n  Spans via 3D Gaussian Splatting",
    "authors": [
      "Yijun Xu",
      "Jingrui Zhang",
      "Yuhan Chen",
      "Dingwen Wang",
      "Lei Yu",
      "Chu He"
    ],
    "abstract": "Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Futhermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.",
    "arxiv_url": "http://arxiv.org/abs/2508.02660v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02660v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "acceleration",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian\n  Splatting",
    "authors": [
      "Jianchao Wang",
      "Peng Zhou",
      "Cen Li",
      "Rong Quan",
      "Jie Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. We provide our implementation in https://jcwang-gh.github.io/EFA-GS.",
    "arxiv_url": "http://arxiv.org/abs/2508.02493v2",
    "pdf_url": "http://arxiv.org/pdf/2508.02493v2",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from\n  Primitive-Based Representations of Error and Visibility",
    "authors": [
      "Thomas Gottwald",
      "Edgar Heinert",
      "Matthias Rottmann"
    ],
    "abstract": "In this work, we present a novel method for uncertainty estimation (UE) in Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical applications such as robotics and medicine. Previous methods typically estimate the variance of Gaussian primitives and use the rendering process to obtain pixel-wise uncertainties. Our method establishes primitive representations of error and visibility of trainings views, which carries meaningful uncertainty information. This representation is obtained by projection of training error and visibility onto the primitives. Uncertainties of novel views are obtained by rendering the primitive representations of uncertainty for those novel views, yielding uncertainty feature maps. To aggregate these uncertainty feature maps of novel views, we perform a pixel-wise regression on holdout data. In our experiments, we analyze the different components of our method, investigating various combinations of uncertainty feature maps and regression models. Furthermore, we considered the effect of separating splatting into foreground and background. Our UEs show high correlations to true errors, outperforming state-of-the-art methods, especially on foreground objects. The trained regression models show generalization capabilities to new scenes, allowing uncertainty estimation without the need for holdout data.",
    "arxiv_url": "http://arxiv.org/abs/2508.02443v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02443v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction",
    "authors": [
      "Yikuang Yuluo",
      "Yue Ma",
      "Kuan Shen",
      "Tongtong Jin",
      "Wang Liao",
      "Yangpu Ma",
      "Fuquan Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT reconstruction. However, existing methods rely on the average gradient magnitude of points within the view, often leading to severe needle-like artifacts under sparse-view conditions. To address this challenge, we propose GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses needle-like artifacts and improves reconstruction accuracy under sparse-view conditions. Our framework introduces two key innovations: (1) a Denoised Point Cloud Initialization Strategy that reduces initialization errors and accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that refines gradient computation using graph-based density differences, improving splitting accuracy and density representation. Experiments on X-3D and real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These results highlight the applicability of GR-Gaussian for accurate CT reconstruction under challenging sparse-view conditions.",
    "arxiv_url": "http://arxiv.org/abs/2508.02408v2",
    "pdf_url": "http://arxiv.org/pdf/2508.02408v2",
    "published_date": "2025-08-04",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "sparse-view",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning\n  via Gaussian Splatting",
    "authors": [
      "Lei Yao",
      "Yi Wang",
      "Yi Zhang",
      "Moyun Liu",
      "Lap-Pui Chau"
    ],
    "abstract": "The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
    "arxiv_url": "http://arxiv.org/abs/2508.02172v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02172v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
    "authors": [
      "Seungyeon Kim",
      "Junsu Ha",
      "Young Hun Kim",
      "Yonghyeon Lee",
      "Frank C. Park"
    ],
    "abstract": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model. See the project website at: https://screwsplat.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2508.02146v2",
    "pdf_url": "http://arxiv.org/pdf/2508.02146v2",
    "published_date": "2025-08-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "geometry",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic\n  Urban Scenes Modeling",
    "authors": [
      "Yuru Xiao",
      "Zihan Lin",
      "Chao Lu",
      "Deming Zhai",
      "Kui Jiang",
      "Wenbo Zhao",
      "Wei Zhang",
      "Junjun Jiang",
      "Huanran Wang",
      "Xianming Liu"
    ],
    "abstract": "Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.",
    "arxiv_url": "http://arxiv.org/abs/2508.02129v1",
    "pdf_url": "http://arxiv.org/pdf/2508.02129v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "fast",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "dynamic",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of\n  Objective Property Assessment",
    "authors": [
      "Petteri Teikari",
      "Mike Jarrell",
      "Irene Bandera Moreno",
      "Harri Pesola"
    ],
    "abstract": "The convergence of autonomous indoor drones with physics-aware sensing technologies promises to transform property assessment from subjective visual inspection to objective, quantitative measurement. This comprehensive review examines the technical foundations enabling this paradigm shift across four critical domains: (1) platform architectures optimized for indoor navigation, where weight constraints drive innovations in heterogeneous computing, collision-tolerant design, and hierarchical control systems; (2) advanced sensing modalities that extend perception beyond human vision, including hyperspectral imaging for material identification, polarimetric sensing for surface characterization, and computational imaging with metaphotonics enabling radical miniaturization; (3) intelligent autonomy through active reconstruction algorithms, where drones equipped with 3D Gaussian Splatting make strategic decisions about viewpoint selection to maximize information gain within battery constraints; and (4) integration pathways with existing property workflows, including Building Information Modeling (BIM) systems and industry standards like Uniform Appraisal Dataset (UAD) 3.6.",
    "arxiv_url": "http://arxiv.org/abs/2508.01965v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01965v1",
    "published_date": "2025-08-04",
    "categories": [
      "cs.RO",
      "cs.CV",
      "I.2.9; I.4.8; I.2.10; C.3; J.2"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for\n  Instance-Level 3D Scene Understanding and Editing",
    "authors": [
      "Zhaonan Wang",
      "Manyi Li",
      "Changhe Tu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2508.01740v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01740v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "compact",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for\n  Autonomous Driving",
    "authors": [
      "Luqi Cheng",
      "Zhangshuo Qi",
      "Zijie Zhou",
      "Chao Lu",
      "Guangming Xiong"
    ],
    "abstract": "Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at https://github.com/ChengLuqi/LT-gaussian.",
    "arxiv_url": "http://arxiv.org/abs/2508.01704v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01704v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing",
    "authors": [
      "Yufeng Chi",
      "Huimin Ma",
      "Kafeng Wang",
      "Jianmin Li"
    ],
    "abstract": "While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \\textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.",
    "arxiv_url": "http://arxiv.org/abs/2508.01684v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01684v1",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D\n  Gaussians",
    "authors": [
      "Quankai Gao",
      "Iliyan Georgiev",
      "Tuanfeng Y. Wang",
      "Krishna Kumar Singh",
      "Ulrich Neumann",
      "Jae Shin Yoon"
    ],
    "abstract": "3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2508.01464v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01464v1",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "semantic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OCSplats: Observation Completeness Quantification and Label Noise\n  Separation in 3DGS",
    "authors": [
      "Han Ling",
      "Xian Xu",
      "Yinghui Sun",
      "Quansen Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become one of the most promising 3D reconstruction technologies. However, label noise in real-world scenarios-such as moving objects, non-Lambertian surfaces, and shadows-often leads to reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods either fail to separate noise effectively or require scene-specific fine-tuning of hyperparameters, making them difficult to apply in practice. This paper re-examines the problem of anti-noise reconstruction from the perspective of epistemic uncertainty, proposing a novel framework, OCSplats. By combining key technologies such as hybrid noise assessment and observation-based cognitive correction, the accuracy of noise classification in areas with cognitive differences has been significantly improved. Moreover, to address the issue of varying noise proportions in different scenarios, we have designed a label noise classification pipeline based on dynamic anchor points. This pipeline enables OCSplats to be applied simultaneously to scenarios with vastly different noise proportions without adjusting parameters. Extensive experiments demonstrate that OCSplats always achieve leading reconstruction performance and precise label noise classification in scenes of different complexity levels.",
    "arxiv_url": "http://arxiv.org/abs/2508.01239v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01239v1",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "shadow",
      "gaussian splatting",
      "face",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from\n  Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.",
    "arxiv_url": "http://arxiv.org/abs/2508.01171v1",
    "pdf_url": "http://arxiv.org/pdf/2508.01171v1",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "sparse view",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a\n  Bimanual Robot with Handover and Gaussian Splat Merging",
    "authors": [
      "Tianshuang Qiu",
      "Zehan Ma",
      "Karim El-Refai",
      "Hiya Shah",
      "Chung Min Kim",
      "Justin Kerr",
      "Ken Goldberg"
    ],
    "abstract": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view images. Such \"digital twins\" are useful for simulations, virtual reality, marketing, robot policy fine-tuning, and part inspection. 3D object scanning usually requires multi-camera arrays, precise laser scanners, or robot wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan, a pipeline for producing high-quality 3D Gaussian Splat models using a bi-manual robot that grasps an object with one gripper and rotates the object with respect to a stationary camera. The object is then re-grasped by a second gripper to expose surfaces that were occluded by the first gripper. We present the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as RAFT optical flow models to identify and isolate objects held by a robot gripper while removing the gripper and the background. We then modify the 3DGS training pipeline to support concatenated datasets with gripper occlusion, producing an omni-directional (360 degree view) model of the object. We apply Omni-Scan to part defect inspection, finding that it can identify visual or geometric defects in 12 different industrial and household objects with an average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be found at https://berkeleyautomation.github.io/omni-scan/",
    "arxiv_url": "http://arxiv.org/abs/2508.00354v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00354v1",
    "published_date": "2025-08-01",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian\n  Splatting",
    "authors": [
      "Wentao Sun",
      "Hanqing Xu",
      "Quanyun Wu",
      "Dedong Zhang",
      "Yiping Chen",
      "Lingfei Ma",
      "John S. Zelek",
      "Jonathan Li"
    ],
    "abstract": "We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.",
    "arxiv_url": "http://arxiv.org/abs/2508.00259v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00259v1",
    "published_date": "2025-08-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
    "authors": [
      "Bowen Zhang",
      "Sicheng Xu",
      "Chuxin Wang",
      "Jiaolong Yang",
      "Feng Zhao",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2507.23785v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23785v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "compact",
      "4d",
      "ar",
      "animation",
      "high-fidelity",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D\n  Gaussian Splatting",
    "authors": [
      "Di Li",
      "Jie Feng",
      "Jiahao Chen",
      "Weisheng Dong",
      "Guanbin Li",
      "Yuhui Zheng",
      "Mingtao Feng",
      "Guangming Shi"
    ],
    "abstract": "3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.",
    "arxiv_url": "http://arxiv.org/abs/2507.23772v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23772v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "human",
      "understanding",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction",
    "authors": [
      "Zhenyang Li",
      "Xiaoyang Bai",
      "Tongchen Zhang",
      "Pengfei Shen",
      "Weiwei Xu",
      "Yifan Peng"
    ],
    "abstract": "High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2507.23704v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23704v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian\n  Splatting for Autonomous Driving Data Generation",
    "authors": [
      "Jialei Chen",
      "Wuhao Xu",
      "Sipeng He",
      "Baoru Huang",
      "Dongchun Ren"
    ],
    "abstract": "Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2507.23683v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23683v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "ar",
      "lighting",
      "gaussian splatting",
      "efficient",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes",
    "authors": [
      "Xiaohan Li",
      "Ziren Gong",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia",
      "Dong Liu",
      "Jun Wu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM applications due to its fast rendering and high-fidelity representation. However, existing 3DGS-SLAM systems have predominantly focused on indoor environments and relied on active depth sensors, leaving a gap for large-scale outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian Splatting SLAM system designed for outdoor scenarios. Our approach uses only RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages depth estimates from pre-trained deep stereo networks to guide 3D Gaussian optimization with a multi-loss strategy enhancing both geometric consistency and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM achieves superior tracking accuracy and mapping performance compared to other 3DGS-based solutions in complex outdoor environments.",
    "arxiv_url": "http://arxiv.org/abs/2507.23677v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23677v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "slam",
      "mapping",
      "high-fidelity",
      "gaussian splatting",
      "outdoor",
      "tracking",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual\n  Localization",
    "authors": [
      "Maxime Pietrantoni",
      "Gabriela Csurka",
      "Torsten Sattler"
    ],
    "abstract": "Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.",
    "arxiv_url": "http://arxiv.org/abs/2507.23569v2",
    "pdf_url": "http://arxiv.org/pdf/2507.23569v2",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "gaussian splatting",
      "geometry",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Takeo Igarashi",
      "Yufeng Wang",
      "ZeSheng Wang",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou"
    ],
    "abstract": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.",
    "arxiv_url": "http://arxiv.org/abs/2507.23374v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23374v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle\n  Inpainting",
    "authors": [
      "Xingyue Peng",
      "Yuandong Lyu",
      "Lang Zhang",
      "Jian Zhu",
      "Songtao Wang",
      "Jiaxin Deng",
      "Songxin Lu",
      "Weiliang Ma",
      "Dangen She",
      "Peng Jia",
      "XianPeng Lang"
    ],
    "abstract": "Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.",
    "arxiv_url": "http://arxiv.org/abs/2507.23340v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23340v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "3d gaussian",
      "segmentation",
      "ar",
      "mapping",
      "lighting",
      "gaussian splatting",
      "efficient",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iLRM: An Iterative Large 3D Reconstruction Model",
    "authors": [
      "Gyeongjin Kang",
      "Seungtae Nam",
      "Xiangyu Sun",
      "Sameh Khamis",
      "Abdelrahman Mohamed",
      "Eunbyung Park"
    ],
    "abstract": "Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.",
    "arxiv_url": "http://arxiv.org/abs/2507.23277v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23277v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian\n  Splatting",
    "authors": [
      "Jaeseok Park",
      "Chanoh Park",
      "Minsu Kim",
      "Soohwan Kim"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.23273v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23273v1",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "mapping",
      "illumination",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to\n  Unfavorable Views",
    "authors": [
      "Yuki Fujimura",
      "Takahiro Kushida",
      "Kazuya Kitano",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa"
    ],
    "abstract": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views.",
    "arxiv_url": "http://arxiv.org/abs/2507.22342v2",
    "pdf_url": "http://arxiv.org/pdf/2507.22342v2",
    "published_date": "2025-07-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity\n  Environments",
    "authors": [
      "Yufei Jia",
      "Guangyu Wang",
      "Yuhang Dong",
      "Junzhe Wu",
      "Yupei Zeng",
      "Haonan Lin",
      "Zifan Wang",
      "Haizhou Ge",
      "Weibin Gu",
      "Kairui Ding",
      "Zike Yan",
      "Yunjie Cheng",
      "Yue Li",
      "Ziming Wang",
      "Chuxuan Li",
      "Wei Sui",
      "Lu Shi",
      "Guanzhong Tian",
      "Ruqi Huang",
      "Guyue Zhou"
    ],
    "abstract": "We present the first unified, modular, open-source 3DGS-based simulation framework for Real2Sim2Real robot learning. It features a holistic Real2Sim pipeline that synthesizes hyper-realistic geometry and appearance of complex real-world scenarios, paving the way for analyzing and bridging the Sim2Real gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively parallel simulation of multiple sensor modalities and accurate physics, with inclusive supports for existing 3D assets, robot models, and ROS plugins, empowering large-scale robot learning and complex robotic benchmarks. Through extensive experiments on imitation learning, Discoverse demonstrates state-of-the-art zero-shot Sim2Real transfer performance compared to existing simulators. For code and demos: https://air-discoverse.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2507.21981v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21981v1",
    "published_date": "2025-07-29",
    "categories": [
      "cs.RO",
      "68T40",
      "I.2.9"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MultiEditor: Controllable Multimodal Object Editing for Driving\n  Scenarios Using 3D Gaussian Splatting Priors",
    "authors": [
      "Shouyi Lu",
      "Zihan Lin",
      "Chao Lu",
      "Huanran Wang",
      "Guirong Zhuo",
      "Lianqing Zheng"
    ],
    "abstract": "Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.",
    "arxiv_url": "http://arxiv.org/abs/2507.21872v3",
    "pdf_url": "http://arxiv.org/pdf/2507.21872v3",
    "published_date": "2025-07-29",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for\n  Real-time Rendering",
    "authors": [
      "Linye Wei",
      "Jiajun Tang",
      "Fan Fei",
      "Boxin Shi",
      "Runsheng Wang",
      "Meng Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and is getting increasing adoption in domains like autonomous driving and embodied intelligence. However, 3DGS still faces major efficiency challenges when faced with high frame rate requirements and resource-constrained edge deployment. To enable efficient 3DGS, in this paper, we propose LS-Gaussian, an algorithm/hardware co-design framework for lightweight streaming 3D rendering. LS-Gaussian is motivated by the core observation that 3DGS suffers from substantial computation redundancy and stalls. On one hand, in practical scenarios, high-frame-rate 3DGS is often applied in settings where a camera observes and renders the same scene continuously but from slightly different viewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian proposes a viewpoint transformation algorithm that leverages inter-frame continuity for efficient sparse rendering. On the other hand, as different tiles within an image are rendered in parallel but have imbalanced workloads, frequent hardware stalls also slow down the rendering process. LS-Gaussian predicts the workload for each tile based on viewpoint transformation to enable more balanced parallel computation and co-designs a customized 3DGS accelerator to support the workload-aware mapping in real-time. Experimental results demonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline on average and up to 17.3x speedup with the customized accelerator, while incurring only minimal visual quality degradation.",
    "arxiv_url": "http://arxiv.org/abs/2507.21572v2",
    "pdf_url": "http://arxiv.org/pdf/2507.21572v2",
    "published_date": "2025-07-29",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "autonomous driving",
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "efficient",
      "face",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from\n  Unconstrained Photo Collections",
    "authors": [
      "Haiyang Bai",
      "Jiaqi Zhu",
      "Songru Jiang",
      "Wei Huang",
      "Tao Lu",
      "Yuanqi Li",
      "Jie Guo",
      "Runze Fu",
      "Yanwen Guo",
      "Lijun Chen"
    ],
    "abstract": "We propose a 3D Gaussian splatting-based framework for outdoor relighting that leverages intrinsic image decomposition to precisely integrate sunlight, sky radiance, and indirect lighting from unconstrained photo collections. Unlike prior methods that compress the per-image global illumination into a single latent vector, our approach enables simultaneously diverse shading manipulation and the generation of dynamic shadow effects. This is achieved through three key innovations: (1) a residual-based sun visibility extraction method to accurately separate direct sunlight effects, (2) a region-based supervision framework with a structural consistency loss for physically interpretable and coherent illumination decomposition, and (3) a ray-tracing-based technique for realistic shadow simulation. Extensive experiments demonstrate that our framework synthesizes novel views with competitive fidelity against state-of-the-art relighting solutions and produces more natural and multifaceted illumination and shadow effects.",
    "arxiv_url": "http://arxiv.org/abs/2507.20512v1",
    "pdf_url": "http://arxiv.org/pdf/2507.20512v1",
    "published_date": "2025-07-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "global illumination",
      "3d gaussian",
      "relighting",
      "ar",
      "lighting",
      "shadow",
      "illumination",
      "gaussian splatting",
      "outdoor",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Automated 3D-GS Registration and Fusion via Skeleton Alignment and\n  Gaussian-Adaptive Features",
    "authors": [
      "Shiyang Liu",
      "Dianyi Yang",
      "Yu Gao",
      "Bohan Ren",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "abstract": "In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation demonstrates significant potential in real-time rendering and training efficiency. However, most existing methods primarily focus on single-map reconstruction, while the registration and fusion of multiple 3D-GS sub-maps remain underexplored. Existing methods typically rely on manual intervention to select a reference sub-map as a template and use point cloud matching for registration. Moreover, hard-threshold filtering of 3D-GS primitives often degrades rendering quality after fusion. In this paper, we present a novel approach for automated 3D-GS sub-map alignment and fusion, eliminating the need for manual intervention while enhancing registration accuracy and fusion quality. First, we extract geometric skeletons across multiple scenes and leverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating robust scene registration. Second, we introduce a multi-factor Gaussian fusion strategy to mitigate the scene element loss caused by rigid thresholding. Experiments on the ScanNet-GSReg and our Coord datasets demonstrate the effectiveness of the proposed method in registration and fusion. For registration, it achieves a 41.9\\% reduction in RRE on complex scenes, ensuring more precise pose estimation. For fusion, it improves PSNR by 10.11 dB, highlighting superior structural preservation. These results confirm its ability to enhance scene alignment and reconstruction fidelity, ensuring more consistent and accurate 3D scene representation for robotic perception and autonomous navigation.",
    "arxiv_url": "http://arxiv.org/abs/2507.20480v1",
    "pdf_url": "http://arxiv.org/pdf/2507.20480v1",
    "published_date": "2025-07-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos",
    "authors": [
      "Chenjian Gao",
      "Lihe Ding",
      "Rui Han",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue"
    ],
    "abstract": "Inserting 3D objects into videos is a longstanding challenge in computer graphics with applications in augmented reality, virtual try-on, and video composition. Achieving both temporal consistency, or realistic lighting remains difficult, particularly in dynamic scenarios with complex object motion, perspective changes, and varying illumination. While 2D diffusion models have shown promise for producing photorealistic edits, they often struggle with maintaining temporal coherence across frames. Conversely, traditional 3D rendering methods excel in spatial and temporal consistency but fall short in achieving photorealistic lighting. In this work, we propose a hybrid object insertion pipeline that combines the strengths of both paradigms. Specifically, we focus on inserting bracelets into dynamic wrist scenes, leveraging the high temporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and refining the results using a 2D diffusion-based enhancement model to ensure realistic lighting interactions. Our method introduces a shading-driven pipeline that separates intrinsic object properties (albedo, shading, reflectance) and refines both shading and sRGB images for photorealism. To maintain temporal coherence, we optimize the 3DGS model with multi-frame weighted adjustments. This is the first approach to synergize 3D rendering and 2D diffusion for video object insertion, offering a robust solution for realistic and consistent video editing. Project Page: https://cjeen.github.io/BraceletPaper/",
    "arxiv_url": "http://arxiv.org/abs/2507.20331v2",
    "pdf_url": "http://arxiv.org/pdf/2507.20331v2",
    "published_date": "2025-07-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "lighting",
      "illumination",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decomposing Densification in Gaussian Splatting for Faster 3D Scene\n  Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhengwu Liu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (GS) has emerged as a powerful representation for high-quality scene reconstruction, offering compelling rendering quality. However, the training process of GS often suffers from slow convergence due to inefficient densification and suboptimal spatial distribution of Gaussian primitives. In this work, we present a comprehensive analysis of the split and clone operations during the densification phase, revealing their distinct roles in balancing detail preservation and computational efficiency. Building upon this analysis, we propose a global-to-local densification strategy, which facilitates more efficient growth of Gaussians across the scene space, promoting both global coverage and local refinement. To cooperate with the proposed densification strategy and promote sufficient diffusion of Gaussian primitives in space, we introduce an energy-guided coarse-to-fine multi-resolution training framework, which gradually increases resolution based on energy density in 2D images. Additionally, we dynamically prune unnecessary Gaussian primitives to speed up the training. Extensive experiments on MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our approach significantly accelerates training,achieving over 2x speedup with fewer Gaussian primitives and superior reconstruction performance.",
    "arxiv_url": "http://arxiv.org/abs/2507.20239v1",
    "pdf_url": "http://arxiv.org/pdf/2507.20239v1",
    "published_date": "2025-07-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "nerf",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Shell Texture Splatting: More Details and Fewer Primitives",
    "authors": [
      "Xin Zhang",
      "Anpei Chen",
      "Jincheng Xiong",
      "Pinxuan Dai",
      "Yujun Shen",
      "Weiwei Xu"
    ],
    "abstract": "Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.",
    "arxiv_url": "http://arxiv.org/abs/2507.20200v1",
    "pdf_url": "http://arxiv.org/pdf/2507.20200v1",
    "published_date": "2025-07-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues\n  for 3D Object Detection",
    "authors": [
      "Xiaokai Bai",
      "Chenxu Zhou",
      "Lianqing Zheng",
      "Si-Yuan Cao",
      "Jianan Liu",
      "Xiaohan Zhang",
      "Zhengzhuang Zhang",
      "Hui-liang Shen"
    ],
    "abstract": "4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2507.19856v2",
    "pdf_url": "http://arxiv.org/pdf/2507.19856v2",
    "published_date": "2025-07-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "3d gaussian",
      "4d",
      "understanding",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "geometry",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Taking Language Embedded 3D Gaussian Splatting into the Wild",
    "authors": [
      "Yuze Wang",
      "Yue Qi"
    ],
    "abstract": "Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.",
    "arxiv_url": "http://arxiv.org/abs/2507.19830v2",
    "pdf_url": "http://arxiv.org/pdf/2507.19830v2",
    "published_date": "2025-07-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
    "authors": [
      "David Bauer",
      "Qi Wu",
      "Hamid Gadirov",
      "Kwan-Liu Ma"
    ],
    "abstract": "Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.19718v2",
    "pdf_url": "http://arxiv.org/pdf/2507.19718v2",
    "published_date": "2025-07-25",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "path tracing",
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit\n  Representations",
    "authors": [
      "Ziren Gong",
      "Xiaohan Li",
      "Fabio Tosi",
      "Youmin Zhang",
      "Stefano Mattoccia",
      "Jun Wu",
      "Matteo Poggi"
    ],
    "abstract": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2507.19474v1",
    "pdf_url": "http://arxiv.org/pdf/2507.19474v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive\n  Initialization",
    "authors": [
      "Pol Francesch Huc",
      "Emily Bates",
      "Simone D'Amico"
    ],
    "abstract": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.",
    "arxiv_url": "http://arxiv.org/abs/2507.19459v1",
    "pdf_url": "http://arxiv.org/pdf/2507.19459v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "nerf",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time\n  Dynamic Scene Rendering",
    "authors": [
      "Jie Chen",
      "Zhangchi Hu",
      "Peixi Wu",
      "Huyue Zhu",
      "Hebei Li",
      "Xiaoyan Sun"
    ],
    "abstract": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing plane-based methods in dynamic Gaussian splatting suffer from an unsuitable low-rank assumption, causing feature overlap and poor rendering quality. Although 4D hash encoding provides an explicit representation without low-rank constraints, directly applying it to the entire dynamic scene leads to substantial hash collisions and redundancy. To address these challenges, we present DASH, a real-time dynamic scene rendering framework that employs 4D hash encoding coupled with self-supervised decomposition. Our approach begins with a self-supervised decomposition mechanism that separates dynamic and static components without manual annotations or precomputed masks. Next, we introduce a multiresolution 4D hash encoder for dynamic elements, providing an explicit representation that avoids the low-rank assumption. Finally, we present a spatio-temporal smoothness regularization strategy to mitigate unstable deformation artifacts. Experiments on real-world datasets demonstrate that DASH achieves state-of-the-art dynamic rendering performance, exhibiting enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU. Code: https://github.com/chenj02/DASH.",
    "arxiv_url": "http://arxiv.org/abs/2507.19141v2",
    "pdf_url": "http://arxiv.org/pdf/2507.19141v2",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "4d",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital\n  CIM for High Frame Rate Real-Time Edge Rendering",
    "authors": [
      "Wei-Hsing Huang",
      "Cheng-Jhih Shih",
      "Jian-Wei Su",
      "Samuel Wade Wang",
      "Vaidehi Garg",
      "Yuyao Kong",
      "Jen-Chun Tien",
      "Nealson Li",
      "Arijit Raychowdhury",
      "Meng-Fan Chang",
      "Yingyan",
      "Lin",
      "Shimeng Yu"
    ],
    "abstract": "Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic scenes, enabling AR/VR applications with moving objects. However, implementing dynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian parameters from DRAM for frustum culling incurs high energy costs. (2) Increased parameters for dynamic scenes elevate sorting latency and energy consumption. (3) Limited on-chip buffer capacity with higher parameters reduces buffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not readily compatible with digital compute-in-memory (DCIM). These challenges hinder real-time performance and power efficiency on edge devices, leading to reduced battery life or requiring bulky batteries. To tackle these challenges, we propose algorithm-hardware co-design techniques. At the algorithmic level, we introduce three optimizations: (1) DRAM-access reduction frustum culling to lower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip buffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to reduce sorting latency. At the hardware level, we present a DCIM-friendly computation flow that is evaluated using the measured data from a 16nm DCIM prototype chip. Our experimental results on Large-Scale Real-World Static/Dynamic Datasets demonstrate the ability to achieve high frame rate real-time rendering exceeding 200 frame per second (FPS) with minimal power consumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W for dynamic Large-Scale Real-World scenes. This work successfully addresses the significant challenges of implementing static/dynamic 3DGS technology on resource-constrained edge devices.",
    "arxiv_url": "http://arxiv.org/abs/2507.19133v1",
    "pdf_url": "http://arxiv.org/pdf/2507.19133v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "head",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization",
    "authors": [
      "Zhentao Huang",
      "Di Wu",
      "Zhenbang He",
      "Minglun Gong"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2507.18923v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18923v1",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time",
    "authors": [
      "Yun Chen",
      "Matthew Haines",
      "Jingkang Wang",
      "Krzysztof Baron-Lis",
      "Sivabalan Manivasagam",
      "Ze Yang",
      "Raquel Urtasun"
    ],
    "abstract": "High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/",
    "arxiv_url": "http://arxiv.org/abs/2507.18713v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18713v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "3d gaussian",
      "ar",
      "fast",
      "nerf",
      "high-fidelity",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping",
    "authors": [
      "Chong Cheng",
      "Zijian Wang",
      "Sicheng Yu",
      "Yu Hu",
      "Nanjie Yao",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D representation. Its effectiveness largely depends on precise camera poses and accurate point cloud initialization, which are often derived from pretrained Multi-View Stereo (MVS) models. However, in unposed reconstruction task from hundreds of outdoor images, existing MVS models may struggle with memory limits and lose accuracy as the number of input images grows. To address this limitation, we propose a novel unposed 3DGS reconstruction framework that integrates pretrained MVS priors with the probabilistic Procrustes mapping strategy. The method partitions input images into subsets, maps submaps into a global space, and jointly optimizes geometry and poses with 3DGS. Technically, we formulate the mapping of tens of millions of point clouds as a probabilistic Procrustes problem and solve a closed-form alignment. By employing probabilistic coupling along with a soft dustbin mechanism to reject uncertain correspondences, our method globally aligns point clouds and poses within minutes across hundreds of images. Moreover, we propose a joint optimization framework for 3DGS and camera poses. It constructs Gaussians from confidence-aware anchor points and integrates 3DGS differentiable rendering with an analytical Jacobian to jointly refine scene and poses, enabling accurate reconstruction and pose estimation. Experiments on Waymo and KITTI datasets show that our method achieves accurate reconstruction from unposed image sequences, setting a new state of the art for unposed 3DGS reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2507.18541v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18541v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "outdoor",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using\n  Gaussian Splatting",
    "authors": [
      "Haoran Xu",
      "Saining Zhang",
      "Peishuo Li",
      "Baijun Ye",
      "Xiaoxue Chen",
      "Huan-ang Gao",
      "Jv Zheng",
      "Xiaowei Song",
      "Ziqiao Peng",
      "Run Miao",
      "Jinrang Jia",
      "Yifeng Shi",
      "Guangqi Yi",
      "Hang Zhao",
      "Hao Tang",
      "Hongyang Li",
      "Kaicheng Yu",
      "Hao Zhao"
    ],
    "abstract": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.",
    "arxiv_url": "http://arxiv.org/abs/2507.18473v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18473v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "ar",
      "tracking",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D\n  Content Creation from a Single Image",
    "authors": [
      "DongFu Yin",
      "Xiaotian Chen",
      "Fei Richard Yu",
      "Xuanchen Li",
      "Xinhao Zhang"
    ],
    "abstract": "Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.",
    "arxiv_url": "http://arxiv.org/abs/2507.18371v2",
    "pdf_url": "http://arxiv.org/pdf/2507.18371v2",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "vr",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM",
    "authors": [
      "Gyuhyeon Pak",
      "Hae Min Cho",
      "Euntai Kim"
    ],
    "abstract": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D reconstruction and robust camera pose tracking in real-time by representing each scene element using a Gaussian distribution constrained to the local tangent plane. This effectively models the local surface as a 2D Gaussian disk aligned with the underlying geometry, leading to more consistent depth interpretation across multiple viewpoints compared to conventional 3D ellipsoid-based representations with isotropic uncertainty. To integrate this representation into the SLAM pipeline, we embed the surface-aligned Gaussian disks into a Generalized ICP framework by introducing anisotropic covariance prior without altering the underlying registration formulation. Furthermore we propose a geometry-aware loss that supervises photometric, depth, and normal consistency. Our system achieves real-time operation while preserving both visual and geometric fidelity. Extensive experiments on the Replica and TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems in terms of localization accuracy, reconstruction completeness, while maintaining the rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2507.18344v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18344v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "slam",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "tracking",
      "geometry",
      "localization",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo",
    "authors": [
      "Yixiao Chen",
      "Bin Liang",
      "Hanzhi Guo",
      "Yongqing Cheng",
      "Jiayi Zhao",
      "Dongdong Weng"
    ],
    "abstract": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.18231v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18231v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "ar",
      "lighting",
      "illumination",
      "gaussian splatting",
      "efficient",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
    "authors": [
      "SeungJun Moon",
      "Hah Min Lew",
      "Seungeun Lee",
      "Ji-Su Kang",
      "Gyeong-Moon Park"
    ],
    "abstract": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2507.18155v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18155v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "head",
      "avatar",
      "ar",
      "animation",
      "face",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency\n  and photorealistic details",
    "authors": [
      "Jun Zhou",
      "Dinghao Li",
      "Nannan Li",
      "Mingjie Wang"
    ],
    "abstract": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.",
    "arxiv_url": "http://arxiv.org/abs/2507.18023v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18023v1",
    "published_date": "2025-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "localization",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian\n  Splatting",
    "authors": [
      "Hyeongmin Lee",
      "Kyungjune Baek"
    ],
    "abstract": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91$\\times$ compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments. The source code is available at https://github.com/HyeongminLEE/RD4DGS.",
    "arxiv_url": "http://arxiv.org/abs/2507.17336v2",
    "pdf_url": "http://arxiv.org/pdf/2507.17336v2",
    "published_date": "2025-07-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
    "authors": [
      "Luchuan Song",
      "Yang Zhou",
      "Zhan Xu",
      "Yi Zhou",
      "Deepali Aneja",
      "Chenliang Xu"
    ],
    "abstract": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.",
    "arxiv_url": "http://arxiv.org/abs/2507.17029v1",
    "pdf_url": "http://arxiv.org/pdf/2507.17029v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "relighting",
      "3d gaussian",
      "head",
      "avatar",
      "ar",
      "fast",
      "lighting",
      "animation",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
    "authors": [
      "Shang Liu",
      "Chenjie Cao",
      "Chaohui Yu",
      "Wen Qian",
      "Jing Wang",
      "Fan Wang"
    ],
    "abstract": "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/",
    "arxiv_url": "http://arxiv.org/abs/2507.16535v2",
    "pdf_url": "http://arxiv.org/pdf/2507.16535v2",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "compact",
      "ar",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges",
    "authors": [
      "Tanveer Younis",
      "Zhanglin Cheng"
    ],
    "abstract": "Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2507.16406v1",
    "pdf_url": "http://arxiv.org/pdf/2507.16406v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "sparse-view",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "robotics",
      "survey",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence\n  Images",
    "authors": [
      "Guichen Huang",
      "Ruoyu Wang",
      "Xiangjun Gao",
      "Che Sun",
      "Yuwei Wu",
      "Shenghua Gao",
      "Yunde Jia"
    ],
    "abstract": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\\% compared to existing per-pixel Gaussian prediction methods.",
    "arxiv_url": "http://arxiv.org/abs/2507.16144v1",
    "pdf_url": "http://arxiv.org/pdf/2507.16144v1",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian\n  Splatting",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.",
    "arxiv_url": "http://arxiv.org/abs/2507.15690v3",
    "pdf_url": "http://arxiv.org/pdf/2507.15690v3",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "sparse-view",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization\n  for Remote Sensing",
    "authors": [
      "Boni Hu",
      "Zhenyu Xia",
      "Lin Chen",
      "Pengcheng Han",
      "Shuhui Bu"
    ],
    "abstract": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera pose from query images, is fundamental to remote sensing and UAV applications. Existing methods face inherent trade-offs: image-based retrieval and pose regression approaches lack precision, while structure-based methods that register queries to Structure-from-Motion (SfM) models suffer from computational complexity and limited scalability. These challenges are particularly pronounced in remote sensing scenarios due to large-scale scenes, high altitude variations, and domain gaps of existing visual priors. To overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel scene representation that compactly encodes both 3D geometry and appearance. We introduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting the rich semantic information and geometric constraints inherent in Gaussian primitives. To handle large-scale remote sensing scenarios, we incorporate partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic memory management strategies. Our approach consists of two stages: (1) a sparse stage featuring a Gaussian-specific consistent render-aware sampling strategy and landmark-guided detector for robust and accurate initial pose estimation, and (2) a dense stage that iteratively refines poses through coarse-to-fine dense rasterization matching while incorporating reliability verification. Through comprehensive evaluation on simulation data, public datasets, and real flight experiments, we demonstrate that our method delivers competitive localization accuracy, recall rate, and computational efficiency while effectively filtering unreliable pose estimates. The results confirm the effectiveness of our approach for practical remote sensing applications.",
    "arxiv_url": "http://arxiv.org/abs/2507.15683v1",
    "pdf_url": "http://arxiv.org/pdf/2507.15683v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "3d gaussian",
      "compact",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic",
      "geometry",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "authors": [
      "Zuo-Liang Zhu",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
    "arxiv_url": "http://arxiv.org/abs/2507.15629v1",
    "pdf_url": "http://arxiv.org/pdf/2507.15629v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "3d gaussian",
      "ar",
      "efficient rendering",
      "lighting",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry",
      "ray marching"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting",
    "authors": [
      "Zihui Gao",
      "Jia-Wang Bian",
      "Guosheng Lin",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "abstract": "Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines the strengths of both approaches: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine the details of SDF for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on the DTU and MobileBrick datasets. Code will be released at https://github.com/aim-uofa/SurfaceSplat.",
    "arxiv_url": "http://arxiv.org/abs/2507.15602v2",
    "pdf_url": "http://arxiv.org/pdf/2507.15602v2",
    "published_date": "2025-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting",
    "authors": [
      "Ruijie Zhu",
      "Mulin Yu",
      "Linning Xu",
      "Lihan Jiang",
      "Yixuan Li",
      "Tianzhu Zhang",
      "Jiangmiao Pang",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
    "arxiv_url": "http://arxiv.org/abs/2507.15454v1",
    "pdf_url": "http://arxiv.org/pdf/2507.15454v1",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage\n  Conditional Processing",
    "authors": [
      "Minnan Pei",
      "Gang Li",
      "Junwen Si",
      "Zeyu Zhu",
      "Zitao Mo",
      "Peisong Wang",
      "Zhuoran Song",
      "Xiaoyao Liang",
      "Jian Cheng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering technique for high-fidelity view synthesis, prompting the development of dedicated 3DGS accelerators for resource-constrained platforms. The conventional decoupled preprocessing-rendering dataflow in existing accelerators has two major limitations: 1) a significant portion of preprocessed Gaussians are not used in rendering, and 2) the same Gaussian gets repeatedly loaded across different tile renderings, resulting in substantial computational and data movement overhead. To address these issues, we propose GCC, a novel accelerator designed for fast and energy-efficient 3DGS inference. GCC introduces a novel dataflow featuring: 1) \\textit{cross-stage conditional processing}, which interleaves preprocessing and rendering to dynamically skip unnecessary Gaussian preprocessing; and 2) \\textit{Gaussian-wise rendering}, ensuring that all rendering operations for a given Gaussian are completed before moving to the next, thereby eliminating duplicated Gaussian loading. We also propose an alpha-based boundary identification method to derive compact and accurate Gaussian regions, thereby reducing rendering costs. We implement our GCC accelerator in 28nm technology. Extensive experiments demonstrate that GCC significantly outperforms the state-of-the-art 3DGS inference accelerator, GSCore, in both performance and energy efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.15300v3",
    "pdf_url": "http://arxiv.org/pdf/2507.15300v3",
    "published_date": "2025-07-21",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian\n  Splatting Reconstruction",
    "authors": [
      "Xiufeng Huang",
      "Ka Chun Cheung",
      "Runmin Cong",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced Image-to-3D content creation but requires substantial computational resources and large datasets, posing challenges to training models from scratch. Current methods usually entangle the prediction of 3D Gaussian geometry and appearance, which rely heavily on data-driven priors and result in slow regression speeds. To address this, we propose \\method, a disentangled framework for efficient 3D Gaussian prediction. Our method extracts features from local image pairs using a stereo vision backbone and fuses them via global attention blocks. Dedicated point and Gaussian prediction heads generate multi-view point-maps for geometry and Gaussian features for appearance, combined as GS-maps to represent the 3DGS object. A refinement network enhances these GS-maps for high-quality reconstruction. Unlike existing methods that depend on camera parameters, our approach achieves pose-free 3D reconstruction, improving robustness and practicality. By reducing resource demands while maintaining high-quality outputs, \\method provides an efficient, scalable solution for real-world 3D content generation.",
    "arxiv_url": "http://arxiv.org/abs/2507.14921v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14921v1",
    "published_date": "2025-07-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "gaussian splatting",
      "efficient",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection",
    "authors": [
      "Jiahao Ma",
      "Tianyu Wang",
      "Miaomiao Liu",
      "David Ahmedt-Aristizabal",
      "Chuong Nguyen"
    ],
    "abstract": "Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.",
    "arxiv_url": "http://arxiv.org/abs/2507.14505v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14505v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "segmentation",
      "human",
      "ar",
      "gaussian splatting",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey",
    "authors": [
      "Jiahui Zhang",
      "Yuelei Li",
      "Anpei Chen",
      "Muyu Xu",
      "Kunhao Liu",
      "Jianyuan Wang",
      "Xiao-Xiao Long",
      "Hanxue Liang",
      "Zexiang Xu",
      "Hao Su",
      "Christian Theobalt",
      "Christian Rupprecht",
      "Andrea Vedaldi",
      "Kaichen Zhou",
      "Paul Pu Liang",
      "Shijian Lu",
      "Fangneng Zhan"
    ],
    "abstract": "3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.",
    "arxiv_url": "http://arxiv.org/abs/2507.14501v3",
    "pdf_url": "http://arxiv.org/pdf/2507.14501v3",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "human",
      "ar",
      "fast",
      "nerf",
      "slam",
      "robotics",
      "lighting",
      "survey",
      "gaussian splatting",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware\n  Tiling and Meta-Learning-Based Bitrate Adaptation",
    "authors": [
      "Han Gong",
      "Qiyue Li",
      "Jie Li",
      "Zhi Liu"
    ],
    "abstract": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a research hotspot in both academia and industry, owing to its impressive ability to deliver immersive 3D video experiences. However, research in this area is still in its early stages, and several fundamental challenges, such as tiling, quality assessment, and bitrate adaptation, require further investigation. In this paper, we tackle these challenges by proposing a comprehensive set of solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by saliency analysis, which integrates both spatial and temporal features. Each tile is encoded into versions possessing dedicated deformation fields and multiple quality levels for adaptive selection. We also introduce a novel quality assessment framework for 3DGS video that jointly evaluates spatial-domain degradation in 3DGS representations during streaming and the quality of the resulting 2D rendered images. Additionally, we develop a meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS video streaming, achieving optimal performance across varying network conditions. Extensive experiments demonstrate that our proposed approaches significantly outperform state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2507.14454v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14454v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "deformation",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive 3D Gaussian Splatting Video Streaming",
    "authors": [
      "Han Gong",
      "Qiyue Li",
      "Zhi Liu",
      "Hao Zhou",
      "Peng Yuan Zhou",
      "Zhu Li",
      "Jie Li"
    ],
    "abstract": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.",
    "arxiv_url": "http://arxiv.org/abs/2507.14432v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14432v1",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural-GASh: A CGA-based neural radiance prediction pipeline for\n  real-time shading",
    "authors": [
      "Efstratios Geronikolakis",
      "Manos Kamarianakis",
      "Antonis Protopsaltis",
      "George Papagiannakis"
    ],
    "abstract": "This paper presents Neural-GASh, a novel real-time shading pipeline for 3D meshes, that leverages a neural radiance field architecture to perform image-based rendering (IBR) using Conformal Geometric Algebra (CGA)-encoded vertex information as input. Unlike traditional Precomputed Radiance Transfer (PRT) methods, that require expensive offline precomputations, our learned model directly consumes CGA-based representations of vertex positions and normals, enabling dynamic scene shading without precomputation. Integrated seamlessly into the Unity engine, Neural-GASh facilitates accurate shading of animated and deformed 3D meshes - capabilities essential for dynamic, interactive environments. The shading of the scene is implemented within Unity, where rotation of scene lights in terms of Spherical Harmonics is also performed optimally using CGA. This neural field approach is designed to deliver fast and efficient light transport simulation across diverse platforms, including mobile and VR, while preserving high rendering quality. Additionally, we evaluate our method on scenes generated via 3D Gaussian splats, further demonstrating the flexibility and robustness of Neural-GASh in diverse scenarios. Performance is evaluated in comparison to conventional PRT, demonstrating competitive rendering speeds even with complex geometries.",
    "arxiv_url": "http://arxiv.org/abs/2507.13917v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13917v1",
    "published_date": "2025-07-18",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "light transport",
      "3d gaussian",
      "ar",
      "fast",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
    "authors": [
      "Yu Wei",
      "Jiahui Zhang",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2507.13891v2",
    "pdf_url": "http://arxiv.org/pdf/2507.13891v2",
    "published_date": "2025-07-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "semantic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via\n  Textured Gaussian Splatting",
    "authors": [
      "Kaiyuan Tang",
      "Kuangshi Ai",
      "Jun Han",
      "Chaoli Wang"
    ],
    "abstract": "Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.",
    "arxiv_url": "http://arxiv.org/abs/2507.13586v1",
    "pdf_url": "http://arxiv.org/pdf/2507.13586v1",
    "published_date": "2025-07-18",
    "categories": [
      "cs.GR",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "lighting",
      "gaussian splatting",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via\n  Deformable 3D Gaussians",
    "authors": [
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.",
    "arxiv_url": "http://arxiv.org/abs/2507.12667v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12667v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "tracking",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
    "authors": [
      "Davide Di Nucci",
      "Matteo Tomei",
      "Guido Borghi",
      "Luca Ciuffreda",
      "Roberto Vezzani",
      "Rita Cucchiara"
    ],
    "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.",
    "arxiv_url": "http://arxiv.org/abs/2507.12095v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12095v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D\n  Gaussian Splatting Representation",
    "authors": [
      "Beining Xu",
      "Siting Zhu",
      "Hesheng Wang"
    ],
    "abstract": "We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at https://github.com/IRMVLab/SGLoc.",
    "arxiv_url": "http://arxiv.org/abs/2507.12027v1",
    "pdf_url": "http://arxiv.org/pdf/2507.12027v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV",
      "cs.RO",
      "I.4.8; I.2.9"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark",
    "authors": [
      "Jingqian Wu",
      "Peiqi Duan",
      "Zongqiang Wang",
      "Changwei Wang",
      "Boxin Shi",
      "Edmund Y. Lam"
    ],
    "abstract": "In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.",
    "arxiv_url": "http://arxiv.org/abs/2507.11931v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11931v1",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Sicheng Yu",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction, which effectively balances rendering quality, efficiency, and speed. However, existing 3DGS approaches usually generate plausible outputs and face significant challenges in complex scene reconstruction, manifesting as incomplete holistic structural outlines and unclear local lighting effects. To address these issues simultaneously, we propose a novel decoupled optimization framework, which integrates wavelet decomposition into 3D Gaussian Splatting and 2D sampling. Technically, through 3D wavelet decomposition, our approach divides point clouds into high-frequency and low-frequency components, enabling targeted optimization for each. The low-frequency component captures global structural outlines and manages the distribution of Gaussians through voxelization. In contrast, the high-frequency component restores intricate geometric and textural details while incorporating a relight module to mitigate lighting artifacts and enhance photorealistic rendering. Additionally, a 2D wavelet decomposition is applied to the training images, simulating radiance variations. This provides critical guidance for high-frequency detail reconstruction, ensuring seamless integration of details with the global structure. Extensive experiments on challenging datasets demonstrate our method achieves state-of-the-art performance across various metrics, surpassing existing approaches and advancing the field of 3D scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2507.12498v2",
    "pdf_url": "http://arxiv.org/pdf/2507.12498v2",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface\n  Reconstruction",
    "authors": [
      "Haoxuan Qu",
      "Yujun Cai",
      "Hossein Rahmani",
      "Ajay Kumar",
      "Junsong Yuan",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
    "arxiv_url": "http://arxiv.org/abs/2507.11321v1",
    "pdf_url": "http://arxiv.org/pdf/2507.11321v1",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "high quality"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth\n  Reconstruction via Physics Simulation for Scene Update",
    "authors": [
      "Jeongyun Kim",
      "Seunghoon Jeong",
      "Giseop Kim",
      "Myung-Hwan Jeon",
      "Eunji Jun",
      "Ayoung Kim"
    ],
    "abstract": "Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.",
    "arxiv_url": "http://arxiv.org/abs/2507.11069v3",
    "pdf_url": "http://arxiv.org/pdf/2507.11069v3",
    "published_date": "2025-07-15",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "understanding",
      "ar",
      "sparse view",
      "reflection",
      "gaussian splatting",
      "face",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
    "authors": [
      "Hayeon Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ],
    "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.",
    "arxiv_url": "http://arxiv.org/abs/2507.11061v2",
    "pdf_url": "http://arxiv.org/pdf/2507.11061v2",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "slam",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
    "authors": [
      "Shivangi Aneja",
      "Sebastian Weiss",
      "Irene Baeza",
      "Prashanth Chandran",
      "Gaspard Zoss",
      "Matthias Nießner",
      "Derek Bradley"
    ],
    "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
    "arxiv_url": "http://arxiv.org/abs/2507.10542v1",
    "pdf_url": "http://arxiv.org/pdf/2507.10542v1",
    "published_date": "2025-07-14",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "head",
      "avatar",
      "human",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for\n  Autonomous Driving",
    "authors": [
      "Yixun Zhang",
      "Lizhi Wang",
      "Junjun Zhao",
      "Wending Zhao",
      "Feng Zhou",
      "Yonghao Dang",
      "Jianqin Yin"
    ],
    "abstract": "Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. Existing 2D and 3D physical attacks, due to their focus on texture optimization, often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture optimization, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module that filters outliers to preserve geometric fidelity, and a physical augmentation module that simulates complex physical scenarios to enhance attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21\\% to 7.38\\%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks.",
    "arxiv_url": "http://arxiv.org/abs/2507.09993v3",
    "pdf_url": "http://arxiv.org/pdf/2507.09993v3",
    "published_date": "2025-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning human-to-robot handovers through 3D scene reconstruction",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.",
    "arxiv_url": "http://arxiv.org/abs/2507.08726v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08726v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "human",
      "sparse-view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and\n  View-Consistent 3D Scene Inpainting",
    "authors": [
      "Ji Hyun Seo",
      "Byounhyun Yoo",
      "Gerard Jounghyun Kim"
    ],
    "abstract": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian Splatting, have emerged as seminal 3D representations for synthesizing realistic novel views. For practical applications, there is ongoing research on flexible scene editing techniques, among which object removal is a representative task. However, removing objects exposes occluded regions, often leading to unnatural appearances. Thus, studies have employed image inpainting techniques to replace such regions with plausible content - a task referred to as 3D scene inpainting. However, image inpainting methods produce one of many plausible completions for each view, leading to inconsistencies between viewpoints. A widely adopted approach leverages perceptual cues to blend inpainted views smoothly. However, it is prone to detail loss and can fail when there are perceptual inconsistencies across views. In this paper, we propose a novel 3D scene inpainting method that reliably produces realistic and perceptually consistent results even for complex scenes by leveraging a reference view. Given the inpainted reference view, we estimate the inpainting similarity of the other views to adjust their contribution in constructing an accurate geometry tailored to the reference. This geometry is then used to warp the reference inpainting to other views as pseudo-ground truth, guiding the optimization to match the reference appearance. Comparative evaluation studies have shown that our approach improves both the geometric fidelity and appearance consistency of inpainted scenes.",
    "arxiv_url": "http://arxiv.org/abs/2507.08434v1",
    "pdf_url": "http://arxiv.org/pdf/2507.08434v1",
    "published_date": "2025-07-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Occlusion-Aware Temporally Consistent Amodal Completion for 3D\n  Human-Object Interaction Reconstruction",
    "authors": [
      "Hyungjun Doh",
      "Dong In Lee",
      "Seunggeun Chi",
      "Pin-Hao Huang",
      "Kwonjoon Lee",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "abstract": "We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.",
    "arxiv_url": "http://arxiv.org/abs/2507.08137v3",
    "pdf_url": "http://arxiv.org/pdf/2507.08137v3",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration",
    "authors": [
      "Chong Cheng",
      "Yu Hu",
      "Sicheng Yu",
      "Beizhen Zhao",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.",
    "arxiv_url": "http://arxiv.org/abs/2507.08136v2",
    "pdf_url": "http://arxiv.org/pdf/2507.08136v2",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "sparse view",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance\n  Transfer and Reflection",
    "authors": [
      "Yongyang Zhou",
      "Fang-Lue Zhang",
      "Zichen Wang",
      "Lei Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.",
    "arxiv_url": "http://arxiv.org/abs/2507.07733v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07733v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "3d gaussian",
      "ar",
      "lighting",
      "reflection",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A\n  Benchmark for 3D Segmentation",
    "authors": [
      "Bangning Wei",
      "Joshua Maraval",
      "Meriem Outtas",
      "Kidiyo Kpalma",
      "Nicolas Ramin",
      "Lu Zhang"
    ],
    "abstract": "The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.",
    "arxiv_url": "http://arxiv.org/abs/2507.07519v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07519v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "segmentation",
      "4d",
      "understanding",
      "nerf",
      "ar",
      "gaussian splatting",
      "outdoor",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene\n  Reconstruction",
    "authors": [
      "Wei Yao",
      "Shuzhao Xie",
      "Letian Li",
      "Weixiang Zhang",
      "Zhixin Lai",
      "Shiqi Dai",
      "Ke Zhang",
      "Zhi Wang"
    ],
    "abstract": "Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\\% reduction in model size and an average of 100\\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2507.07465v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07465v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "compact",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for\n  Unconstrained Image Collections",
    "authors": [
      "Yongtang Bao",
      "Chengjie Tang",
      "Yuze Wang",
      "Haojie Li"
    ],
    "abstract": "Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.",
    "arxiv_url": "http://arxiv.org/abs/2507.07395v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07395v1",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian\n  Splatting",
    "authors": [
      "Wijayathunga W. M. R. D. B"
    ],
    "abstract": "We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.",
    "arxiv_url": "http://arxiv.org/abs/2507.07000v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07000v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "animation",
      "face",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
    "authors": [
      "Matéo Ducastel",
      "David Tschumperlé",
      "Yvain Quéau"
    ],
    "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural networks and operate either through prior learning or inverse rendering optimization. Here, we revisit the problem of calibrated photometric stereo by leveraging recent advances in 3D inverse rendering using the Gaussian Splatting formalism. This allows us to parameterize the 3D scene to be reconstructed and optimize it in a more interpretable manner. Our approach incorporates a simplified model for light representation and demonstrates the potential of the Gaussian Splatting rendering engine for the photometric stereo problem.",
    "arxiv_url": "http://arxiv.org/abs/2507.06684v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06684v1",
    "published_date": "2025-07-09",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for\n  3D Gaussian Splatting",
    "authors": [
      "Boyuan Tian",
      "Qizhe Gao",
      "Siran Xianyu",
      "Xiaotong Cui",
      "Minjia Zhang"
    ],
    "abstract": "3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian",
    "arxiv_url": "http://arxiv.org/abs/2507.06671v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06671v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic\n  Visualization of Volumetric Medical Data",
    "authors": [
      "Chengkun Li",
      "Yuqi Tong",
      "Kai Chen",
      "Zhenya Yang",
      "Ruiyang Li",
      "Shi Qiu",
      "Jason Ying-Kuen Chan",
      "Pheng-Ann Heng",
      "Qi Dou"
    ],
    "abstract": "The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2507.06647v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06647v1",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "medical",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
    "authors": [
      "Wanhua Li",
      "Yujie Zhao",
      "Minghan Qin",
      "Yang Liu",
      "Yuanhao Cai",
      "Chuang Gan",
      "Hanspeter Pfister"
    ],
    "abstract": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\\times$ speedup and a 47 $\\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2507.07136v2",
    "pdf_url": "http://arxiv.org/pdf/2507.07136v2",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "fast",
      "high quality",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for\n  Panorama-Style Mobile Captures",
    "authors": [
      "Seungoh Han",
      "Jaehoon Jang",
      "Hyunsu Kim",
      "Jaeheung Surh",
      "Junhyung Kwak",
      "Hyowon Ha",
      "Kyungdon Joo"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement.",
    "arxiv_url": "http://arxiv.org/abs/2507.06109v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06109v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D\n  Gaussian Splatting for Photorealistic Scenes Rendering",
    "authors": [
      "Jiayi Song",
      "Zihan Ye",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ben Fei",
      "Jingyi Xu",
      "Ying He",
      "Wanli Ouyang"
    ],
    "abstract": "Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2507.06103v1",
    "pdf_url": "http://arxiv.org/pdf/2507.06103v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "reflection",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis",
    "authors": [
      "Alexandre Symeonidis-Herzig",
      "Özge Mercanoğlu Sincan",
      "Richard Bowden"
    ],
    "abstract": "Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.",
    "arxiv_url": "http://arxiv.org/abs/2507.06060v2",
    "pdf_url": "http://arxiv.org/pdf/2507.06060v2",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "human",
      "ar",
      "animation",
      "high-fidelity",
      "gaussian splatting",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for\n  Free-Viewpoint Videos",
    "authors": [
      "Wenkang Zhang",
      "Yan Zhao",
      "Qiang Wang",
      "Li Song",
      "Zhengxue Cheng"
    ],
    "abstract": "Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications.",
    "arxiv_url": "http://arxiv.org/abs/2507.05859v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05859v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamArt: Generating Interactable Articulated Objects from a Single\n  Image",
    "authors": [
      "Ruijie Lu",
      "Yu Liu",
      "Jiaxiang Tang",
      "Junfeng Ni",
      "Yuxiang Wang",
      "Diwen Wan",
      "Gang Zeng",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "abstract": "Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.",
    "arxiv_url": "http://arxiv.org/abs/2507.05763v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05763v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "segmentation",
      "ar",
      "nerf",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D\n  Gaussian Splatting",
    "authors": [
      "Haitao Lu",
      "Haijier Chen",
      "Haoze Liu",
      "Shoujian Zhang",
      "Bo Xu",
      "Ziao Liu"
    ],
    "abstract": "In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails.",
    "arxiv_url": "http://arxiv.org/abs/2507.05661v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05661v1",
    "published_date": "2025-07-08",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "outdoor",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry\n  with Neural Signed Distance Fields",
    "authors": [
      "Rushil Desai"
    ],
    "abstract": "Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.",
    "arxiv_url": "http://arxiv.org/abs/2507.06269v3",
    "pdf_url": "http://arxiv.org/pdf/2507.06269v3",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with\n  Diverse 2D Priors",
    "authors": [
      "Lanqing Guo",
      "Yufei Wang",
      "Hezhen Hu",
      "Yan Zheng",
      "Yeying Jin",
      "Siyu Huang",
      "Zhangyang Wang"
    ],
    "abstract": "Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.",
    "arxiv_url": "http://arxiv.org/abs/2507.05426v1",
    "pdf_url": "http://arxiv.org/pdf/2507.05426v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with\n  Segmented Consistency Trajectory Distillation",
    "authors": [
      "Jiahao Zhu",
      "Zixuan Chen",
      "Guangcong Wang",
      "Xiaohua Xie",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).",
    "arxiv_url": "http://arxiv.org/abs/2507.05256v2",
    "pdf_url": "http://arxiv.org/pdf/2507.05256v2",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "high-fidelity",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D\n  Geometry-Consistent Attention Prior",
    "authors": [
      "Minghao Wen",
      "Shengjie Wu",
      "Kangkan Wang",
      "Dong Liang"
    ],
    "abstract": "3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a \"one-shot deal\", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.",
    "arxiv_url": "http://arxiv.org/abs/2507.04961v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04961v1",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "deformation",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated\n  Rendering in Virtual Reality",
    "authors": [
      "Shuo Xin",
      "Haiyu Wang",
      "Sai Qian Zhang"
    ],
    "abstract": "Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\\times$ while maintaining visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2507.04147v1",
    "pdf_url": "http://arxiv.org/pdf/2507.04147v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "neural rendering",
      "3d gaussian",
      "head",
      "human",
      "ar",
      "efficient rendering",
      "tracking",
      "gaussian splatting",
      "efficient",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM",
    "authors": [
      "Xiaolei Lang",
      "Jiajun Lv",
      "Kai Tang",
      "Laijian Li",
      "Jianxin Huang",
      "Lina Liu",
      "Yong Liu",
      "Xingxing Zuo"
    ],
    "abstract": "This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis. Both the dataset and code will be made publicly available on project page https://xingxingzuo.github.io/gaussian_lic2.",
    "arxiv_url": "http://arxiv.org/abs/2507.04004v2",
    "pdf_url": "http://arxiv.org/pdf/2507.04004v2",
    "published_date": "2025-07-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "slam",
      "gaussian splatting",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic\n  Urban Environments",
    "authors": [
      "Guile Wu",
      "Dongfeng Bai",
      "Bingbing Liu"
    ],
    "abstract": "This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2507.03886v1",
    "pdf_url": "http://arxiv.org/pdf/2507.03886v1",
    "published_date": "2025-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "urban scene",
      "high-fidelity",
      "gaussian splatting",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian\n  Pointmaps",
    "authors": [
      "Chong Cheng",
      "Sicheng Yu",
      "Zijian Wang",
      "Yifan Zhou",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, lack geometric priors in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to scale drift. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.",
    "arxiv_url": "http://arxiv.org/abs/2507.03737v2",
    "pdf_url": "http://arxiv.org/pdf/2507.03737v2",
    "published_date": "2025-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "mapping",
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "outdoor",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. Bühler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "arxiv_url": "http://arxiv.org/abs/2507.02803v2",
    "pdf_url": "http://arxiv.org/pdf/2507.02803v2",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "ar",
      "fast",
      "lighting",
      "face",
      "high-fidelity",
      "reflection",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and\n  Manipulation of Articulated Objects",
    "authors": [
      "Qiaojun Yu",
      "Xibin Yuan",
      "Yu jiang",
      "Junting Chen",
      "Dongzhe Zheng",
      "Ce Hao",
      "Yang You",
      "Yixing Chen",
      "Yao Mu",
      "Liu Liu",
      "Cewu Lu"
    ],
    "abstract": "Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: https://sites.google.com/view/artgs/home",
    "arxiv_url": "http://arxiv.org/abs/2507.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02600v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "3d gaussian",
      "understanding",
      "ar",
      "robotics",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local\n  Implicit Feature Decoupling",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Jianbo Jiao",
      "Jiayu Yang",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jie Liang",
      "Jinbo Yan",
      "Runling Liu",
      "Ronggang Wang"
    ],
    "abstract": "Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
    "arxiv_url": "http://arxiv.org/abs/2507.02363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02363v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes",
    "authors": [
      "Stephen Pasch",
      "Joel K. Salzman",
      "Changxi Zheng"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "arxiv_url": "http://arxiv.org/abs/2507.02257v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02257v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping",
      "lighting",
      "reflection",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial\n  Camouflage Generation",
    "authors": [
      "Tianrui Lou",
      "Xiaojun Jia",
      "Siyuan Liang",
      "Jiawei Liang",
      "Ming Zhang",
      "Yanjun Xiao",
      "Xiaochun Cao"
    ],
    "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.",
    "arxiv_url": "http://arxiv.org/abs/2507.01367v2",
    "pdf_url": "http://arxiv.org/pdf/2507.01367v2",
    "published_date": "2025-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "autonomous driving",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online\n  Semantic Gaussian Splatting",
    "authors": [
      "Keiko Nagami",
      "Timothy Chen",
      "Javier Yu",
      "Ola Shorinwa",
      "Maximilian Adang",
      "Carlyn Dougherty",
      "Eric Cristofalo",
      "Mac Schwager"
    ],
    "abstract": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2507.01125v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01125v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
    "authors": [
      "Felix Windisch",
      "Lukas Radl",
      "Thomas Köhler",
      "Michael Steiner",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
    "arxiv_url": "http://arxiv.org/abs/2507.01110v2",
    "pdf_url": "http://arxiv.org/pdf/2507.01110v2",
    "published_date": "2025-07-01",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "vr",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Masks make discriminative models great again!",
    "authors": [
      "Tianshi Cao",
      "Marie-Julie Rakotosaona",
      "Ben Poole",
      "Federico Tombari",
      "Michael Niemeyer"
    ],
    "abstract": "We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.",
    "arxiv_url": "http://arxiv.org/abs/2507.00916v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00916v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using\n  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
    "authors": [
      "Anna-Maria Halacheva",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
    "arxiv_url": "http://arxiv.org/abs/2507.00886v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00886v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "understanding",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail\n  Conserved Anti-Aliasing",
    "authors": [
      "Zhenya Yang",
      "Bingchen Gong",
      "Kai Chen"
    ],
    "abstract": "Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2507.00554v3",
    "pdf_url": "http://arxiv.org/pdf/2507.00554v3",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And\n  Dynamic Density Control",
    "authors": [
      "Xingjun Wang",
      "Lianlei Shan"
    ],
    "abstract": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.",
    "arxiv_url": "http://arxiv.org/abs/2507.00363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00363v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "face",
      "dynamic",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction",
    "authors": [
      "Antoine Guédon",
      "Diego Gomez",
      "Nissim Maruani",
      "Bingchen Gong",
      "George Drettakis",
      "Maks Ovsjanikov"
    ],
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "arxiv_url": "http://arxiv.org/abs/2506.24096v1",
    "pdf_url": "http://arxiv.org/pdf/2506.24096v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "animation",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local\n  Reconstruction and Rendering",
    "authors": [
      "Zinuo You",
      "Stamatios Georgoulis",
      "Anpei Chen",
      "Siyu Tang",
      "Dengxin Dai"
    ],
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.",
    "arxiv_url": "http://arxiv.org/abs/2506.23957v2",
    "pdf_url": "http://arxiv.org/pdf/2506.23957v2",
    "published_date": "2025-06-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via\n  Structural Attention",
    "authors": [
      "Ziao Liu",
      "Zhenjia Li",
      "Yifeng Shi",
      "Xiangang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2506.23611v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23611v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "efficient rendering",
      "gaussian splatting",
      "efficient",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image\n  Representation via 2D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Chao Yang",
      "Tao Guan",
      "Lili Ju"
    ],
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2506.23479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23479v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable\n  Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Kun Yuan",
      "Guankun Wang",
      "Mobarak I. Hoque",
      "Nicolas Padoy",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.23309v2",
    "pdf_url": "http://arxiv.org/pdf/2506.23309v2",
    "published_date": "2025-06-29",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "understanding",
      "ar",
      "lighting",
      "gaussian splatting",
      "deformation",
      "tracking",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination\n  Correction with Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Yanheng Li",
      "Tong Chen",
      "Jie Wang",
      "Jinlin Wu",
      "Zhen Lei",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "arxiv_url": "http://arxiv.org/abs/2506.23308v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23308v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "4d",
      "ar",
      "lighting",
      "illumination",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric\n  Constraints",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Lei Feng",
      "Yangbing Ge",
      "Shuaifeng Zhi",
      "Jiaxiong Liu",
      "Dewen Hu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
    "arxiv_url": "http://arxiv.org/abs/2506.23207v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23207v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "tracking",
      "ar",
      "slam",
      "mapping",
      "lighting",
      "high-fidelity",
      "illumination",
      "gaussian splatting",
      "outdoor",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STD-GS: Exploring Frame-Event Interaction for\n  SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic\n  Scene",
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ],
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2506.23157v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23157v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient\n  3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "An Le",
      "Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.",
    "arxiv_url": "http://arxiv.org/abs/2506.23042v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23042v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "efficient",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Elaheh Badali Golezani",
      "Shohreh Kasaei"
    ],
    "abstract": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting",
    "arxiv_url": "http://arxiv.org/abs/2506.22973v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22973v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via\n  Diffusion Priors",
    "authors": [
      "Sicong Du",
      "Jiarun Liu",
      "Qifeng Chen",
      "Hao-Xiang Chen",
      "Tai-Jiang Mu",
      "Sheng Yang"
    ],
    "abstract": "A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22800v3",
    "pdf_url": "http://arxiv.org/pdf/2506.22800v3",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
    "authors": [
      "Minchao Jiang",
      "Shunyu Jia",
      "Jiaming Gu",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Anqi Dong",
      "Liang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/",
    "arxiv_url": "http://arxiv.org/abs/2506.22799v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22799v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting",
      "localization",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "authors": [
      "Tao Tang",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "xia zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "abstract": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.22756v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22756v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian\n  Splatting and a Low-Rank Free-Form Deformation Model",
    "authors": [
      "Yuliang Huang",
      "Imraj Singh",
      "Thomas Joyce",
      "Kris Thielemans",
      "Jamie R. McClelland"
    ],
    "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22280v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22280v1",
    "published_date": "2025-06-27",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve\n  Gaussian Splatting",
    "authors": [
      "Zipei Ma",
      "Junzhe Jiang",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.22099v3",
    "pdf_url": "http://arxiv.org/pdf/2506.22099v3",
    "published_date": "2025-06-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "motion",
      "ar",
      "gaussian splatting",
      "dynamic",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MADrive: Memory-Augmented Driving Scene Modeling",
    "authors": [
      "Polina Karpikova",
      "Daniil Selikhanovych",
      "Kirill Struminsky",
      "Ruslan Musaev",
      "Maria Golitsyna",
      "Dmitry Baranchuk"
    ],
    "abstract": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/",
    "arxiv_url": "http://arxiv.org/abs/2506.21520v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21520v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "relighting",
      "3d gaussian",
      "ar",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian\n  Splatting",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Zhuoxiao Li",
      "Haocheng Zhao",
      "Kang Dang",
      "Jionglong Su",
      "Limin Yu",
      "Haoang Li"
    ],
    "abstract": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.21420v2",
    "pdf_url": "http://arxiv.org/pdf/2506.21420v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "slam",
      "mapping",
      "gaussian splatting",
      "efficient",
      "face",
      "dynamic",
      "localization",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image",
    "authors": [
      "Pufan Li",
      "Bi'an Du",
      "Wei Hu"
    ],
    "abstract": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To tackle these issues, we present a novel method that seamlessly integrates geometry and perception information without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we incorporate geometry and perception priors to initialize the Gaussian branches and guide their parameter optimization. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we introduce a stable Score Distillation Sampling for fine-grained prior distillation to ensure effective knowledge transfer. The model is further enhanced by a reprojection-based strategy that enforces depth consistency. Experimental results show that we outperform existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.21152v2",
    "pdf_url": "http://arxiv.org/pdf/2506.21152v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "68",
      "I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local\n  Optimization",
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ],
    "abstract": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.21117v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21117v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "segmentation",
      "ar",
      "robotics",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "User-in-the-Loop View Sampling with Error Peaking Visualization",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Shohei Mori"
    ],
    "abstract": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2506.21009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21009v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via\n  Sparse-Controlled Gaussian Splatting",
    "authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Byung-Ju Kim",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.",
    "arxiv_url": "http://arxiv.org/abs/2506.20998v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20998v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGH: 3D Head Generation with Composable Hair and Face",
    "authors": [
      "Chengan He",
      "Junxuan Li",
      "Tobias Kirschstein",
      "Artem Sevastopolsky",
      "Shunsuke Saito",
      "Qingyang Tan",
      "Javier Romero",
      "Chen Cao",
      "Holly Rushmeier",
      "Giljoo Nam"
    ],
    "abstract": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.",
    "arxiv_url": "http://arxiv.org/abs/2506.20875v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20875v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "human",
      "ar",
      "gaussian splatting",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and\n  Rasterizer",
    "authors": [
      "Da Li",
      "Donggang Jia",
      "Yousef Rajeh",
      "Dominik Engel",
      "Ivan Viola"
    ],
    "abstract": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.",
    "arxiv_url": "http://arxiv.org/abs/2506.20202v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20202v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAR-GS: Gaussian Splatting based SAR Images Rendering and Target\n  Reconstruction",
    "authors": [
      "Aobo Li",
      "Zhengxin Lei",
      "Jiangtao Wei",
      "Feng Xu"
    ],
    "abstract": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.",
    "arxiv_url": "http://arxiv.org/abs/2506.21633v2",
    "pdf_url": "http://arxiv.org/pdf/2506.21633v2",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded\n  Scenes",
    "authors": [
      "Chenhao Zhang",
      "Yezhi Shen",
      "Fengqing Zhu"
    ],
    "abstract": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.21629v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21629v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "neural rendering",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical\n  Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "arxiv_url": "http://arxiv.org/abs/2506.19842v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19842v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "understanding",
      "ar",
      "body",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtual Memory for 3D Gaussian Splatting",
    "authors": [
      "Jonathan Haberl",
      "Philipp Fleck",
      "Clemens Arth"
    ],
    "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
    "arxiv_url": "http://arxiv.org/abs/2506.19415v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19415v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "authors": [
      "Xiaoyuan Wang",
      "Yizhou Zhao",
      "Botao Ye",
      "Xiaojun Shan",
      "Weijie Lyu",
      "Lu Qi",
      "Kelvin C. K. Chan",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2506.19291v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19291v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "4d",
      "ar",
      "nerf",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale\n  Multi-Agent Gaussian SLAM",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "abstract": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.",
    "arxiv_url": "http://arxiv.org/abs/2506.18885v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18885v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "gaussian splatting",
      "outdoor",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "authors": [
      "Michal Nazarczuk",
      "Sibi Catley-Chandar",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Gregory Slabaugh",
      "Eduardo Pérez-Pellitero"
    ],
    "abstract": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io",
    "arxiv_url": "http://arxiv.org/abs/2506.18792v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18792v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
    "authors": [
      "Dylan Ebert"
    ],
    "abstract": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons.   Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource.   Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.18787v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18787v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "understanding",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
    "authors": [
      "Adam Yang",
      "Nadula Kadawedduwa",
      "Tianfu Wang",
      "Sunny Sharma",
      "Emily F. Wisinski",
      "Jhayron S. Pérez-Carrasquilla",
      "Kyle J. C. Hall",
      "Dean Calhoun",
      "Jonathan Starfeldt",
      "Timothy P. Canty",
      "Maria Molina",
      "Christopher Metzler"
    ],
    "abstract": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.18677v2",
    "pdf_url": "http://arxiv.org/pdf/2506.18677v2",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "understanding",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in\n  Large-Scale Scene",
    "authors": [
      "Shihan Chen",
      "Zhaojin Li",
      "Zeyu Chen",
      "Qingsong Yan",
      "Gaoyang Shen",
      "Ran Duan"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2506.17636v1",
    "pdf_url": "http://arxiv.org/pdf/2506.17636v1",
    "published_date": "2025-06-21",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "nerf",
      "efficient",
      "survey",
      "high-fidelity",
      "gaussian splatting",
      "outdoor",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement\n  for 3D Low-Level Vision",
    "authors": [
      "Weeyoung Kwon",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2506.16262v2",
    "pdf_url": "http://arxiv.org/pdf/2506.16262v2",
    "published_date": "2025-06-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "autonomous driving",
      "neural rendering",
      "3d gaussian",
      "ar",
      "nerf",
      "robotics",
      "survey",
      "high-fidelity",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Information-computation trade-offs in non-linear transforms",
    "authors": [
      "Connor Ding",
      "Abhiram Rao Gorle",
      "Jiwon Jeong",
      "Naomi Sagan",
      "Tsachy Weissman"
    ],
    "abstract": "In this work, we explore the interplay between information and computation in non-linear transform-based compression for broad classes of modern information-processing tasks. We first investigate two emerging nonlinear data transformation frameworks for image compression: Implicit Neural Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their representational properties, behavior under lossy compression, and convergence dynamics. Our results highlight key trade-offs between INR's compact, resolution-flexible neural field representations and GS's highly parallelizable, spatially interpretable fitting, providing insights for future hybrid and compression-aware frameworks. Next, we introduce the textual transform that enables efficient compression at ultra-low bitrate regimes and simultaneously enhances human perceptual satisfaction. When combined with the concept of denoising via lossy compression, the textual transform becomes a powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78) \"transform\", a universal method that, when applied to any member of a broad compressor family, produces new compressors that retain the asymptotic universality guarantees of the LZ78 algorithm. Collectively, these three transforms illuminate the fundamental trade-offs between coding efficiency and computational cost. We discuss how these insights extend beyond compression to tasks such as classification, denoising, and generative AI, suggesting new pathways for using non-linear transformations to balance resource constraints and performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.15948v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15948v1",
    "published_date": "2025-06-19",
    "categories": [
      "cs.IT",
      "eess.IV",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "human",
      "ar",
      "compression",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos",
    "authors": [
      "Kaifeng Zhang",
      "Baoyu Li",
      "Kris Hauser",
      "Yunzhu Li"
    ],
    "abstract": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .",
    "arxiv_url": "http://arxiv.org/abs/2506.15680v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15680v1",
    "published_date": "2025-06-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate\n  Camera Pose Estimation under Complex Trajectories",
    "authors": [
      "Qingsong Yan",
      "Qiang Wang",
      "Kaiyong Zhao",
      "Jie Chen",
      "Bo Li",
      "Xiaowen Chu",
      "Fei Deng"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks\\&Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2506.15242v2",
    "pdf_url": "http://arxiv.org/pdf/2506.15242v2",
    "published_date": "2025-06-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "slam",
      "nerf",
      "gaussian splatting",
      "localization",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads\n  Synthesis Using Gaussian Splatting",
    "authors": [
      "Ziqiao Peng",
      "Wentao Hu",
      "Junyuan Ma",
      "Xiangyu Zhu",
      "Xiaomei Zhang",
      "Hao Zhao",
      "Hui Tian",
      "Jun He",
      "Hongyan Liu",
      "Zhaoxin Fan"
    ],
    "abstract": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.",
    "arxiv_url": "http://arxiv.org/abs/2506.14742v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14742v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "face",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D\n  Gaussian-Splatting",
    "authors": [
      "Yuke Xing",
      "Jiarui Wang",
      "Peizhi Niu",
      "Wenjie Huang",
      "Guangtao Zhai",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.",
    "arxiv_url": "http://arxiv.org/abs/2506.14642v2",
    "pdf_url": "http://arxiv.org/pdf/2506.14642v2",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "compression",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty\n  Maps for 3D Reconstruction",
    "authors": [
      "Zhengquan Zhang",
      "Feng Xu",
      "Mengmi Zhang"
    ],
    "abstract": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.",
    "arxiv_url": "http://arxiv.org/abs/2506.14856v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14856v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "neural rendering",
      "3d gaussian",
      "head",
      "ar",
      "nerf",
      "mapping",
      "gaussian splatting",
      "efficient",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient\n  High-Resolution 3D Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Juan Zhang",
      "Tongfei Chen",
      "Shuo Yang",
      "Shuwei Shao",
      "Wenhao Dong",
      "Baochang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.14229v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14229v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "face",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAF: Gaussian Action Field as a 4D Representation for Dynamic World\n  Modeling in Robotic Manipulation",
    "authors": [
      "Ying Chai",
      "Litao Deng",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Kangchen Lv",
      "Liangjun Xing",
      "Xiang Li",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Accurate scene perception is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we adopt a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing 4D modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF provides three interrelated outputs: reconstruction of the current scene, prediction of future frames, and estimation of init action via Gaussian motion. Furthermore, we employ an action-vision-aligned denoising framework, conditioned on a unified representation that combines the init action and the Gaussian perception, both generated by the GAF, to further obtain more precise actions. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR, +0.3864 SSIM and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average +7.3% success rate in robotic manipulation tasks over state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.14135v4",
    "pdf_url": "http://arxiv.org/pdf/2506.14135v4",
    "published_date": "2025-06-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with\n  Gaussian Radiance Fields and Differentiable Dynamics",
    "authors": [
      "Qianzhong Chen",
      "Naixiang Gao",
      "Suning Huang",
      "JunEn Low",
      "Timothy Chen",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.",
    "arxiv_url": "http://arxiv.org/abs/2506.14009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14009v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "head",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated\n  Human Images",
    "authors": [
      "Lingteng Qiu",
      "Peihao Li",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Weihao Yuan",
      "Siyu Zhu",
      "Xiaoguang Han",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.",
    "arxiv_url": "http://arxiv.org/abs/2506.13766v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13766v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "avatar",
      "ar",
      "high-fidelity",
      "human",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for\n  Unconstrained Scene Reconstruction",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.13516v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13516v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "illumination",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate\n  Radiance Fields",
    "authors": [
      "Jungeon Kim",
      "Geonsoo Park",
      "Seungyong Lee"
    ],
    "abstract": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.13508v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13508v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "outdoor",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian\n  Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.",
    "arxiv_url": "http://arxiv.org/abs/2506.13348v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13348v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object\n  Reconstruction",
    "authors": [
      "Jinguang Tong",
      "Xuesong li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ],
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS",
    "arxiv_url": "http://arxiv.org/abs/2506.13110v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13110v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "3d gaussian",
      "ar",
      "fast",
      "lighting",
      "gaussian splatting",
      "face",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "authors": [
      "Hyunjin Kim",
      "Haebeom Jung",
      "Jaesik Park"
    ],
    "abstract": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.",
    "arxiv_url": "http://arxiv.org/abs/2506.12945v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12945v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ],
    "abstract": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "arxiv_url": "http://arxiv.org/abs/2506.12787v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12787v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "fast",
      "nerf",
      "gaussian splatting",
      "lightweight",
      "deformation",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient multi-view training for 3D Gaussian Splatting",
    "authors": [
      "Minhyuk Choi",
      "Injae Kim",
      "Hyunwoo J. Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize \"single-view\" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's \"multi-view\" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.",
    "arxiv_url": "http://arxiv.org/abs/2506.12727v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12727v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "ar",
      "nerf",
      "lighting",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors",
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ],
    "abstract": "We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.12716v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12716v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian\n  Splatting",
    "authors": [
      "Hongbi Zhou",
      "Zhangkai Ni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS",
    "arxiv_url": "http://arxiv.org/abs/2506.12400v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12400v2",
    "published_date": "2025-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure",
    "authors": [
      "Stanley Lewis",
      "Vishal Chandra",
      "Tom Gao",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Representing articulated objects remains a difficult problem within the field of robotics. Objects such as pliers, clamps, or cabinets require representations that capture not only geometry and color information, but also part seperation, connectivity, and joint parametrization. Furthermore, learning these representations becomes even more difficult with each additional degree of freedom. Complex articulated objects such as robot arms may have seven or more degrees of freedom, and the depth of their kinematic tree may be notably greater than the tools, drawers, and cabinets that are the typical subjects of articulated object research. To address these concerns, we introduce SPLATART - a pipeline for learning Gaussian splat representations of articulated objects from posed images, of which a subset contains image space part segmentations. SPLATART disentangles the part separation task from the articulation estimation task, allowing for post-facto determination of joint estimation and representation of articulated objects with deeper kinematic trees than previously exhibited. In this work, we present data on the SPLATART pipeline as applied to the syntheic Paris dataset objects, and qualitative results on a real-world object under spare segmentation supervision. We additionally present on articulated serial chain manipulators to demonstrate usage on deeper kinematic tree structures.",
    "arxiv_url": "http://arxiv.org/abs/2506.12184v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12184v1",
    "published_date": "2025-06-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "robotics",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static\n  Decoupling for 3D Gaussian Splatting-based Occupancy Prediction",
    "authors": [
      "Ke Song",
      "Yunhe Wu",
      "Chunchit Siu",
      "Huiyuan Xiong"
    ],
    "abstract": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.",
    "arxiv_url": "http://arxiv.org/abs/2506.14825v2",
    "pdf_url": "http://arxiv.org/pdf/2506.14825v2",
    "published_date": "2025-06-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "3d gaussian",
      "compact",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Anti-Aliased 2D Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an antialiased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.",
    "arxiv_url": "http://arxiv.org/abs/2506.11252v1",
    "pdf_url": "http://arxiv.org/pdf/2506.11252v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "mapping",
      "gaussian splatting",
      "efficient",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian\n  Splatting",
    "authors": [
      "Lintao Xiang",
      "Hongpei Zheng",
      "Yating Huang",
      "Qijun Yang",
      "Hujun Yin"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.10335v1",
    "pdf_url": "http://arxiv.org/pdf/2506.10335v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "lightweight",
      "sparse view",
      "few-shot"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular\n  Videos",
    "authors": [
      "Chieh Hubert Lin",
      "Zhaoyang Lv",
      "Songyin Wu",
      "Zhen Xu",
      "Thu Nguyen-Phuoc",
      "Hung-Yu Tseng",
      "Julian Straub",
      "Numair Khan",
      "Lei Xiao",
      "Ming-Hsuan Yang",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong",
      "Zhengqin Li"
    ],
    "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09997v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09997v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "tracking",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.09952v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09952v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion\n  Decomposition for Scene Reconstruction",
    "authors": [
      "Junli Deng",
      "Ping Shi",
      "Qipei Li",
      "Jinyang Guo"
    ],
    "abstract": "Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09836v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09836v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global\n  Gaussian Reduction for 3DGS",
    "authors": [
      "Tao Wang",
      "Mengyu Li",
      "Geduo Zeng",
      "Cheng Meng",
      "Qiong Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering. The code is publicly available at https://github.com/DrunkenPoet/GHAP",
    "arxiv_url": "http://arxiv.org/abs/2506.09534v2",
    "pdf_url": "http://arxiv.org/pdf/2506.09534v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "neural rendering",
      "3d gaussian",
      "compact",
      "ar",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene",
    "authors": [
      "Jianing Chen",
      "Zehao Li",
      "Yujun Cai",
      "Hao Jiang",
      "Chengxuan Qian",
      "Juyuan Kang",
      "Shuqin Gao",
      "Honglong Zhao",
      "Tianlu Mao",
      "Yucheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2506.09518v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09518v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene\n  Representation",
    "authors": [
      "Zetian Song",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Xiaohan Lu",
      "Chuanmin Jia",
      "Siwei Ma",
      "Wen Gao"
    ],
    "abstract": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.",
    "arxiv_url": "http://arxiv.org/abs/2506.09479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09479v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "authors": [
      "Yunxiao Shi",
      "Yinhao Zhu",
      "Shizhong Han",
      "Jisoo Jeong",
      "Amin Ansari",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "abstract": "Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.",
    "arxiv_url": "http://arxiv.org/abs/2506.09417v2",
    "pdf_url": "http://arxiv.org/pdf/2506.09417v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic",
      "geometry",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via\n  Feed-Forward Gaussian Splatting from Only Sparse-View Images",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Jingyu Gong",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09378v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09378v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "sparse-view",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "authors": [
      "Zike Wu",
      "Qi Yan",
      "Xuanyu Yi",
      "Lele Wang",
      "Renjie Liao"
    ],
    "abstract": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.",
    "arxiv_url": "http://arxiv.org/abs/2506.08862v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08862v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "deformation",
      "dynamic",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised\n  Learning with 3D Gaussian Splatting",
    "authors": [
      "Keyi Liu",
      "Weidong Yang",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.08777v2",
    "pdf_url": "http://arxiv.org/pdf/2506.08777v2",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "understanding",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language\n  Gaussian Splatting",
    "authors": [
      "Mengjiao Ma",
      "Qi Ma",
      "Yue Li",
      "Jiahuan Cheng",
      "Runyi Yang",
      "Bin Ren",
      "Nikola Popovic",
      "Mingqiang Wei",
      "Nicu Sebe",
      "Luc Van Gool",
      "Theo Gevers",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2506.08710v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08710v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "fast",
      "ar",
      "efficient",
      "gaussian splatting",
      "outdoor",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary\n  Large-Scale Scene Rendering",
    "authors": [
      "Xiaohan Zhang",
      "Sitong Wang",
      "Yushen Yan",
      "Yi Yang",
      "Mingda Xu",
      "Qi Liu"
    ],
    "abstract": "High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.08704v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08704v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex-Valued Holographic Radiance Fields",
    "authors": [
      "Yicheng Zhan",
      "Dong-Ha Shin",
      "Seung-Hwan Baek",
      "Kaan Akşit"
    ],
    "abstract": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.",
    "arxiv_url": "http://arxiv.org/abs/2506.08350v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08350v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.ET"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression\n  of Dynamic Scenes",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "abstract": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve high-quality novel view synthesis by using neural networks to predict the time-varying deformation of each Gaussian. However, performing per-Gaussian neural inference at every frame poses a significant bottleneck, limiting rendering speed and increasing memory and compute requirements. In this paper, we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS representations by reducing neural inference through two complementary techniques. First, we propose a temporal sensitivity pruning score that identifies and removes Gaussians with low contribution to the dynamic scene reconstruction. We also introduce an annealing smooth pruning mechanism that improves pruning robustness in real-world scenes with imprecise camera poses. Second, we propose GroupFlow, a motion analysis technique that clusters Gaussians by trajectory similarity and predicts a single rigid transformation per group instead of separate deformations for each Gaussian. Together, our techniques accelerate rendering by $10.37\\times$, reduce model size by $7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset. SpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be integrated into any deformable 3DGS or 4DGS framework.",
    "arxiv_url": "http://arxiv.org/abs/2506.07917v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07917v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "3d gaussian",
      "4d",
      "fast",
      "nerf",
      "ar",
      "compression",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for\n  High-Fidelity Super-Resolution",
    "authors": [
      "Shuja Khalid",
      "Mohamed Ibrahim",
      "Yang Liu"
    ],
    "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
    "arxiv_url": "http://arxiv.org/abs/2506.07897v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07897v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "lightweight",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving\n  Simulation",
    "authors": [
      "William Ljungbergh",
      "Bernardo Taveira",
      "Wenzhao Zheng",
      "Adam Tonderski",
      "Chensheng Peng",
      "Fredrik Kahl",
      "Christoffer Petersson",
      "Michael Felsberg",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.",
    "arxiv_url": "http://arxiv.org/abs/2506.07826v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07826v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "neural rendering",
      "3d gaussian",
      "ar",
      "lighting",
      "shadow",
      "illumination",
      "gaussian splatting",
      "lightweight",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian\n  Splatting",
    "authors": [
      "Jens Piekenbrinck",
      "Christian Schmidt",
      "Alexander Hermans",
      "Narunas Vaskevicius",
      "Timm Linder",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2506.07697v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07697v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline\n  Sparse Views",
    "authors": [
      "Xiaohan Lu",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Zetian Song",
      "Chuanmin Jia",
      "Siwei Ma"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.07670v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07670v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "high-fidelity",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians",
    "authors": [
      "Zeyu Xiao",
      "Zhenyi Wu",
      "Mingyang Sun",
      "Qipeng Yan",
      "Yufan Guo",
      "Zhuoer Liang",
      "Lihua Zhang"
    ],
    "abstract": "3D Gaussian Splatting has achieved remarkable success in reconstructing both static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian primitives, interactions between objects suffer from inaccurate 3D segmentation, imprecise deformation among different materials, and severe rendering artifacts. To address these challenges, we introduce PIG: Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach that combines 3D object segmentation with the simulation of interacting objects in high precision. Firstly, our method facilitates fast and accurate mapping from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation. Secondly, we assign unique physical properties to correspondingly segmented objects within the scene for multi-material coupled interactions. Finally, we have successfully embedded constraint scales into deformation gradients, specifically clamping the scaling and rotation properties of the Gaussian primitives to eliminate artifacts and achieve geometric fidelity and visual consistency. Experimental results demonstrate that our method not only outperforms the state-of-the-art (SOTA) in terms of visual quality, but also opens up new directions and pipelines for the field of physically realistic scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.07657v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07657v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "fast",
      "mapping",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory\n  Optimization and Architectural Support",
    "authors": [
      "Chenqi Zhang",
      "Yu Feng",
      "Jieru Zhao",
      "Guangda Liu",
      "Wenchao Ding",
      "Chentao Wu",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
    "arxiv_url": "http://arxiv.org/abs/2506.09070v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09070v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal\n  Navigation",
    "authors": [
      "Yijie Deng",
      "Shuaihang Yuan",
      "Geeta Chandra Raju Bethala",
      "Anthony Tzes",
      "Yu-Shen Liu",
      "Yi Fang"
    ],
    "abstract": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.",
    "arxiv_url": "http://arxiv.org/abs/2506.07338v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07338v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "head",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented\n  Rasterization",
    "authors": [
      "Zhican Wang",
      "Guanghui He",
      "Dantong Liu",
      "Lingjun Gao",
      "Shell Xu Hu",
      "Chen Zhang",
      "Zhuoran Song",
      "Nicholas Lane",
      "Wayne Luk",
      "Hongxiang Fan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy savings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.",
    "arxiv_url": "http://arxiv.org/abs/2506.07069v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07069v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "autonomous driving",
      "head",
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting",
      "efficient",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene\n  Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiao Tang",
      "Jiajun Tang",
      "Jiaqi Lin",
      "Yuxin Cheng",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
    "arxiv_url": "http://arxiv.org/abs/2506.06988v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06988v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "3d reconstruction",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Mapping for Evolving Scenes",
    "authors": [
      "Vladimir Yugay",
      "Thies Kersten",
      "Luca Carlone",
      "Theo Gevers",
      "Martin R. Oswald",
      "Lukas Schmid"
    ],
    "abstract": "Mapping systems with novel view synthesis (NVS) capabilities are widely used in computer vision, with augmented reality, robotics, and autonomous driving applications. Most notably, 3D Gaussian Splatting-based systems show high NVS performance; however, many current approaches are limited to static scenes. While recent works have started addressing short-term dynamics (motion within the view of the camera), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene adaptation mechanism that continuously updates the 3D representation to reflect the latest changes. In addition, since maintaining geometric and semantic consistency remains challenging due to stale observations disrupting the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We evaluate Gaussian Mapping for Evolving Scenes (GaME) on both synthetic and real-world datasets and find it to be more accurate than the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2506.06909v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06909v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "autonomous driving",
      "motion",
      "3d gaussian",
      "ar",
      "robotics",
      "mapping",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation",
    "authors": [
      "Sumit Sharma",
      "Gopi Raju Matta",
      "Kaushik Mitra"
    ],
    "abstract": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline.",
    "arxiv_url": "http://arxiv.org/abs/2506.06890v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06890v1",
    "published_date": "2025-06-07",
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "nerf",
      "illumination",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles",
    "authors": [
      "Yangkai Lin",
      "Jiabao Lei",
      "Kui jia"
    ],
    "abstract": "In recent years, there has been a growing demand to stylize a given 3D scene to align with the artistic style of reference images for creative purposes. While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method for realistic 3D scene modeling, there remains a challenge in adapting it to stylize 3D GS to match with multiple styles through automatic local style transfer or manual designation, while maintaining memory efficiency for stylization training. In this paper, we introduce a novel 3D GS stylization solution termed Multi-StyleGS to tackle these challenges. In particular, we employ a bipartite matching mechanism to au tomatically identify correspondences between the style images and the local regions of the rendered images. To facilitate local style transfer, we introduce a novel semantic style loss function that employs a segmentation network to apply distinct styles to various objects of the scene and propose a local-global feature matching to enhance the multi-view consistency. Furthermore, this technique can achieve memory efficient training, more texture details and better color match. To better assign a robust semantic label to each Gaussian, we propose several techniques to regularize the segmentation network. As demonstrated by our comprehensive experiments, our approach outperforms existing ones in producing plausible stylization results and offering flexible editing.",
    "arxiv_url": "http://arxiv.org/abs/2506.06846v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06846v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "segmentation",
      "ar",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.06822v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06822v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "understanding",
      "ar",
      "gaussian splatting",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and\n  Realistic Human Avatar Modeling",
    "authors": [
      "Cheng Peng",
      "Jingxiang Sun",
      "Yushuo Chen",
      "Zhaoqi Su",
      "Zhuo Su",
      "Yebin Liu"
    ],
    "abstract": "Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.",
    "arxiv_url": "http://arxiv.org/abs/2506.06645v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06645v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "avatar",
      "compact",
      "human",
      "ar",
      "fast",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian\n  Mixtures and Part Discovery",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "Marcus A. Brubaker",
      "David J. Fleet"
    ],
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09063v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09063v1",
    "published_date": "2025-06-06",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "4d",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4: Generalizable Sparse Splatting Semantic SLAM",
    "authors": [
      "Mingqi Jiang",
      "Chanho Kim",
      "Chen Ziwen",
      "Li Fuxin"
    ],
    "abstract": "Traditional SLAM algorithms are excellent at camera tracking but might generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting (GS) approaches have emerged as an option for SLAM with accurate, dense 3D map building. However, existing GS-based SLAM methods rely on per-scene optimization which is time-consuming and does not generalize to diverse scenes well. In this work, we introduce the first generalizable GS-based semantic SLAM algorithm that incrementally builds and updates a 3D scene representation from an RGB-D video stream using a learned generalizable network. Our approach starts from an RGB-D image recognition backbone to predict the Gaussian parameters from every downsampled and backprojected image location. Additionally, we seamlessly integrate 3D semantic segmentation into our GS framework, bridging 3D mapping and recognition through a shared backbone. To correct localization drifting and floaters, we propose to optimize the GS for only 1 iteration following global localization. We demonstrate state-of-the-art semantic SLAM performance on the real-world benchmark ScanNet with an order of magnitude fewer Gaussians compared to other recent GS-based methods, and showcase our model's generalization capability through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.06517v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06517v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "segmentation",
      "ar",
      "slam",
      "mapping",
      "gaussian splatting",
      "tracking",
      "localization",
      "recognition"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements",
    "authors": [
      "Nicolás Violante",
      "Andreas Meuleman",
      "Alban Gauthier",
      "Frédo Durand",
      "Thibault Groueix",
      "George Drettakis"
    ],
    "abstract": "We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.06462v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06462v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic\n  Environments",
    "authors": [
      "Mingrui Li",
      "Yiming Zhou",
      "Hongxing Zhou",
      "Xinggang Hu",
      "Florian Roemer",
      "Hongyu Wang",
      "Ahmad Osman"
    ],
    "abstract": "Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.05965v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05965v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "slam",
      "mapping",
      "tracking",
      "gaussian splatting",
      "dynamic",
      "geometry",
      "localization"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for\n  Surgical Scene Reconstruction",
    "authors": [
      "Yuchao Zheng",
      "Jianing Zhang",
      "Guochen Ning",
      "Hongen Liao"
    ],
    "abstract": "Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page https://surgsplat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05935v2",
    "pdf_url": "http://arxiv.org/pdf/2506.05935v2",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "lighting",
      "high-fidelity",
      "gaussian splatting",
      "efficient",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy",
    "authors": [
      "Yu Feng",
      "Weikai Lin",
      "Yuge Cheng",
      "Zihan Liu",
      "Jingwen Leng",
      "Minyi Guo",
      "Chen Chen",
      "Shixuan Sun",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural rendering, but it remains computationally demanding on today's mobile SoCs. To address this challenge, we propose Lumina, a hardware-algorithm co-designed system, which integrates two principal optimizations: a novel algorithm, S^2, and a radiance caching mechanism, RC, to improve the efficiency of neural rendering. S2 algorithm exploits temporal coherence in rendering to reduce the computational overhead, while RC leverages the color integration process of 3DGS to decrease the frequency of intensive rasterization computations. Coupled with these techniques, we propose an accelerator architecture, LuminCore, to further accelerate cache lookup and address the fundamental inefficiencies in Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB peak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.05682v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05682v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "head",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for\n  Occupancy and Flow Prediction",
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ],
    "abstract": "Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancing performance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at https://zzy816.github.io/VoxelSplat-Demo/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05563v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05563v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from\n  Unposed Images",
    "authors": [
      "Andreas Meuleman",
      "Ishaan Shah",
      "Alexandre Lanvin",
      "Bernhard Kerbl",
      "George Drettakis"
    ],
    "abstract": "Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that our solution can provide on-the-fly processing of all the capture scenarios and scene sizes we target while remaining competitive with other methods that only handle specific capture styles or scene sizes in speed, image quality, or both.",
    "arxiv_url": "http://arxiv.org/abs/2506.05558v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05558v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "large scene",
      "motion",
      "3d gaussian",
      "ar",
      "fast",
      "slam",
      "gaussian splatting",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian\n  Splatting",
    "authors": [
      "Daniel Wang",
      "Patrick Rim",
      "Tian Tian",
      "Dong Lao",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "We introduce ODE-GS, a novel approach that integrates 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to enable future extrapolation of dynamic 3D scenes. Unlike existing dynamic scene reconstruction methods, which rely on time-conditioned deformation networks and are limited to interpolation within a fixed time window, ODE-GS eliminates timestamp dependency by modeling Gaussian parameter trajectories as continuous-time latent dynamics. Our approach first learns an interpolation model to generate accurate Gaussian trajectories within the observed window, then trains a Transformer encoder to aggregate past trajectories into a latent state evolved via a neural ODE. Finally, numerical integration produces smooth, physically plausible future Gaussian trajectories, enabling rendering at arbitrary future timestamps. On the D-NeRF, NVFi, and HyperNeRF benchmarks, ODE-GS achieves state-of-the-art extrapolation performance, improving metrics by 19.8% compared to leading baselines, demonstrating its ability to accurately represent and predict 3D scene dynamics.",
    "arxiv_url": "http://arxiv.org/abs/2506.05480v3",
    "pdf_url": "http://arxiv.org/pdf/2506.05480v3",
    "published_date": "2025-06-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting",
      "deformation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Duochao Shi",
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Jia-Wang Bian",
      "Bohan Zhuang",
      "Chunhua Shen"
    ],
    "abstract": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss",
    "arxiv_url": "http://arxiv.org/abs/2506.05327v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05327v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "efficient",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian\n  Splatting",
    "authors": [
      "Nan Wang",
      "Yuantao Chen",
      "Lixing Xiao",
      "Weiqing Xiao",
      "Bohan Li",
      "Zhaoxi Chen",
      "Chongjie Ye",
      "Shaocong Xu",
      "Saining Zhang",
      "Ziyang Yan",
      "Pierre Merriaux",
      "Lei Lei",
      "Tianfan Xue",
      "Hao Zhao"
    ],
    "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
    "arxiv_url": "http://arxiv.org/abs/2506.05280v3",
    "pdf_url": "http://arxiv.org/pdf/2506.05280v3",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "neural rendering",
      "ar",
      "nerf",
      "mapping",
      "gaussian splatting",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D\n  Gaussian Splatting for Vision Training",
    "authors": [
      "Aneesh Deogan",
      "Wout Beks",
      "Peter Teurlings",
      "Koen de Vos",
      "Mark van den Brand",
      "Rene van de Molengraft"
    ],
    "abstract": "Annotated datasets are critical for training neural networks for object detection, yet their manual creation is time- and labour-intensive, subjective to human error, and often limited in diversity. This challenge is particularly pronounced in the domain of robotics, where diverse and dynamic scenarios further complicate the creation of representative datasets. To address this, we propose a novel method for automatically generating annotated synthetic data in Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for rapid synthetic data generation. We demonstrate that synthetic datasets can achieve performance comparable to that of real-world datasets while significantly reducing the time required to generate and annotate data. Additionally, combining real-world and synthetic data significantly increases object detection performance by leveraging the quality of real-world images with the easier scalability of synthetic data. To our knowledge, this is the first application of synthetic data for training object detection algorithms in the highly dynamic and varied environment of robot soccer. Validation experiments reveal that a detector trained on synthetic images performs on par with one trained on manually annotated real-world images when tested on robot soccer match scenarios. Our method offers a scalable and comprehensive alternative to traditional dataset creation, eliminating the labour-intensive error-prone manual annotation process. By generating datasets in a simulator where all elements are intrinsically known, we ensure accurate annotations while significantly reducing manual effort, which makes it particularly valuable for robotics applications requiring diverse and scalable training data.",
    "arxiv_url": "http://arxiv.org/abs/2506.05092v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05092v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "robotics",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using\n  Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Christopher Maxey",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "Despite significant advancements in dynamic neural rendering, existing methods fail to address the unique challenges posed by UAV-captured scenarios, particularly those involving monocular camera setups, top-down perspective, and multiple small, moving humans, which are not adequately represented in existing datasets. In this work, we introduce UAV4D, a framework for enabling photorealistic rendering for dynamic real-world scenes captured by UAVs. Specifically, we address the challenge of reconstructing dynamic scenes with multiple moving pedestrians from monocular video data without the need for additional sensors. We use a combination of a 3D foundation model and a human mesh reconstruction model to reconstruct both the scene background and humans. We propose a novel approach to resolve the scene scale ambiguity and place both humans and the scene in world coordinates by identifying human-scene contact points. Additionally, we exploit the SMPL model and background mesh to initialize Gaussian splats, enabling holistic scene rendering. We evaluated our method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and Okutama-Action, each with distinct characteristics and 10~50 humans. Our results demonstrate the benefits of our approach over existing methods in novel view synthesis, achieving a 1.5 dB PSNR improvement and superior visual sharpness.",
    "arxiv_url": "http://arxiv.org/abs/2506.05011v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05011v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "human",
      "4d",
      "ar",
      "gaussian splatting",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]