[
  {
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. Bühler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "arxiv_url": "http://arxiv.org/abs/2507.02803v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02803v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "reflection",
      "face",
      "avatar",
      "deformation",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and\n  Manipulation of Articulated Objects",
    "authors": [
      "Qiaojun Yu",
      "Xibin Yuan",
      "Yu jiang",
      "Junting Chen",
      "Dongzhe Zheng",
      "Ce Hao",
      "Yang You",
      "Yixing Chen",
      "Yao Mu",
      "Liu Liu",
      "Cewu Lu"
    ],
    "abstract": "Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: https://sites.google.com/view/artgs/home",
    "arxiv_url": "http://arxiv.org/abs/2507.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02600v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "3d gaussian",
      "motion",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local\n  Implicit Feature Decoupling",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Jianbo Jiao",
      "Jiayu Yang",
      "Luyang Tang",
      "Kaiqiang Xiong",
      "Jie Liang",
      "Jinbo Yan",
      "Runling Liu",
      "Ronggang Wang"
    ],
    "abstract": "Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
    "arxiv_url": "http://arxiv.org/abs/2507.02363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02363v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes",
    "authors": [
      "Stephen Pasch",
      "Joel K. Salzman",
      "Changxi Zheng"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "arxiv_url": "http://arxiv.org/abs/2507.02257v1",
    "pdf_url": "http://arxiv.org/pdf/2507.02257v1",
    "published_date": "2025-07-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "reflection",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial\n  Camouflage Generation",
    "authors": [
      "Tianrui Lou",
      "Xiaojun Jia",
      "Siyuan Liang",
      "Jiawei Liang",
      "Ming Zhang",
      "Yanjun Xiao",
      "Xiaochun Cao"
    ],
    "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:https://github.com/TRLou/PGA.",
    "arxiv_url": "http://arxiv.org/abs/2507.01367v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01367v1",
    "published_date": "2025-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online\n  Semantic Gaussian Splatting",
    "authors": [
      "Keiko Nagami",
      "Timothy Chen",
      "Javier Yu",
      "Ola Shorinwa",
      "Maximilian Adang",
      "Carlyn Dougherty",
      "Eric Cristofalo",
      "Mac Schwager"
    ],
    "abstract": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2507.01125v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01125v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
    "authors": [
      "Felix Windisch",
      "Lukas Radl",
      "Thomas Köhler",
      "Michael Steiner",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
    "arxiv_url": "http://arxiv.org/abs/2507.01110v1",
    "pdf_url": "http://arxiv.org/pdf/2507.01110v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "lightweight",
      "vr",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Masks make discriminative models great again!",
    "authors": [
      "Tianshi Cao",
      "Marie-Julie Rakotosaona",
      "Ben Poole",
      "Federico Tombari",
      "Michael Niemeyer"
    ],
    "abstract": "We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.",
    "arxiv_url": "http://arxiv.org/abs/2507.00916v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00916v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using\n  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
    "authors": [
      "Anna-Maria Halacheva",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
    "arxiv_url": "http://arxiv.org/abs/2507.00886v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00886v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "understanding",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail\n  Conserved Anti-Aliasing",
    "authors": [
      "Zhenya Yang",
      "Bingchen Gong",
      "Kai Chen",
      "Qi Dou"
    ],
    "abstract": "Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2507.00554v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00554v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And\n  Dynamic Density Control",
    "authors": [
      "Xingjun Wang",
      "Lianlei Shan"
    ],
    "abstract": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.",
    "arxiv_url": "http://arxiv.org/abs/2507.00363v1",
    "pdf_url": "http://arxiv.org/pdf/2507.00363v1",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction",
    "authors": [
      "Antoine Guédon",
      "Diego Gomez",
      "Nissim Maruani",
      "Bingchen Gong",
      "George Drettakis",
      "Maks Ovsjanikov"
    ],
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "arxiv_url": "http://arxiv.org/abs/2506.24096v1",
    "pdf_url": "http://arxiv.org/pdf/2506.24096v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "3d gaussian",
      "geometry",
      "animation",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local\n  Reconstruction and Rendering",
    "authors": [
      "Zinuo You",
      "Stamatios Georgoulis",
      "Anpei Chen",
      "Siyu Tang",
      "Dengxin Dai"
    ],
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.",
    "arxiv_url": "http://arxiv.org/abs/2506.23957v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23957v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via\n  Structural Attention",
    "authors": [
      "Ziao Liu",
      "Zhenjia Li",
      "Yifeng Shi",
      "Xiangang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2506.23611v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23611v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "efficient",
      "efficient rendering",
      "nerf",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image\n  Representation via 2D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Chao Yang",
      "Tao Guan",
      "Lili Ju"
    ],
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2506.23479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23479v1",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable\n  Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Kun Yuan",
      "Guankun Wang",
      "Mobarak I. Hoque",
      "Nicolas Padoy",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.23309v2",
    "pdf_url": "http://arxiv.org/pdf/2506.23309v2",
    "published_date": "2025-06-29",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d reconstruction",
      "tracking",
      "deformation",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination\n  Correction with Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Yanheng Li",
      "Tong Chen",
      "Jie Wang",
      "Jinlin Wu",
      "Zhen Lei",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "arxiv_url": "http://arxiv.org/abs/2506.23308v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23308v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "4d",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric\n  Constraints",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Lei Feng",
      "Yangbing Ge",
      "Shuaifeng Zhi",
      "Jiaxiong Liu",
      "Dewen Hu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
    "arxiv_url": "http://arxiv.org/abs/2506.23207v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23207v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "slam",
      "tracking",
      "dynamic",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "illumination",
      "mapping",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STD-GS: Exploring Frame-Event Interaction for\n  SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic\n  Scene",
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ],
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2506.23157v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23157v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "dynamic",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient\n  3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "An Le",
      "Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.",
    "arxiv_url": "http://arxiv.org/abs/2506.23042v1",
    "pdf_url": "http://arxiv.org/pdf/2506.23042v1",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Elaheh Badali Golezani",
      "Shohreh Kasaei"
    ],
    "abstract": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting",
    "arxiv_url": "http://arxiv.org/abs/2506.22973v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22973v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "compression",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via\n  Diffusion Priors",
    "authors": [
      "Sicong Du",
      "Jiarun Liu",
      "Qifeng Chen",
      "Hao-Xiang Chen",
      "Tai-Jiang Mu",
      "Sheng Yang"
    ],
    "abstract": "A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22800v2",
    "pdf_url": "http://arxiv.org/pdf/2506.22800v2",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
    "authors": [
      "Minchao Jiang",
      "Shunyu Jia",
      "Jiaming Gu",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Anqi Dong",
      "Liang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/",
    "arxiv_url": "http://arxiv.org/abs/2506.22799v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22799v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "3d gaussian",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "authors": [
      "Tao Tang",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "xia zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "abstract": "The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.22756v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22756v1",
    "published_date": "2025-06-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian\n  Splatting and a Low-Rank Free-Form Deformation Model",
    "authors": [
      "Yuliang Huang",
      "Imraj Singh",
      "Thomas Joyce",
      "Kris Thielemans",
      "Jamie R. McClelland"
    ],
    "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.22280v1",
    "pdf_url": "http://arxiv.org/pdf/2506.22280v1",
    "published_date": "2025-06-27",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "4d",
      "deformation",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve\n  Gaussian Splatting",
    "authors": [
      "Zipei Ma",
      "Junzhe Jiang",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.22099v2",
    "pdf_url": "http://arxiv.org/pdf/2506.22099v2",
    "published_date": "2025-06-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "urban scene",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MADrive: Memory-Augmented Driving Scene Modeling",
    "authors": [
      "Polina Karpikova",
      "Daniil Selikhanovych",
      "Kirill Struminsky",
      "Ruslan Musaev",
      "Maria Golitsyna",
      "Dmitry Baranchuk"
    ],
    "abstract": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/",
    "arxiv_url": "http://arxiv.org/abs/2506.21520v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21520v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "relighting",
      "3d gaussian",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian\n  Splatting",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Zhuoxiao Li",
      "Haocheng Zhao",
      "Kang Dang",
      "Jionglong Su",
      "Limin Yu",
      "Haoang Li"
    ],
    "abstract": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.21420v2",
    "pdf_url": "http://arxiv.org/pdf/2506.21420v2",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "slam",
      "dynamic",
      "efficient",
      "localization",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image",
    "authors": [
      "Pufan Li",
      "Bi'an Du",
      "Wei Hu"
    ],
    "abstract": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.21152v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21152v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV",
      "68",
      "I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local\n  Optimization",
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ],
    "abstract": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.21117v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21117v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "segmentation",
      "head",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "User-in-the-Loop View Sampling with Error Peaking Visualization",
    "authors": [
      "Ayaka Yasunaga",
      "Hideo Saito",
      "Shohei Mori"
    ],
    "abstract": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2506.21009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21009v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via\n  Sparse-Controlled Gaussian Splatting",
    "authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Byung-Ju Kim",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.",
    "arxiv_url": "http://arxiv.org/abs/2506.20998v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20998v1",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGH: 3D Head Generation with Composable Hair and Face",
    "authors": [
      "Chengan He",
      "Junxuan Li",
      "Tobias Kirschstein",
      "Artem Sevastopolsky",
      "Shunsuke Saito",
      "Qingyang Tan",
      "Javier Romero",
      "Chen Cao",
      "Holly Rushmeier",
      "Giljoo Nam"
    ],
    "abstract": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.",
    "arxiv_url": "http://arxiv.org/abs/2506.20875v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20875v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and\n  Rasterizer",
    "authors": [
      "Da Li",
      "Donggang Jia",
      "Yousef Rajeh",
      "Dominik Engel",
      "Ivan Viola"
    ],
    "abstract": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.",
    "arxiv_url": "http://arxiv.org/abs/2506.20202v1",
    "pdf_url": "http://arxiv.org/pdf/2506.20202v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "ray tracing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target\n  Reconstruction",
    "authors": [
      "Aobo Li",
      "Zhengxin Lei",
      "Jiangtao Wei",
      "Feng Xu"
    ],
    "abstract": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.",
    "arxiv_url": "http://arxiv.org/abs/2506.21633v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21633v1",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded\n  Scenes",
    "authors": [
      "Chenhao Zhang",
      "Yezhi Shen",
      "Fengqing Zhu"
    ],
    "abstract": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.21629v1",
    "pdf_url": "http://arxiv.org/pdf/2506.21629v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "nerf",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical\n  Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "arxiv_url": "http://arxiv.org/abs/2506.19842v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19842v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "body",
      "deformation",
      "motion",
      "understanding",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtual Memory for 3D Gaussian Splatting",
    "authors": [
      "Jonathan Haberl",
      "Philipp Fleck",
      "Clemens Arth"
    ],
    "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
    "arxiv_url": "http://arxiv.org/abs/2506.19415v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19415v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "authors": [
      "Xiaoyuan Wang",
      "Yizhou Zhao",
      "Botao Ye",
      "Xiaojun Shan",
      "Weijie Lyu",
      "Lu Qi",
      "Kelvin C. K. Chan",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2506.19291v1",
    "pdf_url": "http://arxiv.org/pdf/2506.19291v1",
    "published_date": "2025-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "deformation",
      "nerf",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale\n  Multi-Agent Gaussian SLAM",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "abstract": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.",
    "arxiv_url": "http://arxiv.org/abs/2506.18885v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18885v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "authors": [
      "Michal Nazarczuk",
      "Sibi Catley-Chandar",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Gregory Slabaugh",
      "Eduardo Pérez-Pellitero"
    ],
    "abstract": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io",
    "arxiv_url": "http://arxiv.org/abs/2506.18792v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18792v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
    "authors": [
      "Dylan Ebert"
    ],
    "abstract": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons.   Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource.   Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.18787v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18787v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
    "authors": [
      "Adam Yang",
      "Nadula Kadawedduwa",
      "Tianfu Wang",
      "Maria Molina",
      "Christopher Metzler"
    ],
    "abstract": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2506.18677v1",
    "pdf_url": "http://arxiv.org/pdf/2506.18677v1",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in\n  Large-Scale Scene",
    "authors": [
      "Shihan Chen",
      "Zhaojin Li",
      "Zeyu Chen",
      "Qingsong Yan",
      "Gaoyang Shen",
      "Ran Duan"
    ],
    "abstract": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2506.17636v1",
    "pdf_url": "http://arxiv.org/pdf/2506.17636v1",
    "published_date": "2025-06-21",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "efficient",
      "nerf",
      "3d gaussian",
      "high-fidelity",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "survey",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement\n  for 3D Low-Level Vision",
    "authors": [
      "Weeyoung Kwon",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2506.16262v2",
    "pdf_url": "http://arxiv.org/pdf/2506.16262v2",
    "published_date": "2025-06-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "3d gaussian",
      "nerf",
      "vr",
      "high-fidelity",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "survey",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Information-computation trade-offs in non-linear transforms",
    "authors": [
      "Connor Ding",
      "Abhiram Rao Gorle",
      "Jiwon Jeong",
      "Naomi Sagan",
      "Tsachy Weissman"
    ],
    "abstract": "In this work, we explore the interplay between information and computation in non-linear transform-based compression for broad classes of modern information-processing tasks. We first investigate two emerging nonlinear data transformation frameworks for image compression: Implicit Neural Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their representational properties, behavior under lossy compression, and convergence dynamics. Our results highlight key trade-offs between INR's compact, resolution-flexible neural field representations and GS's highly parallelizable, spatially interpretable fitting, providing insights for future hybrid and compression-aware frameworks. Next, we introduce the textual transform that enables efficient compression at ultra-low bitrate regimes and simultaneously enhances human perceptual satisfaction. When combined with the concept of denoising via lossy compression, the textual transform becomes a powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78) \"transform\", a universal method that, when applied to any member of a broad compressor family, produces new compressors that retain the asymptotic universality guarantees of the LZ78 algorithm. Collectively, these three transforms illuminate the fundamental trade-offs between coding efficiency and computational cost. We discuss how these insights extend beyond compression to tasks such as classification, denoising, and generative AI, suggesting new pathways for using non-linear transformations to balance resource constraints and performance.",
    "arxiv_url": "http://arxiv.org/abs/2506.15948v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15948v1",
    "published_date": "2025-06-19",
    "categories": [
      "cs.IT",
      "eess.IV",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "efficient",
      "compression",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos",
    "authors": [
      "Kaifeng Zhang",
      "Baoyu Li",
      "Kris Hauser",
      "Yunzhu Li"
    ],
    "abstract": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd .",
    "arxiv_url": "http://arxiv.org/abs/2506.15680v1",
    "pdf_url": "http://arxiv.org/pdf/2506.15680v1",
    "published_date": "2025-06-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate\n  Camera Pose Estimation under Complex Trajectories",
    "authors": [
      "Qingsong Yan",
      "Qiang Wang",
      "Kaiyong Zhao",
      "Jie Chen",
      "Bo Li",
      "Xiaowen Chu",
      "Fei Deng"
    ],
    "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks\\&Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories.",
    "arxiv_url": "http://arxiv.org/abs/2506.15242v2",
    "pdf_url": "http://arxiv.org/pdf/2506.15242v2",
    "published_date": "2025-06-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "slam",
      "localization",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads\n  Synthesis Using Gaussian Splatting",
    "authors": [
      "Ziqiao Peng",
      "Wentao Hu",
      "Junyuan Ma",
      "Xiangyu Zhu",
      "Xiaomei Zhang",
      "Hao Zhao",
      "Hui Tian",
      "Jun He",
      "Hongyan Liu",
      "Zhaoxin Fan"
    ],
    "abstract": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.",
    "arxiv_url": "http://arxiv.org/abs/2506.14742v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14742v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "efficient",
      "head",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D\n  Gaussian-Splatting",
    "authors": [
      "Yuke Xing",
      "Jiarui Wang",
      "Peizhi Niu",
      "Wenjie Huang",
      "Guangtao Zhai",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.",
    "arxiv_url": "http://arxiv.org/abs/2506.14642v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14642v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty\n  Maps for 3D Reconstruction",
    "authors": [
      "Zhengquan Zhang",
      "Feng Xu",
      "Mengmi Zhang"
    ],
    "abstract": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.",
    "arxiv_url": "http://arxiv.org/abs/2506.14856v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14856v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "efficient",
      "lightweight",
      "3d gaussian",
      "nerf",
      "head",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient\n  High-Resolution 3D Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Juan Zhang",
      "Tongfei Chen",
      "Shuo Yang",
      "Shuwei Shao",
      "Wenhao Dong",
      "Baochang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.14229v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14229v1",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAF: Gaussian Action Field as a Dynamic World Model for Robotic\n  Manipulation",
    "authors": [
      "Ying Chai",
      "Litao Deng",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Liangjun Xing",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a Vision-to-4D-to-Action (V-4D-A) framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/",
    "arxiv_url": "http://arxiv.org/abs/2506.14135v2",
    "pdf_url": "http://arxiv.org/pdf/2506.14135v2",
    "published_date": "2025-06-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "3d gaussian",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with\n  Gaussian Radiance Fields and Differentiable Dynamics",
    "authors": [
      "Qianzhong Chen",
      "Naixiang Gao",
      "Suning Huang",
      "JunEn Low",
      "Timothy Chen",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.",
    "arxiv_url": "http://arxiv.org/abs/2506.14009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.14009v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "efficient",
      "lightweight",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated\n  Human Images",
    "authors": [
      "Lingteng Qiu",
      "Peihao Li",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Weihao Yuan",
      "Siyu Zhu",
      "Xiaoguang Han",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.",
    "arxiv_url": "http://arxiv.org/abs/2506.13766v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13766v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for\n  Unconstrained Scene Reconstruction",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.",
    "arxiv_url": "http://arxiv.org/abs/2506.13516v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13516v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "motion",
      "gaussian splatting",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate\n  Radiance Fields",
    "authors": [
      "Jungeon Kim",
      "Geonsoo Park",
      "Seungyong Lee"
    ],
    "abstract": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.13508v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13508v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian\n  Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.",
    "arxiv_url": "http://arxiv.org/abs/2506.13348v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13348v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "face",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object\n  Reconstruction",
    "authors": [
      "Jinguang Tong",
      "Xuesong li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ],
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS",
    "arxiv_url": "http://arxiv.org/abs/2506.13110v1",
    "pdf_url": "http://arxiv.org/pdf/2506.13110v1",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "relighting",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "authors": [
      "Hyunjin Kim",
      "Haebeom Jung",
      "Jaesik Park"
    ],
    "abstract": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.",
    "arxiv_url": "http://arxiv.org/abs/2506.12945v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12945v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ],
    "abstract": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "arxiv_url": "http://arxiv.org/abs/2506.12787v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12787v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "localization",
      "lightweight",
      "deformation",
      "nerf",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient multi-view training for 3D Gaussian Splatting",
    "authors": [
      "Minhyuk Choi",
      "Injae Kim",
      "Hyunwoo J. Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize \"single-view\" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's \"multi-view\" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.",
    "arxiv_url": "http://arxiv.org/abs/2506.12727v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12727v2",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "efficient",
      "nerf",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors",
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ],
    "abstract": "We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.12716v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12716v1",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian\n  Splatting",
    "authors": [
      "Hongbi Zhou",
      "Zhangkai Ni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS",
    "arxiv_url": "http://arxiv.org/abs/2506.12400v2",
    "pdf_url": "http://arxiv.org/pdf/2506.12400v2",
    "published_date": "2025-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure",
    "authors": [
      "Stanley Lewis",
      "Vishal Chandra",
      "Tom Gao",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Representing articulated objects remains a difficult problem within the field of robotics. Objects such as pliers, clamps, or cabinets require representations that capture not only geometry and color information, but also part seperation, connectivity, and joint parametrization. Furthermore, learning these representations becomes even more difficult with each additional degree of freedom. Complex articulated objects such as robot arms may have seven or more degrees of freedom, and the depth of their kinematic tree may be notably greater than the tools, drawers, and cabinets that are the typical subjects of articulated object research. To address these concerns, we introduce SPLATART - a pipeline for learning Gaussian splat representations of articulated objects from posed images, of which a subset contains image space part segmentations. SPLATART disentangles the part separation task from the articulation estimation task, allowing for post-facto determination of joint estimation and representation of articulated objects with deeper kinematic trees than previously exhibited. In this work, we present data on the SPLATART pipeline as applied to the syntheic Paris dataset objects, and qualitative results on a real-world object under spare segmentation supervision. We additionally present on articulated serial chain manipulators to demonstrate usage on deeper kinematic tree structures.",
    "arxiv_url": "http://arxiv.org/abs/2506.12184v1",
    "pdf_url": "http://arxiv.org/pdf/2506.12184v1",
    "published_date": "2025-06-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static\n  Decoupling for 3D Gaussian Splatting-based Occupancy Prediction",
    "authors": [
      "Ke Song",
      "Yunhe Wu",
      "Chunchit Siu",
      "Huiyuan Xiong"
    ],
    "abstract": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.",
    "arxiv_url": "http://arxiv.org/abs/2506.14825v2",
    "pdf_url": "http://arxiv.org/pdf/2506.14825v2",
    "published_date": "2025-06-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "3d gaussian",
      "semantic",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Anti-Aliased 2D Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an antialiased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.",
    "arxiv_url": "http://arxiv.org/abs/2506.11252v1",
    "pdf_url": "http://arxiv.org/pdf/2506.11252v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian\n  Splatting",
    "authors": [
      "Lintao Xiang",
      "Hongpei Zheng",
      "Yating Huang",
      "Qijun Yang",
      "Hujun Yin"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.10335v1",
    "pdf_url": "http://arxiv.org/pdf/2506.10335v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "sparse view",
      "few-shot",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular\n  Videos",
    "authors": [
      "Chieh Hubert Lin",
      "Zhaoyang Lv",
      "Songyin Wu",
      "Zhen Xu",
      "Thu Nguyen-Phuoc",
      "Hung-Yu Tseng",
      "Julian Straub",
      "Numair Khan",
      "Lei Xiao",
      "Ming-Hsuan Yang",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong",
      "Zhengqin Li"
    ],
    "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09997v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09997v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "deformation",
      "3d gaussian",
      "motion",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.09952v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09952v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion\n  Decomposition for Scene Reconstruction",
    "authors": [
      "Junli Deng",
      "Ping Shi",
      "Qipei Li",
      "Jinyang Guo"
    ],
    "abstract": "Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09836v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09836v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "efficient",
      "deformation",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global\n  Gaussian Reduction for 3DGS",
    "authors": [
      "Tao Wang",
      "Mengyu Li",
      "Geduo Zeng",
      "Cheng Meng",
      "Qiong Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering.",
    "arxiv_url": "http://arxiv.org/abs/2506.09534v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09534v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "compact",
      "efficient",
      "lightweight",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene",
    "authors": [
      "Jianing Chen",
      "Zehao Li",
      "Yujun Cai",
      "Hao Jiang",
      "Chengxuan Qian",
      "Juyuan Kang",
      "Shuqin Gao",
      "Honglong Zhao",
      "Tianlu Mao",
      "Yucheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2506.09518v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09518v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "deformation",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene\n  Representation",
    "authors": [
      "Zetian Song",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Xiaohan Lu",
      "Chuanmin Jia",
      "Siwei Ma",
      "Wen Gao"
    ],
    "abstract": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.",
    "arxiv_url": "http://arxiv.org/abs/2506.09479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09479v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "authors": [
      "Yunxiao Shi",
      "Yinhao Zhu",
      "Shizhong Han",
      "Jisoo Jeong",
      "Amin Ansari",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "abstract": "Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.",
    "arxiv_url": "http://arxiv.org/abs/2506.09417v2",
    "pdf_url": "http://arxiv.org/pdf/2506.09417v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "geometry",
      "semantic",
      "autonomous driving",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via\n  Feed-Forward Gaussian Splatting from Only Sparse-View Images",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Jingyu Gong",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09378v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09378v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "authors": [
      "Zike Wu",
      "Qi Yan",
      "Xuanyu Yi",
      "Lele Wang",
      "Renjie Liao"
    ],
    "abstract": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.",
    "arxiv_url": "http://arxiv.org/abs/2506.08862v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08862v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "dynamic",
      "efficient",
      "deformation",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised\n  Learning with 3D Gaussian Splatting",
    "authors": [
      "Keyi Liu",
      "Weidong Yang",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.08777v2",
    "pdf_url": "http://arxiv.org/pdf/2506.08777v2",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language\n  Gaussian Splatting",
    "authors": [
      "Mengjiao Ma",
      "Qi Ma",
      "Yue Li",
      "Jiahuan Cheng",
      "Runyi Yang",
      "Bin Ren",
      "Nikola Popovic",
      "Mingqiang Wei",
      "Nicu Sebe",
      "Luc Van Gool",
      "Theo Gevers",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2506.08710v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08710v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "geometry",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "fast",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary\n  Large-Scale Scene Rendering",
    "authors": [
      "Xiaohan Zhang",
      "Sitong Wang",
      "Yushen Yan",
      "Yi Yang",
      "Mingda Xu",
      "Qi Liu"
    ],
    "abstract": "High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.08704v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08704v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex-Valued Holographic Radiance Fields",
    "authors": [
      "Yicheng Zhan",
      "Dong-Ha Shin",
      "Seung-Hwan Baek",
      "Kaan Akşit"
    ],
    "abstract": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.",
    "arxiv_url": "http://arxiv.org/abs/2506.08350v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08350v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.ET"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression\n  of Dynamic Scenes",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "abstract": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve high-quality novel view synthesis by using neural networks to predict the time-varying deformation of each Gaussian. However, performing per-Gaussian neural inference at every frame poses a significant bottleneck, limiting rendering speed and increasing memory and compute requirements. In this paper, we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS representations by reducing neural inference through two complementary techniques. First, we propose a temporal sensitivity pruning score that identifies and removes Gaussians with low contribution to the dynamic scene reconstruction. We also introduce an annealing smooth pruning mechanism that improves pruning robustness in real-world scenes with imprecise camera poses. Second, we propose GroupFlow, a motion analysis technique that clusters Gaussians by trajectory similarity and predicts a single rigid transformation per group instead of separate deformations for each Gaussian. Together, our techniques accelerate rendering by $10.37\\times$, reduce model size by $7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset. SpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be integrated into any deformable 3DGS or 4DGS framework.",
    "arxiv_url": "http://arxiv.org/abs/2506.07917v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07917v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "deformation",
      "nerf",
      "3d gaussian",
      "motion",
      "compression",
      "vr",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for\n  High-Fidelity Super-Resolution",
    "authors": [
      "Shuja Khalid",
      "Mohamed Ibrahim",
      "Yang Liu"
    ],
    "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
    "arxiv_url": "http://arxiv.org/abs/2506.07897v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07897v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "lightweight",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving\n  Simulation",
    "authors": [
      "William Ljungbergh",
      "Bernardo Taveira",
      "Wenzhao Zheng",
      "Adam Tonderski",
      "Chensheng Peng",
      "Fredrik Kahl",
      "Christoffer Petersson",
      "Michael Felsberg",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.",
    "arxiv_url": "http://arxiv.org/abs/2506.07826v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07826v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "neural rendering",
      "shadow",
      "dynamic",
      "lightweight",
      "3d gaussian",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian\n  Splatting",
    "authors": [
      "Jens Piekenbrinck",
      "Christian Schmidt",
      "Alexander Hermans",
      "Narunas Vaskevicius",
      "Timm Linder",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2506.07697v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07697v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "understanding",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline\n  Sparse Views",
    "authors": [
      "Xiaohan Lu",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Zetian Song",
      "Chuanmin Jia",
      "Siwei Ma"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.07670v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07670v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "sparse view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians",
    "authors": [
      "Zeyu Xiao",
      "Zhenyi Wu",
      "Mingyang Sun",
      "Qipeng Yan",
      "Yufan Guo",
      "Zhuoer Liang",
      "Lihua Zhang"
    ],
    "abstract": "3D Gaussian Splatting has achieved remarkable success in reconstructing both static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian primitives, interactions between objects suffer from inaccurate 3D segmentation, imprecise deformation among different materials, and severe rendering artifacts. To address these challenges, we introduce PIG: Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach that combines 3D object segmentation with the simulation of interacting objects in high precision. Firstly, our method facilitates fast and accurate mapping from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation. Secondly, we assign unique physical properties to correspondingly segmented objects within the scene for multi-material coupled interactions. Finally, we have successfully embedded constraint scales into deformation gradients, specifically clamping the scaling and rotation properties of the Gaussian primitives to eliminate artifacts and achieve geometric fidelity and visual consistency. Experimental results demonstrate that our method not only outperforms the state-of-the-art (SOTA) in terms of visual quality, but also opens up new directions and pipelines for the field of physically realistic scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.07657v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07657v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "deformation",
      "3d gaussian",
      "segmentation",
      "gaussian splatting",
      "fast",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory\n  Optimization and Architectural Support",
    "authors": [
      "Chenqi Zhang",
      "Yu Feng",
      "Jieru Zhao",
      "Guangda Liu",
      "Wenchao Ding",
      "Chentao Wu",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
    "arxiv_url": "http://arxiv.org/abs/2506.09070v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09070v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal\n  Navigation",
    "authors": [
      "Yijie Deng",
      "Shuaihang Yuan",
      "Geeta Chandra Raju Bethala",
      "Anthony Tzes",
      "Yu-Shen Liu",
      "Yi Fang"
    ],
    "abstract": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.",
    "arxiv_url": "http://arxiv.org/abs/2506.07338v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07338v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented\n  Rasterization",
    "authors": [
      "Zhican Wang",
      "Guanghui He",
      "Dantong Liu",
      "Lingjun Gao",
      "Shell Xu Hu",
      "Chen Zhang",
      "Zhuoran Song",
      "Nicholas Lane",
      "Wayne Luk",
      "Hongxiang Fan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy savings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.",
    "arxiv_url": "http://arxiv.org/abs/2506.07069v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07069v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "vr",
      "head",
      "autonomous driving",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene\n  Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiao Tang",
      "Jiajun Tang",
      "Jiaqi Lin",
      "Yuxin Cheng",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
    "arxiv_url": "http://arxiv.org/abs/2506.06988v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06988v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Mapping for Evolving Scenes",
    "authors": [
      "Vladimir Yugay",
      "Thies Kersten",
      "Luca Carlone",
      "Theo Gevers",
      "Martin R. Oswald",
      "Lukas Schmid"
    ],
    "abstract": "Mapping systems with novel view synthesis (NVS) capabilities are widely used in computer vision, with augmented reality, robotics, and autonomous driving applications. Most notably, 3D Gaussian Splatting-based systems show high NVS performance; however, many current approaches are limited to static scenes. While recent works have started addressing short-term dynamics (motion within the view of the camera), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene adaptation mechanism that continuously updates the 3D representation to reflect the latest changes. In addition, since maintaining geometric and semantic consistency remains challenging due to stale observations disrupting the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We evaluate Gaussian Mapping for Evolving Scenes (GaME) on both synthetic and real-world datasets and find it to be more accurate than the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2506.06909v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06909v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "motion",
      "semantic",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "mapping",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation",
    "authors": [
      "Sumit Sharma",
      "Gopi Raju Matta",
      "Kaushik Mitra"
    ],
    "abstract": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline.",
    "arxiv_url": "http://arxiv.org/abs/2506.06890v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06890v1",
    "published_date": "2025-06-07",
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "nerf",
      "gaussian splatting",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles",
    "authors": [
      "Yangkai Lin",
      "Jiabao Lei",
      "Kui jia"
    ],
    "abstract": "In recent years, there has been a growing demand to stylize a given 3D scene to align with the artistic style of reference images for creative purposes. While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method for realistic 3D scene modeling, there remains a challenge in adapting it to stylize 3D GS to match with multiple styles through automatic local style transfer or manual designation, while maintaining memory efficiency for stylization training. In this paper, we introduce a novel 3D GS stylization solution termed Multi-StyleGS to tackle these challenges. In particular, we employ a bipartite matching mechanism to au tomatically identify correspondences between the style images and the local regions of the rendered images. To facilitate local style transfer, we introduce a novel semantic style loss function that employs a segmentation network to apply distinct styles to various objects of the scene and propose a local-global feature matching to enhance the multi-view consistency. Furthermore, this technique can achieve memory efficient training, more texture details and better color match. To better assign a robust semantic label to each Gaussian, we propose several techniques to regularize the segmentation network. As demonstrated by our comprehensive experiments, our approach outperforms existing ones in producing plausible stylization results and offering flexible editing.",
    "arxiv_url": "http://arxiv.org/abs/2506.06846v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06846v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.06822v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06822v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "segmentation",
      "understanding",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and\n  Realistic Human Avatar Modeling",
    "authors": [
      "Cheng Peng",
      "Jingxiang Sun",
      "Yushuo Chen",
      "Zhaoqi Su",
      "Zhuo Su",
      "Yebin Liu"
    ],
    "abstract": "Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.",
    "arxiv_url": "http://arxiv.org/abs/2506.06645v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06645v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "compact",
      "avatar",
      "efficient",
      "3d gaussian",
      "geometry",
      "head",
      "high-fidelity",
      "gaussian splatting",
      "fast",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian\n  Mixtures and Part Discovery",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "Marcus A. Brubaker",
      "David J. Fleet"
    ],
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09063v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09063v1",
    "published_date": "2025-06-06",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "4d",
      "segmentation",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4: Generalizable Sparse Splatting Semantic SLAM",
    "authors": [
      "Mingqi Jiang",
      "Chanho Kim",
      "Chen Ziwen",
      "Li Fuxin"
    ],
    "abstract": "Traditional SLAM algorithms are excellent at camera tracking but might generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting (GS) approaches have emerged as an option for SLAM with accurate, dense 3D map building. However, existing GS-based SLAM methods rely on per-scene optimization which is time-consuming and does not generalize to diverse scenes well. In this work, we introduce the first generalizable GS-based semantic SLAM algorithm that incrementally builds and updates a 3D scene representation from an RGB-D video stream using a learned generalizable network. Our approach starts from an RGB-D image recognition backbone to predict the Gaussian parameters from every downsampled and backprojected image location. Additionally, we seamlessly integrate 3D semantic segmentation into our GS framework, bridging 3D mapping and recognition through a shared backbone. To correct localization drifting and floaters, we propose to optimize the GS for only 1 iteration following global localization. We demonstrate state-of-the-art semantic SLAM performance on the real-world benchmark ScanNet with an order of magnitude fewer Gaussians compared to other recent GS-based methods, and showcase our model's generalization capability through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.06517v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06517v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "recognition",
      "localization",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements",
    "authors": [
      "Nicolás Violante",
      "Andreas Meuleman",
      "Alban Gauthier",
      "Frédo Durand",
      "Thibault Groueix",
      "George Drettakis"
    ],
    "abstract": "We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.06462v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06462v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic\n  Environments",
    "authors": [
      "Mingrui Li",
      "Yiming Zhou",
      "Hongxing Zhou",
      "Xinggang Hu",
      "Florian Roemer",
      "Hongyu Wang",
      "Ahmad Osman"
    ],
    "abstract": "Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.05965v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05965v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "dynamic",
      "localization",
      "nerf",
      "3d gaussian",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for\n  Surgical Scene Reconstruction",
    "authors": [
      "Yuchao Zheng",
      "Jianing Zhang",
      "Guochen Ning",
      "Hongen Liao"
    ],
    "abstract": "Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page https://surgsplat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05935v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05935v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d reconstruction",
      "efficient",
      "3d gaussian",
      "geometry",
      "motion",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy",
    "authors": [
      "Yu Feng",
      "Weikai Lin",
      "Yuge Cheng",
      "Zihan Liu",
      "Jingwen Leng",
      "Minyi Guo",
      "Chen Chen",
      "Shixuan Sun",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural rendering, but it remains computationally demanding on today's mobile SoCs. To address this challenge, we propose Lumina, a hardware-algorithm co-designed system, which integrates two principal optimizations: a novel algorithm, S^2, and a radiance caching mechanism, RC, to improve the efficiency of neural rendering. S2 algorithm exploits temporal coherence in rendering to reduce the computational overhead, while RC leverages the color integration process of 3DGS to decrease the frequency of intensive rasterization computations. Coupled with these techniques, we propose an accelerator architecture, LuminCore, to further accelerate cache lookup and address the fundamental inefficiencies in Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB peak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.05682v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05682v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for\n  Occupancy and Flow Prediction",
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ],
    "abstract": "Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancing performance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at https://zzy816.github.io/VoxelSplat-Demo/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05563v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05563v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "motion",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from\n  Unposed Images",
    "authors": [
      "Andreas Meuleman",
      "Ishaan Shah",
      "Alexandre Lanvin",
      "Bernhard Kerbl",
      "George Drettakis"
    ],
    "abstract": "Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that our solution can provide on-the-fly processing of all the capture scenarios and scene sizes we target while remaining competitive with other methods that only handle specific capture styles or scene sizes in speed, image quality, or both.",
    "arxiv_url": "http://arxiv.org/abs/2506.05558v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05558v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "efficient",
      "large scene",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian\n  Splatting",
    "authors": [
      "Daniel Wang",
      "Patrick Rim",
      "Tian Tian",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to forecast dynamic 3D scenes far beyond the time span seen during training. Existing neural rendering systems - whether NeRF- or 3DGS-based - embed time directly in a deformation network and therefore excel at interpolation but collapse when asked to predict the future, where timestamps are strictly out-of-distribution. ODE-GS eliminates this dependency: after learning a high-fidelity, time-conditioned deformation model for the training window, we freeze it and train a Transformer encoder that summarizes past Gaussian trajectories into a latent state whose continuous evolution is governed by a neural ODE. Numerical integration of this latent flow yields smooth, physically plausible Gaussian trajectories that can be queried at any future instant and rendered in real time. Coupled with a variational objective and a lightweight second-derivative regularizer, ODE-GS attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the strongest baselines. Our results demonstrate that continuous-time latent dynamics are a powerful, practical route to photorealistic prediction of complex 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.05480v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05480v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "dynamic",
      "lightweight",
      "deformation",
      "nerf",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Duochao Shi",
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Jia-Wang Bian",
      "Bohan Zhuang",
      "Chunhua Shen"
    ],
    "abstract": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss",
    "arxiv_url": "http://arxiv.org/abs/2506.05327v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05327v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian\n  Splatting",
    "authors": [
      "Nan Wang",
      "Yuantao Chen",
      "Lixing Xiao",
      "Weiqing Xiao",
      "Bohan Li",
      "Zhaoxi Chen",
      "Chongjie Ye",
      "Shaocong Xu",
      "Saining Zhang",
      "Ziyang Yan",
      "Pierre Merriaux",
      "Lei Lei",
      "Tianfan Xue",
      "Hao Zhao"
    ],
    "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
    "arxiv_url": "http://arxiv.org/abs/2506.05280v2",
    "pdf_url": "http://arxiv.org/pdf/2506.05280v2",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "dynamic",
      "nerf",
      "geometry",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D\n  Gaussian Splatting for Vision Training",
    "authors": [
      "Aneesh Deogan",
      "Wout Beks",
      "Peter Teurlings",
      "Koen de Vos",
      "Mark van den Brand",
      "Rene van de Molengraft"
    ],
    "abstract": "Annotated datasets are critical for training neural networks for object detection, yet their manual creation is time- and labour-intensive, subjective to human error, and often limited in diversity. This challenge is particularly pronounced in the domain of robotics, where diverse and dynamic scenarios further complicate the creation of representative datasets. To address this, we propose a novel method for automatically generating annotated synthetic data in Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for rapid synthetic data generation. We demonstrate that synthetic datasets can achieve performance comparable to that of real-world datasets while significantly reducing the time required to generate and annotate data. Additionally, combining real-world and synthetic data significantly increases object detection performance by leveraging the quality of real-world images with the easier scalability of synthetic data. To our knowledge, this is the first application of synthetic data for training object detection algorithms in the highly dynamic and varied environment of robot soccer. Validation experiments reveal that a detector trained on synthetic images performs on par with one trained on manually annotated real-world images when tested on robot soccer match scenarios. Our method offers a scalable and comprehensive alternative to traditional dataset creation, eliminating the labour-intensive error-prone manual annotation process. By generating datasets in a simulator where all elements are intrinsically known, we ensure accurate annotations while significantly reducing manual effort, which makes it particularly valuable for robotics applications requiring diverse and scalable training data.",
    "arxiv_url": "http://arxiv.org/abs/2506.05092v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05092v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "human",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using\n  Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Christopher Maxey",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "Despite significant advancements in dynamic neural rendering, existing methods fail to address the unique challenges posed by UAV-captured scenarios, particularly those involving monocular camera setups, top-down perspective, and multiple small, moving humans, which are not adequately represented in existing datasets. In this work, we introduce UAV4D, a framework for enabling photorealistic rendering for dynamic real-world scenes captured by UAVs. Specifically, we address the challenge of reconstructing dynamic scenes with multiple moving pedestrians from monocular video data without the need for additional sensors. We use a combination of a 3D foundation model and a human mesh reconstruction model to reconstruct both the scene background and humans. We propose a novel approach to resolve the scene scale ambiguity and place both humans and the scene in world coordinates by identifying human-scene contact points. Additionally, we exploit the SMPL model and background mesh to initialize Gaussian splats, enabling holistic scene rendering. We evaluated our method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and Okutama-Action, each with distinct characteristics and 10~50 humans. Our results demonstrate the benefits of our approach over existing methods in novel view synthesis, achieving a 1.5 dB PSNR improvement and superior visual sharpness.",
    "arxiv_url": "http://arxiv.org/abs/2506.05011v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05011v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "dynamic",
      "4d",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian\n  Splatting",
    "authors": [
      "Alfred T. Christiansen",
      "Andreas H. Højrup",
      "Morten K. Stephansen",
      "Md Ibtihaj A. Sakib",
      "Taman S. Poojary",
      "Filip Slezak",
      "Morten S. Laursen",
      "Thomas B. Moeslund",
      "Joakim B. Haurum"
    ],
    "abstract": "Training neural networks for tasks such as 3D point cloud semantic segmentation demands extensive datasets, yet obtaining and annotating real-world point clouds is costly and labor-intensive. This work aims to introduce a novel pipeline for generating realistic synthetic data, by leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to generate 3D assets of multiple different agricultural vehicles instead of using generic models. These assets are placed in a simulated environment, where the point clouds are generated using a simulated LiDAR. This is a flexible approach that allows changing the LiDAR specifications without incurring additional costs. We evaluated the impact of synthetic data on segmentation models such as PointNet++, Point Transformer V3, and OACNN, by training and validating the models only on synthetic data. Remarkably, the PTv3 model had an mIoU of 91.35\\%, a noteworthy result given that the model had neither been trained nor validated on any real data. Further studies even suggested that in certain scenarios the models trained only on synthetically generated data performed better than models trained on real-world data. Finally, experiments demonstrated that the models can generalize across semantic classes, enabling accurate predictions on mesh models they were never trained on.",
    "arxiv_url": "http://arxiv.org/abs/2506.05009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05009v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and\n  Expert Knowledge Transfer",
    "authors": [
      "Filip Slezak",
      "Magnus K. Gjerde",
      "Joakim B. Haurum",
      "Ivan Nikolov",
      "Morten S. Laursen",
      "Thomas B. Moeslund"
    ],
    "abstract": "In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for stereo dataset generation, offering an efficient alternative to Neural Radiance Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore utilizing the reconstructed geometry from the explicit 3D representations as well as depth estimates from the FoundationStereo model in an expert knowledge transfer setup. We find that when fine-tuning stereo models on 3DGS-generated datasets, we demonstrate competitive performance in zero-shot generalization benchmarks. When using the reconstructed geometry directly, we observe that it is often noisy and contains artifacts, which propagate noise to the trained model. In contrast, we find that the disparity estimates from FoundationStereo are cleaner and consequently result in a better performance on the zero-shot generalization benchmarks. Our method highlights the potential for low-cost, high-fidelity dataset creation and fast fine-tuning for deep stereo models. Moreover, we also reveal that while the latest Gaussian Splatting based methods have achieved superior performance on established benchmarks, their robustness falls short in challenging in-the-wild settings warranting further exploration.",
    "arxiv_url": "http://arxiv.org/abs/2506.04908v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04908v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
    "authors": [
      "Gaia Di Lorenzo",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.",
    "arxiv_url": "http://arxiv.org/abs/2506.04789v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04789v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "3d gaussian",
      "geometry",
      "semantic",
      "high-fidelity",
      "gaussian splatting",
      "understanding",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photoreal Scene Reconstruction from an Egocentric Device",
    "authors": [
      "Zhaoyang Lv",
      "Maurizio Monge",
      "Ka Chen",
      "Yufeng Zhu",
      "Michael Goesele",
      "Jakob Engel",
      "Zhao Dong",
      "Richard Newcombe"
    ],
    "abstract": "In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at http://www.projectaria.com/photoreal-reconstruction/",
    "arxiv_url": "http://arxiv.org/abs/2506.04444v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04444v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.",
    "arxiv_url": "http://arxiv.org/abs/2506.04351v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04351v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "gaussian splatting",
      "ar",
      "human",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pseudo-Simulation for Autonomous Driving",
    "authors": [
      "Wei Cao",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Daniel Dauner",
      "Xunjiang Gu",
      "Caojun Wang",
      "Yakov Miron",
      "Marco Aiello",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations (R^2=0.8) than the best existing open-loop approach (R^2=0.7). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim.",
    "arxiv_url": "http://arxiv.org/abs/2506.04218v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04218v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "3d gaussian",
      "head",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D\n  Gaussian Splatting",
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands relatively significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the effectiveness of our approach. Code is available at https://flexgs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2506.04174v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04174v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data",
    "authors": [
      "Ben Moran",
      "Mauro Comi",
      "Arunkumar Byravan",
      "Steven Bohez",
      "Tom Erez",
      "Zhibin Li",
      "Leonard Hasenclever"
    ],
    "abstract": "Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.",
    "arxiv_url": "http://arxiv.org/abs/2506.04120v2",
    "pdf_url": "http://arxiv.org/pdf/2506.04120v2",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "geometry",
      "motion",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D\n  Gaussian Splatting",
    "authors": [
      "Shengjie Lin",
      "Jiading Fang",
      "Muhammad Zubair Irshad",
      "Vitor Campagnolo Guizilini",
      "Rares Andrei Ambrus",
      "Greg Shakhnarovich",
      "Matthew R. Walter"
    ],
    "abstract": "Reconstructing articulated objects prevalent in daily environments is crucial for applications in augmented/virtual reality and robotics. However, existing methods face scalability limitations (requiring 3D supervision or costly annotations), robustness issues (being susceptible to local optima), and rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a self-supervised, category-agnostic framework that leverages 3D Gaussian Splatting (3DGS) to reconstruct articulated objects and infer kinematics from two sets of posed RGB images captured at different articulation states, enabling real-time photorealistic rendering for novel viewpoints and articulations. SplArt augments 3DGS with a differentiable mobility parameter per Gaussian, achieving refined part segmentation. A multi-stage optimization strategy is employed to progressively handle reconstruction, part segmentation, and articulation estimation, significantly enhancing robustness and accuracy. SplArt exploits geometric self-supervision, effectively addressing challenging scenarios without requiring 3D annotations or category-specific priors. Evaluations on established and newly proposed benchmarks, along with applications to real-world scenarios using a handheld RGB camera, demonstrate SplArt's state-of-the-art performance and real-world practicality. Code is publicly available at https://github.com/ripl/splart.",
    "arxiv_url": "http://arxiv.org/abs/2506.03594v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03594v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "segmentation",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
    "authors": [
      "Chengqi Li",
      "Zhihao Shi",
      "Yangdi Lu",
      "Wenbo He",
      "Xiangyu Xu"
    ],
    "abstract": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2506.03538v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03538v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "neural rendering",
      "3d reconstruction",
      "dynamic",
      "lightweight",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation",
    "authors": [
      "Lukas Meyer",
      "Josef Grün",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS) framework that is able to generate multi-view consistent novel views from images of multiple, independent cameras with different spectral domains. In contrast to previous approaches, our method does not require cross-modal camera calibration and is versatile enough to model a variety of different spectra, including thermal and near-infra red, without any algorithmic changes.   Unlike existing 3DGS-based frameworks that treat each modality separately (by optimizing per-channel spherical harmonics) and therefore fail to exploit the underlying spectral and spatial correlations, our method leverages a novel neural color representation that encodes multi-spectral information into a learned, compact, per-splat feature embedding. A shallow multi-layer perceptron (MLP) then decodes this embedding to obtain spectral color values, enabling joint learning of all bands within a unified representation.   Our experiments show that this simple yet effective strategy is able to improve multi-spectral rendering quality, while also leading to improved per-spectra rendering quality over state-of-the-art methods. We demonstrate the effectiveness of this new technique in agricultural applications to render vegetation indices, such as normalized difference vegetation index (NDVI).",
    "arxiv_url": "http://arxiv.org/abs/2506.03407v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03407v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "compact",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gen4D: Synthesizing Humans and Scenes in the Wild",
    "authors": [
      "Jerrin Bright",
      "Zhibo Wang",
      "Yuhao Chen",
      "Sirisha Rambhatla",
      "John Zelek",
      "David Clausi"
    ],
    "abstract": "Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.",
    "arxiv_url": "http://arxiv.org/abs/2506.05397v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05397v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "4d",
      "animation",
      "motion",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM",
    "authors": [
      "Roman Titkov",
      "Egor Zubkov",
      "Dmitry Yudin",
      "Jaafar Mahmoud",
      "Malik Mohrat",
      "Gennady Sidorov"
    ],
    "abstract": "Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM -- a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera's ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: https://titrom025.github.io/LEG-SLAM/",
    "arxiv_url": "http://arxiv.org/abs/2506.03073v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03073v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "localization",
      "3d gaussian",
      "motion",
      "semantic",
      "gaussian splatting",
      "ar",
      "mapping",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Processor Chip Model",
    "authors": [
      "Kaiyan Chang",
      "Mingzhi Chen",
      "Yunji Chen",
      "Zhirong Chen",
      "Dongrui Fan",
      "Junfeng Gong",
      "Nan Guo",
      "Yinhe Han",
      "Qinfen Hao",
      "Shuo Hou",
      "Xuan Huang",
      "Pengwei Jin",
      "Changxin Ke",
      "Cangyuan Li",
      "Guangli Li",
      "Huawei Li",
      "Kuan Li",
      "Naipeng Li",
      "Shengwen Liang",
      "Cheng Liu",
      "Hongwei Liu",
      "Jiahua Liu",
      "Junliang Lv",
      "Jianan Mu",
      "Jin Qin",
      "Bin Sun",
      "Chenxi Wang",
      "Duo Wang",
      "Mingjun Wang",
      "Ying Wang",
      "Chenggang Wu",
      "Peiyang Wu",
      "Teng Wu",
      "Xiao Xiao",
      "Mengyao Xie",
      "Chenwei Xiong",
      "Ruiyuan Xu",
      "Mingyu Yan",
      "Xiaochun Ye",
      "Kuai Yu",
      "Rui Zhang",
      "Shuoming Zhang",
      "Jiacheng Zhao"
    ],
    "abstract": "Computer System Architecture serves as a crucial bridge between software applications and the underlying hardware, encompassing components like compilers, CPUs, coprocessors, and RTL designs. Its development, from early mainframes to modern domain-specific architectures, has been driven by rising computational demands and advancements in semiconductor technology. However, traditional paradigms in computer system architecture design are confronting significant challenges, including a reliance on manual expertise, fragmented optimization across software and hardware layers, and high costs associated with exploring expansive design spaces. While automated methods leveraging optimization algorithms and machine learning have improved efficiency, they remain constrained by a single-stage focus, limited data availability, and a lack of comprehensive human domain knowledge. The emergence of large language models offers transformative opportunities for the design of computer system architecture. By leveraging the capabilities of LLMs in areas such as code generation, data analysis, and performance modeling, the traditional manual design process can be transitioned to a machine-based automated design approach. To harness this potential, we present the Large Processor Chip Model (LPCM), an LLM-driven framework aimed at achieving end-to-end automated computer architecture design. The LPCM is structured into three levels: Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D Gaussian Splatting as a representative workload and employs the concept of software-hardware collaborative design to examine the implementation of the LPCM at Level 1, demonstrating the effectiveness of the proposed approach. Furthermore, this paper provides an in-depth discussion on the pathway to implementing Level 2 and Level 3 of the LPCM, along with an analysis of the existing challenges.",
    "arxiv_url": "http://arxiv.org/abs/2506.02929v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02929v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone",
    "authors": [
      "Zheng Liu",
      "He Zhu",
      "Xinyang Li",
      "Yirun Wang",
      "Yujiao Shi",
      "Wei Li",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D scene rendering. However, rendering city-scale 3DGS scenes on mobile devices, e.g., your smartphones, remains a significant challenge due to the limited resources on mobile devices. A natural solution is to offload computation to the cloud; however, naively streaming rendered frames from the cloud to the client introduces high latency and requires bandwidth far beyond the capacity of current wireless networks.   In this paper, we propose an effective solution to enable city-scale 3DGS rendering on mobile devices. Our key insight is that, under normal user motion, the number of newly visible Gaussians per second remains roughly constant. Leveraging this, we stream only the necessary Gaussians to the client. Specifically, on the cloud side, we propose asynchronous level-of-detail search to identify the necessary Gaussians for the client. On the client side, we accelerate rendering via a lookup table-based rasterization. Combined with holistic runtime optimizations, our system can deliver low-latency, city-scale 3DGS rendering on mobile devices. Compared to existing solutions, Voyager achieves over 100$\\times$ reduction on data transfer and up to 8.9$\\times$ speedup while retaining comparable rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2506.02774v2",
    "pdf_url": "http://arxiv.org/pdf/2506.02774v2",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS",
    "authors": [
      "Chuanyu Fu",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Guanying Chen",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.",
    "arxiv_url": "http://arxiv.org/abs/2506.02751v2",
    "pdf_url": "http://arxiv.org/pdf/2506.02751v2",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for\n  Real-World 3DGS Scenes in VR",
    "authors": [
      "Zihao Ding",
      "Cheng-Tse Lee",
      "Mufeng Zhu",
      "Tao Guan",
      "Yuan-Chun Sun",
      "Cheng-Hsin Hsu",
      "Yao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an emerging media representation that reconstructs real-world 3D scenes in high fidelity, enabling 6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However, developing and evaluating 3DGS-enabled applications and optimizing their rendering performance, require realistic user navigation data. Such data is currently unavailable for photorealistic 3DGS reconstructions of real-world scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available 6-DoF navigation dataset featuring traces from 46 participants exploring twelve diverse, real-world 3DGS scenes. The dataset was collected at two sites, using the Meta Quest Pro headsets, recording the head pose and eye gaze data for each rendered frame during free world standing 6-DoF navigation. For each of the twelve scenes, we performed careful scene initialization to correct for scene tilt and scale, ensuring a perceptually-comfortable VR experience. We also release our open-source SIBR viewer software fork with record-and-replay functionalities and a suite of utility tools for data processing, conversion, and visualization. The EyeNavGS dataset and its accompanying software tools provide valuable resources for advancing research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.",
    "arxiv_url": "http://arxiv.org/abs/2506.02380v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02380v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "vr",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCodec Studio: A Modular Framework for Gaussian Splat Compression",
    "authors": [
      "Sicheng Li",
      "Chengzhen Wu",
      "Hao Li",
      "Xiang Gao",
      "Yiyi Liao",
      "Lu Yu"
    ],
    "abstract": "3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available at https://github.com/JasonLSC/GSCodec_Studio , to advance the research on Gaussian Splats compression.",
    "arxiv_url": "http://arxiv.org/abs/2506.01822v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01822v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "4d",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes",
    "authors": [
      "Manuel-Andreas Schneider",
      "Lukas Höllein",
      "Matthias Nießner"
    ],
    "abstract": "Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.",
    "arxiv_url": "http://arxiv.org/abs/2506.01799v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01799v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
    "authors": [
      "Tenny Yin",
      "Zhiting Mei",
      "Tao Sun",
      "Lihan Zha",
      "Emily Zhou",
      "Jeremy Bao",
      "Miyu Yamane",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.",
    "arxiv_url": "http://arxiv.org/abs/2506.01600v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01600v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "localization",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis\n  and 3D Reconstruction of Autonomous Driving Scenes",
    "authors": [
      "Pou-Chun Kung",
      "Skanda Harisha",
      "Ram Vasudevan",
      "Aline Eid",
      "Katherine A. Skinner"
    ],
    "abstract": "High-Fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR / 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction. A project page is available at https://umautobots.github.io/radarsplat.",
    "arxiv_url": "http://arxiv.org/abs/2506.01379v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01379v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "3d reconstruction",
      "high-fidelity",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic\n  Gaussian Splatting",
    "authors": [
      "Fengze Li",
      "Yangle Liu",
      "Jieming Ma",
      "Hai-Ning Liang",
      "Yaochun Shen",
      "Huangxiang Li",
      "Zhijing Wu"
    ],
    "abstract": "Accurate fruit counting in real-world agricultural environments is a longstanding challenge due to visual occlusions, semantic ambiguity, and the high computational demands of 3D reconstruction. Existing methods based on neural radiance fields suffer from low inference speed, limited generalization, and lack support for open-set semantic control. This paper presents FruitLangGS, a real-time 3D fruit counting framework that addresses these limitations through spatial reconstruction, semantic embedding, and language-guided instance estimation. FruitLangGS first reconstructs orchard-scale scenes using an adaptive Gaussian splatting pipeline with radius-aware pruning and tile-based rasterization for efficient rendering. To enable semantic control, each Gaussian encodes a compressed CLIP-aligned language embedding, forming a compact and queryable 3D representation. At inference time, prompt-based semantic filtering is applied directly in 3D space, without relying on image-space segmentation or view-level fusion. The selected Gaussians are then converted into dense point clouds via distribution-aware sampling and clustered to estimate fruit counts. Experimental results on real orchard data demonstrate that FruitLangGS achieves higher rendering speed, semantic flexibility, and counting accuracy compared to prior approaches, offering a new perspective for language-driven, real-time neural rendering across open-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2506.01109v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01109v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "compact",
      "efficient",
      "efficient rendering",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Globally Consistent RGB-D SLAM with 2D Gaussian Splatting",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Liren Jin",
      "Marija Popović",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable performance of high-fidelity 3D reconstruction. However, the lack of depth rendering consistency and efficient loop closure limits the quality of its geometric reconstructions and its ability to perform globally consistent mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using 2D Gaussian splatting as the map representation. By leveraging the depth-consistent rendering property of the 2D variant, we propose an accurate camera pose optimization method and achieve geometrically accurate 3D reconstruction. In addition, we implement efficient loop detection and camera relocalization by leveraging MASt3R, a 3D foundation model, and achieve efficient map updates by maintaining a local active map. Experiments show that our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface reconstruction quality, and more consistent global map reconstruction compared to existing rendering-based SLAM methods, while maintaining high-fidelity image rendering and improved computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2506.00970v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00970v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "slam",
      "tracking",
      "efficient",
      "localization",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splat Vulnerabilities",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haoran Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Willian T. Lunardi",
      "Martin Andreoni",
      "Polo Chau"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.",
    "arxiv_url": "http://arxiv.org/abs/2506.00280v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00280v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "fast",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Voxelization for Transform coding of 3D Gaussian splatting data",
    "authors": [
      "Chenjunjie Wang",
      "Shashank N. Sridhara",
      "Eduardo Pavez",
      "Antonio Ortega",
      "Cheng Chang"
    ],
    "abstract": "We present a novel compression framework for 3D Gaussian splatting (3DGS) data that leverages transform coding tools originally developed for point clouds. Contrary to existing 3DGS compression methods, our approach can produce compressed 3DGS models at multiple bitrates in a computationally efficient way. Point cloud voxelization is a discretization technique that point cloud codecs use to improve coding efficiency while enabling the use of fast transform coding algorithms. We propose an adaptive voxelization algorithm tailored to 3DGS data, to avoid the inefficiencies introduced by uniform voxelization used in point cloud codecs. We ensure the positions of larger volume Gaussians are represented at high resolution, as these significantly impact rendering quality. Meanwhile, a low-resolution representation is used for dense regions with smaller Gaussians, which have a relatively lower impact on rendering quality. This adaptive voxelization approach significantly reduces the number of Gaussians and the bitrate required to encode the 3DGS data. After voxelization, many Gaussians are moved or eliminated. Thus, we propose to fine-tune/recolor the remaining 3DGS attributes with an initialization that can reduce the amount of retraining required. Experimental results on pre-trained datasets show that our proposed compression framework outperforms existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.00271v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00271v1",
    "published_date": "2025-05-30",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Understanding while Exploring: Semantics-driven Active Mapping",
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Hairong Yin",
      "Yi Xu",
      "Philippos Mordohai"
    ],
    "abstract": "Effective robotic autonomy in unknown environments demands proactive exploration and precise understanding of both geometry and semantics. In this paper, we propose ActiveSGM, an active semantic mapping framework designed to predict the informativeness of potential observations before execution. Built upon a 3D Gaussian Splatting (3DGS) mapping backbone, our approach employs semantic and geometric uncertainty quantification, coupled with a sparse semantic representation, to guide exploration. By enabling robots to strategically select the most beneficial viewpoints, ActiveSGM efficiently enhances mapping completeness, accuracy, and robustness to noisy semantic data, ultimately supporting more adaptive scene exploration. Our experiments on the Replica and Matterport3D datasets highlight the effectiveness of ActiveSGM in active semantic mapping tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.00225v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00225v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "geometry",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional\n  Multiview Diffusion",
    "authors": [
      "Yangyi Huang",
      "Ye Yuan",
      "Xueting Li",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.",
    "arxiv_url": "http://arxiv.org/abs/2505.24877v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24877v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "body",
      "animation",
      "3d gaussian",
      "motion",
      "high-fidelity",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores",
    "authors": [
      "Zimu Liao",
      "Jifeng Ding",
      "Rong Fu",
      "Siwei Cui",
      "Ruixuan Gong",
      "Li Wang",
      "Boni Hu",
      "Yi Wang",
      "Hengjie Li",
      "XIngcheng Zhang",
      "Hui Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where conditional alpha-blending dominates the time cost in the rendering pipeline. This paper proposes TC-GS, an algorithm-independent universal module that expands Tensor Core (TCU) applicability for 3DGS, leading to substantial speedups and seamless integration into existing 3DGS optimization frameworks. The key innovation lies in mapping alpha computation to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS implementations. TC-GS provides plug-and-play acceleration for existing top-tier acceleration algorithms tightly coupled with rendering pipeline designs, like Gaussian compression and redundancy elimination algorithms. Additionally, we introduce a global-to-local coordinate transformation to mitigate rounding errors from quadratic terms of pixel coordinates caused by Tensor Core half-precision computation. Extensive experiments demonstrate that our method maintains rendering quality while providing an additional 2.18x speedup over existing Gaussian acceleration algorithms, thus reaching up to a total 5.6x acceleration. The code is currently available at anonymous \\href{https://github.com/TensorCore3DGS/3DGSTensorCore}",
    "arxiv_url": "http://arxiv.org/abs/2505.24796v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24796v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC",
      "I.3.6; I.3.2; D.1.3"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "fast",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting",
    "authors": [
      "Jiazhong Cen",
      "Xudong Zhou",
      "Jiemin Fang",
      "Changsong Wen",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints--a phenomenon we term view-dependent semantics. To address this challenge, we propose LaGa (Language Gaussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of +18.7% mIoU over the previous SOTA on the LERF-OVS dataset. Our code is available at: https://github.com/SJTU-DeepVisionLab/LaGa.",
    "arxiv_url": "http://arxiv.org/abs/2505.24746v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24746v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "understanding",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GARLIC: GAussian Representation LearnIng for spaCe partitioning",
    "authors": [
      "Panagiotis Rigas",
      "Panagiotis Drivas",
      "Charalambos Tzamos",
      "Ioannis Chamodrakas",
      "George Ioannakis",
      "Leonidas J. Guibas",
      "Ioannis Z. Emiris"
    ],
    "abstract": "We introduce GARLIC (GAussian Representation LearnIng for spaCe partitioning), a novel indexing structure based on \\(N\\)-dimensional Gaussians for efficiently learning high-dimensional vector spaces. Our approach is inspired from Gaussian splatting techniques, typically used in 3D rendering, which we adapt for high-dimensional search and classification. We optimize Gaussian parameters using information-theoretic objectives that balance coverage, assignment confidence, and structural and semantic consistency. A key contribution is to progressively refine the representation through split and clone operations, handling hundreds of dimensions, thus handling varying data densities. GARLIC offers the fast building times of traditional space partitioning methods (e.g., under \\(\\sim5\\) min build time for SIFT1M) while achieving \\(\\sim50\\%\\) Recall10@10 in low-candidate regimes. Experimental results on standard benchmarks demonstrate our method's consistency in (a) \\(k\\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by using about half their probes for the same Recall10@10 in Fashion-MNIST, and (b) in classification tasks, beating by \\(\\sim15\\%\\) accuracy other majority voting methods. Further, we show strong generalization capabilities, maintaining high accuracy even with downsampled training data: using just \\(1\\%\\) of the training data returns \\(\\sim 45\\%\\) Recall@1, thus making GARLIC quite powerful for applications requiring both speed and accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2505.24608v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24608v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with\n  Auto-Regressive Diffusion Framework",
    "authors": [
      "Xin Kang",
      "Zihan Zheng",
      "Lei Chu",
      "Yue Gao",
      "Jiahao Li",
      "Hao Pan",
      "Xuejin Chen",
      "Yan Lu"
    ],
    "abstract": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.24245v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24245v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians",
    "authors": [
      "Zixun Huang",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the quality and efficiency of differentiable rendering. However, its high efficiency stems from an approximation of projecting 3D Gaussians onto the image plane as 2D Gaussians, which inherently limits rendering quality--particularly under large Field-of-View (FoV) camera inputs. While several recent works have extended 3DGS to mitigate these approximation errors, none have successfully achieved both exactness and high efficiency simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient Volumetric Gaussian Rendering method. Starting from first principles, we derive a closed-form expression for the density integral along a ray traversing a 3D Gaussian distribution. This formulation enables precise forward rendering with arbitrary camera models and supports gradient-based optimization of 3D Gaussian parameters. To ensure both exactness and real-time performance, we propose an efficient method for computing a tight Particle Bounding Frustum (PBF) for each 3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also introduce a novel Bipolar Equiangular Projection (BEAP) representation to accelerate ray association under generic camera models. BEAP further provides a more uniform ray sampling strategy to apply supervision, which empirically improves reconstruction quality. Experiments on multiple pinhole and fisheye datasets show that our method consistently outperforms prior methods, establishing a new state-of-the-art in real-time neural rendering.",
    "arxiv_url": "http://arxiv.org/abs/2505.24053v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24053v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "authors": [
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Duochao Shi",
      "Akide Liu",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.",
    "arxiv_url": "http://arxiv.org/abs/2505.23734v2",
    "pdf_url": "http://arxiv.org/pdf/2505.23734v2",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "efficient",
      "lightweight",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mobi-$π$: Mobilizing Your Robot Learning Policy",
    "authors": [
      "Jingyun Yang",
      "Isabella Huang",
      "Brandon Vu",
      "Max Bajracharya",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.",
    "arxiv_url": "http://arxiv.org/abs/2505.23692v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23692v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D\n  Reconstruction and Novel View Synthesis",
    "authors": [
      "Nathaniel Burgdorfer",
      "Philippos Mordohai"
    ],
    "abstract": "In this work, we introduce an inference-time optimization framework utilizing triangles to represent the geometry and appearance of the scene. More specifically, we develop a scene optimization algorithm for triangle soup, a collection of disconnected semi-transparent triangle primitives. Compared to the current most-widely used primitives for 3D scene representation, namely Gaussian splats, triangles allow for more expressive color interpolation, and benefit from a large algorithmic infrastructure for downstream tasks. Triangles, unlike full-rank Gaussian kernels, naturally combine to form surfaces. We formulate connectivity forces between triangles during optimization, encouraging explicit, but soft, surface continuity in 3D. We perform experiments on a representative 3D reconstruction dataset and show competitive photometric and geometric results.",
    "arxiv_url": "http://arxiv.org/abs/2505.23642v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23642v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "face",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "authors": [
      "Chuandong Liu",
      "Huijiao Wang",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.23280v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23280v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient\n  Rendering",
    "authors": [
      "Jonas Kulhanek",
      "Marie-Julie Rakotosaona",
      "Fabian Manhardt",
      "Christina Tsalicoglou",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Songyou Peng",
      "Federico Tombari"
    ],
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.",
    "arxiv_url": "http://arxiv.org/abs/2505.23158v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23158v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "nerf",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2505.22978v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22978v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS Compression with Sparsity-guided Hierarchical Transform Coding",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its fast and high-quality rendering, but it has a very large memory footprint incurring high transmission and storage overhead. Recently, some neural compression methods, such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach of end-to-end optimized analysis-synthesis transforms which has been proven highly effective in neural signal compression. Without an appropriate analysis transform, signal correlations cannot be removed by sparse representation. Without such transforms the only way to remove signal redundancies is through entropy coding driven by a complex and expensive context modeling, which results in slower speed and suboptimal rate-distortion (R-D) performance. To overcome this weakness, we propose Sparsity-guided Hierarchical Transform Coding (SHTC), the first end-to-end optimized transform coding framework for 3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight context model. This joint optimization enables the transform to produce representations that approach the best R-D performance possible. The SHTC framework consists of a base layer using KLT for data decorrelation, and a sparsity-coded enhancement layer that compresses the KLT residuals to refine the representation. The enhancement encoder learns a linear transform to project high-dimensional inputs into a low-dimensional space, while the decoder unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct the residuals. All components are designed to be interpretable, allowing the incorporation of signal priors and fewer parameters than black-box transforms. This novel design significantly improves R-D performance with minimal additional parameters and computational overhead.",
    "arxiv_url": "http://arxiv.org/abs/2505.22908v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22908v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "head",
      "compression",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian\n  Splatting",
    "authors": [
      "Kornel Howil",
      "Joanna Waczyńska",
      "Piotr Borycki",
      "Tadeusz Dziarmaga",
      "Marcin Mazur",
      "Przemysław Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussians, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussians approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving a model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussians as a universal and efficient solution for multimodal style transfer.",
    "arxiv_url": "http://arxiv.org/abs/2505.22854v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22854v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "4d",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Zehao Li",
      "Hao Jiang",
      "Yujun Cai",
      "Jianing Chen",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Yiwei Wang",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ],
    "abstract": "Although dynamic scene reconstruction has long been a fundamental challenge in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a promising direction by enabling high-quality, real-time rendering through explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic reconstruction often suffer from \\textit{spatio-temporal incoherence} during initialization, where canonical Gaussians are constructed by aggregating observations from multiple frames without temporal distinction. This results in spatio-temporally entangled representations, making it difficult to model dynamic motion accurately. To overcome this limitation, we propose \\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a plug-and-play module that learns spatio-temporal probability distributions for each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation field, and a consistency regularization to jointly disentangle spatial and temporal patterns. Extensive experiments demonstrate that incorporating our module into existing 3DGS-based dynamic scene reconstruction frameworks leads to notable improvements in both reconstruction quality and spatio-temporal consistency across synthetic and real-world benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2505.22400v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22400v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "deformation",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction\n  in Dynamic Environments",
    "authors": [
      "Wancai Zheng",
      "Linlin Ou",
      "Jiajie He",
      "Libo Zhou",
      "Xinyi Yu",
      "Yan Wei"
    ],
    "abstract": "Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous Localization and Mapping (SLAM) have significantly progressed in tracking and high-fidelity mapping. However, their sequential optimization framework and sensitivity to dynamic objects limit real-time performance and robustness in real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for dynamic environments that decouples tracking and mapping through a parallelized framework. A probabilistic octree is employed to manage Gaussian primitives adaptively, enabling efficient initialization and pruning without hand-crafted thresholds. To robustly filter dynamic regions during tracking, we propose a training-free uncertainty estimator that fuses multi-modal residuals to estimate per-pixel motion uncertainty, achieving open-set dynamic object handling without reliance on semantic labels. Furthermore, a temporal encoder is designed to enhance rendering quality. Concurrently, low-dimensional features are efficiently transformed via a shallow multilayer perceptron to construct DINO features, which are then employed to enrich the Gaussian field and improve the robustness of uncertainty prediction. Extensive experiments on multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57 dB PSNR), while maintaining real-time performance and producing reusable, artifact-free static maps in dynamic environments.The project: https://aczheng-cai.github.io/up_slam.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2505.22335v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22335v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "dynamic",
      "efficient",
      "localization",
      "3d gaussian",
      "motion",
      "high-fidelity",
      "semantic",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade\n  Depth Loss",
    "authors": [
      "Wenjun Lu",
      "Haodong Chen",
      "Anqi Yi",
      "Yuk Ying Chung",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Novel view synthesis is a fundamental task in 3D computer vision that aims to reconstruct realistic images from a set of posed input views. However, reconstruction quality degrades significantly under sparse-view conditions due to limited geometric cues. Existing methods, such as Neural Radiance Fields (NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from blurred details and structural artifacts when trained with insufficient views. Recent works have identified the quality of rendered depth as a key factor in mitigating these artifacts, as it directly affects geometric accuracy and view consistency. In this paper, we address these challenges by introducing Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that progressively refines geometry from coarse to fine levels. Central to HDGS is a novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and estimated monocular depths across multiple spatial scales. By enforcing multi-scale depth consistency, our method substantially improves structural fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU benchmarks demonstrate that HDGS achieves state-of-the-art performance under sparse-view settings while maintaining efficient and high-quality rendering",
    "arxiv_url": "http://arxiv.org/abs/2505.22279v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22279v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "nerf",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hyperspectral Gaussian Splatting",
    "authors": [
      "Sunil Kumar Narayanan",
      "Lingjun Zhao",
      "Lu Gan",
      "Yongsheng Chen"
    ],
    "abstract": "Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise determination of nutritional elements in samples. Recently, 3D reconstruction methods have been used to create implicit neural representations of HSI scenes, which can help localize the target object's nutrient composition spatially and spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit representation that can render hyperspectral channel compositions of each spatial location from any viewing direction. However, it faces limitations in training time and rendering speed. In this paper, we propose Hyperspectral Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of the hyperspectral scenes and novel view synthesis for the entire spectral range. To enhance the model's ability to capture fine-grained reflectance variations across the light spectrum and leverage correlations between adjacent wavelengths for denoising, we introduce a wavelength encoder to generate wavelength-specific spherical harmonics offsets. We also introduce a novel Kullback--Leibler divergence-based loss to mitigate the spectral distribution gap between the rendered image and the ground truth. A diffusion model is further applied for denoising the rendered images and generating photorealistic hyperspectral images. We present extensive evaluations on five diverse hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our proposed HS-GS framework. The results demonstrate that HS-GS achieves new state-of-the-art performance among all previously published methods. Code will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2505.21890v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21890v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and\n  View-dependent Visibility",
    "authors": [
      "Yidi Li",
      "Jun Xiao",
      "Zhengda Lu",
      "Yiqun Wang",
      "Haiyong Jiang"
    ],
    "abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling.",
    "arxiv_url": "http://arxiv.org/abs/2505.21377v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21377v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure from Collision",
    "authors": [
      "Takuhiro Kaneko"
    ],
    "abstract": "Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.",
    "arxiv_url": "http://arxiv.org/abs/2505.21335v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21335v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics\n  Based Appearance-Medium Decoupling",
    "authors": [
      "Jieyu Yuan",
      "Yujun Li",
      "Yuanlin Zhang",
      "Chunle Guo",
      "Xiongxin Tang",
      "Ruixing Wang",
      "Chongyi Li"
    ],
    "abstract": "Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at https://bilityniu.github.io/3D-UIR.",
    "arxiv_url": "http://arxiv.org/abs/2505.21238v2",
    "pdf_url": "http://arxiv.org/pdf/2505.21238v2",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings\n  and Residual Gaussians",
    "authors": [
      "Weihang Liu",
      "Yuhui Zhong",
      "Yuke Li",
      "Xi Chen",
      "Jiadi Cui",
      "Honglong Zhang",
      "Lan Xu",
      "Xin Lou",
      "Yujiao Shi",
      "Jingyi Yu",
      "Yingliang Zhang"
    ],
    "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for applications such as AR navigation, UAV based inspection, and smart city digital twins. While aerial imagery offers broad coverage and complements limitations of ground-based data, reconstructing city-scale environments from such views remains challenging due to occlusions, incomplete geometry, and high memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve scalability and visual quality but remain limited by dense primitive usage, long training times, and poor suit ability for edge devices. We propose CityGo, a hybrid framework that combines textured proxy geometry with residual and surrounding 3D Gaussians for lightweight, photorealistic rendering of urban scenes from aerial perspectives. Our approach first extracts compact building proxy meshes from MVS point clouds, then uses zero order SH Gaussians to generate occlusion-free textures via image-based rendering and back-projection. To capture high-frequency details, we introduce residual Gaussians placed based on proxy-photo discrepancies and guided by depth priors. Broader urban context is represented by surrounding Gaussians, with importance-aware downsampling applied to non-critical regions to reduce redundancy. A tailored optimization strategy jointly refines proxy textures and Gaussian parameters, enabling real-time rendering of complex urban scenes on mobile GPUs with significantly reduced training and memory requirements. Extensive experiments on real-world aerial datasets demonstrate that our hybrid representation significantly reduces training time, achieving on average 1.4x speedup, while delivering comparable visual fidelity to pure 3D Gaussian Splatting approaches. Furthermore, CityGo enables real-time rendering of large-scale urban scenes on mobile consumer GPUs, with substantially reduced memory usage and energy consumption.",
    "arxiv_url": "http://arxiv.org/abs/2505.21041v3",
    "pdf_url": "http://arxiv.org/pdf/2505.21041v3",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "efficient",
      "lightweight",
      "3d gaussian",
      "geometry",
      "urban scene",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Runnan Chen",
      "Mingming Gong",
      "Dong Xu",
      "Tongliang Liu"
    ],
    "abstract": "Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.",
    "arxiv_url": "http://arxiv.org/abs/2505.20729v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20729v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "sparse-view",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian\n  Splatting",
    "authors": [
      "Zechen Li",
      "Lanqing Yang",
      "Yiheng Bian",
      "Hao Pan",
      "Yongjian Fu",
      "Yezhou Wang",
      "Yi-Chao Chen",
      "Guangtao Xue",
      "Ju Ren"
    ],
    "abstract": "This paper presents an innovative frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling, offering an advancement over the existing works limited to single-frequency modeling. Grounded in fundamental physics, we uncover the complex relationship between EM wave propagation behaviors and RF frequencies. Inspired by this, we design an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. By training the frequency-embedded 3DGS model, we can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. Finally, we propose a large-scale power angular spectrum (PAS) dataset containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments, and conduct extensive experiments to verify the effectiveness of our method. Our approach achieves an average Structural Similarity Index Measure (SSIM) up to 0.72, and a significant improvement up to 17.8% compared to the current state-of-the-art (SOTA) methods trained on individual test frequencies. Additionally, our method achieves an SSIM of 0.70 without prior training on these frequencies, which represents only a 2.8% performance drop compared to models trained with full PAS data. This demonstrates our model's capability to estimate PAS at unknown frequencies. For related code and datasets, please refer to https://github.com/sim-2-real/Wideband3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.20714v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20714v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for\n  Prior-free Motion Extrapolation",
    "authors": [
      "Jinsheng Quan",
      "Chunshi Wang",
      "Yawei Luo"
    ],
    "abstract": "This paper aims to model the dynamics of 3D Gaussians from visual observations to support temporal extrapolation. Existing dynamic 3D reconstruction methods often struggle to effectively learn underlying dynamics or rely heavily on manually defined physical priors, which limits their extrapolation capabilities. To address this issue, we propose a novel dynamic 3D Gaussian Splatting prior-free motion extrapolation framework based on particle dynamics systems. The core advantage of our method lies in its ability to learn differential equations that describe the dynamics of 3D Gaussians, and follow them during future frame extrapolation. Instead of simply fitting to the observed visual frame sequence, we aim to more effectively model the gaussian particle dynamics system. To this end, we introduce a dynamics latent state vector into the standard Gaussian kernel and design a dynamics latent space encoder to extract initial state. Subsequently, we introduce a Neural ODEs-based dynamics module that models the temporal evolution of Gaussian in dynamics latent space. Finally, a Gaussian kernel space decoder is used to decode latent state at the specific time step into the deformation. Experimental results demonstrate that the proposed method achieves comparable rendering quality with existing approaches in reconstruction tasks, and significantly outperforms them in future frame extrapolation. Our code is available at https://github.com/QuanJinSheng/ParticleGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.20270v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20270v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "dynamic",
      "deformation",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction\n  Using Blender",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Toshiki Watanabe",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Recent advancements in radiance field rendering, exemplified by Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly progressed 3D modeling and reconstruction. The use of multiple 360-degree omnidirectional images for these tasks is increasingly favored due to advantages in data acquisition and comprehensive scene capture. However, the inherent geometric distortions in common omnidirectional representations, such as equirectangular projection (particularly severe in polar regions and varying with latitude), pose substantial challenges to achieving high-fidelity 3D reconstructions. Current datasets, while valuable, often lack the specific focus, scene composition, and ground truth granularity required to systematically benchmark and drive progress in overcoming these omnidirectional-specific challenges. To address this critical gap, we introduce Omnidirectional Blender 3D (OB3D), a new synthetic dataset curated for advancing 3D reconstruction from multiple omnidirectional images. OB3D features diverse and complex 3D scenes generated from Blender 3D projects, with a deliberate emphasis on challenging scenarios. The dataset provides comprehensive ground truth, including omnidirectional RGB images, precise omnidirectional camera parameters, and pixel-aligned equirectangular maps for depth and normals, alongside evaluation metrics. By offering a controlled yet challenging environment, OB3Daims to facilitate the rigorous evaluation of existing methods and prompt the development of new techniques to enhance the accuracy and reliability of 3D reconstruction from omnidirectional images.",
    "arxiv_url": "http://arxiv.org/abs/2505.20126v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20126v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather\n  Synthesis In Real Time",
    "authors": [
      "Chen Sang",
      "Yeqiang Qian",
      "Jiale Zhang",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "For tasks such as urban digital twins, VR/AR/game scene design, or creating synthetic films, the traditional industrial approach often involves manually modeling scenes and using various rendering engines to complete the rendering process. This approach typically requires high labor costs and hardware demands, and can result in poor quality when replicating complex real-world scenes. A more efficient approach is to use data from captured real-world scenes, then apply reconstruction and rendering algorithms to quickly recreate the authentic scene. However, current algorithms are unable to effectively reconstruct and render real-world weather effects. To address this, we propose a framework based on gaussian splatting, that can reconstruct real scenes and render them under synthesized 4D weather effects. Our work can simulate various common weather effects by applying Gaussians modeling and rendering techniques. It supports continuous dynamic weather changes and can easily control the details of the effects. Additionally, our work has low hardware requirements and achieves real-time rendering performance. The result demos can be accessed on our project homepage: weathermagician.github.io",
    "arxiv_url": "http://arxiv.org/abs/2505.19919v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19919v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "4d",
      "vr",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian\n  Splatting with Dense Point Cloud",
    "authors": [
      "Natsuki Takama",
      "Shintaro Ito",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Gaussian Splatting (GS) has gained attention as a fast and effective method for novel view synthesis. It has also been applied to 3D reconstruction using multi-view images and can achieve fast and accurate 3D reconstruction. However, GS assumes that the input contains a large number of multi-view images, and therefore, the reconstruction accuracy significantly decreases when only a limited number of input images are available. One of the main reasons is the insufficient number of 3D points in the sparse point cloud obtained through Structure from Motion (SfM), which results in a poor initialization for optimizing the Gaussian primitives. We propose a new 3D reconstruction method, called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along with COLMAP MVS to generate highly accurate and dense 3D point clouds, which are then used to initialize 2D Gaussians. Through experiments on the DTU dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of objects using just three images. The project page is available at https://gsisaoki.github.io/SPARSE2DGS/",
    "arxiv_url": "http://arxiv.org/abs/2505.19854v2",
    "pdf_url": "http://arxiv.org/pdf/2505.19854v2",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "sparse-view",
      "motion",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple\n  Buffers",
    "authors": [
      "Haofan Ren",
      "Zunjie Zhu",
      "Xiang Chen",
      "Ming Lu",
      "Rongfeng Lu",
      "Chenggang Yan"
    ],
    "abstract": "Neural fields are now the central focus of research in 3D vision and computer graphics. Existing methods mainly focus on various scene representations, such as neural points and 3D Gaussians. However, few works have studied the rendering process to enhance the neural fields. In this work, we propose a plug-in method named K-Buffers that leverages multiple buffers to improve the rendering performance. Our method first renders K buffers from scene representations and constructs K pixel-wise feature maps. Then, We introduce a K-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally, we adopt a feature decoder to generate the rendering image. We also introduce an acceleration strategy to improve rendering speed and quality. We apply our method to well-known radiance field baselines, including neural point fields and 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our method effectively enhances the rendering performance of neural point fields and 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.19564v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19564v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "acceleration"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse\n  Views by Jointly Training Hemisphere Sampled Synthetic Images",
    "authors": [
      "Guangan Chen",
      "Anh Minh Truong",
      "Hanhe Lin",
      "Michiel Vlaminck",
      "Wilfried Philips",
      "Hiep Luong"
    ],
    "abstract": "Novel view synthesis in 360$^\\circ$ scenes from extremely sparse input views is essential for applications like virtual reality and augmented reality. This paper presents a novel framework for novel view synthesis in extremely sparse-view cases. As typical structure-from-motion methods are unable to estimate camera poses in extremely sparse-view cases, we apply DUSt3R to estimate camera poses and generate a dense point cloud. Using the poses of estimated cameras, we densely sample additional views from the upper hemisphere space of the scenes, from which we render synthetic images together with the point cloud. Training 3D Gaussian Splatting model on a combination of reference images from sparse views and densely sampled synthetic images allows a larger scene coverage in 3D space, addressing the overfitting challenge due to the limited input in sparse-view cases. Retraining a diffusion-based image enhancement model on our created dataset, we further improve the quality of the point-cloud-rendered images by removing artifacts. We compare our framework with benchmark methods in cases of only four input views, demonstrating significant improvement in novel view synthesis under extremely sparse-view conditions for 360$^\\circ$ scenes.",
    "arxiv_url": "http://arxiv.org/abs/2505.19264v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19264v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "sparse view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Adrien Deliege",
      "Abdullah Hamdi",
      "Silvio Giancola",
      "Anthony Cioppa",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Andrea Tagliasacchi",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for a triangle comeback. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible with standard graphics stacks and GPU hardware, and highly efficient: for the \\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks. The project page is https://trianglesplatting.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2505.19175v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19175v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FHGS: Feature-Homogenized Gaussian Splatting",
    "authors": [
      "Q. G. Duan",
      "Benyun Zhao",
      "Mingqiao Han Yijun Huang",
      "Ben M. Chen"
    ],
    "abstract": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently achieved notable advances. Although 3DGS related methods have efficient rendering capabilities, they fail to address the inherent contradiction between the anisotropic color representation of gaussian primitives and the isotropic requirements of semantic features, leading to insufficient cross-view feature consistency. To overcome the limitation, we proposes $\\textit{FHGS}$ (Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework inspired by physical models, which can achieve high-precision mapping of arbitrary 2D features from pre-trained models to 3D scenes while preserving the real-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$ introduces the following innovations: Firstly, a universal feature fusion architecture is proposed, enabling robust embedding of large-scale pre-trained models' semantic features (e.g., SAM, CLIP) into sparse 3D structures. Secondly, a non-differentiable feature fusion mechanism is introduced, which enables semantic features to exhibit viewpoint independent isotropic distributions. This fundamentally balances the anisotropic rendering of gaussian primitives and the isotropic expression of features; Thirdly, a dual-driven optimization strategy inspired by electric potential fields is proposed, which combines external supervision from semantic feature fields with internal primitive clustering guidance. This mechanism enables synergistic optimization of global semantic alignment and local structural consistency. More interactive results can be accessed on: https://fhgs.cuastro.org/.",
    "arxiv_url": "http://arxiv.org/abs/2505.19154v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19154v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "semantic",
      "understanding",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal\n  infrared Novel-view Synthesis",
    "authors": [
      "Myeongseok Nam",
      "Wongi Park",
      "Minsol Kim",
      "Hyejin Hur",
      "Soomok Lee"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR) imaging has gained attention in novel-view synthesis, showing real-time rendering. However, novel-view synthesis with thermal infrared images suffers from transmission effects, emissivity, and low resolution, leading to floaters and blur effects in rendered images. To address these problems, we introduce Veta-GS, which leverages a view-dependent deformation field and a Thermal Feature Extractor (TFE) to precisely capture subtle thermal variations and maintain robustness. Specifically, we design view-dependent deformation field that leverages camera position and viewing direction, which capture thermal variations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and MonoSSIM loss, which consider appearance, edge, and frequency to maintain robustness. Extensive experiments on the TI-NSD benchmark show that our method achieves better performance over existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.19138v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19138v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale\n  Scenes",
    "authors": [
      "Tianchen Deng",
      "Wenhua Wu",
      "Junjie He",
      "Yue Pan",
      "Xirui Jiang",
      "Shenghai Yuan",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "3D Gaussian Splatting has recently shown promising results in dense visual SLAM. However, existing 3DGS-based SLAM methods are all constrained to small-room scenarios and struggle with memory explosion in large-scale scenes and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We design a novel voxel-based progressive 3D Gaussian mapping method with multiple submaps for compact and accurate scene representation in large-scale and long-sequence scenes. This allows us to scale up to arbitrary scenes and improves robustness (even under pose drifts). In addition, we propose a 2D-3D fusion camera tracking method to achieve robust and accurate camera tracking in both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D Gaussian loop closure method to eliminate pose drift. We further propose a submap fusion method with online distillation to achieve global consistency in large-scale scenes when detecting a loop. Experiments on various indoor and outdoor datasets demonstrate the superiority and generalizability of the proposed framework. The code will be open source on https://github.com/dtc111111/vpgs-slam.",
    "arxiv_url": "http://arxiv.org/abs/2505.18992v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18992v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "compact",
      "tracking",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "mapping",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian\n  Splatting",
    "authors": [
      "Yitian Yuan",
      "Qianyue He"
    ],
    "abstract": "Recent works demonstrate the advantages of hardware rasterization for 3D Gaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized graphics and fixed memory footprint. However, extending these benefits to backward-pass gradient computation remains challenging due to graphics pipeline constraints. We present a differentiable hardware rasterizer for 3DGS that overcomes the memory and performance limitations of tile-based software rasterization. Our solution employs programmable blending for per-pixel gradient computation combined with a hybrid gradient reduction strategy (quad-level + subgroup) in fragment shaders, achieving over 10x faster backward rasterization versus naive atomic operations and 3x speedup over the canonical tile-based rasterizer. Systematic evaluation reveals 16-bit render targets (float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving higher gradient accuracy among mixed-precision rendering formats with execution speeds second only to unorm8, while float32 texture incurs severe forward pass performance degradation due to suboptimal hardware optimizations. Our method with float16 formats demonstrates 3.07x acceleration in full pipeline execution (forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset, outperforming the baseline tile-based renderer while preserving hardware rasterization's memory efficiency advantages -- incurring merely 2.67% of the memory overhead required for splat sorting operations. This work presents a unified differentiable hardware rasterization method that simultaneously optimizes runtime and memory usage for 3DGS, making it particularly suitable for resource-constrained devices with limited memory capacity.",
    "arxiv_url": "http://arxiv.org/abs/2505.18764v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18764v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.GR",
      "I.3.7; I.3.1"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "efficient",
      "nerf",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene\n  Reconstruction via Gaussian Splatting",
    "authors": [
      "Shiyun Xie",
      "Zhiru Wang",
      "Yinghao Zhu",
      "Xu Wang",
      "Chengwei Pan",
      "Xiwang Dong"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis (NVS) with its real-time rendering capabilities and superior quality. However, it encounters challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose SuperGS, an expansion of Scaffold-GS designed with a two-stage coarse-to-fine training framework. In the low-resolution stage, we introduce a latent feature field to represent the low-resolution scene, which serves as both the initialization and foundational information for super-resolution optimization. In the high-resolution stage, we propose a multi-view consistent densification strategy that backprojects high-resolution depth maps based on error maps and employs a multi-view voting mechanism, mitigating ambiguities caused by multi-view inconsistencies in the pseudo labels provided by 2D prior models while avoiding Gaussian redundancy. Furthermore, we model uncertainty through variational feature learning and use it to guide further scene representation refinement and adjust the supervisory effect of pseudo-labels, ensuring consistent and detailed scene reconstruction. Extensive experiments demonstrate that SuperGS outperforms state-of-the-art HRNVS methods on both forward-facing and 360-degree datasets.",
    "arxiv_url": "http://arxiv.org/abs/2505.18649v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18649v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose\n  and Appearance",
    "authors": [
      "Jack Goffinet",
      "Youngjo Min",
      "Carlo Tomasi",
      "David E. Carlson"
    ],
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose Pose Splatter, a novel framework leveraging shape carving and 3D Gaussian splatting to model the complete pose and appearance of laboratory animals without prior knowledge of animal geometry, per-frame optimization, or manual annotations. We also propose a novel rotation-invariant visual embedding technique for encoding pose and appearance, designed to be a plug-in replacement for 3D keypoint data in downstream behavioral analyses. Experiments on datasets of mice, rats, and zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably, Pose Splatter represents subtle variations in pose, provides better low-dimensional pose embeddings over state-of-the-art as evaluated by humans, and generalizes to unseen data. By eliminating annotation and per-frame optimization bottlenecks, Pose Splatter enables analysis of large-scale, longitudinal behavior needed to map genotype, neural activity, and micro-behavior at unprecedented resolution.",
    "arxiv_url": "http://arxiv.org/abs/2505.18342v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18342v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting",
    "authors": [
      "Karly Hou",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Recently, Gaussian Splatting methods have emerged as a desirable substitute for prior Radiance Field methods for novel-view synthesis of scenes captured with multi-view images or videos. In this work, we propose a novel extension to 4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual learning, we hierarchically decompose the dynamic scene into a \"video-segment-frame\" structure, with segments dynamically adjusted by optical flow. Then, instead of directly predicting the time-dependent signals, we model the signal as the sum of video-constant values, segment-constant values, and frame-specific residuals, as inspired by the success of residual learning. This approach allows more flexible models that adapt to highly variable scenes. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets, with the greatest improvements on complex scenes with large movements, occlusions, and fine details, where current methods degrade most.",
    "arxiv_url": "http://arxiv.org/abs/2505.18306v2",
    "pdf_url": "http://arxiv.org/pdf/2505.18306v2",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for\n  Detail-Preserving Rendering of Large-Scale Unbounded Scenes",
    "authors": [
      "Haihong Xiao",
      "Jianan Zou",
      "Yuxin Zhou",
      "Ying He",
      "Wenxiong Kang"
    ],
    "abstract": "We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor environments. SplatCo builds upon two novel components: (1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features that represent fine surface details. This fusion is achieved through a novel hierarchical compensation strategy, ensuring both global consistency and local detail preservation; and (2) a cross-view assisted training strategy that enhances multi-view consistency by synchronizing gradient updates across viewpoints, applying visibility-aware densification, and pruning overfitted or inaccurate Gaussians based on structural consistency. Through joint optimization of structural representation and multi-view coherence, SplatCo effectively reconstructs fine-grained geometric structures and complex textures in large-scale scenes. Comprehensive evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity, Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo consistently achieves higher reconstruction quality than state-of-the-art methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These results establish a new benchmark for high-fidelity rendering of large-scale unbounded scenes. Code and additional information are available at https://github.com/SCUT-BIP-Lab/SplatCo.",
    "arxiv_url": "http://arxiv.org/abs/2505.17951v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17951v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human\n  Head Synthesis",
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
    "arxiv_url": "http://arxiv.org/abs/2505.17590v2",
    "pdf_url": "http://arxiv.org/pdf/2505.17590v2",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection\n  via Gaussian Splatting and Language-Guided Segmentation",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Homayoun Najjaran"
    ],
    "abstract": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such as infrastructure monitoring, structural assessment, and environmental surveying. While traditional photogrammetry techniques enable geometric modeling, they lack semantic interpretability, limiting their effectiveness for automated inspection workflows. Recent advances in neural rendering and 3D Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but similarly lack scene-level understanding.   In this work, we present a UAV-based pipeline that extends Feature-3DGS for language-guided 3D segmentation. We leverage LSeg-based feature fields with CLIP embeddings to generate heatmaps in response to language prompts. These are thresholded to produce rough segmentations, and the highest-scoring point is then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view renderings. Our results highlight the strengths and limitations of various feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful structure in large-scale outdoor environments. We demonstrate that this hybrid approach enables flexible, language-driven interaction with photorealistic 3D reconstructions, opening new possibilities for semantic aerial inspection and scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2505.17402v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17402v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "efficient",
      "3d gaussian",
      "segmentation",
      "high-fidelity",
      "semantic",
      "gaussian splatting",
      "understanding",
      "ar",
      "survey",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric\n  Rendering",
    "authors": [
      "Zhongpai Gao",
      "Meng Zheng",
      "Benjamin Planche",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: https://gaozhongpai.github.io/renderfm/.",
    "arxiv_url": "http://arxiv.org/abs/2505.17338v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17338v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "neural rendering",
      "efficient",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane\n  Deformation and Latent Diffusion",
    "authors": [
      "Asrar Alruwayqi"
    ],
    "abstract": "We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.   In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.   Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs.",
    "arxiv_url": "http://arxiv.org/abs/2505.16535v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16535v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "sparse-view",
      "dynamic",
      "compact",
      "efficient",
      "4d",
      "deformation",
      "motion",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video\n  Reconstruction",
    "authors": [
      "Jiacong Chen",
      "Qingyu Mao",
      "Youneng Bao",
      "Xiandong Meng",
      "Fanyang Meng",
      "Ronggang Wang",
      "Yongsheng Liang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers rapid responsiveness and immersive experiences. However, existing online methods face challenge in prohibitive storage requirements primarily due to point-wise modeling that fails to exploit the motion properties. To address this limitation, we propose a novel Compact Gaussian Streaming (ComGS) framework, leveraging the locality and consistency of motion in dynamic scene, that models object-consistent Gaussian point motion through keypoint-driven motion representation. By transmitting only the keypoint attributes, this framework provides a more storage-efficient solution. Specifically, we first identify a sparse set of motion-sensitive keypoints localized within motion regions using a viewspace gradient difference strategy. Equipped with these keypoints, we propose an adaptive motion-driven mechanism that predicts a spatial influence field for propagating keypoint motion to neighboring Gaussian points with similar motion. Moreover, ComGS adopts an error-aware correction strategy for key frame reconstruction that selectively refines erroneous regions and mitigates error accumulation without unnecessary overhead. Overall, ComGS achieves a remarkable storage reduction of over 159 X compared to 3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining competitive visual fidelity and rendering speed. Our code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2505.16533v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16533v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "compact",
      "dynamic",
      "efficient",
      "3d gaussian",
      "motion",
      "head",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater\n  Scene Reconstruction",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics.",
    "arxiv_url": "http://arxiv.org/abs/2505.15737v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15737v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided\n  Gaussian Splatting",
    "authors": [
      "Zane K J Hartley",
      "Lewis A G Stuart",
      "Andrew P French",
      "Michael P Pound"
    ],
    "abstract": "Recent years have seen substantial improvements in the ability to generate synthetic 3D objects using AI. However, generating complex 3D objects, such as plants, remains a considerable challenge. Current generative 3D models struggle with plant generation compared to general objects, limiting their usability in plant analysis tools, which require fine detail and accurate geometry. We introduce PlantDreamer, a novel approach to 3D synthetic plant generation, which can achieve greater levels of realism for complex plant geometry and textures than available text-to-3D models. To achieve this, our new generation pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an adaptable Gaussian culling algorithm, which directly improve textural realism and geometric integrity of generated 3D plant models. Additionally, PlantDreamer enables both purely synthetic plant generation, by leveraging L-System-generated meshes, and the enhancement of real-world plant point clouds by converting them into 3D Gaussian Splats. We evaluate our approach by comparing its outputs with state-of-the-art text-to-3D models, demonstrating that PlantDreamer outperforms existing methods in producing high-fidelity synthetic plants. Our results indicate that our approach not only advances synthetic plant generation, but also facilitates the upgrading of legacy point cloud datasets, making it a valuable tool for 3D phenotyping applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.15528v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15528v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.2.10; I.3.0; I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream\n  Generation",
    "authors": [
      "Yuchen Li",
      "Chaoran Feng",
      "Zhenyu Tang",
      "Kaiyuan Deng",
      "Wangbo Yu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic event dataset for high-fidelity event vision tasks, captured from real-world sparse multi-view RGB images. Existing event datasets are often synthesized from dense RGB videos, which typically lack viewpoint diversity and geometric consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E overcomes these limitations by first reconstructing photorealistic static scenes using 3D Gaussian Splatting, and subsequently employing a novel, physically-informed event simulation pipeline. This pipeline generally integrates adaptive trajectory interpolation with physically-consistent event contrast threshold modeling. Such an approach yields temporally dense and geometrically consistent event streams under diverse motion and lighting conditions, while ensuring strong alignment with underlying scene structures. Experimental results on event-based 3D reconstruction demonstrate GS2E's superior generalization capabilities and its practical value as a benchmark for advancing event vision research.",
    "arxiv_url": "http://arxiv.org/abs/2505.15287v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15287v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d reconstruction",
      "3d gaussian",
      "motion",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to\n  Computed Tomography",
    "authors": [
      "Yifan Liu",
      "Wuyang Li",
      "Weihao Yu",
      "Chenxin Li",
      "Alexandre Alahi",
      "Max Meng",
      "Yixuan Yuan"
    ],
    "abstract": "Computed Tomography serves as an indispensable tool in clinical workflows, providing non-invasive visualization of internal anatomical structures. Existing CT reconstruction works are limited to small-capacity model architecture and inflexible volume representation. In this work, we present X-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for reconstructing 3D CT volumes from sparse-view 2D X-ray projections. X-GRM employs a scalable transformer-based architecture to encode sparse-view X-ray inputs, where tokens from different views are integrated efficiently. Then, these tokens are decoded into a novel volume representation, named Voxel-based Gaussian Splatting (VoxGS), which enables efficient CT volume extraction and differentiable X-ray rendering. This combination of a high-capacity model and flexible volume representation, empowers our model to produce high-quality reconstructions from various testing inputs, including in-domain and out-domain X-ray projections. Our codes are available at: https://github.com/CUHK-AIM-Group/X-GRM.",
    "arxiv_url": "http://arxiv.org/abs/2505.15235v2",
    "pdf_url": "http://arxiv.org/pdf/2505.15235v2",
    "published_date": "2025-05-21",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "sparse-view",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth\n  Foundation Models",
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ],
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at https://github.com/CUHK-AIM-Group/MonoSplat.",
    "arxiv_url": "http://arxiv.org/abs/2505.15185v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15185v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with\n  Gaussian Point Cloud Compression",
    "authors": [
      "Kangli Wang",
      "Shihao Li",
      "Qianxi Yi",
      "Wei Gao"
    ],
    "abstract": "Recently, immersive media and autonomous driving applications have significantly advanced through 3D Gaussian Splatting (3DGS), which offers high-fidelity rendering and computational efficiency. Despite these advantages, 3DGS as a display-oriented representation requires substantial storage due to its numerous Gaussian attributes. Current compression methods have shown promising results but typically neglect the compression of Gaussian spatial positions, creating unnecessary bitstream overhead. We conceptualize Gaussian primitives as point clouds and propose leveraging point cloud compression techniques for more effective storage. AI-based point cloud compression demonstrates superior performance and faster inference compared to MPEG Geometry-based Point Cloud Compression (G-PCC). However, direct application of existing models to Gaussian compression may yield suboptimal results, as Gaussian point clouds tend to exhibit globally sparse yet locally dense geometric distributions that differ from conventional point cloud characteristics. To address these challenges, we introduce GausPcgc for Gaussian point cloud geometry compression along with a specialized training dataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud compression into Gaussian compression pipelines, achieving superior compression ratios. The framework complements existing Gaussian compression methods while delivering significant performance improvements. All code, data, and pre-trained models will be publicly released to facilitate further research advances in this field.",
    "arxiv_url": "http://arxiv.org/abs/2505.18197v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18197v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "efficient",
      "3d gaussian",
      "geometry",
      "head",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scan, Materialize, Simulate: A Generalizable Framework for Physically\n  Grounded Robot Planning",
    "authors": [
      "Amine Elhafsi",
      "Daniel Morton",
      "Marco Pavone"
    ],
    "abstract": "Autonomous robots must reason about the physical consequences of their actions to operate effectively in unstructured, real-world environments. We present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D Gaussian Splatting for accurate scene reconstruction, visual foundation models for semantic segmentation, vision-language models for material property inference, and physics simulation for reliable prediction of action outcomes. By integrating these components, SMS enables generalizable physical reasoning and object-centric planning without the need to re-learn foundational physical dynamics. We empirically validate SMS in a billiards-inspired manipulation task and a challenging quadrotor landing scenario, demonstrating robust performance on both simulated domain transfer and real-world experiments. Our results highlight the potential of bridging differentiable rendering for scene reconstruction, foundation models for semantic understanding, and physics-based simulation to achieve physically grounded robot planning across diverse settings.",
    "arxiv_url": "http://arxiv.org/abs/2505.14938v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14938v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d gaussian",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a\n  Single Image",
    "authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Qingshan Xu",
      "Yuan Zhou",
      "Long Chen",
      "Hanwang Zhang"
    ],
    "abstract": "Personalizing 3D scenes from a single reference image enables intuitive user-guided editing, which requires achieving both multi-view consistency across perspectives and referential consistency with the input image. However, these goals are particularly challenging due to the viewpoint bias caused by the limited perspective provided in a single image. Lacking the mechanisms to effectively expand reference information beyond the original view, existing methods of image-conditioned 3DGS personalization often suffer from this viewpoint bias and struggle to produce consistent results. Therefore, in this paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS), a framework that progressively propagates the single-view reference appearance to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D generation and iterative LoRA fine-tuning to extract and extend the reference appearance, and finally produces faithful multi-view guidance images and the personalized 3DGS outputs through a view-consistent generation process guided by geometric cues. Extensive experiments on real-world scenes show that our CP-GS effectively mitigates the viewpoint bias, achieving high-quality personalization that significantly outperforms existing methods. The code will be released at https://github.com/Yuxuan-W/CP-GS.",
    "arxiv_url": "http://arxiv.org/abs/2505.14537v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14537v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene\n  Reconstruction",
    "authors": [
      "Zhenyu Bao",
      "Qing Li",
      "Guibiao Liao",
      "Zhongyuan Zhao",
      "Kanglin Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention in streamable dynamic novel view synthesis (DNVS) for its photorealistic rendering capability and computational efficiency. Despite much progress in improving rendering quality and optimization strategies, 3DGS-based streamable dynamic scene reconstruction still suffers from flickering artifacts and storage inefficiency, and struggles to model the emerging objects. To tackle this, we introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related 3DGs are implemented according to the motion mask and the clustering-based convex hull algorithm. The rigid deformation is applied to the motion-related 3DGs for modeling the dynamic, and the attention-based optimization on the motion-related 3DGs enables the reconstruction of the emerging objects. As the deformation and optimization are only conducted on the motion-related 3DGs, MGStream avoids flickering artifacts and improves the storage efficiency. Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that MGStream surpasses existing streaming 3DGS-based approaches in terms of rendering quality, training/storage efficiency and temporal consistency. Our code is available at: https://github.com/pcl3dv/MGStream.",
    "arxiv_url": "http://arxiv.org/abs/2505.13839v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13839v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "deformation",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recollection from Pensieve: Novel View Synthesis via Learning from\n  Uncalibrated Videos",
    "authors": [
      "Ruoyu Wang",
      "Yi Ma",
      "Shenghua Gao"
    ],
    "abstract": "Currently almost all state-of-the-art novel view synthesis and reconstruction models rely on calibrated cameras or additional geometric priors for training. These prerequisites significantly limit their applicability to massive uncalibrated data. To alleviate this requirement and unlock the potential for self-supervised training on large-scale uncalibrated videos, we propose a novel two-stage strategy to train a view synthesis model from only raw video frames or multi-view images, without providing camera parameters or other priors. In the first stage, we learn to reconstruct the scene implicitly in a latent space without relying on any explicit 3D representation. Specifically, we predict per-frame latent camera and scene context features, and employ a view synthesis model as a proxy for explicit rendering. This pretraining stage substantially reduces the optimization complexity and encourages the network to learn the underlying 3D consistency in a self-supervised manner. The learned latent camera and implicit scene representation have a large gap compared with the real 3D world. To reduce this gap, we introduce the second stage training by explicitly predicting 3D Gaussian primitives. We additionally apply explicit Gaussian Splatting rendering loss and depth projection loss to align the learned latent representations with physically grounded 3D geometry. In this way, Stage 1 provides a strong initialization and Stage 2 enforces 3D consistency - the two stages are complementary and mutually beneficial. Extensive experiments demonstrate the effectiveness of our approach, achieving high-quality novel view synthesis and accurate camera pose estimation, compared to methods that employ supervision with calibration, pose, or depth information. The code is available at https://github.com/Dwawayu/Pensieve.",
    "arxiv_url": "http://arxiv.org/abs/2505.13440v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13440v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
    "authors": [
      "Seungjun Oh",
      "Younggeun Lee",
      "Hyejin Jeon",
      "Eunbyung Park"
    ],
    "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.13215v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13215v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "3d gaussian",
      "motion",
      "head",
      "high-fidelity",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy",
    "authors": [
      "Chenyu Xu",
      "Zhouyu Jin",
      "Chengkang Shen",
      "Hao Zhu",
      "Zhan Ma",
      "Bo Xiong",
      "You Zhou",
      "Xun Cao",
      "Ning Gu"
    ],
    "abstract": "Compared to light-field microscopy (LFM), which enables high-speed volumetric imaging but suffers from non-uniform spatial sampling, Fourier light-field microscopy (FLFM) introduces sub-aperture division at the pupil plane, thereby ensuring spatially invariant sampling and enhancing spatial resolution. Conventional FLFM reconstruction methods, such as Richardson-Lucy (RL) deconvolution, exhibit poor axial resolution and signal degradation due to the ill-posed nature of the inverse problem. While data-driven approaches enhance spatial resolution by leveraging high-quality paired datasets or imposing structural priors, Neural Radiance Fields (NeRF)-based methods employ physics-informed self-supervised learning to overcome these limitations, yet they are hindered by substantial computational costs and memory demands. Therefore, we propose 3D Gaussian Adaptive Tomography (3DGAT) for FLFM, a 3D gaussian splatting based self-supervised learning framework that significantly improves the volumetric reconstruction quality of FLFM while maintaining computational efficiency. Experimental results indicate that our approach achieves higher resolution and improved reconstruction accuracy, highlighting its potential to advance FLFM imaging and broaden its applications in 3D optical microscopy.",
    "arxiv_url": "http://arxiv.org/abs/2505.12875v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12875v1",
    "published_date": "2025-05-19",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D\n  Semantic Occupancy",
    "authors": [
      "Luyao Lei",
      "Shuo Xu",
      "Yifan Bai",
      "Xing Wei"
    ],
    "abstract": "The performance of multi-modal 3D occupancy prediction is limited by ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion strategies and surface detail loss caused by sparse, noisy annotations. The mismatch stems from the heterogeneous scale and distribution of point cloud and image features, leading to biased matching under fixed neighborhood fusion. To address this, we propose a target-scale adaptive, bidirectional symmetric retrieval mechanism. It expands the neighborhood for large targets to enhance context awareness and shrinks it for small ones to improve efficiency and suppress noise, enabling accurate cross-modal feature alignment. This mechanism explicitly establishes spatial correspondences and improves fusion accuracy. For surface detail loss, sparse labels provide limited supervision, resulting in poor predictions for small objects. We introduce an improved volume rendering pipeline based on 3D Gaussian Splatting, which takes fused features as input to render images, applies photometric consistency supervision, and jointly optimizes 2D-3D consistency. This enhances surface detail reconstruction while suppressing noise propagation. In summary, we propose TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy prediction, enhanced by volume rendering supervision. Experiments on the nuScenes and SemanticKITTI benchmarks validate its effectiveness.",
    "arxiv_url": "http://arxiv.org/abs/2505.12693v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12693v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "geometry",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey",
    "authors": [
      "Calvin Galagain",
      "Martyna Poreba",
      "François Goulette"
    ],
    "abstract": "In embedded systems, robots must perceive and interpret their environment efficiently to operate reliably in real-world conditions. Visual Semantic SLAM (Simultaneous Localization and Mapping) enhances standard SLAM by incorporating semantic information into the map, enabling more informed decision-making. However, implementing such systems on resource-limited hardware involves trade-offs between accuracy, computing efficiency, and power usage.   This paper provides a comparative review of recent Semantic Visual SLAM methods with a focus on their applicability to embedded platforms. We analyze three main types of architectures - Geometric SLAM, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting - and evaluate their performance on constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their accuracy, segmentation quality, memory usage, and energy consumption.   Our results show that methods based on NeRF and Gaussian Splatting achieve high semantic detail but demand substantial computing resources, limiting their use on embedded devices. In contrast, Semantic Geometric SLAM offers a more practical balance between computational cost and accuracy. The review highlights a need for SLAM algorithms that are better adapted to embedded environments, and it discusses key directions for improving their efficiency through algorithm-hardware co-design.",
    "arxiv_url": "http://arxiv.org/abs/2505.12384v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12384v1",
    "published_date": "2025-05-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "efficient",
      "localization",
      "mapping",
      "nerf",
      "3d gaussian",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar",
      "survey"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects\n  Based on Appearance and Geometric Complexity",
    "authors": [
      "Takuya Ikeda",
      "Sergey Zakharov",
      "Muhammad Zubair Irshad",
      "Istvan Balazs Opra",
      "Shun Iwase",
      "Dian Chen",
      "Mark Tjersland",
      "Robert Lee",
      "Alexandre Dilly",
      "Rares Ambrus",
      "Koichi Nishiwaki"
    ],
    "abstract": "We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2505.11905v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11905v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "tracking",
      "3d gaussian",
      "geometry",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos",
    "authors": [
      "Hongyi Zhou",
      "Xiaogang Wang",
      "Yulan Guo",
      "Kai Xu"
    ],
    "abstract": "Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.11868v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11868v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting as a Unified Representation for Autonomy in\n  Unstructured Environments",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "In this work, we argue that Gaussian splatting is a suitable unified representation for autonomous robot navigation in large-scale unstructured outdoor environments. Such environments require representations that can capture complex structures while remaining computationally tractable for real-time navigation. We demonstrate that the dense geometric and photometric information provided by a Gaussian splatting representation is useful for navigation in unstructured environments. Additionally, semantic information can be embedded in the Gaussian map to enable large-scale task-driven navigation. From the lessons learned through our experiments, we highlight several challenges and opportunities arising from the use of such a representation for robot autonomy.",
    "arxiv_url": "http://arxiv.org/abs/2505.11794v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11794v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "semantic",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views",
    "authors": [
      "Abhishek Kashyap",
      "Henrik Andreasson",
      "Todor Stoyanov"
    ],
    "abstract": "Vision based robot manipulation uses cameras to capture one or more images of a scene containing the objects to be manipulated. Taking multiple images can help if any object is occluded from one viewpoint but more visible from another viewpoint. However, the camera has to be moved to a sequence of suitable positions for capturing multiple images, which requires time and may not always be possible, due to reachability constraints. So while additional images can produce more accurate grasp poses due to the extra information available, the time-cost goes up with the number of additional views sampled. Scene representations like Gaussian Splatting are capable of rendering accurate photorealistic virtual images from user-specified novel viewpoints. In this work, we show initial results which indicate that novel view synthesis can provide additional context in generating grasp poses. Our experiments on the Graspnet-1billion dataset show that novel views contributed force-closure grasps in addition to the force-closure grasps obtained from sparsely sampled real views while also improving grasp coverage. In the future we hope this work can be extended to improve grasp extraction from radiance fields constructed with a single input image, using for example diffusion models or generalizable radiance fields.",
    "arxiv_url": "http://arxiv.org/abs/2505.11467v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11467v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian\n  Splats",
    "authors": [
      "Simeon Adebola",
      "Shuangyu Xie",
      "Chung Min Kim",
      "Justin Kerr",
      "Bart M. van Marrewijk",
      "Mieke van Vlaardingen",
      "Tim van Daalen",
      "E. N. van Loo",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Rick van de Zedde",
      "Ken Goldberg"
    ],
    "abstract": "Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/",
    "arxiv_url": "http://arxiv.org/abs/2505.10923v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10923v2",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "deformation",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced\n  Quality for outdoor scenes",
    "authors": [
      "Jianlin Guo",
      "Haihong Xiao",
      "Wenxiong Kang"
    ],
    "abstract": "Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent performance with its high-quality rendering and real-time speed, especially for objects and small-scale scenes. However, in outdoor scenes, its point-based explicit representation lacks an effective adjustment mechanism, and the millions of Gaussian points required often lead to memory constraints during training. To address these challenges, we propose EA-3DGS, a high-quality real-time rendering method designed for outdoor scenes. First, we introduce a mesh structure to regulate the initialization of Gaussian components by leveraging an adaptive tetrahedral mesh that partitions the grid and initializes Gaussian components on each face, effectively capturing geometric structures in low-texture regions. Second, we propose an efficient Gaussian pruning strategy that evaluates each 3D Gaussian's contribution to the view and prunes accordingly. To retain geometry-critical Gaussian points, we also present a structure-aware densification strategy that densifies Gaussian points in low-curvature regions. Additionally, we employ vector quantization for parameter quantization of Gaussian components, significantly reducing disk space requirements with only a minimal impact on rendering quality. Extensive experiments on 13 scenes, including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \\& Temples, WHU) and five self-collected scenes acquired through UAV photogrammetry measurement from SCUT-CA and plateau regions, further demonstrate the superiority of our method.",
    "arxiv_url": "http://arxiv.org/abs/2505.10787v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10787v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "nerf",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting",
    "authors": [
      "Fengdi Zhang",
      "Hongkun Cao",
      "Ruqi Huang"
    ],
    "abstract": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks to minimize the number of Gaussians used while preserving high rendering quality, introducing an inherent trade-off between Gaussian quantity and rendering quality. Existing methods strive for better quantity-quality performance, but lack the ability for users to intuitively adjust this trade-off to suit practical needs such as model deployment under diverse hardware and communication constraints. Here, we present ControlGS, a 3DGS optimization method that achieves semantically meaningful and cross-scene consistent quantity-quality control. Through a single training run using a fixed setup and a user-specified hyperparameter reflecting quantity-quality preference, ControlGS can automatically find desirable quantity-quality trade-off points across diverse scenes, from compact objects to large outdoor scenes. It also outperforms baselines by achieving higher rendering quality with fewer Gaussians, and supports a broad adjustment range with stepless control over the trade-off. Project page: https://zhang-fengdi.github.io/ControlGS/",
    "arxiv_url": "http://arxiv.org/abs/2505.10473v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10473v2",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
    "authors": [
      "Xuechang Tu",
      "Lukas Radl",
      "Michael Steiner",
      "Markus Steinberger",
      "Bernhard Kerbl",
      "Fernando de la Torre"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.",
    "arxiv_url": "http://arxiv.org/abs/2505.10144v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10144v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "3d gaussian",
      "vr",
      "head",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to\n  Gaussian Field",
    "authors": [
      "Jinlong Fan",
      "Xuepu Zeng",
      "Jing Zhang",
      "Mingming Gong",
      "Yuxiang Yang",
      "Dacheng Tao"
    ],
    "abstract": "Dynamic scene representation and reconstruction have undergone transformative advances in recent years, catalyzed by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques. While initially developed for static environments, these methodologies have rapidly evolved to address the complexities inherent in 4D dynamic scenes through an expansive body of research. Coupled with innovations in differentiable volumetric rendering, these approaches have significantly enhanced the quality of motion representation and dynamic scene reconstruction, thereby garnering substantial attention from the computer vision and graphics communities. This survey presents a systematic analysis of over 200 papers focused on dynamic scene representation using radiance field, spanning the spectrum from implicit neural representations to explicit Gaussian primitives. We categorize and evaluate these works through multiple critical lenses: motion representation paradigms, reconstruction techniques for varied scene dynamics, auxiliary information integration strategies, and regularization approaches that ensure temporal consistency and physical plausibility. We organize diverse methodological approaches under a unified representational framework, concluding with a critical examination of persistent challenges and promising research directions. By providing this comprehensive overview, we aim to establish a definitive reference for researchers entering this rapidly evolving field while offering experienced practitioners a systematic understanding of both conceptual principles and practical frontiers in dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2505.10049v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10049v2",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "body",
      "3d gaussian",
      "motion",
      "understanding",
      "gaussian splatting",
      "ar",
      "survey"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large-Scale Gaussian Splatting SLAM",
    "authors": [
      "Zhe Xin",
      "Chenyang Wu",
      "Penghui Huang",
      "Yanyong Zhang",
      "Yinian Mao",
      "Guoquan Huang"
    ],
    "abstract": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2505.09915v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09915v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "recognition",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware",
    "authors": [
      "Justin Yu",
      "Letian Fu",
      "Huang Huang",
      "Karim El-Refai",
      "Rares Andrei Ambrus",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "abstract": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com",
    "arxiv_url": "http://arxiv.org/abs/2505.09601v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09601v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "3d gaussian",
      "geometry",
      "motion",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Video Compression using 2D Gaussian Splatting",
    "authors": [
      "Lakshya Gupta",
      "Imran N. Junejo"
    ],
    "abstract": "The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.",
    "arxiv_url": "http://arxiv.org/abs/2505.09324v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09324v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "motion",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: a vision-based low overhead framework for 3D scene\n  reconstruction",
    "authors": [
      "Yunji Feng",
      "Chengpu Yu",
      "Fengrui Ran",
      "Zhi Yang",
      "Yinni Liu"
    ],
    "abstract": "This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.10578v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10578v1",
    "published_date": "2025-05-14",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "head"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance",
    "authors": [
      "Wenzhe Cai",
      "Jiaqi Peng",
      "Yuqiang Yang",
      "Yujian Zhang",
      "Meng Wei",
      "Hanqing Wang",
      "Yilun Chen",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "abstract": "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20$\\times$ more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.",
    "arxiv_url": "http://arxiv.org/abs/2505.08712v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08712v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "localization",
      "gaussian splatting",
      "ar",
      "human",
      "mapping",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian\n  Splatting",
    "authors": [
      "Holly Dinkel",
      "Marcel Büsching",
      "Alberta Longhini",
      "Brian Coltin",
      "Trey Smith",
      "Danica Kragic",
      "Mårten Björkman",
      "Timothy Bretl"
    ],
    "abstract": "This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.08644v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08644v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FOCI: Trajectory Optimization on Gaussian Splats",
    "authors": [
      "Mario Gomez Andreu",
      "Maximum Wilder-Smith",
      "Victor Klemm",
      "Vaishakh Patil",
      "Jesus Tordesillas",
      "Marco Hutter"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at https://rffr.leggedrobotics.com/works/foci/",
    "arxiv_url": "http://arxiv.org/abs/2505.08510v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08510v1",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey of 3D Reconstruction with Event Cameras",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Langyi Chen",
      "Haodong Chen",
      "Ying Zhou",
      "Vera Chung",
      "Qiang Qu",
      "Weidong Cai"
    ],
    "abstract": "Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2505.08438v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08438v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "dynamic",
      "3d gaussian",
      "nerf",
      "geometry",
      "motion",
      "autonomous driving",
      "gaussian splatting",
      "ar",
      "illumination",
      "survey",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for\n  Dynamic Scene Reconstruction",
    "authors": [
      "He Huang",
      "Qi Yang",
      "Mufan Liu",
      "Yiling Xu",
      "Zhu Li"
    ],
    "abstract": "Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.",
    "arxiv_url": "http://arxiv.org/abs/2505.08196v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08196v1",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "efficient",
      "4d",
      "deformation",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAG: Scalable Language-Augmented Gaussian Splatting",
    "authors": [
      "Laszlo Szilagyi",
      "Francis Engelmann",
      "Jeannette Bohg"
    ],
    "abstract": "Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2505.08124v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08124v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "large scene",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ],
    "abstract": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream",
    "arxiv_url": "http://arxiv.org/abs/2505.07539v1",
    "pdf_url": "http://arxiv.org/pdf/2505.07539v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "deformation",
      "motion",
      "compression",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin\n  Benchmark Dataset",
    "authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias Nießner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win",
    "arxiv_url": "http://arxiv.org/abs/2505.07396v2",
    "pdf_url": "http://arxiv.org/pdf/2505.07396v2",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "segmentation",
      "semantic",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by\n  Tensorized Gaussian",
    "authors": [
      "Shijie Lian",
      "Ziyi Zhang",
      "Laurence Tianruo Yang and",
      "Mengyu Ren",
      "Debin Liu",
      "Hua Li"
    ],
    "abstract": "Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications",
    "arxiv_url": "http://arxiv.org/abs/2505.08811v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08811v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "compact",
      "efficient",
      "lightweight",
      "nerf",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System\n  for Real-Time Rendering of Composed Scenes",
    "authors": [
      "Xijie Yang",
      "Linning Xu",
      "Lihan Jiang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2505.06523v1",
    "pdf_url": "http://arxiv.org/pdf/2505.06523v1",
    "published_date": "2025-05-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head\n  Modeling",
    "authors": [
      "Gengyan Li",
      "Paulo Gotardo",
      "Timo Bolkart",
      "Stephan Garbin",
      "Kripasindhu Sarkar",
      "Abhimitra Meka",
      "Alexandros Lattas",
      "Thabo Beeler"
    ],
    "abstract": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.",
    "arxiv_url": "http://arxiv.org/abs/2505.05672v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05672v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV",
      "I.3.7; I.3.5"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "avatar",
      "deformation",
      "3d gaussian",
      "motion",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes",
    "authors": [
      "Mark C. Eid",
      "Ana I. L. Namburete",
      "João F. Henriques"
    ],
    "abstract": "Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2505.05643v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05643v1",
    "published_date": "2025-05-08",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "efficient",
      "gaussian splatting",
      "fast",
      "ar",
      "survey"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian\n  Initialization",
    "authors": [
      "Yueh-Cheng Liu",
      "Lukas Höllein",
      "Matthias Nießner",
      "Angela Dai"
    ],
    "abstract": "Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.05591v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05591v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "gaussian splatting",
      "fast",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2505.05587v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05587v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with\n  Video Diffusion and Data Augmentation",
    "authors": [
      "Yonwoo Choi"
    ],
    "abstract": "Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.",
    "arxiv_url": "http://arxiv.org/abs/2505.05475v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05475v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "avatar",
      "3d gaussian",
      "animation",
      "high-fidelity",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in\n  Dynamic Radiance Fields",
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ],
    "abstract": "We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf",
    "arxiv_url": "http://arxiv.org/abs/2505.05356v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05356v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "dynamic",
      "3d gaussian",
      "geometry",
      "motion",
      "high-fidelity",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View\n  Diffusion Priors and Gaussian Splatting",
    "authors": [
      "Feng Yang",
      "Wenliang Qian",
      "Wangmeng Zuo",
      "Hui Li"
    ],
    "abstract": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2505.04262v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04262v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ],
    "abstract": "Neural rendering techniques have made substantial progress in generating photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has achieved high quality novel view synthesis as well as fast rendering speed. However, 3D Gaussians lack proficiency in defining accurate 3D geometric structures despite their explicit primitive representations. This is due to the fact that Gaussian's attributes are primarily tailored and fine-tuned for rendering diverse 2D images by their anisotropic nature. To pave the way for efficient 3D reconstruction, we present Spherical Gaussians, a simple and effective representation for 3D geometric boundaries, from which we can directly reconstruct 3D feature curves from a set of calibrated multi-view images. Spherical Gaussians is optimized from grid initialization with a view-based rendering loss, where a 2D edge map is rendered at a specific view and then compared to the ground-truth edge map extracted from the corresponding image, without the need for any 3D guidance or supervision. Given Spherical Gaussians serve as intermedia for the robust edge representation, we further introduce a novel optimization-based algorithm called SGCR to directly extract accurate parametric curves from aligned Spherical Gaussians. We demonstrate that SGCR outperforms existing state-of-the-art methods in 3D edge reconstruction while enjoying great efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2505.04668v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04668v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d reconstruction",
      "high quality",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view\n  Synthesis in 3D Scenes",
    "authors": [
      "Feng Xiao",
      "Hongbin Xu",
      "Wanlin Liang",
      "Wenxiong Kang"
    ],
    "abstract": "The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.",
    "arxiv_url": "http://arxiv.org/abs/2505.04659v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04659v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "semantic",
      "understanding",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Data Compression with Mixture of Priors",
    "authors": [
      "Lei Liu",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.",
    "arxiv_url": "http://arxiv.org/abs/2505.03310v1",
    "pdf_url": "http://arxiv.org/pdf/2505.03310v1",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "nerf",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
    "authors": [
      "Shubhendu Jena",
      "Amine Ouasfi",
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",
    "arxiv_url": "http://arxiv.org/abs/2505.02178v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02178v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "sparse view",
      "geometry",
      "head",
      "gaussian splatting",
      "fast",
      "shape reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian\n  Splatting",
    "authors": [
      "Shubhendu Jena",
      "Shishir Reddy Vutukur",
      "Adnane Boukhayma"
    ],
    "abstract": "Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",
    "arxiv_url": "http://arxiv.org/abs/2505.02175v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02175v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "sparse-view",
      "sparse view",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "fast",
      "shape reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity\n  Non-Watertight 3D Garment Reconstruction",
    "authors": [
      "Zhihao Tang",
      "Shenghao Yang",
      "Hongtao Zhang",
      "Mingbo Zhao"
    ],
    "abstract": "Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.02126v2",
    "pdf_url": "http://arxiv.org/pdf/2505.02126v2",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2505.02108v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02108v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "body",
      "motion",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using\n  Dual-Channel Sparse Representation and Point Cloud Encoder",
    "authors": [
      "Qi Yang",
      "Le Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "abstract": "Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.01938v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01938v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GenSync: A Generalized Talking Head Framework for Audio-driven\n  Multi-Subject Lip-Sync using 3D Gaussian Splatting",
    "authors": [
      "Anushka Agarwal",
      "Muhammad Yusuf Hassan",
      "Talha Chafekar"
    ],
    "abstract": "We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.01928v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01928v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "head",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual enhancement and 3D representation for underwater scenes: a review",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Brett Seymour",
      "Evan Kovacs",
      "John Ellerbrock",
      "Dave Blackham",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in   computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an   in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and   3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian   Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple   benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.",
    "arxiv_url": "http://arxiv.org/abs/2505.01869v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01869v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "survey"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian\n  Splatting",
    "authors": [
      "Junhao Shi",
      "Jisheng Xu",
      "Jianping He",
      "Zhiliang Lin"
    ],
    "abstract": "Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.",
    "arxiv_url": "http://arxiv.org/abs/2505.01799v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01799v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "nerf",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research",
    "authors": [
      "Yan Miao",
      "Will Shen",
      "Hang Cui",
      "Sayan Mitra"
    ],
    "abstract": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing platform for autonomy research. The hardware platform integrates a small camera, a standard airframe, offboard computation, and radio communication for manual overrides. We demonstrate FalconWing's capabilities by developing and deploying a purely vision-based control policy for autonomous landing (without IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our learning approach: (1) constructs a photorealistic simulation environment via 3D Gaussian splatting trained on real-world images; (2) identifies nonlinear dynamics from vision-estimated real-flight data; and (3) trains a multi-modal Vision Transformer (ViT) policy through simulation-only imitation learning. The ViT architecture fuses single RGB image with the history of control actions via self-attention, preserving temporal context while maintaining real-time 20 Hz inference. When deployed zero-shot on the hardware platform, this policy achieves an 80% success rate in vision-based autonomous landings. Together with the hardware specifications, we also open-source the system dynamics, the software for photorealistic simulator and the learning approach.",
    "arxiv_url": "http://arxiv.org/abs/2505.01383v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01383v1",
    "published_date": "2025-05-02",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "lightweight",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular\n  Videos",
    "authors": [
      "Xia Yuan",
      "Hai Yuan",
      "Wenyi Ge",
      "Ying Fu",
      "Xi Wu",
      "Guanyu Xing"
    ],
    "abstract": "High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2505.00421v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00421v1",
    "published_date": "2025-05-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "avatar",
      "deformation",
      "animation",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation",
    "authors": [
      "Haiyang Zhou",
      "Wangbo Yu",
      "Jiawen Guan",
      "Xinhua Cheng",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.21650v2",
    "pdf_url": "http://arxiv.org/pdf/2504.21650v2",
    "published_date": "2025-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "vr",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From\n  Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting\n  (3DGS), and Beyond",
    "authors": [
      "Jiajia Li",
      "Xinda Qi",
      "Seyed Hamidreza Nabaei",
      "Meiqi Liu",
      "Dong Chen",
      "Xin Zhang",
      "Xunyuan Yin",
      "Zhaojian Li"
    ],
    "abstract": "Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.",
    "arxiv_url": "http://arxiv.org/abs/2505.00737v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00737v1",
    "published_date": "2025-04-30",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "ar",
      "nerf",
      "3d gaussian",
      "geometry",
      "understanding",
      "gaussian splatting",
      "sparse view",
      "survey",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for\n  Targeted Scene Confusion",
    "authors": [
      "Jiaxin Hong",
      "Sixu Chen",
      "Shuoyang Sun",
      "Hongyao Yu",
      "Hao Fang",
      "Yuqi Tan",
      "Bin Chen",
      "Shuhan Qi",
      "Jiawei Li"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.",
    "arxiv_url": "http://arxiv.org/abs/2504.20829v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20829v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D\n  Reconstruction",
    "authors": [
      "Yuhan Xie",
      "Yixi Cai",
      "Yinqiang Zhang",
      "Lei Yang",
      "Jia Pan"
    ],
    "abstract": "This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",
    "arxiv_url": "http://arxiv.org/abs/2504.21067v1",
    "pdf_url": "http://arxiv.org/pdf/2504.21067v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "efficient",
      "nerf",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human\n  using Articulated 2D Gaussian",
    "authors": [
      "Hao Tian",
      "Rui Liu",
      "Wen Shen",
      "Yilong Hu",
      "Zhihao Zheng",
      "Xiaolin Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2504.20607v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20607v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "dynamic",
      "efficient",
      "body",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Your Editable 3D Photorealistic Avatar with\n  Tetrahedron-constrained Gaussian Splatting",
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ],
    "abstract": "Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.",
    "arxiv_url": "http://arxiv.org/abs/2504.20403v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20403v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "localization",
      "3d gaussian",
      "geometry",
      "vr",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D\n  Gaussian Splatting",
    "authors": [
      "Jongwon Lee",
      "Timothy Bretl"
    ],
    "abstract": "In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.",
    "arxiv_url": "http://arxiv.org/abs/2504.20379v2",
    "pdf_url": "http://arxiv.org/pdf/2504.20379v2",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface\n  Reconstruction from Sparse Views",
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ],
    "abstract": "We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach.",
    "arxiv_url": "http://arxiv.org/abs/2504.20378v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20378v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "sparse-view",
      "nerf",
      "geometry",
      "motion",
      "gaussian splatting",
      "fast",
      "sparse view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics",
    "authors": [
      "Yunfei Wan",
      "Jianheng Liu",
      "Chunran Zheng",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "abstract": "In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.",
    "arxiv_url": "http://arxiv.org/abs/2504.19938v2",
    "pdf_url": "http://arxiv.org/pdf/2504.19938v2",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar",
      "mapping",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel\n  View Synthesis in Autonomous Driving Scenes",
    "authors": [
      "Mohammad Altillawi",
      "Fengyi Shen",
      "Liudi Yang",
      "Sai Manoj Prakhya",
      "Ziyuan Liu"
    ],
    "abstract": "Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2504.19557v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19557v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "geometry",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field",
    "authors": [
      "Zuxing Lu",
      "Xin Yuan",
      "Shaowen Yang",
      "Jingyu Liu",
      "Changyin Sun"
    ],
    "abstract": "Semantic-aware 3D scene reconstruction is essential for autonomous robots to perform complex interactions. Semantic SLAM, an online approach, integrates pose tracking, geometric reconstruction, and semantic mapping into a unified framework, shows significant potential. However, existing systems, which rely on 2D ground truth priors for supervision, are often limited by the sparsity and noise of these signals in real-world environments. To address this challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D Gaussian Splatting that leverages feature fields to achieve joint rendering of appearance, geometry, and N-dimensional semantic features. By independently optimizing feature gradients, our method supports semantic reconstruction using various forms of 2D priors, particularly sparse and noisy signals. Experimental results demonstrate that our approach outperforms previous methods in both tracking accuracy and photorealistic rendering quality. When utilizing 2D ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation performance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with only marginal performance degradation.",
    "arxiv_url": "http://arxiv.org/abs/2504.19409v2",
    "pdf_url": "http://arxiv.org/pdf/2504.19409v2",
    "published_date": "2025-04-28",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "3d gaussian",
      "geometry",
      "segmentation",
      "semantic",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rendering Anywhere You See: Renderability Field-guided Gaussian\n  Splatting",
    "authors": [
      "Xiaofeng Jin",
      "Yan Fang",
      "Matteo Frosi",
      "Jianfei Ge",
      "Jiangjian Xiao",
      "Matteo Matteucci"
    ],
    "abstract": "Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360{\\deg} views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.",
    "arxiv_url": "http://arxiv.org/abs/2504.19261v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19261v1",
    "published_date": "2025-04-27",
    "categories": [
      "cs.CV",
      "65D18, 68U05",
      "I.3.7; I.4.8"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data\n  Compression",
    "authors": [
      "Zicong Chen",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Lei Liu",
      "Dong Xu"
    ],
    "abstract": "Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints. Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively. Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding. Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook. By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.",
    "arxiv_url": "http://arxiv.org/abs/2504.18925v2",
    "pdf_url": "http://arxiv.org/pdf/2504.18925v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with\n  Gaussians",
    "authors": [
      "Letian Huang",
      "Dongwei Ye",
      "Jialin Dan",
      "Chengzhi Tao",
      "Huiwen Liu",
      "Kun Zhou",
      "Bo Ren",
      "Yuanqi Li",
      "Yanwen Guo",
      "Jie Guo"
    ],
    "abstract": "The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",
    "arxiv_url": "http://arxiv.org/abs/2504.18768v2",
    "pdf_url": "http://arxiv.org/pdf/2504.18768v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny\n  Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.",
    "arxiv_url": "http://arxiv.org/abs/2504.18468v3",
    "pdf_url": "http://arxiv.org/pdf/2504.18468v3",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "reflection",
      "face",
      "relighting",
      "nerf",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian\n  Splatting and Vision Models",
    "authors": [
      "Michel Gokan Khan",
      "Renan Guarese",
      "Fabian Johnson",
      "Xi Vincent Wang",
      "Anders Bergman",
      "Benjamin Edvinsson",
      "Mario Romero",
      "Jérémy Vachier",
      "Jan Kronqvist"
    ],
    "abstract": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.",
    "arxiv_url": "http://arxiv.org/abs/2504.18165v1",
    "pdf_url": "http://arxiv.org/pdf/2504.18165v1",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "tracking",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via\n  Editable 3D Gaussian Splatting",
    "authors": [
      "Kaiyuan Tang",
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "In volume visualization, users can interactively explore the three-dimensional data by specifying color and opacity mappings in the transfer function (TF) or adjusting lighting parameters, facilitating meaningful interpretation of the underlying structure. However, rendering large-scale volumes demands powerful GPUs and high-speed memory access for real-time performance. While existing novel view synthesis (NVS) methods offer faster rendering speeds with lower hardware requirements, the visible parts of a reconstructed scene are fixed and constrained by preset TF settings, significantly limiting user exploration. This paper introduces inverse volume rendering via Gaussian splatting (iVR-GS), an innovative NVS method that reduces the rendering cost while enabling scene editing for interactive volume exploration. Specifically, we compose multiple iVR-GS models associated with basic TFs covering disjoint visible parts to make the entire volumetric scene visible. Each basic model contains a collection of 3D editable Gaussians, where each Gaussian is a 3D spatial point that supports real-time scene rendering and editing. We demonstrate the superior reconstruction quality and composability of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on various volume datasets. The code is available at https://github.com/TouKaienn/iVR-GS.",
    "arxiv_url": "http://arxiv.org/abs/2504.17954v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17954v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "nerf",
      "3d gaussian",
      "vr",
      "gaussian splatting",
      "fast",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from\n  Casually Captured Videos",
    "authors": [
      "Shucheng Gong",
      "Lingzhe Zhao",
      "Wenpu Li",
      "Hong Xie",
      "Yin Zhang",
      "Shiyu Zhao",
      "Peidong Liu"
    ],
    "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered widespread attention due to their superior performance. However, most works rely on low dynamic range (LDR) images, which limits the capturing of richer scene details. Some prior works have focused on high dynamic range (HDR) scene reconstruction, typically require capturing of multi-view sharp images with different exposure times at fixed camera positions during exposure times, which is time-consuming and challenging in practice. For a more flexible data acquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily and robustly reconstruct the 3D HDR scene from casually captured videos with auto-exposure enabled, even in the presence of severe motion blur and varying unknown exposure time. \\textbf{CasualHDRSplat} contains a unified differentiable physical imaging model which first applies continuous-time trajectory constraint to imaging process so that we can jointly optimize exposure time, camera response function (CRF), camera poses, and sharp 3D HDR scene. Extensive experiments demonstrate that our approach outperforms existing methods in terms of robustness and rendering quality. Our source code will be available at https://github.com/WU-CVGL/CasualHDRSplat",
    "arxiv_url": "http://arxiv.org/abs/2504.17728v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17728v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "nerf",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting is an Effective Data Generator for 3D Object\n  Detection",
    "authors": [
      "Farhad G. Zanjani",
      "Davide Abati",
      "Auke Wiggers",
      "Dimitris Kalatzis",
      "Jens Petersen",
      "Hong Cai",
      "Amirhossein Habibian"
    ],
    "abstract": "We investigate data augmentation for 3D object detection in autonomous driving. We utilize recent advancements in 3D reconstruction based on Gaussian Splatting for 3D object placement in driving scenes. Unlike existing diffusion-based methods that synthesize images conditioned on BEV layouts, our approach places 3D objects directly in the reconstructed 3D space with explicitly imposed geometric transformations. This ensures both the physical plausibility of object placement and highly accurate 3D pose and position annotations.   Our experiments demonstrate that even by integrating a limited number of external 3D objects into real scenes, the augmented data significantly enhances 3D object detection performance and outperforms existing diffusion-based 3D augmentation for object detection. Extensive testing on the nuScenes dataset reveals that imposing high geometric diversity in object placement has a greater impact compared to the appearance diversity of objects. Additionally, we show that generating hard examples, either by maximizing detection loss or imposing high visual occlusion in camera images, does not lead to more efficient 3D data augmentation for camera-based 3D object detection in autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2504.16740v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16740v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "efficient",
      "autonomous driving",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile\n  Manipulation",
    "authors": [
      "Wenxuan Li",
      "Hang Zhao",
      "Zhiyuan Yu",
      "Yu Du",
      "Qin Zou",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "abstract": "While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.",
    "arxiv_url": "http://arxiv.org/abs/2504.16693v2",
    "pdf_url": "http://arxiv.org/pdf/2504.16693v2",
    "published_date": "2025-04-23",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "body",
      "gaussian splatting",
      "ar",
      "few-shot"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and\n  Multi-Frame Integration",
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Martin R. Oswald",
      "Stefano Mattoccia"
    ],
    "abstract": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.",
    "arxiv_url": "http://arxiv.org/abs/2504.16545v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16545v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "tracking",
      "efficient",
      "3d gaussian",
      "geometry",
      "vr",
      "gaussian splatting",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene\n  Conceptional Learning",
    "authors": [
      "Mingxuan Cui",
      "Qing Guo",
      "Yuyi Wang",
      "Hongkai Yu",
      "Di Lin",
      "Qin Zou",
      "Ming-Ming Cheng",
      "Xi Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.",
    "arxiv_url": "http://arxiv.org/abs/2504.17815v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17815v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "efficient",
      "nerf",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for\n  Small-Baseline Videos",
    "authors": [
      "Yuxin Yao",
      "Yan Zhang",
      "Zhening Huang",
      "Joan Lasenby"
    ],
    "abstract": "Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS",
    "arxiv_url": "http://arxiv.org/abs/2504.17810v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17810v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
    "authors": [
      "Cailin Zhuang",
      "Yaoqi Hu",
      "Xuanyang Zhang",
      "Wei Cheng",
      "Jiacheng Bao",
      "Shengqi Liu",
      "Yiying Yang",
      "Xianfang Zeng",
      "Gang Yu",
      "Ming Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.",
    "arxiv_url": "http://arxiv.org/abs/2504.15281v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15281v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "quality enhancement",
      "nerf",
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Immersive Teleoperation Framework for Locomanipulation Tasks",
    "authors": [
      "Takuya Boehringer",
      "Jonathan Embley-Riches",
      "Karim Hammoud",
      "Valerio Modugno",
      "Dimitrios Kanoulas"
    ],
    "abstract": "Recent advancements in robotic loco-manipulation have leveraged Virtual Reality (VR) to enhance the precision and immersiveness of teleoperation systems, significantly outperforming traditional methods reliant on 2D camera feeds and joystick controls. Despite these advancements, challenges remain, particularly concerning user experience across different setups. This paper introduces a novel VR-based teleoperation framework designed for a robotic manipulator integrated onto a mobile platform. Central to our approach is the application of Gaussian splatting, a technique that abstracts the manipulable scene into a VR environment, thereby enabling more intuitive and immersive interactions. Users can navigate and manipulate within the virtual scene as if interacting with a real robot, enhancing both the engagement and efficacy of teleoperation tasks. An extensive user study validates our approach, demonstrating significant usability and efficiency improvements. Two-thirds (66%) of participants completed tasks faster, achieving an average time reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface overall, with unanimous (100%) recommendations for future use, highlighting improvements in precision, responsiveness, and situational awareness. Finally, we demonstrate the effectiveness of our framework through real-world experiments in two distinct application scenarios, showcasing the practical capabilities and versatility of the Splat-based VR interface.",
    "arxiv_url": "http://arxiv.org/abs/2504.15229v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15229v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "vr",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\n  Monocular Video",
    "authors": [
      "Minh-Quan Viet Bui",
      "Jongmin Park",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.",
    "arxiv_url": "http://arxiv.org/abs/2504.15122v3",
    "pdf_url": "http://arxiv.org/pdf/2504.15122v3",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "4d",
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed\n  Real X-rays",
    "authors": [
      "Sascha Jecklin",
      "Aidana Massalimova",
      "Ruyi Zha",
      "Lilian Calvet",
      "Christoph J. Laux",
      "Mazda Farshad",
      "Philipp Fürnstahl"
    ],
    "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems. Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems. However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions. Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements. While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored. This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions. We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality. Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies. We evaluated our approach using an ex-vivo dataset. Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs. This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation.",
    "arxiv_url": "http://arxiv.org/abs/2504.14699v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14699v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D\n  Open Vocabulary Instance Segmentation",
    "authors": [
      "Junyuan Fang",
      "Zihan Wang",
      "Yejun Zhang",
      "Shuzhe Wang",
      "Iaroslav Melekhov",
      "Juho Kannala"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive zero-shot transfer capabilities in image-level visual perception tasks. However, they fall short in 3D instance-level segmentation tasks that require accurate localization and recognition of individual objects. To bridge this gap, we introduce a novel 3D Gaussian Splatting based hard visual prompting approach that leverages camera interpolation to generate diverse viewpoints around target objects without any 2D-3D optimization or fine-tuning. Our method simulates realistic 3D perspectives, effectively augmenting existing hard visual prompts by enforcing geometric consistency across viewpoints. This training-free strategy seamlessly integrates with prior hard visual prompts, enriching object-descriptive features and enabling VLMs to achieve more robust and accurate 3D instance segmentation in diverse 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2504.14638v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14638v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "localization",
      "3d gaussian",
      "segmentation",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided\n  Gaussian Number Control",
    "authors": [
      "Lifeng Lin",
      "Rongfeng Lu",
      "Quan Chen",
      "Haofan Ren",
      "Ming Lu",
      "Yaoqi Sun",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in practical 3D reconstruction applications. Recently, many methods based on the 3D Gaussian Splatting (3DGS) framework have been proposed to address sparse-view 3D reconstruction. Although these methods have made considerable advancements, they still show significant issues with overfitting. To reduce the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number Control (VGNC) approach based on generative novel view synthesis (NVS) models. To the best of our knowledge, this is the first attempt to alleviate the overfitting issue of sparse-view 3DGS with generative validation images. Specifically, we first introduce a validation image generation method based on a generative NVS model. We then propose a Gaussian number control strategy that utilizes generated validation images to determine the optimal Gaussian numbers, thereby reducing the issue of overfitting. We conducted detailed experiments on various sparse-view 3DGS baselines and datasets to evaluate the effectiveness of VGNC. Extensive experiments show that our approach not only reduces overfitting but also improves rendering quality on the test set while decreasing the number of Gaussian points. This reduction lowers storage demands and accelerates both training and rendering. The code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2504.14548v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14548v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "sparse-view",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metamon-GS: Enhancing Representability with Variance-Guided\n  Densification and Light Encoding",
    "authors": [
      "Junyan Su",
      "Baozhu Zhao",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point features with anchor embeddings has significantly enhanced the performance of newer 3DGS variants. While significant advances have been made, it is still challenging to boost rendering performance. Feature embeddings have difficulty accurately representing colors from different perspectives under varying lighting conditions, which leads to a washed-out appearance. Another reason is the lack of a proper densification strategy that prevents Gaussian point growth in thinly initialized areas, resulting in blurriness and needle-shaped artifacts. To address them, we propose Metamon-GS, from innovative viewpoints of variance-guided densification strategy and multi-level hash grid. The densification strategy guided by variance specifically targets Gaussians with high gradient variance in pixels and compensates for the importance of regions with extra Gaussians to improve reconstruction. The latter studies implicit global lighting conditions and accurately interprets color from different perspectives and feature embeddings. Our thorough experiments on publicly available datasets show that Metamon-GS surpasses its baseline model and previous versions, delivering superior quality in rendering novel views.",
    "arxiv_url": "http://arxiv.org/abs/2504.14460v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14460v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image",
    "authors": [
      "Chen Guo",
      "Zhuo Su",
      "Jian Wang",
      "Shuang Li",
      "Xu Chang",
      "Zhaohu Li",
      "Yang Zhao",
      "Guidong Wang",
      "Ruqi Huang"
    ],
    "abstract": "Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.14373v2",
    "pdf_url": "http://arxiv.org/pdf/2504.14373v2",
    "published_date": "2025-04-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "dynamic",
      "avatar",
      "efficient",
      "3d gaussian",
      "animation",
      "head",
      "gaussian splatting",
      "ar",
      "human"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering,\n  Gaussian Splatting and SLAM",
    "authors": [
      "Samuel Cerezo",
      "Gaetano Meli",
      "Tomás Berriel Martins",
      "Kirill Safronov",
      "Javier Civera"
    ],
    "abstract": "Models and methods originally developed for novel view synthesis and scene rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as multimodality and sequentiality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. To bridge this gap, we introduce SLAM&Render, a novel dataset designed to benchmark methods in the intersection between SLAM and novel view rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of novel SLAM strategies when applied to robot manipulators. The dataset sequences span five different setups featuring consumer and industrial objects under four different lighting conditions, with separate training and test trajectories per scene, as well as object rearrangements. Our experimental results, obtained with several baselines from the literature, validate SLAM&Render as a relevant benchmark for this emerging research area.",
    "arxiv_url": "http://arxiv.org/abs/2504.13713v2",
    "pdf_url": "http://arxiv.org/pdf/2504.13713v2",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "neural rendering",
      "slam",
      "localization",
      "nerf",
      "gaussian splatting",
      "ar",
      "illumination",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Green Robotic Mixed Reality with Gaussian Splatting",
    "authors": [
      "Chenxuan Liu",
      "He Li",
      "Zongze Li",
      "Shuai Wang",
      "Wei Xu",
      "Kejiang Ye",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Realizing green communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images at high frequencies through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS model which enables the simulator to opportunistically render a photo-realistic view from the robot's pose, thereby reducing the need for excessive image uploads. Since the GS model may involve discrepancies compared to the actual environments, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation across different frames. The GSCLO problem is solved by an accelerated penalty optimization (APO) algorithm. Experiments demonstrate that the proposed GSRMR reduces the communication energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with APO outperforms extensive baseline schemes, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM).",
    "arxiv_url": "http://arxiv.org/abs/2504.13697v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13697v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian\n  Splatting",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framework EG-Gaussian, which utilizes epipolar geometry and graph networks for 3D scene reconstruction. Initially, we integrate epipolar geometry into the 3DGS initialization phase to enhance initial 3DGS point construction. Then, we specifically design a graph learning module to refine 3DGS spatial features, in which we incorporate both spatial coordinates and angular relationships among neighboring points. Experiments on indoor and outdoor benchmark datasets demonstrate that our approach significantly improves reconstruction accuracy compared to 3DGS-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2504.13540v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13540v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "3d gaussian",
      "geometry",
      "gaussian splatting",
      "ar",
      "outdoor"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for\n  Volume Rendering",
    "authors": [
      "Landon Dyken",
      "Andres Sewell",
      "Will Usher",
      "Steve Petruzza",
      "Sidharth Kumar"
    ],
    "abstract": "While HPC resources are increasingly being used to produce adaptively refined or unstructured volume datasets, current research in applying machine learning-based representation to visualization has largely ignored this type of data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D Gaussian-based representation for scientific volume visualization focused on unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that store view-dependent color and opacity for each Gaussian, VEG decouple the visual appearance from the data representation by encoding only scalar values, enabling transfer-function-agnostic rendering of 3DGS models for interactive scientific visualization. VEG are directly initialized from volume datasets, eliminating the need for structure-from-motion pipelines like COLMAP. To ensure complete scalar field coverage, we introduce an opacity-guided training strategy, using differentiable rendering with multiple transfer functions to optimize our data representation. This allows VEG to preserve fine features across the full scalar range of a dataset while remaining independent of any specific transfer function. Each Gaussian is scaled and rotated to adapt to local geometry, allowing for efficient representation of unstructured meshes without storing mesh connectivity and while using far fewer primitives. Across a diverse set of data, VEG achieve high reconstruction quality, compress large volume datasets by up to 3600x, and support lightning-fast rendering on commodity GPUs, enabling interactive visualization of large-scale structured and unstructured volumes.",
    "arxiv_url": "http://arxiv.org/abs/2504.13339v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13339v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "geometry",
      "motion",
      "gaussian splatting",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust\n  One-Shot Manipulation",
    "authors": [
      "Sizhe Yang",
      "Wenye Yu",
      "Jia Zeng",
      "Jun Lv",
      "Kerui Ren",
      "Cewu Lu",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world.",
    "arxiv_url": "http://arxiv.org/abs/2504.13175v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13175v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]