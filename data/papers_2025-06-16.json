[
  {
    "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting",
    "authors": [
      "Lintao Xiang",
      "Hongpei Zheng",
      "Yating Huang",
      "Qijun Yang",
      "Hujun Yin"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.10335v1",
    "pdf_url": "http://arxiv.org/pdf/2506.10335v1",
    "published_date": "2025-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "few-shot",
      "lightweight",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
    "authors": [
      "Chieh Hubert Lin",
      "Zhaoyang Lv",
      "Songyin Wu",
      "Zhen Xu",
      "Thu Nguyen-Phuoc",
      "Hung-Yu Tseng",
      "Julian Straub",
      "Numair Khan",
      "Lei Xiao",
      "Ming-Hsuan Yang",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong",
      "Zhengqin Li"
    ],
    "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09997v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09997v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "arxiv_url": "http://arxiv.org/abs/2506.09952v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09952v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/wangzy22/UniPre3D",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge",
    "authors": [
      "Haoru Wang",
      "Kai Ye",
      "Yangyan Li",
      "Wenzheng Chen",
      "Baoquan Chen"
    ],
    "abstract": "We consider the problem of generalizable novel view synthesis (NVS), which aims to generate photorealistic novel views from sparse or even unposed 2D images without per-scene optimization. This task remains fundamentally challenging, as it requires inferring 3D structure from incomplete and ambiguous 2D observations. Early approaches typically rely on strong 3D knowledge, including architectural 3D inductive biases (e.g., embedding explicit 3D representations, such as NeRF or 3DGS, into network design) and ground-truth camera poses for both input and target views. While recent efforts have sought to reduce the 3D inductive bias or the dependence on known camera poses of input views, critical questions regarding the role of 3D knowledge and the necessity of circumventing its use remain under-explored. In this work, we conduct a systematic analysis on the 3D knowledge and uncover a critical trend: the performance of methods that requires less 3D knowledge accelerates more as data scales, eventually achieving performance on par with their 3D knowledge-driven counterparts, which highlights the increasing importance of reducing dependence on 3D knowledge in the era of large-scale data. Motivated by and following this trend, we propose a novel NVS framework that minimizes 3D inductive bias and pose dependence for both input and target views. By eliminating this 3D knowledge, our method fully leverages data scaling and learns implicit 3D awareness directly from sparse 2D images, without any 3D inductive bias or pose annotation during training. Extensive experiments demonstrate that our model generates photorealistic and 3D-consistent novel views, achieving even comparable performance with methods that rely on posed inputs, thereby validating the feasibility and effectiveness of our data-centric paradigm. Project page: https://pku-vcl-geometry.github.io/Less3Depend/ .",
    "arxiv_url": "http://arxiv.org/abs/2506.09885v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09885v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "geometry",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction",
    "authors": [
      "Junli Deng",
      "Ping Shi",
      "Qipei Li",
      "Jinyang Guo"
    ],
    "abstract": "Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09836v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09836v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation",
    "authors": [
      "Haowen Wang",
      "Xiaoping Yuan",
      "Zhao Jin",
      "Zhen Zhao",
      "Zhengping Che",
      "Yousong Xue",
      "Jin Tian",
      "Yakun Huang",
      "Jian Tang"
    ],
    "abstract": "Articulated objects are ubiquitous in everyday life, and accurate 3D representations of their geometry and motion are critical for numerous applications. However, in the absence of human annotation, existing approaches still struggle to build a unified representation for objects that contain multiple movable parts. We introduce DeGSS, a unified framework that encodes articulated objects as deformable 3D Gaussian fields, embedding geometry, appearance, and motion in one compact representation. Each interaction state is modeled as a smooth deformation of a shared field, and the resulting deformation trajectories guide a progressive coarse-to-fine part segmentation that identifies distinct rigid components, all in an unsupervised manner. The refined field provides a spatially continuous, fully decoupled description of every part, supporting part-level reconstruction and precise modeling of their kinematic relationships. To evaluate generalization and realism, we enlarge the synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset that pairs RGB captures with accurately reverse-engineered 3D models. Extensive experiments demonstrate that our method outperforms existing methods in both accuracy and stability.",
    "arxiv_url": "http://arxiv.org/abs/2506.09663v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09663v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "segmentation",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields",
    "authors": [
      "Qijing Li",
      "Jingxiang Sun",
      "Liang An",
      "Zhaoqi Su",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Holistic 3D scene understanding, which jointly models geometry, appearance, and semantics, is crucial for applications like augmented reality and robotic interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM) are limited to extracting language-based semantics from scenes, failing to achieve holistic scene comprehension. Additionally, they suffer from low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene optimization methods rely on dense input views, which reduces practicality and increases complexity during deployment. In this paper, we propose SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which unifies 3D Gaussians with latent semantic attributes for joint geometry-appearance-semantics modeling. To predict the semantic anisotropic Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a cost volume representation that stores cross-view feature similarities, enhancing coherent and accurate scene comprehension. Leveraging a two-stage distillation framework, SemanticSplat reconstructs a holistic multi-modal semantic feature field from sparse-view images. Experiments demonstrate the effectiveness of our method for 3D scene understanding tasks like promptable and open-vocabulary segmentation. Video results are available at https://semanticsplat.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2506.09565v2",
    "pdf_url": "http://arxiv.org/pdf/2506.09565v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
    "authors": [
      "Tao Wang",
      "Mengyu Li",
      "Geduo Zeng",
      "Cheng Meng",
      "Qiong Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering.",
    "arxiv_url": "http://arxiv.org/abs/2506.09534v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09534v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene",
    "authors": [
      "Jianing Chen",
      "Zehao Li",
      "Yujun Cai",
      "Hao Jiang",
      "Chengxuan Qian",
      "Juyuan Kang",
      "Shuqin Gao",
      "Honglong Zhao",
      "Tianlu Mao",
      "Yucheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2506.09518v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09518v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation",
    "authors": [
      "Zetian Song",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Xiaohan Lu",
      "Chuanmin Jia",
      "Siwei Ma",
      "Wen Gao"
    ],
    "abstract": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.",
    "arxiv_url": "http://arxiv.org/abs/2506.09479v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09479v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "authors": [
      "Yunxiao Shi",
      "Yinhao Zhu",
      "Shizhong Han",
      "Jisoo Jeong",
      "Amin Ansari",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "abstract": "Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.",
    "arxiv_url": "http://arxiv.org/abs/2506.09417v2",
    "pdf_url": "http://arxiv.org/pdf/2506.09417v2",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Human Action Video Data Generation with Pose Transfer",
    "authors": [
      "Vaclav Knapp",
      "Matyas Bohacek"
    ],
    "abstract": "In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.",
    "arxiv_url": "http://arxiv.org/abs/2506.09411v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09411v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "autonomous driving",
      "motion",
      "few-shot",
      "understanding",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Jingyu Gong",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.09378v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09378v1",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams",
    "authors": [
      "Zike Wu",
      "Qi Yan",
      "Xuanyu Yi",
      "Lele Wang",
      "Renjie Liao"
    ],
    "abstract": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.",
    "arxiv_url": "http://arxiv.org/abs/2506.08862v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08862v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/nickwzk/StreamSplat",
    "keywords": [
      "efficient",
      "deformation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting",
    "authors": [
      "Keyi Liu",
      "Weidong Yang",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.08777v2",
    "pdf_url": "http://arxiv.org/pdf/2506.08777v2",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting",
    "authors": [
      "Mengjiao Ma",
      "Qi Ma",
      "Yue Li",
      "Jiahuan Cheng",
      "Runyi Yang",
      "Bin Ren",
      "Nikola Popovic",
      "Mingqiang Wei",
      "Nicu Sebe",
      "Luc Van Gool",
      "Theo Gevers",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2506.08710v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08710v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "fast",
      "semantic",
      "understanding",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering",
    "authors": [
      "Xiaohan Zhang",
      "Sitong Wang",
      "Yushen Yan",
      "Yi Yang",
      "Mingda Xu",
      "Qi Liu"
    ],
    "abstract": "High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2506.08704v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08704v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex-Valued Holographic Radiance Fields",
    "authors": [
      "Yicheng Zhan",
      "Dong-Ha Shin",
      "Seung-Hwan Baek",
      "Kaan Ak≈üit"
    ],
    "abstract": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.",
    "arxiv_url": "http://arxiv.org/abs/2506.08350v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08350v1",
    "published_date": "2025-06-10",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.ET"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos",
    "authors": [
      "Zhen Xu",
      "Zhengqin Li",
      "Zhao Dong",
      "Xiaowei Zhou",
      "Richard Newcombe",
      "Zhaoyang Lv"
    ],
    "abstract": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input and remain efficient rendering at runtime. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos. Project page: https://4dgt.github.io",
    "arxiv_url": "http://arxiv.org/abs/2506.08015v1",
    "pdf_url": "http://arxiv.org/pdf/2506.08015v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "abstract": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve high-quality novel view synthesis by using neural networks to predict the time-varying deformation of each Gaussian. However, performing per-Gaussian neural inference at every frame poses a significant bottleneck, limiting rendering speed and increasing memory and compute requirements. In this paper, we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS representations by reducing neural inference through two complementary techniques. First, we propose a temporal sensitivity pruning score that identifies and removes Gaussians with low contribution to the dynamic scene reconstruction. We also introduce an annealing smooth pruning mechanism that improves pruning robustness in real-world scenes with imprecise camera poses. Second, we propose GroupFlow, a motion analysis technique that clusters Gaussians by trajectory similarity and predicts a single rigid transformation per group instead of separate deformations for each Gaussian. Together, our techniques accelerate rendering by $10.37\\times$, reduce model size by $7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset. SpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be integrated into any deformable 3DGS or 4DGS framework.",
    "arxiv_url": "http://arxiv.org/abs/2506.07917v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07917v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "vr",
      "motion",
      "fast",
      "deformation",
      "3d gaussian",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution",
    "authors": [
      "Shuja Khalid",
      "Mohamed Ibrahim",
      "Yang Liu"
    ],
    "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
    "arxiv_url": "http://arxiv.org/abs/2506.07897v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07897v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation",
    "authors": [
      "William Ljungbergh",
      "Bernardo Taveira",
      "Wenzhao Zheng",
      "Adam Tonderski",
      "Chensheng Peng",
      "Fredrik Kahl",
      "Christoffer Petersson",
      "Michael Felsberg",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.",
    "arxiv_url": "http://arxiv.org/abs/2506.07826v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07826v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "lighting",
      "shadow",
      "lightweight",
      "3d gaussian",
      "ar",
      "illumination",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting",
    "authors": [
      "Jens Piekenbrinck",
      "Christian Schmidt",
      "Alexander Hermans",
      "Narunas Vaskevicius",
      "Timm Linder",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2506.07697v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07697v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views",
    "authors": [
      "Xiaohan Lu",
      "Jiaye Fu",
      "Jiaqi Zhang",
      "Zetian Song",
      "Chuanmin Jia",
      "Siwei Ma"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.07670v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07670v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians",
    "authors": [
      "Zeyu Xiao",
      "Zhenyi Wu",
      "Mingyang Sun",
      "Qipeng Yan",
      "Yufan Guo",
      "Zhuoer Liang",
      "Lihua Zhang"
    ],
    "abstract": "3D Gaussian Splatting has achieved remarkable success in reconstructing both static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian primitives, interactions between objects suffer from inaccurate 3D segmentation, imprecise deformation among different materials, and severe rendering artifacts. To address these challenges, we introduce PIG: Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach that combines 3D object segmentation with the simulation of interacting objects in high precision. Firstly, our method facilitates fast and accurate mapping from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation. Secondly, we assign unique physical properties to correspondingly segmented objects within the scene for multi-material coupled interactions. Finally, we have successfully embedded constraint scales into deformation gradients, specifically clamping the scaling and rotation properties of the Gaussian primitives to eliminate artifacts and achieve geometric fidelity and visual consistency. Experimental results demonstrate that our method not only outperforms the state-of-the-art (SOTA) in terms of visual quality, but also opens up new directions and pipelines for the field of physically realistic scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2506.07657v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07657v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "deformation",
      "mapping",
      "segmentation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support",
    "authors": [
      "Chenqi Zhang",
      "Yu Feng",
      "Jieru Zhao",
      "Guangda Liu",
      "Wenchao Ding",
      "Chentao Wu",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
    "arxiv_url": "http://arxiv.org/abs/2506.09070v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09070v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation",
    "authors": [
      "Yijie Deng",
      "Shuaihang Yuan",
      "Geeta Chandra Raju Bethala",
      "Anthony Tzes",
      "Yu-Shen Liu",
      "Yi Fang"
    ],
    "abstract": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.",
    "arxiv_url": "http://arxiv.org/abs/2506.07338v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07338v1",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization",
    "authors": [
      "Zhican Wang",
      "Guanghui He",
      "Dantong Liu",
      "Lingjun Gao",
      "Shell Xu Hu",
      "Chen Zhang",
      "Zhuoran Song",
      "Nicholas Lane",
      "Wayne Luk",
      "Hongxiang Fan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy savings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.",
    "arxiv_url": "http://arxiv.org/abs/2506.07069v1",
    "pdf_url": "http://arxiv.org/pdf/2506.07069v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "head",
      "vr",
      "autonomous driving",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiao Tang",
      "Jiajun Tang",
      "Jiaqi Lin",
      "Yuxin Cheng",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
    "arxiv_url": "http://arxiv.org/abs/2506.06988v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06988v1",
    "published_date": "2025-06-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Mapping for Evolving Scenes",
    "authors": [
      "Vladimir Yugay",
      "Thies Kersten",
      "Luca Carlone",
      "Theo Gevers",
      "Martin R. Oswald",
      "Lukas Schmid"
    ],
    "abstract": "Mapping systems with novel view synthesis (NVS) capabilities are widely used in computer vision, with augmented reality, robotics, and autonomous driving applications. Most notably, 3D Gaussian Splatting-based systems show high NVS performance; however, many current approaches are limited to static scenes. While recent works have started addressing short-term dynamics (motion within the view of the camera), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene adaptation mechanism that continuously updates the 3D representation to reflect the latest changes. In addition, since maintaining geometric and semantic consistency remains challenging due to stale observations disrupting the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We evaluate Gaussian Mapping for Evolving Scenes (GaME) on both synthetic and real-world datasets and find it to be more accurate than the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2506.06909v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06909v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "autonomous driving",
      "motion",
      "semantic",
      "mapping",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation",
    "authors": [
      "Sumit Sharma",
      "Gopi Raju Matta",
      "Kaushik Mitra"
    ],
    "abstract": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline.",
    "arxiv_url": "http://arxiv.org/abs/2506.06890v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06890v1",
    "published_date": "2025-06-07",
    "categories": [
      "eess.IV",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles",
    "authors": [
      "Yangkai Lin",
      "Jiabao Lei",
      "Kui jia"
    ],
    "abstract": "In recent years, there has been a growing demand to stylize a given 3D scene to align with the artistic style of reference images for creative purposes. While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method for realistic 3D scene modeling, there remains a challenge in adapting it to stylize 3D GS to match with multiple styles through automatic local style transfer or manual designation, while maintaining memory efficiency for stylization training. In this paper, we introduce a novel 3D GS stylization solution termed Multi-StyleGS to tackle these challenges. In particular, we employ a bipartite matching mechanism to au tomatically identify correspondences between the style images and the local regions of the rendered images. To facilitate local style transfer, we introduce a novel semantic style loss function that employs a segmentation network to apply distinct styles to various objects of the scene and propose a local-global feature matching to enhance the multi-view consistency. Furthermore, this technique can achieve memory efficient training, more texture details and better color match. To better assign a robust semantic label to each Gaussian, we propose several techniques to regularize the segmentation network. As demonstrated by our comprehensive experiments, our approach outperforms existing ones in producing plausible stylization results and offering flexible editing.",
    "arxiv_url": "http://arxiv.org/abs/2506.06846v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06846v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.06822v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06822v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling",
    "authors": [
      "Cheng Peng",
      "Jingxiang Sun",
      "Yushuo Chen",
      "Zhaoqi Su",
      "Zhuo Su",
      "Yebin Liu"
    ],
    "abstract": "Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.",
    "arxiv_url": "http://arxiv.org/abs/2506.06645v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06645v1",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "fast",
      "face",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "Marcus A. Brubaker",
      "David J. Fleet"
    ],
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.09063v1",
    "pdf_url": "http://arxiv.org/pdf/2506.09063v1",
    "published_date": "2025-06-06",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4: Generalizable Sparse Splatting Semantic SLAM",
    "authors": [
      "Mingqi Jiang",
      "Chanho Kim",
      "Chen Ziwen",
      "Li Fuxin"
    ],
    "abstract": "Traditional SLAM algorithms are excellent at camera tracking but might generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting (GS) approaches have emerged as an option for SLAM with accurate, dense 3D map building. However, existing GS-based SLAM methods rely on per-scene optimization which is time-consuming and does not generalize to diverse scenes well. In this work, we introduce the first generalizable GS-based semantic SLAM algorithm that incrementally builds and updates a 3D scene representation from an RGB-D video stream using a learned generalizable network. Our approach starts from an RGB-D image recognition backbone to predict the Gaussian parameters from every downsampled and backprojected image location. Additionally, we seamlessly integrate 3D semantic segmentation into our GS framework, bridging 3D mapping and recognition through a shared backbone. To correct localization drifting and floaters, we propose to optimize the GS for only 1 iteration following global localization. We demonstrate state-of-the-art semantic SLAM performance on the real-world benchmark ScanNet with an order of magnitude fewer Gaussians compared to other recent GS-based methods, and showcase our model's generalization capability through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.06517v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06517v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "localization",
      "tracking",
      "semantic",
      "mapping",
      "gaussian splatting",
      "slam",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements",
    "authors": [
      "Nicol√°s Violante",
      "Andreas Meuleman",
      "Alban Gauthier",
      "Fr√©do Durand",
      "Thibault Groueix",
      "George Drettakis"
    ],
    "abstract": "We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2506.06462v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06462v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading",
    "authors": [
      "Jonathan Schmidt",
      "Simon Giebenhain",
      "Matthias Niessner"
    ],
    "abstract": "We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2506.06271v1",
    "pdf_url": "http://arxiv.org/pdf/2506.06271v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "head",
      "relightable",
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "illumination",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments",
    "authors": [
      "Mingrui Li",
      "Yiming Zhou",
      "Hongxing Zhou",
      "Xinggang Hu",
      "Florian Roemer",
      "Hongyu Wang",
      "Ahmad Osman"
    ],
    "abstract": "Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.05965v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05965v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "motion",
      "mapping",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction",
    "authors": [
      "Yuchao Zheng",
      "Jianing Zhang",
      "Guochen Ning",
      "Hongen Liao"
    ],
    "abstract": "Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page https://surgsplat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05935v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05935v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "lighting",
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy",
    "authors": [
      "Yu Feng",
      "Weikai Lin",
      "Yuge Cheng",
      "Zihan Liu",
      "Jingwen Leng",
      "Minyi Guo",
      "Chen Chen",
      "Shixuan Sun",
      "Yuhao Zhu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural rendering, but it remains computationally demanding on today's mobile SoCs. To address this challenge, we propose Lumina, a hardware-algorithm co-designed system, which integrates two principal optimizations: a novel algorithm, S^2, and a radiance caching mechanism, RC, to improve the efficiency of neural rendering. S2 algorithm exploits temporal coherence in rendering to reduce the computational overhead, while RC leverages the color integration process of 3DGS to decrease the frequency of intensive rasterization computations. Coupled with these techniques, we propose an accelerator architecture, LuminCore, to further accelerate cache lookup and address the fundamental inefficiencies in Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB peak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
    "arxiv_url": "http://arxiv.org/abs/2506.05682v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05682v1",
    "published_date": "2025-06-06",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction",
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ],
    "abstract": "Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancing performance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at https://zzy816.github.io/VoxelSplat-Demo/.",
    "arxiv_url": "http://arxiv.org/abs/2506.05563v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05563v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images",
    "authors": [
      "Andreas Meuleman",
      "Ishaan Shah",
      "Alexandre Lanvin",
      "Bernhard Kerbl",
      "George Drettakis"
    ],
    "abstract": "Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that our solution can provide on-the-fly processing of all the capture scenarios and scene sizes we target while remaining competitive with other methods that only handle specific capture styles or scene sizes in speed, image quality, or both.",
    "arxiv_url": "http://arxiv.org/abs/2506.05558v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05558v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "slam",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting",
    "authors": [
      "Daniel Wang",
      "Patrick Rim",
      "Tian Tian",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to forecast dynamic 3D scenes far beyond the time span seen during training. Existing neural rendering systems - whether NeRF- or 3DGS-based - embed time directly in a deformation network and therefore excel at interpolation but collapse when asked to predict the future, where timestamps are strictly out-of-distribution. ODE-GS eliminates this dependency: after learning a high-fidelity, time-conditioned deformation model for the training window, we freeze it and train a Transformer encoder that summarizes past Gaussian trajectories into a latent state whose continuous evolution is governed by a neural ODE. Numerical integration of this latent flow yields smooth, physically plausible Gaussian trajectories that can be queried at any future instant and rendered in real time. Coupled with a variational objective and a lightweight second-derivative regularizer, ODE-GS attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the strongest baselines. Our results demonstrate that continuous-time latent dynamics are a powerful, practical route to photorealistic prediction of complex 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2506.05480v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05480v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "lightweight",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction",
    "authors": [
      "Yifan Wang",
      "Peishan Yang",
      "Zhen Xu",
      "Jiaming Sun",
      "Zhanhua Zhang",
      "Yong Chen",
      "Hujun Bao",
      "Sida Peng",
      "Xiaowei Zhou"
    ],
    "abstract": "This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin. Project page: https://zju3dv.github.io/freetimegs/ .",
    "arxiv_url": "http://arxiv.org/abs/2506.05348v2",
    "pdf_url": "http://arxiv.org/pdf/2506.05348v2",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Duochao Shi",
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Jia-Wang Bian",
      "Bohan Zhuang",
      "Chunhua Shen"
    ],
    "abstract": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss",
    "arxiv_url": "http://arxiv.org/abs/2506.05327v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05327v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting",
    "authors": [
      "Nan Wang",
      "Yuantao Chen",
      "Lixing Xiao",
      "Weiqing Xiao",
      "Bohan Li",
      "Zhaoxi Chen",
      "Chongjie Ye",
      "Shaocong Xu",
      "Saining Zhang",
      "Ziyang Yan",
      "Pierre Merriaux",
      "Lei Lei",
      "Tianfan Xue",
      "Hao Zhao"
    ],
    "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
    "arxiv_url": "http://arxiv.org/abs/2506.05280v2",
    "pdf_url": "http://arxiv.org/pdf/2506.05280v2",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "mapping",
      "geometry",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DSG-World: Learning a 3D Gaussian World Model from Dual State Videos",
    "authors": [
      "Wenhao Hu",
      "Xuexiang Wen",
      "Xi Li",
      "Gaoang Wang"
    ],
    "abstract": "Building an efficient and physically consistent world model from limited observations is a long standing challenge in vision and robotics. Many existing world modeling pipelines are based on implicit generative models, which are hard to train and often lack 3D or physical consistency. On the other hand, explicit 3D methods built from a single state often require multi-stage processing-such as segmentation, background completion, and inpainting-due to occlusions. To address this, we leverage two perturbed observations of the same scene under different object configurations. These dual states offer complementary visibility, alleviating occlusion issues during state transitions and enabling more stable and complete reconstruction. In this paper, we present DSG-World, a novel end-to-end framework that explicitly constructs a 3D Gaussian World model from Dual State observations. Our approach builds dual segmentation-aware Gaussian fields and enforces bidirectional photometric and semantic consistency. We further introduce a pseudo intermediate state for symmetric alignment and design collaborative co-pruning trategies to refine geometric completeness. DSG-World enables efficient real-to-simulation transfer purely in the explicit Gaussian representation space, supporting high-fidelity rendering and object-level scene manipulation without relying on dense observations or multi-stage pipelines. Extensive experiments demonstrate strong generalization to novel views and scene states, highlighting the effectiveness of our approach for real-world 3D reconstruction and simulation.",
    "arxiv_url": "http://arxiv.org/abs/2506.05217v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05217v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "high-fidelity",
      "lighting",
      "semantic",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D Gaussian Splatting for Vision Training",
    "authors": [
      "Aneesh Deogan",
      "Wout Beks",
      "Peter Teurlings",
      "Koen de Vos",
      "Mark van den Brand",
      "Rene van de Molengraft"
    ],
    "abstract": "Annotated datasets are critical for training neural networks for object detection, yet their manual creation is time- and labour-intensive, subjective to human error, and often limited in diversity. This challenge is particularly pronounced in the domain of robotics, where diverse and dynamic scenarios further complicate the creation of representative datasets. To address this, we propose a novel method for automatically generating annotated synthetic data in Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for rapid synthetic data generation. We demonstrate that synthetic datasets can achieve performance comparable to that of real-world datasets while significantly reducing the time required to generate and annotate data. Additionally, combining real-world and synthetic data significantly increases object detection performance by leveraging the quality of real-world images with the easier scalability of synthetic data. To our knowledge, this is the first application of synthetic data for training object detection algorithms in the highly dynamic and varied environment of robot soccer. Validation experiments reveal that a detector trained on synthetic images performs on par with one trained on manually annotated real-world images when tested on robot soccer match scenarios. Our method offers a scalable and comprehensive alternative to traditional dataset creation, eliminating the labour-intensive error-prone manual annotation process. By generating datasets in a simulator where all elements are intrinsically known, we ensure accurate annotations while significantly reducing manual effort, which makes it particularly valuable for robotics applications requiring diverse and scalable training data.",
    "arxiv_url": "http://arxiv.org/abs/2506.05092v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05092v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Christopher Maxey",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "Despite significant advancements in dynamic neural rendering, existing methods fail to address the unique challenges posed by UAV-captured scenarios, particularly those involving monocular camera setups, top-down perspective, and multiple small, moving humans, which are not adequately represented in existing datasets. In this work, we introduce UAV4D, a framework for enabling photorealistic rendering for dynamic real-world scenes captured by UAVs. Specifically, we address the challenge of reconstructing dynamic scenes with multiple moving pedestrians from monocular video data without the need for additional sensors. We use a combination of a 3D foundation model and a human mesh reconstruction model to reconstruct both the scene background and humans. We propose a novel approach to resolve the scene scale ambiguity and place both humans and the scene in world coordinates by identifying human-scene contact points. Additionally, we exploit the SMPL model and background mesh to initialize Gaussian splats, enabling holistic scene rendering. We evaluated our method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and Okutama-Action, each with distinct characteristics and 10~50 humans. Our results demonstrate the benefits of our approach over existing methods in novel view synthesis, achieving a 1.5 dB PSNR improvement and superior visual sharpness.",
    "arxiv_url": "http://arxiv.org/abs/2506.05011v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05011v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "human",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting",
    "authors": [
      "Alfred T. Christiansen",
      "Andreas H. H√∏jrup",
      "Morten K. Stephansen",
      "Md Ibtihaj A. Sakib",
      "Taman S. Poojary",
      "Filip Slezak",
      "Morten S. Laursen",
      "Thomas B. Moeslund",
      "Joakim B. Haurum"
    ],
    "abstract": "Training neural networks for tasks such as 3D point cloud semantic segmentation demands extensive datasets, yet obtaining and annotating real-world point clouds is costly and labor-intensive. This work aims to introduce a novel pipeline for generating realistic synthetic data, by leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to generate 3D assets of multiple different agricultural vehicles instead of using generic models. These assets are placed in a simulated environment, where the point clouds are generated using a simulated LiDAR. This is a flexible approach that allows changing the LiDAR specifications without incurring additional costs. We evaluated the impact of synthetic data on segmentation models such as PointNet++, Point Transformer V3, and OACNN, by training and validating the models only on synthetic data. Remarkably, the PTv3 model had an mIoU of 91.35\\%, a noteworthy result given that the model had neither been trained nor validated on any real data. Further studies even suggested that in certain scenarios the models trained only on synthetically generated data performed better than models trained on real-world data. Finally, experiments demonstrated that the models can generalize across semantic classes, enabling accurate predictions on mesh models they were never trained on.",
    "arxiv_url": "http://arxiv.org/abs/2506.05009v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05009v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer",
    "authors": [
      "Filip Slezak",
      "Magnus K. Gjerde",
      "Joakim B. Haurum",
      "Ivan Nikolov",
      "Morten S. Laursen",
      "Thomas B. Moeslund"
    ],
    "abstract": "In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for stereo dataset generation, offering an efficient alternative to Neural Radiance Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore utilizing the reconstructed geometry from the explicit 3D representations as well as depth estimates from the FoundationStereo model in an expert knowledge transfer setup. We find that when fine-tuning stereo models on 3DGS-generated datasets, we demonstrate competitive performance in zero-shot generalization benchmarks. When using the reconstructed geometry directly, we observe that it is often noisy and contains artifacts, which propagate noise to the trained model. In contrast, we find that the disparity estimates from FoundationStereo are cleaner and consequently result in a better performance on the zero-shot generalization benchmarks. Our method highlights the potential for low-cost, high-fidelity dataset creation and fast fine-tuning for deep stereo models. Moreover, we also reveal that while the latest Gaussian Splatting based methods have achieved superior performance on established benchmarks, their robustness falls short in challenging in-the-wild settings warranting further exploration.",
    "arxiv_url": "http://arxiv.org/abs/2506.04908v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04908v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
    "authors": [
      "Gaia Di Lorenzo",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.",
    "arxiv_url": "http://arxiv.org/abs/2506.04789v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04789v1",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "high-fidelity",
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photoreal Scene Reconstruction from an Egocentric Device",
    "authors": [
      "Zhaoyang Lv",
      "Maurizio Monge",
      "Ka Chen",
      "Yufeng Zhu",
      "Michael Goesele",
      "Jakob Engel",
      "Zhao Dong",
      "Richard Newcombe"
    ],
    "abstract": "In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at http://www.projectaria.com/photoreal-reconstruction/",
    "arxiv_url": "http://arxiv.org/abs/2506.04444v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04444v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "lighting",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available.",
    "arxiv_url": "http://arxiv.org/abs/2506.04351v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04351v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "mapping",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pseudo-Simulation for Autonomous Driving",
    "authors": [
      "Wei Cao",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Daniel Dauner",
      "Xunjiang Gu",
      "Caojun Wang",
      "Yakov Miron",
      "Marco Aiello",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations (R^2=0.8) than the best existing open-loop approach (R^2=0.7). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim.",
    "arxiv_url": "http://arxiv.org/abs/2506.04218v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04218v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/autonomousvision/navsim",
    "keywords": [
      "efficient",
      "head",
      "autonomous driving",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands relatively significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the effectiveness of our approach. Code is available at https://flexgs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2506.04174v1",
    "pdf_url": "http://arxiv.org/pdf/2506.04174v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data",
    "authors": [
      "Ben Moran",
      "Mauro Comi",
      "Arunkumar Byravan",
      "Steven Bohez",
      "Tom Erez",
      "Zhibin Li",
      "Leonard Hasenclever"
    ],
    "abstract": "Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.",
    "arxiv_url": "http://arxiv.org/abs/2506.04120v2",
    "pdf_url": "http://arxiv.org/pdf/2506.04120v2",
    "published_date": "2025-06-04",
    "categories": [
      "cs.RO",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting",
    "authors": [
      "Yang Xiao",
      "Guoan Xu",
      "Qiang Wu",
      "Wenjing Jia"
    ],
    "abstract": "Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view reconstruction methods provide an efficient solution for real-time novel view synthesis by leveraging geometric priors learned from large-scale multi-view datasets and computing 3D Gaussian centers via back-projection. Despite offering strong geometric cues, both feed-forward multi-view depth estimation and flow-depth joint estimation face key limitations: the former suffers from mislocation and artifact issues in low-texture or repetitive regions, while the latter is prone to local noise and global inconsistency due to unreliable matches when ground-truth flow supervision is unavailable. To overcome this, we propose JointSplat, a unified framework that leverages the complementarity between optical flow and depth via a novel probabilistic optimization mechanism. Specifically, this pixel-level mechanism scales the information fusion between depth and flow based on the matching probability of optical flow during training. Building upon the above mechanism, we further propose a novel multi-view depth-consistency loss to leverage the reliability of supervision while suppressing misleading gradients in uncertain areas. Evaluated on RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art (SOTA) methods, demonstrating the effectiveness and robustness of our proposed probabilistic joint flow-depth optimization approach for high-fidelity sparse-view 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2506.03872v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03872v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "efficient",
      "high-fidelity",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting",
    "authors": [
      "Shengjie Lin",
      "Jiading Fang",
      "Muhammad Zubair Irshad",
      "Vitor Campagnolo Guizilini",
      "Rares Andrei Ambrus",
      "Greg Shakhnarovich",
      "Matthew R. Walter"
    ],
    "abstract": "Reconstructing articulated objects prevalent in daily environments is crucial for applications in augmented/virtual reality and robotics. However, existing methods face scalability limitations (requiring 3D supervision or costly annotations), robustness issues (being susceptible to local optima), and rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a self-supervised, category-agnostic framework that leverages 3D Gaussian Splatting (3DGS) to reconstruct articulated objects and infer kinematics from two sets of posed RGB images captured at different articulation states, enabling real-time photorealistic rendering for novel viewpoints and articulations. SplArt augments 3DGS with a differentiable mobility parameter per Gaussian, achieving refined part segmentation. A multi-stage optimization strategy is employed to progressively handle reconstruction, part segmentation, and articulation estimation, significantly enhancing robustness and accuracy. SplArt exploits geometric self-supervision, effectively addressing challenging scenarios without requiring 3D annotations or category-specific priors. Evaluations on established and newly proposed benchmarks, along with applications to real-world scenarios using a handheld RGB camera, demonstrate SplArt's state-of-the-art performance and real-world practicality. Code is publicly available at https://github.com/ripl/splart.",
    "arxiv_url": "http://arxiv.org/abs/2506.03594v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03594v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "github_url": "https://github.com/ripl/splart",
    "keywords": [
      "robotics",
      "face",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
    "authors": [
      "Chengqi Li",
      "Zhihao Shi",
      "Yangdi Lu",
      "Wenbo He",
      "Xiangyu Xu"
    ],
    "abstract": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2506.03538v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03538v1",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "lightweight",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation",
    "authors": [
      "Lukas Meyer",
      "Josef Gr√ºn",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS) framework that is able to generate multi-view consistent novel views from images of multiple, independent cameras with different spectral domains. In contrast to previous approaches, our method does not require cross-modal camera calibration and is versatile enough to model a variety of different spectra, including thermal and near-infra red, without any algorithmic changes.   Unlike existing 3DGS-based frameworks that treat each modality separately (by optimizing per-channel spherical harmonics) and therefore fail to exploit the underlying spectral and spatial correlations, our method leverages a novel neural color representation that encodes multi-spectral information into a learned, compact, per-splat feature embedding. A shallow multi-layer perceptron (MLP) then decodes this embedding to obtain spectral color values, enabling joint learning of all bands within a unified representation.   Our experiments show that this simple yet effective strategy is able to improve multi-spectral rendering quality, while also leading to improved per-spectra rendering quality over state-of-the-art methods. We demonstrate the effectiveness of this new technique in agricultural applications to render vegetation indices, such as normalized difference vegetation index (NDVI).",
    "arxiv_url": "http://arxiv.org/abs/2506.03407v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03407v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gen4D: Synthesizing Humans and Scenes in the Wild",
    "authors": [
      "Jerrin Bright",
      "Zhibo Wang",
      "Yuhao Chen",
      "Sirisha Rambhatla",
      "John Zelek",
      "David Clausi"
    ],
    "abstract": "Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.",
    "arxiv_url": "http://arxiv.org/abs/2506.05397v1",
    "pdf_url": "http://arxiv.org/pdf/2506.05397v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM",
    "authors": [
      "Roman Titkov",
      "Egor Zubkov",
      "Dmitry Yudin",
      "Jaafar Mahmoud",
      "Malik Mohrat",
      "Gennady Sidorov"
    ],
    "abstract": "Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM -- a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera's ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: https://titrom025.github.io/LEG-SLAM/",
    "arxiv_url": "http://arxiv.org/abs/2506.03073v1",
    "pdf_url": "http://arxiv.org/pdf/2506.03073v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "motion",
      "semantic",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Processor Chip Model",
    "authors": [
      "Kaiyan Chang",
      "Mingzhi Chen",
      "Yunji Chen",
      "Zhirong Chen",
      "Dongrui Fan",
      "Junfeng Gong",
      "Nan Guo",
      "Yinhe Han",
      "Qinfen Hao",
      "Shuo Hou",
      "Xuan Huang",
      "Pengwei Jin",
      "Changxin Ke",
      "Cangyuan Li",
      "Guangli Li",
      "Huawei Li",
      "Kuan Li",
      "Naipeng Li",
      "Shengwen Liang",
      "Cheng Liu",
      "Hongwei Liu",
      "Jiahua Liu",
      "Junliang Lv",
      "Jianan Mu",
      "Jin Qin",
      "Bin Sun",
      "Chenxi Wang",
      "Duo Wang",
      "Mingjun Wang",
      "Ying Wang",
      "Chenggang Wu",
      "Peiyang Wu",
      "Teng Wu",
      "Xiao Xiao",
      "Mengyao Xie",
      "Chenwei Xiong",
      "Ruiyuan Xu",
      "Mingyu Yan",
      "Xiaochun Ye",
      "Kuai Yu",
      "Rui Zhang",
      "Shuoming Zhang",
      "Jiacheng Zhao"
    ],
    "abstract": "Computer System Architecture serves as a crucial bridge between software applications and the underlying hardware, encompassing components like compilers, CPUs, coprocessors, and RTL designs. Its development, from early mainframes to modern domain-specific architectures, has been driven by rising computational demands and advancements in semiconductor technology. However, traditional paradigms in computer system architecture design are confronting significant challenges, including a reliance on manual expertise, fragmented optimization across software and hardware layers, and high costs associated with exploring expansive design spaces. While automated methods leveraging optimization algorithms and machine learning have improved efficiency, they remain constrained by a single-stage focus, limited data availability, and a lack of comprehensive human domain knowledge. The emergence of large language models offers transformative opportunities for the design of computer system architecture. By leveraging the capabilities of LLMs in areas such as code generation, data analysis, and performance modeling, the traditional manual design process can be transitioned to a machine-based automated design approach. To harness this potential, we present the Large Processor Chip Model (LPCM), an LLM-driven framework aimed at achieving end-to-end automated computer architecture design. The LPCM is structured into three levels: Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D Gaussian Splatting as a representative workload and employs the concept of software-hardware collaborative design to examine the implementation of the LPCM at Level 1, demonstrating the effectiveness of the proposed approach. Furthermore, this paper provides an in-depth discussion on the pathway to implementing Level 2 and Level 3 of the LPCM, along with an analysis of the existing challenges.",
    "arxiv_url": "http://arxiv.org/abs/2506.02929v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02929v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone",
    "authors": [
      "Zheng Liu",
      "He Zhu",
      "Xinyang Li",
      "Yirun Wang",
      "Yujiao Shi",
      "Wei Li",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D scene rendering. However, rendering city-scale 3DGS scenes on mobile devices, e.g., your smartphones, remains a significant challenge due to the limited resources on mobile devices. A natural solution is to offload computation to the cloud; however, naively streaming rendered frames from the cloud to the client introduces high latency and requires bandwidth far beyond the capacity of current wireless networks.   In this paper, we propose an effective solution to enable city-scale 3DGS rendering on mobile devices. Our key insight is that, under normal user motion, the number of newly visible Gaussians per second remains roughly constant. Leveraging this, we stream only the necessary Gaussians to the client. Specifically, on the cloud side, we propose asynchronous level-of-detail search to identify the necessary Gaussians for the client. On the client side, we accelerate rendering via a lookup table-based rasterization. Combined with holistic runtime optimizations, our system can deliver low-latency, city-scale 3DGS rendering on mobile devices. Compared to existing solutions, Voyager achieves over 100$\\times$ reduction on data transfer and up to 8.9$\\times$ speedup while retaining comparable rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2506.02774v2",
    "pdf_url": "http://arxiv.org/pdf/2506.02774v2",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
    "authors": [
      "Chuanyu Fu",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Guanying Chen",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.",
    "arxiv_url": "http://arxiv.org/abs/2506.02751v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02751v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians",
    "authors": [
      "Pengchong Hu",
      "Zhizhong Han"
    ],
    "abstract": "Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). State-of-the-art methods employ 3D Gaussians to represent a scene, and render these Gaussians through splatting for higher efficiency and better rendering. However, these methods cannot scale up to extremely large scenes, due to the inefficient tracking and mapping strategies that need to optimize all 3D Gaussians in the limited GPU memories throughout the training to maintain the geometry and color consistency to previous RGBD observations. To resolve this issue, we propose novel tracking and mapping strategies to work with a novel 3D representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied 3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels, without needing to learn locations, rotations, and multi-dimensional variances. Tying Gaussians to views not only significantly saves storage but also allows us to employ many more Gaussians to represent local details in the limited GPU memory. Moreover, our strategies remove the need of maintaining all Gaussians learnable throughout the training, while improving rendering quality, and tracking accuracy. We justify the effectiveness of these designs, and report better performance over the latest methods on the widely used benchmarks in terms of rendering and tracking accuracy and scalability. Please see our project page for code and videos at https://machineperceptionlab.github.io/VTGaussian-SLAM-Project .",
    "arxiv_url": "http://arxiv.org/abs/2506.02741v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02741v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "slam",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR",
    "authors": [
      "Zihao Ding",
      "Cheng-Tse Lee",
      "Mufeng Zhu",
      "Tao Guan",
      "Yuan-Chun Sun",
      "Cheng-Hsin Hsu",
      "Yao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an emerging media representation that reconstructs real-world 3D scenes in high fidelity, enabling 6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However, developing and evaluating 3DGS-enabled applications and optimizing their rendering performance, require realistic user navigation data. Such data is currently unavailable for photorealistic 3DGS reconstructions of real-world scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available 6-DoF navigation dataset featuring traces from 46 participants exploring twelve diverse, real-world 3DGS scenes. The dataset was collected at two sites, using the Meta Quest Pro headsets, recording the head pose and eye gaze data for each rendered frame during free world standing 6-DoF navigation. For each of the twelve scenes, we performed careful scene initialization to correct for scene tilt and scale, ensuring a perceptually-comfortable VR experience. We also release our open-source SIBR viewer software fork with record-and-replay functionalities and a suite of utility tools for data processing, conversion, and visualization. The EyeNavGS dataset and its accompanying software tools provide valuable resources for advancing research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.",
    "arxiv_url": "http://arxiv.org/abs/2506.02380v1",
    "pdf_url": "http://arxiv.org/pdf/2506.02380v1",
    "published_date": "2025-06-03",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCodec Studio: A Modular Framework for Gaussian Splat Compression",
    "authors": [
      "Sicheng Li",
      "Chengzhen Wu",
      "Hao Li",
      "Xiang Gao",
      "Yiyi Liao",
      "Lu Yu"
    ],
    "abstract": "3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available at https://github.com/JasonLSC/GSCodec_Studio , to advance the research on Gaussian Splats compression.",
    "arxiv_url": "http://arxiv.org/abs/2506.01822v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01822v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/JasonLSC/GSCodec_Studio",
    "keywords": [
      "compression",
      "4d",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes",
    "authors": [
      "Manuel-Andreas Schneider",
      "Lukas H√∂llein",
      "Matthias Nie√üner"
    ],
    "abstract": "Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.",
    "arxiv_url": "http://arxiv.org/abs/2506.01799v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01799v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
    "authors": [
      "Tenny Yin",
      "Zhiting Mei",
      "Tao Sun",
      "Lihan Zha",
      "Emily Zhou",
      "Jeremy Bao",
      "Miyu Yamane",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.",
    "arxiv_url": "http://arxiv.org/abs/2506.01600v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01600v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes",
    "authors": [
      "Pou-Chun Kung",
      "Skanda Harisha",
      "Ram Vasudevan",
      "Aline Eid",
      "Katherine A. Skinner"
    ],
    "abstract": "High-Fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR / 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction. A project page is available at https://umautobots.github.io/radarsplat.",
    "arxiv_url": "http://arxiv.org/abs/2506.01379v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01379v1",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "autonomous driving",
      "reflection",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting",
    "authors": [
      "Fengze Li",
      "Yangle Liu",
      "Jieming Ma",
      "Hai-Ning Liang",
      "Yaochun Shen",
      "Huangxiang Li",
      "Zhijing Wu"
    ],
    "abstract": "Accurate fruit counting in real-world agricultural environments is a longstanding challenge due to visual occlusions, semantic ambiguity, and the high computational demands of 3D reconstruction. Existing methods based on neural radiance fields suffer from low inference speed, limited generalization, and lack support for open-set semantic control. This paper presents FruitLangGS, a real-time 3D fruit counting framework that addresses these limitations through spatial reconstruction, semantic embedding, and language-guided instance estimation. FruitLangGS first reconstructs orchard-scale scenes using an adaptive Gaussian splatting pipeline with radius-aware pruning and tile-based rasterization for efficient rendering. To enable semantic control, each Gaussian encodes a compressed CLIP-aligned language embedding, forming a compact and queryable 3D representation. At inference time, prompt-based semantic filtering is applied directly in 3D space, without relying on image-space segmentation or view-level fusion. The selected Gaussians are then converted into dense point clouds via distribution-aware sampling and clustered to estimate fruit counts. Experimental results on real orchard data demonstrate that FruitLangGS achieves higher rendering speed, semantic flexibility, and counting accuracy compared to prior approaches, offering a new perspective for language-driven, real-time neural rendering across open-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2506.01109v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01109v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "semantic",
      "segmentation",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation",
    "authors": [
      "Mert Kiray",
      "Paul Uhlenbruck",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "abstract": "Visual effects (VFX) are key to immersion in modern films, games, and AR/VR. Creating 3D effects requires specialized expertise and training in 3D animation software and can be time consuming. Generative solutions typically rely on computationally intense methods such as diffusion models which can be slow at 4D inference. We reformulate 3D animation as a field prediction task and introduce a text-driven framework that infers a time-varying 4D flow field acting on 3D Gaussians. By leveraging large language models (LLMs) and vision-language models (VLMs) for function generation, our approach interprets arbitrary prompts (e.g., \"make the vase glow orange, then explode\") and instantly updates color, opacity, and positions of 3D Gaussians in real time. This design avoids overheads such as mesh extraction, manual or physics-based simulations and allows both novice and expert users to animate volumetric scenes with minimal effort on a consumer device even in a web browser. Experimental results show that simple textual instructions suffice to generate compelling time-varying VFX, reducing the manual effort typically required for rigging or advanced modeling. We thus present a fast and accessible pathway to language-driven 3D content creation that can pave the way to democratize VFX further.",
    "arxiv_url": "http://arxiv.org/abs/2506.01091v1",
    "pdf_url": "http://arxiv.org/pdf/2506.01091v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "fast",
      "3d gaussian",
      "4d",
      "ar",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Globally Consistent RGB-D SLAM with 2D Gaussian Splatting",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Liren Jin",
      "Marija Popoviƒá",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable performance of high-fidelity 3D reconstruction. However, the lack of depth rendering consistency and efficient loop closure limits the quality of its geometric reconstructions and its ability to perform globally consistent mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using 2D Gaussian splatting as the map representation. By leveraging the depth-consistent rendering property of the 2D variant, we propose an accurate camera pose optimization method and achieve geometrically accurate 3D reconstruction. In addition, we implement efficient loop detection and camera relocalization by leveraging MASt3R, a 3D foundation model, and achieve efficient map updates by maintaining a local active map. Experiments show that our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface reconstruction quality, and more consistent global map reconstruction compared to existing rendering-based SLAM methods, while maintaining high-fidelity image rendering and improved computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2506.00970v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00970v1",
    "published_date": "2025-06-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "face",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splat Vulnerabilities",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haoran Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Willian T. Lunardi",
      "Martin Andreoni",
      "Polo Chau"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.",
    "arxiv_url": "http://arxiv.org/abs/2506.00280v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00280v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Voxelization for Transform coding of 3D Gaussian splatting data",
    "authors": [
      "Chenjunjie Wang",
      "Shashank N. Sridhara",
      "Eduardo Pavez",
      "Antonio Ortega",
      "Cheng Chang"
    ],
    "abstract": "We present a novel compression framework for 3D Gaussian splatting (3DGS) data that leverages transform coding tools originally developed for point clouds. Contrary to existing 3DGS compression methods, our approach can produce compressed 3DGS models at multiple bitrates in a computationally efficient way. Point cloud voxelization is a discretization technique that point cloud codecs use to improve coding efficiency while enabling the use of fast transform coding algorithms. We propose an adaptive voxelization algorithm tailored to 3DGS data, to avoid the inefficiencies introduced by uniform voxelization used in point cloud codecs. We ensure the positions of larger volume Gaussians are represented at high resolution, as these significantly impact rendering quality. Meanwhile, a low-resolution representation is used for dense regions with smaller Gaussians, which have a relatively lower impact on rendering quality. This adaptive voxelization approach significantly reduces the number of Gaussians and the bitrate required to encode the 3DGS data. After voxelization, many Gaussians are moved or eliminated. Thus, we propose to fine-tune/recolor the remaining 3DGS attributes with an initialization that can reduce the amount of retraining required. Experimental results on pre-trained datasets show that our proposed compression framework outperforms existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2506.00271v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00271v1",
    "published_date": "2025-05-30",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Understanding while Exploring: Semantics-driven Active Mapping",
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Hairong Yin",
      "Yi Xu",
      "Philippos Mordohai"
    ],
    "abstract": "Effective robotic autonomy in unknown environments demands proactive exploration and precise understanding of both geometry and semantics. In this paper, we propose ActiveSGM, an active semantic mapping framework designed to predict the informativeness of potential observations before execution. Built upon a 3D Gaussian Splatting (3DGS) mapping backbone, our approach employs semantic and geometric uncertainty quantification, coupled with a sparse semantic representation, to guide exploration. By enabling robots to strategically select the most beneficial viewpoints, ActiveSGM efficiently enhances mapping completeness, accuracy, and robustness to noisy semantic data, ultimately supporting more adaptive scene exploration. Our experiments on the Replica and Matterport3D datasets highlight the effectiveness of ActiveSGM in active semantic mapping tasks.",
    "arxiv_url": "http://arxiv.org/abs/2506.00225v1",
    "pdf_url": "http://arxiv.org/pdf/2506.00225v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "mapping",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion",
    "authors": [
      "Yangyi Huang",
      "Ye Yuan",
      "Xueting Li",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.",
    "arxiv_url": "http://arxiv.org/abs/2505.24877v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24877v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "high-fidelity",
      "motion",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores",
    "authors": [
      "Zimu Liao",
      "Jifeng Ding",
      "Rong Fu",
      "Siwei Cui",
      "Ruixuan Gong",
      "Li Wang",
      "Boni Hu",
      "Yi Wang",
      "Hengjie Li",
      "XIngcheng Zhang",
      "Hui Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where conditional alpha-blending dominates the time cost in the rendering pipeline. This paper proposes TC-GS, an algorithm-independent universal module that expands Tensor Core (TCU) applicability for 3DGS, leading to substantial speedups and seamless integration into existing 3DGS optimization frameworks. The key innovation lies in mapping alpha computation to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS implementations. TC-GS provides plug-and-play acceleration for existing top-tier acceleration algorithms tightly coupled with rendering pipeline designs, like Gaussian compression and redundancy elimination algorithms. Additionally, we introduce a global-to-local coordinate transformation to mitigate rounding errors from quadratic terms of pixel coordinates caused by Tensor Core half-precision computation. Extensive experiments demonstrate that our method maintains rendering quality while providing an additional 2.18x speedup over existing Gaussian acceleration algorithms, thus reaching up to a total 5.6x acceleration. The code is currently available at anonymous \\href{https://github.com/TensorCore3DGS/3DGSTensorCore}",
    "arxiv_url": "http://arxiv.org/abs/2505.24796v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24796v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC",
      "I.3.6; I.3.2; D.1.3"
    ],
    "github_url": "https://github.com/TensorCore3DGS/3DGSTensorCore",
    "keywords": [
      "fast",
      "mapping",
      "3d gaussian",
      "acceleration",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting",
    "authors": [
      "Jiazhong Cen",
      "Xudong Zhou",
      "Jiemin Fang",
      "Changsong Wen",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints--a phenomenon we term view-dependent semantics. To address this challenge, we propose LaGa (Language Gaussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of +18.7% mIoU over the previous SOTA on the LERF-OVS dataset. Our code is available at: https://github.com/SJTU-DeepVisionLab/LaGa.",
    "arxiv_url": "http://arxiv.org/abs/2505.24746v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24746v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/SJTU-DeepVisionLab/LaGa",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GARLIC: GAussian Representation LearnIng for spaCe partitioning",
    "authors": [
      "Panagiotis Rigas",
      "Panagiotis Drivas",
      "Charalambos Tzamos",
      "Ioannis Chamodrakas",
      "George Ioannakis",
      "Leonidas J. Guibas",
      "Ioannis Z. Emiris"
    ],
    "abstract": "We introduce GARLIC (GAussian Representation LearnIng for spaCe partitioning), a novel indexing structure based on \\(N\\)-dimensional Gaussians for efficiently learning high-dimensional vector spaces. Our approach is inspired from Gaussian splatting techniques, typically used in 3D rendering, which we adapt for high-dimensional search and classification. We optimize Gaussian parameters using information-theoretic objectives that balance coverage, assignment confidence, and structural and semantic consistency. A key contribution is to progressively refine the representation through split and clone operations, handling hundreds of dimensions, thus handling varying data densities. GARLIC offers the fast building times of traditional space partitioning methods (e.g., under \\(\\sim5\\) min build time for SIFT1M) while achieving \\(\\sim50\\%\\) Recall10@10 in low-candidate regimes. Experimental results on standard benchmarks demonstrate our method's consistency in (a) \\(k\\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by using about half their probes for the same Recall10@10 in Fashion-MNIST, and (b) in classification tasks, beating by \\(\\sim15\\%\\) accuracy other majority voting methods. Further, we show strong generalization capabilities, maintaining high accuracy even with downsampled training data: using just \\(1\\%\\) of the training data returns \\(\\sim 45\\%\\) Recall@1, thus making GARLIC quite powerful for applications requiring both speed and accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2505.24608v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24608v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework",
    "authors": [
      "Xin Kang",
      "Zihan Zheng",
      "Lei Chu",
      "Yue Gao",
      "Jiahao Li",
      "Hao Pan",
      "Xuejin Chen",
      "Yan Lu"
    ],
    "abstract": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.24245v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24245v1",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians",
    "authors": [
      "Zixun Huang",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the quality and efficiency of differentiable rendering. However, its high efficiency stems from an approximation of projecting 3D Gaussians onto the image plane as 2D Gaussians, which inherently limits rendering quality--particularly under large Field-of-View (FoV) camera inputs. While several recent works have extended 3DGS to mitigate these approximation errors, none have successfully achieved both exactness and high efficiency simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient Volumetric Gaussian Rendering method. Starting from first principles, we derive a closed-form expression for the density integral along a ray traversing a 3D Gaussian distribution. This formulation enables precise forward rendering with arbitrary camera models and supports gradient-based optimization of 3D Gaussian parameters. To ensure both exactness and real-time performance, we propose an efficient method for computing a tight Particle Bounding Frustum (PBF) for each 3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also introduce a novel Bipolar Equiangular Projection (BEAP) representation to accelerate ray association under generic camera models. BEAP further provides a more uniform ray sampling strategy to apply supervision, which empirically improves reconstruction quality. Experiments on multiple pinhole and fisheye datasets show that our method consistently outperforms prior methods, establishing a new state-of-the-art in real-time neural rendering.",
    "arxiv_url": "http://arxiv.org/abs/2505.24053v1",
    "pdf_url": "http://arxiv.org/pdf/2505.24053v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "authors": [
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Duochao Shi",
      "Akide Liu",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.",
    "arxiv_url": "http://arxiv.org/abs/2505.23734v2",
    "pdf_url": "http://arxiv.org/pdf/2505.23734v2",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "authors": [
      "Lihan Jiang",
      "Yucheng Mao",
      "Linning Xu",
      "Tao Lu",
      "Kerui Ren",
      "Yichen Jin",
      "Xudong Xu",
      "Mulin Yu",
      "Jiangmiao Pang",
      "Feng Zhao",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/",
    "arxiv_url": "http://arxiv.org/abs/2505.23716v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23716v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mobi-$œÄ$: Mobilizing Your Robot Learning Policy",
    "authors": [
      "Jingyun Yang",
      "Isabella Huang",
      "Brandon Vu",
      "Max Bajracharya",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.",
    "arxiv_url": "http://arxiv.org/abs/2505.23692v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23692v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis",
    "authors": [
      "Nathaniel Burgdorfer",
      "Philippos Mordohai"
    ],
    "abstract": "In this work, we introduce an inference-time optimization framework utilizing triangles to represent the geometry and appearance of the scene. More specifically, we develop a scene optimization algorithm for triangle soup, a collection of disconnected semi-transparent triangle primitives. Compared to the current most-widely used primitives for 3D scene representation, namely Gaussian splats, triangles allow for more expressive color interpolation, and benefit from a large algorithmic infrastructure for downstream tasks. Triangles, unlike full-rank Gaussian kernels, naturally combine to form surfaces. We formulate connectivity forces between triangles during optimization, encouraging explicit, but soft, surface continuity in 3D. We perform experiments on a representative 3D reconstruction dataset and show competitive photometric and geometric results.",
    "arxiv_url": "http://arxiv.org/abs/2505.23642v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23642v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "face",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "authors": [
      "Chuandong Liu",
      "Huijiao Wang",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.23280v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23280v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/azhuantou/MixGS",
    "keywords": [
      "3d gaussian",
      "vr",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
    "authors": [
      "Jonas Kulhanek",
      "Marie-Julie Rakotosaona",
      "Fabian Manhardt",
      "Christina Tsalicoglou",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Songyou Peng",
      "Federico Tombari"
    ],
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.",
    "arxiv_url": "http://arxiv.org/abs/2505.23158v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23158v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "outdoor",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images",
    "authors": [
      "Yu Sheng",
      "Jiajun Deng",
      "Xinran Zhang",
      "Yu Zhang",
      "Bei Hua",
      "Yanyong Zhang",
      "Jianmin Ji"
    ],
    "abstract": "A major breakthrough in 3D reconstruction is the feedforward paradigm to generate pixel-wise 3D points or Gaussian primitives from sparse, unposed images. To further incorporate semantics while avoiding the significant memory and storage costs of high-dimensional semantic features, existing methods extend this paradigm by associating each primitive with a compressed semantic feature vector. However, these methods have two major limitations: (a) the naively compressed feature compromises expressiveness, affecting the model's ability to capture fine-grained semantics, and (b) the pixel-wise primitive prediction introduces redundancy in overlapping areas, causing unnecessary memory overhead. To this end, we introduce \\textbf{SpatialSplat}, a feedforward framework that produces redundancy-aware Gaussians and capitalizes on a dual-field semantic representation. Particularly, with the insight that primitives within the same instance exhibit high semantic consistency, we decompose the semantic representation into a coarse feature field that encodes uncompressed semantics with minimal primitives, and a fine-grained yet low-dimensional feature field that captures detailed inter-instance relationships. Moreover, we propose a selective Gaussian mechanism, which retains only essential Gaussians in the scene, effectively eliminating redundant primitives. Our proposed Spatialsplat learns accurate semantic information and detailed instances prior with more compact 3D Gaussians, making semantic 3D reconstruction more applicable. We conduct extensive experiments to evaluate our method, demonstrating a remarkable 60\\% reduction in scene representation parameters while achieving superior performance over state-of-the-art methods. The code will be made available for future investigation.",
    "arxiv_url": "http://arxiv.org/abs/2505.23044v1",
    "pdf_url": "http://arxiv.org/pdf/2505.23044v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "semantic",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2505.22978v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22978v1",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS Compression with Sparsity-guided Hierarchical Transform Coding",
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its fast and high-quality rendering, but it has a very large memory footprint incurring high transmission and storage overhead. Recently, some neural compression methods, such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach of end-to-end optimized analysis-synthesis transforms which has been proven highly effective in neural signal compression. Without an appropriate analysis transform, signal correlations cannot be removed by sparse representation. Without such transforms the only way to remove signal redundancies is through entropy coding driven by a complex and expensive context modeling, which results in slower speed and suboptimal rate-distortion (R-D) performance. To overcome this weakness, we propose Sparsity-guided Hierarchical Transform Coding (SHTC), the first end-to-end optimized transform coding framework for 3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight context model. This joint optimization enables the transform to produce representations that approach the best R-D performance possible. The SHTC framework consists of a base layer using KLT for data decorrelation, and a sparsity-coded enhancement layer that compresses the KLT residuals to refine the representation. The enhancement encoder learns a linear transform to project high-dimensional inputs into a low-dimensional space, while the decoder unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct the residuals. All components are designed to be interpretable, allowing the incorporation of signal priors and fewer parameters than black-box transforms. This novel design significantly improves R-D performance with minimal additional parameters and computational overhead.",
    "arxiv_url": "http://arxiv.org/abs/2505.22908v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22908v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "compression",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians",
    "authors": [
      "Hidenobu Matsuki",
      "Gwangbin Bae",
      "Andrew J. Davison"
    ],
    "abstract": "We propose the first 4D tracking and mapping method that jointly performs camera localization and non-rigid surface reconstruction via differentiable rendering. Our approach captures 4D scenes from an online stream of color images with depth measurements or predictions by jointly optimizing scene geometry, appearance, dynamics, and camera ego-motion. Although natural environments exhibit complex non-rigid motions, 4D-SLAM remains relatively underexplored due to its inherent challenges; even with 2.5D signals, the problem is ill-posed because of the high dimensionality of the optimization space. To overcome these challenges, we first introduce a SLAM method based on Gaussian surface primitives that leverages depth signals more effectively than 3D Gaussians, thereby achieving accurate surface reconstruction. To further model non-rigid deformations, we employ a warp-field represented by a multi-layer perceptron (MLP) and introduce a novel camera pose estimation technique along with surface regularization terms that facilitate spatio-temporal reconstruction. In addition to these algorithmic challenges, a significant hurdle in 4D SLAM research is the lack of reliable ground truth and evaluation protocols, primarily due to the difficulty of 4D capture using commodity sensors. To address this, we present a novel open synthetic dataset of everyday objects with diverse motions, leveraging large-scale object models and animation modeling. In summary, we open up the modern 4D-SLAM research by introducing a novel method and evaluation protocols grounded in modern vision and rendering techniques.",
    "arxiv_url": "http://arxiv.org/abs/2505.22859v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22859v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "motion",
      "face",
      "deformation",
      "mapping",
      "geometry",
      "3d gaussian",
      "4d",
      "slam",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting",
    "authors": [
      "Kornel Howil",
      "Joanna Waczy≈Ñska",
      "Piotr Borycki",
      "Tadeusz Dziarmaga",
      "Marcin Mazur",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussians, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussians approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving a model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussians as a universal and efficient solution for multimodal style transfer.",
    "arxiv_url": "http://arxiv.org/abs/2505.22854v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22854v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Zehao Li",
      "Hao Jiang",
      "Yujun Cai",
      "Jianing Chen",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Yiwei Wang",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ],
    "abstract": "Although dynamic scene reconstruction has long been a fundamental challenge in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a promising direction by enabling high-quality, real-time rendering through explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic reconstruction often suffer from \\textit{spatio-temporal incoherence} during initialization, where canonical Gaussians are constructed by aggregating observations from multiple frames without temporal distinction. This results in spatio-temporally entangled representations, making it difficult to model dynamic motion accurately. To overcome this limitation, we propose \\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a plug-and-play module that learns spatio-temporal probability distributions for each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation field, and a consistency regularization to jointly disentangle spatial and temporal patterns. Extensive experiments demonstrate that incorporating our module into existing 3DGS-based dynamic scene reconstruction frameworks leads to notable improvements in both reconstruction quality and spatio-temporal consistency across synthetic and real-world benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2505.22400v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22400v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments",
    "authors": [
      "Wancai Zheng",
      "Linlin Ou",
      "Jiajie He",
      "Libo Zhou",
      "Xinyi Yu",
      "Yan Wei"
    ],
    "abstract": "Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous Localization and Mapping (SLAM) have significantly progressed in tracking and high-fidelity mapping. However, their sequential optimization framework and sensitivity to dynamic objects limit real-time performance and robustness in real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for dynamic environments that decouples tracking and mapping through a parallelized framework. A probabilistic octree is employed to manage Gaussian primitives adaptively, enabling efficient initialization and pruning without hand-crafted thresholds. To robustly filter dynamic regions during tracking, we propose a training-free uncertainty estimator that fuses multi-modal residuals to estimate per-pixel motion uncertainty, achieving open-set dynamic object handling without reliance on semantic labels. Furthermore, a temporal encoder is designed to enhance rendering quality. Concurrently, low-dimensional features are efficiently transformed via a shallow multilayer perceptron to construct DINO features, which are then employed to enrich the Gaussian field and improve the robustness of uncertainty prediction. Extensive experiments on multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57 dB PSNR), while maintaining real-time performance and producing reusable, artifact-free static maps in dynamic environments.The project: https://aczheng-cai.github.io/up_slam.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2505.22335v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22335v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "motion",
      "semantic",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss",
    "authors": [
      "Wenjun Lu",
      "Haodong Chen",
      "Anqi Yi",
      "Yuk Ying Chung",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Novel view synthesis is a fundamental task in 3D computer vision that aims to reconstruct realistic images from a set of posed input views. However, reconstruction quality degrades significantly under sparse-view conditions due to limited geometric cues. Existing methods, such as Neural Radiance Fields (NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from blurred details and structural artifacts when trained with insufficient views. Recent works have identified the quality of rendered depth as a key factor in mitigating these artifacts, as it directly affects geometric accuracy and view consistency. In this paper, we address these challenges by introducing Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that progressively refines geometry from coarse to fine levels. Central to HDGS is a novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and estimated monocular depths across multiple spatial scales. By enforcing multi-scale depth consistency, our method substantially improves structural fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU benchmarks demonstrate that HDGS achieves state-of-the-art performance under sparse-view settings while maintaining efficient and high-quality rendering",
    "arxiv_url": "http://arxiv.org/abs/2505.22279v1",
    "pdf_url": "http://arxiv.org/pdf/2505.22279v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hyperspectral Gaussian Splatting",
    "authors": [
      "Sunil Kumar Narayanan",
      "Lingjun Zhao",
      "Lu Gan",
      "Yongsheng Chen"
    ],
    "abstract": "Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise determination of nutritional elements in samples. Recently, 3D reconstruction methods have been used to create implicit neural representations of HSI scenes, which can help localize the target object's nutrient composition spatially and spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit representation that can render hyperspectral channel compositions of each spatial location from any viewing direction. However, it faces limitations in training time and rendering speed. In this paper, we propose Hyperspectral Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of the hyperspectral scenes and novel view synthesis for the entire spectral range. To enhance the model's ability to capture fine-grained reflectance variations across the light spectrum and leverage correlations between adjacent wavelengths for denoising, we introduce a wavelength encoder to generate wavelength-specific spherical harmonics offsets. We also introduce a novel Kullback--Leibler divergence-based loss to mitigate the spectral distribution gap between the rendered image and the ground truth. A diffusion model is further applied for denoising the rendered images and generating photorealistic hyperspectral images. We present extensive evaluations on five diverse hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our proposed HS-GS framework. The results demonstrate that HS-GS achieves new state-of-the-art performance among all previously published methods. Code will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2505.21890v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21890v1",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis",
    "authors": [
      "Yipengjing Sun",
      "Chenyang Wang",
      "Shunyuan Zheng",
      "Zonglin Li",
      "Shengping Zhang",
      "Xiangyang Ji"
    ],
    "abstract": "We propose GRGS, a generalizable and relightable 3D Gaussian framework for high-fidelity human novel view synthesis under diverse lighting conditions. Unlike existing methods that rely on per-character optimization or ignore physical constraints, GRGS adopts a feed-forward, fully supervised strategy that projects geometry, material, and illumination cues from multi-view 2D observations into 3D Gaussian representations. Specifically, to reconstruct lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement (LGR) module trained on synthetically relit data to predict accurate depth and surface normals. Based on the high-quality geometry, a Physically Grounded Neural Rendering (PGNR) module is further proposed to integrate neural prediction with physics-based shading, supporting editable relighting with shadows and indirect illumination. Besides, we design a 2D-to-3D projection training scheme that leverages differentiable supervision from ambient occlusion, direct, and indirect lighting maps, which alleviates the computational cost of explicit ray tracing. Extensive experiments demonstrate that GRGS achieves superior visual quality, geometric consistency, and generalization across characters and lighting conditions.",
    "arxiv_url": "http://arxiv.org/abs/2505.21502v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21502v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "relightable",
      "ray tracing",
      "relighting",
      "lighting",
      "shadow",
      "face",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "illumination",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation",
    "authors": [
      "Kerui Ren",
      "Jiayang Bai",
      "Linning Xu",
      "Lihan Jiang",
      "Jiangmiao Pang",
      "Mulin Yu",
      "Bo Dai"
    ],
    "abstract": "Object compositing offers significant promise for augmented reality (AR) and embodied intelligence applications. Existing approaches predominantly focus on single-image scenarios or intrinsic decomposition techniques, facing challenges with multi-view consistency, complex scenes, and diverse lighting conditions. Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based methods, have enhanced consistency but are limited by scalability, heavy data requirements, or prolonged reconstruction time per scene. To broaden its applicability, we introduce MV-CoLight, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. Our novel feed-forward architecture models lighting and shadows directly, avoiding the iterative biases of diffusion-based methods. We employ a Hilbert curve-based mapping to align 2D image inputs with 3D Gaussian scene representations seamlessly. To facilitate training and evaluation, we further introduce a large-scale 3D compositing dataset. Experiments demonstrate state-of-the-art harmonized results across standard benchmarks and our dataset, as well as casually captured real-world scenes demonstrate the framework's robustness and wide generalization.",
    "arxiv_url": "http://arxiv.org/abs/2505.21483v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21483v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "shadow",
      "mapping",
      "3d gaussian",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility",
    "authors": [
      "Yidi Li",
      "Jun Xiao",
      "Zhengda Lu",
      "Yiqun Wang",
      "Haiyong Jiang"
    ],
    "abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling.",
    "arxiv_url": "http://arxiv.org/abs/2505.21377v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21377v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure from Collision",
    "authors": [
      "Takuhiro Kaneko"
    ],
    "abstract": "Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.",
    "arxiv_url": "http://arxiv.org/abs/2505.21335v1",
    "pdf_url": "http://arxiv.org/pdf/2505.21335v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling",
    "authors": [
      "Jieyu Yuan",
      "Yujun Li",
      "Yuanlin Zhang",
      "Chunle Guo",
      "Xiongxin Tang",
      "Ruixing Wang",
      "Chongyi Li"
    ],
    "abstract": "Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at https://bilityniu.github.io/3D-UIR.",
    "arxiv_url": "http://arxiv.org/abs/2505.21238v2",
    "pdf_url": "http://arxiv.org/pdf/2505.21238v2",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians",
    "authors": [
      "Weihang Liu",
      "Yuhui Zhong",
      "Yuke Li",
      "Xi Chen",
      "Jiadi Cui",
      "Honglong Zhang",
      "Lan Xu",
      "Xin Lou",
      "Yujiao Shi",
      "Jingyi Yu",
      "Yingliang Zhang"
    ],
    "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for applications such as AR navigation, UAV based inspection, and smart city digital twins. While aerial imagery offers broad coverage and complements limitations of ground-based data, reconstructing city-scale environments from such views remains challenging due to occlusions, incomplete geometry, and high memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve scalability and visual quality but remain limited by dense primitive usage, long training times, and poor suit ability for edge devices. We propose CityGo, a hybrid framework that combines textured proxy geometry with residual and surrounding 3D Gaussians for lightweight, photorealistic rendering of urban scenes from aerial perspectives. Our approach first extracts compact building proxy meshes from MVS point clouds, then uses zero order SH Gaussians to generate occlusion-free textures via image-based rendering and back-projection. To capture high-frequency details, we introduce residual Gaussians placed based on proxy-photo discrepancies and guided by depth priors. Broader urban context is represented by surrounding Gaussians, with importance-aware downsampling applied to non-critical regions to reduce redundancy. A tailored optimization strategy jointly refines proxy textures and Gaussian parameters, enabling real-time rendering of complex urban scenes on mobile GPUs with significantly reduced training and memory requirements. Extensive experiments on real-world aerial datasets demonstrate that our hybrid representation significantly reduces training time, achieving on average 1.4x speedup, while delivering comparable visual fidelity to pure 3D Gaussian Splatting approaches. Furthermore, CityGo enables real-time rendering of large-scale urban scenes on mobile consumer GPUs, with substantially reduced memory usage and energy consumption.",
    "arxiv_url": "http://arxiv.org/abs/2505.21041v3",
    "pdf_url": "http://arxiv.org/pdf/2505.21041v3",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient",
    "authors": [
      "Jason Chui",
      "Daniel Cremers"
    ],
    "abstract": "Classical Bundle Adjustment (BA) methods require accurate initial estimates for convergence and typically assume known camera intrinsics, which limits their applicability when such information is uncertain or unavailable. We propose a novel probabilistic formulation of BA (ProBA) that explicitly models and propagates uncertainty in both the 2D observations and the 3D scene structure, enabling optimization without any prior knowledge of camera poses or focal length. Our method uses 3D Gaussians instead of point-like landmarks and we introduce uncertainty-aware reprojection losses by projecting the 3D Gaussians onto the 2D image space, and enforce geometric consistency across multiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap between their corresponding Gaussian distributions. This probabilistic framework leads to more robust and reliable optimization, even in the presence of outliers in the correspondence set, reducing the likelihood of converging to poor local minima. Experimental results show that \\textit{ProBA} outperforms traditional methods in challenging real-world conditions. By removing the need for strong initialization and known intrinsics, ProBA enhances the practicality of SLAM systems deployed in unstructured environments.",
    "arxiv_url": "http://arxiv.org/abs/2505.20858v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20858v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Runnan Chen",
      "Mingming Gong",
      "Dong Xu",
      "Tongliang Liu"
    ],
    "abstract": "Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.",
    "arxiv_url": "http://arxiv.org/abs/2505.20729v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20729v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting",
    "authors": [
      "Zechen Li",
      "Lanqing Yang",
      "Yiheng Bian",
      "Hao Pan",
      "Yongjian Fu",
      "Yezhou Wang",
      "Yi-Chao Chen",
      "Guangtao Xue",
      "Ju Ren"
    ],
    "abstract": "This paper presents an innovative frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling, offering an advancement over the existing works limited to single-frequency modeling. Grounded in fundamental physics, we uncover the complex relationship between EM wave propagation behaviors and RF frequencies. Inspired by this, we design an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. By training the frequency-embedded 3DGS model, we can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. Finally, we propose a large-scale power angular spectrum (PAS) dataset containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments, and conduct extensive experiments to verify the effectiveness of our method. Our approach achieves an average Structural Similarity Index Measure (SSIM) up to 0.72, and a significant improvement up to 17.8% compared to the current state-of-the-art (SOTA) methods trained on individual test frequencies. Additionally, our method achieves an SSIM of 0.70 without prior training on these frequencies, which represents only a 2.8% performance drop compared to models trained with full PAS data. This demonstrates our model's capability to estimate PAS at unknown frequencies. For related code and datasets, please refer to https://github.com/sim-2-real/Wideband3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.20714v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20714v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/sim-2-real/Wideband3DGS",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniIndoor3D: Comprehensive Indoor 3D Reconstruction",
    "authors": [
      "Xiaobao Wei",
      "Xiaoan Zhang",
      "Hao Wang",
      "Qingpo Wuwu",
      "Ming Lu",
      "Wenzhao Zheng",
      "Shanghang Zhang"
    ],
    "abstract": "We propose a novel framework for comprehensive indoor 3D reconstruction using Gaussian representations, called OmniIndoor3D. This framework enables accurate appearance, geometry, and panoptic reconstruction of diverse indoor scenes captured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized for photorealistic rendering, it lacks the precise geometry critical for high-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines multiple RGB-D images to create a coarse 3D reconstruction, which is then used to initialize the 3D Gaussians and guide the 3DGS training. To decouple the optimization conflict between appearance and geometry, we introduce a lightweight MLP that adjusts the geometric properties of 3D Gaussians. The introduced lightweight MLP serves as a low-pass filter for geometry reconstruction and significantly reduces noise in indoor scenes. To improve the distribution of Gaussian primitives, we propose a densification strategy guided by panoptic priors to encourage smoothness on planar surfaces. Through the joint optimization of appearance, geometry, and panoptic reconstruction, OmniIndoor3D provides comprehensive 3D indoor scene understanding, which facilitates accurate and robust robotic navigation. We perform thorough evaluations across multiple datasets, and OmniIndoor3D achieves state-of-the-art results in appearance, geometry, and panoptic reconstruction. We believe our work bridges a critical gap in indoor 3D reconstruction. The code will be released at: https://ucwxb.github.io/OmniIndoor3D/",
    "arxiv_url": "http://arxiv.org/abs/2505.20610v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20610v1",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "understanding",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
    "authors": [
      "Chenghao Qian",
      "Wenjing Li",
      "Yuhu Guo",
      "Gustav Markkula"
    ],
    "abstract": "In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: https://jumponthemoon.github.io/w-edit",
    "arxiv_url": "http://arxiv.org/abs/2505.20471v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20471v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "lighting",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting",
    "authors": [
      "Lei Tian",
      "Xiaomin Li",
      "Liqian Ma",
      "Hefei Huang",
      "Zirui Zheng",
      "Hao Yin",
      "Taiqing Li",
      "Huchuan Lu",
      "Xu Jia"
    ],
    "abstract": "Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual/augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at https://epsilontl.github.io/CCL-LGS/.",
    "arxiv_url": "http://arxiv.org/abs/2505.20469v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20469v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "autonomous driving",
      "semantic",
      "understanding",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation",
    "authors": [
      "Jinsheng Quan",
      "Chunshi Wang",
      "Yawei Luo"
    ],
    "abstract": "This paper aims to model the dynamics of 3D Gaussians from visual observations to support temporal extrapolation. Existing dynamic 3D reconstruction methods often struggle to effectively learn underlying dynamics or rely heavily on manually defined physical priors, which limits their extrapolation capabilities. To address this issue, we propose a novel dynamic 3D Gaussian Splatting prior-free motion extrapolation framework based on particle dynamics systems. The core advantage of our method lies in its ability to learn differential equations that describe the dynamics of 3D Gaussians, and follow them during future frame extrapolation. Instead of simply fitting to the observed visual frame sequence, we aim to more effectively model the gaussian particle dynamics system. To this end, we introduce a dynamics latent state vector into the standard Gaussian kernel and design a dynamics latent space encoder to extract initial state. Subsequently, we introduce a Neural ODEs-based dynamics module that models the temporal evolution of Gaussian in dynamics latent space. Finally, a Gaussian kernel space decoder is used to decode latent state at the specific time step into the deformation. Experimental results demonstrate that the proposed method achieves comparable rendering quality with existing approaches in reconstruction tasks, and significantly outperforms them in future frame extrapolation. Our code is available at https://github.com/QuanJinSheng/ParticleGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.20270v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20270v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/QuanJinSheng/ParticleGS",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes",
    "authors": [
      "Changjian Jiang",
      "Kerui Ren",
      "Linning Xu",
      "Jiong Chen",
      "Jiangmiao Pang",
      "Yu Zhang",
      "Bo Dai",
      "Mulin Yu"
    ],
    "abstract": "High fidelity 3D reconstruction and rendering hinge on capturing precise geometry while preserving photo realistic detail. Most existing methods either fuse these goals into a single cumbersome model or adopt hybrid schemes whose uniform primitives lead to a trade off between efficiency and fidelity. In this paper, we introduce HaloGS, a dual representation that loosely couples coarse triangles for geometry with Gaussian primitives for appearance, motivated by the lightweight classic geometry representations and their proven efficiency in real world applications. Our design yields a compact yet expressive model capable of photo realistic rendering across both indoor and outdoor environments, seamlessly adapting to varying levels of scene complexity. Experiments on multiple benchmark datasets demonstrate that our method yields both compact, accurate geometry and high fidelity renderings, especially in challenging scenarios where robust geometric structure make a clear difference.",
    "arxiv_url": "http://arxiv.org/abs/2505.20267v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20267v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "geometry",
      "3d reconstruction",
      "ar",
      "compact",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Toshiki Watanabe",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Recent advancements in radiance field rendering, exemplified by Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly progressed 3D modeling and reconstruction. The use of multiple 360-degree omnidirectional images for these tasks is increasingly favored due to advantages in data acquisition and comprehensive scene capture. However, the inherent geometric distortions in common omnidirectional representations, such as equirectangular projection (particularly severe in polar regions and varying with latitude), pose substantial challenges to achieving high-fidelity 3D reconstructions. Current datasets, while valuable, often lack the specific focus, scene composition, and ground truth granularity required to systematically benchmark and drive progress in overcoming these omnidirectional-specific challenges. To address this critical gap, we introduce Omnidirectional Blender 3D (OB3D), a new synthetic dataset curated for advancing 3D reconstruction from multiple omnidirectional images. OB3D features diverse and complex 3D scenes generated from Blender 3D projects, with a deliberate emphasis on challenging scenarios. The dataset provides comprehensive ground truth, including omnidirectional RGB images, precise omnidirectional camera parameters, and pixel-aligned equirectangular maps for depth and normals, alongside evaluation metrics. By offering a controlled yet challenging environment, OB3Daims to facilitate the rigorous evaluation of existing methods and prompt the development of new techniques to enhance the accuracy and reliability of 3D reconstruction from omnidirectional images.",
    "arxiv_url": "http://arxiv.org/abs/2505.20126v1",
    "pdf_url": "http://arxiv.org/pdf/2505.20126v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time",
    "authors": [
      "Chen Sang",
      "Yeqiang Qian",
      "Jiale Zhang",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "For tasks such as urban digital twins, VR/AR/game scene design, or creating synthetic films, the traditional industrial approach often involves manually modeling scenes and using various rendering engines to complete the rendering process. This approach typically requires high labor costs and hardware demands, and can result in poor quality when replicating complex real-world scenes. A more efficient approach is to use data from captured real-world scenes, then apply reconstruction and rendering algorithms to quickly recreate the authentic scene. However, current algorithms are unable to effectively reconstruct and render real-world weather effects. To address this, we propose a framework based on gaussian splatting, that can reconstruct real scenes and render them under synthesized 4D weather effects. Our work can simulate various common weather effects by applying Gaussians modeling and rendering techniques. It supports continuous dynamic weather changes and can easily control the details of the effects. Additionally, our work has low hardware requirements and achieves real-time rendering performance. The result demos can be accessed on our project homepage: weathermagician.github.io",
    "arxiv_url": "http://arxiv.org/abs/2505.19919v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19919v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "4d",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "The use of multi-view images acquired by a 360-degree camera can reconstruct a 3D space with a wide area. There are 3D reconstruction methods from equirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis (NVS) methods. On the other hand, it is necessary to overcome the large distortion caused by the projection model of a 360-degree camera when equirectangular images are used. In 3DGS-based methods, the large distortion of the 360-degree camera model generates extremely large 3D Gaussians, resulting in poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based on 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering accuracy improvement techniques: geometric regularization, scale regularization, and distortion-aware weights and a mask to suppress the effects of obstacles in equirectangular images. Through experiments on public datasets, we demonstrate that ErpGS can render novel view images more accurately than conventional methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.19883v2",
    "pdf_url": "http://arxiv.org/pdf/2505.19883v2",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud",
    "authors": [
      "Natsuki Takama",
      "Shintaro Ito",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Gaussian Splatting (GS) has gained attention as a fast and effective method for novel view synthesis. It has also been applied to 3D reconstruction using multi-view images and can achieve fast and accurate 3D reconstruction. However, GS assumes that the input contains a large number of multi-view images, and therefore, the reconstruction accuracy significantly decreases when only a limited number of input images are available. One of the main reasons is the insufficient number of 3D points in the sparse point cloud obtained through Structure from Motion (SfM), which results in a poor initialization for optimizing the Gaussian primitives. We propose a new 3D reconstruction method, called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along with COLMAP MVS to generate highly accurate and dense 3D point clouds, which are then used to initialize 2D Gaussians. Through experiments on the DTU dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of objects using just three images. The project page is available at https://gsisaoki.github.io/SPARSE2DGS/",
    "arxiv_url": "http://arxiv.org/abs/2505.19854v2",
    "pdf_url": "http://arxiv.org/pdf/2505.19854v2",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "fast",
      "face",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers",
    "authors": [
      "Haofan Ren",
      "Zunjie Zhu",
      "Xiang Chen",
      "Ming Lu",
      "Rongfeng Lu",
      "Chenggang Yan"
    ],
    "abstract": "Neural fields are now the central focus of research in 3D vision and computer graphics. Existing methods mainly focus on various scene representations, such as neural points and 3D Gaussians. However, few works have studied the rendering process to enhance the neural fields. In this work, we propose a plug-in method named K-Buffers that leverages multiple buffers to improve the rendering performance. Our method first renders K buffers from scene representations and constructs K pixel-wise feature maps. Then, We introduce a K-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally, we adopt a feature decoder to generate the rendering image. We also introduce an acceleration strategy to improve rendering speed and quality. We apply our method to well-known radiance field baselines, including neural point fields and 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our method effectively enhances the rendering performance of neural point fields and 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.19564v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19564v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "acceleration",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting",
    "authors": [
      "Wenhua Wu",
      "Chenpeng Su",
      "Siting Zhu",
      "Tianchen Deng",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based Simultaneous Localization and Mapping (SLAM) methods have demonstrated exceptional localization precision and remarkable dense mapping performance. However, dynamic objects introduce critical challenges by disrupting scene consistency, leading to tracking drift and mapping artifacts. Existing methods that employ semantic segmentation or object detection for dynamic identification and filtering typically rely on predefined categorical priors, while discarding dynamic scene information crucial for robotic applications such as dynamic obstacle avoidance and environmental interaction. To overcome these challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework based on Gaussian splitting. We design an adaptive dynamic identification mechanism grounded in scene consistency analysis, comparing geometric and textural discrepancies between real-time observations and historical maps. Ours requires no predefined semantic category priors and adaptively discovers scene dynamics. Precise dynamic object recognition effectively mitigates interference from moving targets during localization. Furthermore, we propose a dynamic-static separation mapping strategy that constructs a temporal Gaussian model to achieve online incremental dynamic modeling. Experiments conducted on multiple dynamic datasets demonstrate our method's flexible and accurate dynamic segmentation capabilities, along with state-of-the-art performance in both localization and mapping.",
    "arxiv_url": "http://arxiv.org/abs/2505.19420v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19420v1",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "localization",
      "tracking",
      "semantic",
      "mapping",
      "segmentation",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images",
    "authors": [
      "Guangan Chen",
      "Anh Minh Truong",
      "Hanhe Lin",
      "Michiel Vlaminck",
      "Wilfried Philips",
      "Hiep Luong"
    ],
    "abstract": "Novel view synthesis in 360$^\\circ$ scenes from extremely sparse input views is essential for applications like virtual reality and augmented reality. This paper presents a novel framework for novel view synthesis in extremely sparse-view cases. As typical structure-from-motion methods are unable to estimate camera poses in extremely sparse-view cases, we apply DUSt3R to estimate camera poses and generate a dense point cloud. Using the poses of estimated cameras, we densely sample additional views from the upper hemisphere space of the scenes, from which we render synthetic images together with the point cloud. Training 3D Gaussian Splatting model on a combination of reference images from sparse views and densely sampled synthetic images allows a larger scene coverage in 3D space, addressing the overfitting challenge due to the limited input in sparse-view cases. Retraining a diffusion-based image enhancement model on our created dataset, we further improve the quality of the point-cloud-rendered images by removing artifacts. We compare our framework with benchmark methods in cases of only four input views, demonstrating significant improvement in novel view synthesis under extremely sparse-view conditions for 360$^\\circ$ scenes.",
    "arxiv_url": "http://arxiv.org/abs/2505.19264v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19264v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Adrien Deliege",
      "Abdullah Hamdi",
      "Silvio Giancola",
      "Anthony Cioppa",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Andrea Tagliasacchi",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for a triangle comeback. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible with standard graphics stacks and GPU hardware, and highly efficient: for the \\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks. The project page is https://trianglesplatting.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2505.19175v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19175v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FHGS: Feature-Homogenized Gaussian Splatting",
    "authors": [
      "Q. G. Duan",
      "Benyun Zhao",
      "Mingqiao Han Yijun Huang",
      "Ben M. Chen"
    ],
    "abstract": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently achieved notable advances. Although 3DGS related methods have efficient rendering capabilities, they fail to address the inherent contradiction between the anisotropic color representation of gaussian primitives and the isotropic requirements of semantic features, leading to insufficient cross-view feature consistency. To overcome the limitation, we proposes $\\textit{FHGS}$ (Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework inspired by physical models, which can achieve high-precision mapping of arbitrary 2D features from pre-trained models to 3D scenes while preserving the real-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$ introduces the following innovations: Firstly, a universal feature fusion architecture is proposed, enabling robust embedding of large-scale pre-trained models' semantic features (e.g., SAM, CLIP) into sparse 3D structures. Secondly, a non-differentiable feature fusion mechanism is introduced, which enables semantic features to exhibit viewpoint independent isotropic distributions. This fundamentally balances the anisotropic rendering of gaussian primitives and the isotropic expression of features; Thirdly, a dual-driven optimization strategy inspired by electric potential fields is proposed, which combines external supervision from semantic feature fields with internal primitive clustering guidance. This mechanism enables synergistic optimization of global semantic alignment and local structural consistency. More interactive results can be accessed on: https://fhgs.cuastro.org/.",
    "arxiv_url": "http://arxiv.org/abs/2505.19154v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19154v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "semantic",
      "mapping",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal infrared Novel-view Synthesis",
    "authors": [
      "Myeongseok Nam",
      "Wongi Park",
      "Minsol Kim",
      "Hyejin Hur",
      "Soomok Lee"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR) imaging has gained attention in novel-view synthesis, showing real-time rendering. However, novel-view synthesis with thermal infrared images suffers from transmission effects, emissivity, and low resolution, leading to floaters and blur effects in rendered images. To address these problems, we introduce Veta-GS, which leverages a view-dependent deformation field and a Thermal Feature Extractor (TFE) to precisely capture subtle thermal variations and maintain robustness. Specifically, we design view-dependent deformation field that leverages camera position and viewing direction, which capture thermal variations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and MonoSSIM loss, which consider appearance, edge, and frequency to maintain robustness. Extensive experiments on the TI-NSD benchmark show that our method achieves better performance over existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.19138v1",
    "pdf_url": "http://arxiv.org/pdf/2505.19138v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes",
    "authors": [
      "Tianchen Deng",
      "Wenhua Wu",
      "Junjie He",
      "Yue Pan",
      "Xirui Jiang",
      "Shenghai Yuan",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "3D Gaussian Splatting has recently shown promising results in dense visual SLAM. However, existing 3DGS-based SLAM methods are all constrained to small-room scenarios and struggle with memory explosion in large-scale scenes and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We design a novel voxel-based progressive 3D Gaussian mapping method with multiple submaps for compact and accurate scene representation in large-scale and long-sequence scenes. This allows us to scale up to arbitrary scenes and improves robustness (even under pose drifts). In addition, we propose a 2D-3D fusion camera tracking method to achieve robust and accurate camera tracking in both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D Gaussian loop closure method to eliminate pose drift. We further propose a submap fusion method with online distillation to achieve global consistency in large-scale scenes when detecting a loop. Experiments on various indoor and outdoor datasets demonstrate the superiority and generalizability of the proposed framework. The code will be open source on https://github.com/dtc111111/vpgs-slam.",
    "arxiv_url": "http://arxiv.org/abs/2505.18992v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18992v1",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dtc111111/vpgs-slam",
    "keywords": [
      "tracking",
      "outdoor",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting",
    "authors": [
      "Yitian Yuan",
      "Qianyue He"
    ],
    "abstract": "Recent works demonstrate the advantages of hardware rasterization for 3D Gaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized graphics and fixed memory footprint. However, extending these benefits to backward-pass gradient computation remains challenging due to graphics pipeline constraints. We present a differentiable hardware rasterizer for 3DGS that overcomes the memory and performance limitations of tile-based software rasterization. Our solution employs programmable blending for per-pixel gradient computation combined with a hybrid gradient reduction strategy (quad-level + subgroup) in fragment shaders, achieving over 10x faster backward rasterization versus naive atomic operations and 3x speedup over the canonical tile-based rasterizer. Systematic evaluation reveals 16-bit render targets (float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving higher gradient accuracy among mixed-precision rendering formats with execution speeds second only to unorm8, while float32 texture incurs severe forward pass performance degradation due to suboptimal hardware optimizations. Our method with float16 formats demonstrates 3.07x acceleration in full pipeline execution (forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset, outperforming the baseline tile-based renderer while preserving hardware rasterization's memory efficiency advantages -- incurring merely 2.67% of the memory overhead required for splat sorting operations. This work presents a unified differentiable hardware rasterization method that simultaneously optimizes runtime and memory usage for 3DGS, making it particularly suitable for resource-constrained devices with limited memory capacity.",
    "arxiv_url": "http://arxiv.org/abs/2505.18764v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18764v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.GR",
      "I.3.7; I.3.1"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "3d gaussian",
      "acceleration",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting",
    "authors": [
      "Shiyun Xie",
      "Zhiru Wang",
      "Yinghao Zhu",
      "Xu Wang",
      "Chengwei Pan",
      "Xiwang Dong"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis (NVS) with its real-time rendering capabilities and superior quality. However, it encounters challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose SuperGS, an expansion of Scaffold-GS designed with a two-stage coarse-to-fine training framework. In the low-resolution stage, we introduce a latent feature field to represent the low-resolution scene, which serves as both the initialization and foundational information for super-resolution optimization. In the high-resolution stage, we propose a multi-view consistent densification strategy that backprojects high-resolution depth maps based on error maps and employs a multi-view voting mechanism, mitigating ambiguities caused by multi-view inconsistencies in the pseudo labels provided by 2D prior models while avoiding Gaussian redundancy. Furthermore, we model uncertainty through variational feature learning and use it to guide further scene representation refinement and adjust the supervisory effect of pseudo-labels, ensuring consistent and detailed scene reconstruction. Extensive experiments demonstrate that SuperGS outperforms state-of-the-art HRNVS methods on both forward-facing and 360-degree datasets.",
    "arxiv_url": "http://arxiv.org/abs/2505.18649v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18649v1",
    "published_date": "2025-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance",
    "authors": [
      "Jack Goffinet",
      "Youngjo Min",
      "Carlo Tomasi",
      "David E. Carlson"
    ],
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose Pose Splatter, a novel framework leveraging shape carving and 3D Gaussian splatting to model the complete pose and appearance of laboratory animals without prior knowledge of animal geometry, per-frame optimization, or manual annotations. We also propose a novel rotation-invariant visual embedding technique for encoding pose and appearance, designed to be a plug-in replacement for 3D keypoint data in downstream behavioral analyses. Experiments on datasets of mice, rats, and zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably, Pose Splatter represents subtle variations in pose, provides better low-dimensional pose embeddings over state-of-the-art as evaluated by humans, and generalizes to unseen data. By eliminating annotation and per-frame optimization bottlenecks, Pose Splatter enables analysis of large-scale, longitudinal behavior needed to map genotype, neural activity, and micro-behavior at unprecedented resolution.",
    "arxiv_url": "http://arxiv.org/abs/2505.18342v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18342v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting",
    "authors": [
      "Karly Hou",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Recently, Gaussian Splatting methods have emerged as a desirable substitute for prior Radiance Field methods for novel-view synthesis of scenes captured with multi-view images or videos. In this work, we propose a novel extension to 4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual learning, we hierarchically decompose the dynamic scene into a \"video-segment-frame\" structure, with segments dynamically adjusted by optical flow. Then, instead of directly predicting the time-dependent signals, we model the signal as the sum of video-constant values, segment-constant values, and frame-specific residuals, as inspired by the success of residual learning. This approach allows more flexible models that adapt to highly variable scenes. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets, with the greatest improvements on complex scenes with large movements, occlusions, and fine details, where current methods degrade most.",
    "arxiv_url": "http://arxiv.org/abs/2505.18306v2",
    "pdf_url": "http://arxiv.org/pdf/2505.18306v2",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes",
    "authors": [
      "Haihong Xiao",
      "Jianan Zou",
      "Yuxin Zhou",
      "Ying He",
      "Wenxiong Kang"
    ],
    "abstract": "We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor environments. SplatCo builds upon two novel components: (1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features that represent fine surface details. This fusion is achieved through a novel hierarchical compensation strategy, ensuring both global consistency and local detail preservation; and (2) a cross-view assisted training strategy that enhances multi-view consistency by synchronizing gradient updates across viewpoints, applying visibility-aware densification, and pruning overfitted or inaccurate Gaussians based on structural consistency. Through joint optimization of structural representation and multi-view coherence, SplatCo effectively reconstructs fine-grained geometric structures and complex textures in large-scale scenes. Comprehensive evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity, Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo consistently achieves higher reconstruction quality than state-of-the-art methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These results establish a new benchmark for high-fidelity rendering of large-scale unbounded scenes. Code and additional information are available at https://github.com/SCUT-BIP-Lab/SplatCo.",
    "arxiv_url": "http://arxiv.org/abs/2505.17951v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17951v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/SCUT-BIP-Lab/SplatCo",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
    "arxiv_url": "http://arxiv.org/abs/2505.17590v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17590v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "high quality",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Homayoun Najjaran"
    ],
    "abstract": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such as infrastructure monitoring, structural assessment, and environmental surveying. While traditional photogrammetry techniques enable geometric modeling, they lack semantic interpretability, limiting their effectiveness for automated inspection workflows. Recent advances in neural rendering and 3D Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but similarly lack scene-level understanding.   In this work, we present a UAV-based pipeline that extends Feature-3DGS for language-guided 3D segmentation. We leverage LSeg-based feature fields with CLIP embeddings to generate heatmaps in response to language prompts. These are thresholded to produce rough segmentations, and the highest-scoring point is then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view renderings. Our results highlight the strengths and limitations of various feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful structure in large-scale outdoor environments. We demonstrate that this hybrid approach enables flexible, language-driven interaction with photorealistic 3D reconstructions, opening new possibilities for semantic aerial inspection and scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2505.17402v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17402v1",
    "published_date": "2025-05-23",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "outdoor",
      "survey",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering",
    "authors": [
      "Zhongpai Gao",
      "Meng Zheng",
      "Benjamin Planche",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: https://gaozhongpai.github.io/renderfm/.",
    "arxiv_url": "http://arxiv.org/abs/2505.17338v1",
    "pdf_url": "http://arxiv.org/pdf/2505.17338v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "medical",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion",
    "authors": [
      "Asrar Alruwayqi"
    ],
    "abstract": "We present a novel framework for dynamic 3D scene reconstruction that integrates three key components: an explicit tri-plane deformation field, a view-conditioned canonical radiance field with spherical harmonics (SH) attention, and a temporally-aware latent diffusion prior. Our method encodes 4D scenes using three orthogonal 2D feature planes that evolve over time, enabling efficient and compact spatiotemporal representation. These features are explicitly warped into a canonical space via a deformation offset field, eliminating the need for MLP-based motion modeling.   In canonical space, we replace traditional MLP decoders with a structured SH-based rendering head that synthesizes view-dependent color via attention over learned frequency bands improving both interpretability and rendering efficiency. To further enhance fidelity and temporal consistency, we introduce a transformer-guided latent diffusion module that refines the tri-plane and deformation features in a compressed latent space. This generative module denoises scene representations under ambiguous or out-of-distribution (OOD) motion, improving generalization.   Our model is trained in two stages: the diffusion module is first pre-trained independently, and then fine-tuned jointly with the full pipeline using a combination of image reconstruction, diffusion denoising, and temporal consistency losses. We demonstrate state-of-the-art results on synthetic benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view dynamic inputs.",
    "arxiv_url": "http://arxiv.org/abs/2505.16535v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16535v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "head",
      "motion",
      "deformation",
      "4d",
      "3d reconstruction",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction",
    "authors": [
      "Jiacong Chen",
      "Qingyu Mao",
      "Youneng Bao",
      "Xiandong Meng",
      "Fanyang Meng",
      "Ronggang Wang",
      "Yongsheng Liang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers rapid responsiveness and immersive experiences. However, existing online methods face challenge in prohibitive storage requirements primarily due to point-wise modeling that fails to exploit the motion properties. To address this limitation, we propose a novel Compact Gaussian Streaming (ComGS) framework, leveraging the locality and consistency of motion in dynamic scene, that models object-consistent Gaussian point motion through keypoint-driven motion representation. By transmitting only the keypoint attributes, this framework provides a more storage-efficient solution. Specifically, we first identify a sparse set of motion-sensitive keypoints localized within motion regions using a viewspace gradient difference strategy. Equipped with these keypoints, we propose an adaptive motion-driven mechanism that predicts a spatial influence field for propagating keypoint motion to neighboring Gaussian points with similar motion. Moreover, ComGS adopts an error-aware correction strategy for key frame reconstruction that selectively refines erroneous regions and mitigates error accumulation without unnecessary overhead. Overall, ComGS achieves a remarkable storage reduction of over 159 X compared to 3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining competitive visual fidelity and rendering speed. Our code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2505.16533v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16533v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM",
    "authors": [
      "Siwei Meng",
      "Yawei Luo",
      "Ping Liu"
    ],
    "abstract": "Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models.",
    "arxiv_url": "http://arxiv.org/abs/2505.16456v1",
    "pdf_url": "http://arxiv.org/pdf/2505.16456v1",
    "published_date": "2025-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction",
    "authors": [
      "Zhuodong Jiang",
      "Haoran Wang",
      "Guoxi Huang",
      "Brett Seymour",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics.",
    "arxiv_url": "http://arxiv.org/abs/2505.15737v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15737v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "robotics",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting",
    "authors": [
      "Zane K J Hartley",
      "Lewis A G Stuart",
      "Andrew P French",
      "Michael P Pound"
    ],
    "abstract": "Recent years have seen substantial improvements in the ability to generate synthetic 3D objects using AI. However, generating complex 3D objects, such as plants, remains a considerable challenge. Current generative 3D models struggle with plant generation compared to general objects, limiting their usability in plant analysis tools, which require fine detail and accurate geometry. We introduce PlantDreamer, a novel approach to 3D synthetic plant generation, which can achieve greater levels of realism for complex plant geometry and textures than available text-to-3D models. To achieve this, our new generation pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an adaptable Gaussian culling algorithm, which directly improve textural realism and geometric integrity of generated 3D plant models. Additionally, PlantDreamer enables both purely synthetic plant generation, by leveraging L-System-generated meshes, and the enhancement of real-world plant point clouds by converting them into 3D Gaussian Splats. We evaluate our approach by comparing its outputs with state-of-the-art text-to-3D models, demonstrating that PlantDreamer outperforms existing methods in producing high-fidelity synthetic plants. Our results indicate that our approach not only advances synthetic plant generation, but also facilitates the upgrading of legacy point cloud datasets, making it a valuable tool for 3D phenotyping applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.15528v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15528v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.2.10; I.3.0; I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVA: Expressive Virtual Avatars from Multi-view Videos",
    "authors": [
      "Hendrik Junkawitsch",
      "Guoxing Sun",
      "Heming Zhu",
      "Christian Theobalt",
      "Marc Habermann"
    ],
    "abstract": "With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.",
    "arxiv_url": "http://arxiv.org/abs/2505.15385v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15385v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "motion",
      "face",
      "deformation",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections",
    "authors": [
      "Xu yan",
      "Zhaohui Wang",
      "Rong Wei",
      "Jingbo Yu",
      "Dong Li",
      "Xiangde Liu"
    ],
    "abstract": "We propose R3GS, a robust reconstruction and relocalization framework tailored for unconstrained datasets. Our method uses a hybrid representation during training. Each anchor combines a global feature from a convolutional neural network (CNN) with a local feature encoded by the multiresolution hash grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict the attributes of each Gaussians, including color, opacity, and covariance. To mitigate the adverse effects of transient objects on the reconstruction process, we ffne-tune a lightweight human detection network. Once ffne-tuned, this network generates a visibility map that efffciently generalizes to other transient objects (such as posters, banners, and cars) with minimal need for further adaptation. Additionally, to address the challenges posed by sky regions in outdoor scenes, we propose an effective sky-handling technique that incorporates a depth prior as a constraint. This allows the inffnitely distant sky to be represented on the surface of a large-radius sky sphere, signiffcantly reducing ffoaters caused by errors in sky reconstruction. Furthermore, we introduce a novel relocalization method that remains robust to changes in lighting conditions while estimating the camera pose of a given image within the reconstructed 3DGS scene. As a result, R3GS significantly enhances rendering ffdelity, improves both training and rendering efffciency, and reduces storage requirements. Our method achieves state-of-the-art performance compared to baseline methods on in-the-wild datasets. The code will be made open-source following the acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2505.15294v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15294v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "outdoor",
      "lighting",
      "face",
      "gaussian splatting",
      "human",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation",
    "authors": [
      "Yuchen Li",
      "Chaoran Feng",
      "Zhenyu Tang",
      "Kaiyuan Deng",
      "Wangbo Yu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic event dataset for high-fidelity event vision tasks, captured from real-world sparse multi-view RGB images. Existing event datasets are often synthesized from dense RGB videos, which typically lack viewpoint diversity and geometric consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E overcomes these limitations by first reconstructing photorealistic static scenes using 3D Gaussian Splatting, and subsequently employing a novel, physically-informed event simulation pipeline. This pipeline generally integrates adaptive trajectory interpolation with physically-consistent event contrast threshold modeling. Such an approach yields temporally dense and geometrically consistent event streams under diverse motion and lighting conditions, while ensuring strong alignment with underlying scene structures. Experimental results on event-based 3D reconstruction demonstrate GS2E's superior generalization capabilities and its practical value as a benchmark for advancing event vision research.",
    "arxiv_url": "http://arxiv.org/abs/2505.15287v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15287v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography",
    "authors": [
      "Yifan Liu",
      "Wuyang Li",
      "Weihao Yu",
      "Chenxin Li",
      "Alexandre Alahi",
      "Max Meng",
      "Yixuan Yuan"
    ],
    "abstract": "Computed Tomography serves as an indispensable tool in clinical workflows, providing non-invasive visualization of internal anatomical structures. Existing CT reconstruction works are limited to small-capacity model architecture and inflexible volume representation. In this work, we present X-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for reconstructing 3D CT volumes from sparse-view 2D X-ray projections. X-GRM employs a scalable transformer-based architecture to encode sparse-view X-ray inputs, where tokens from different views are integrated efficiently. Then, these tokens are decoded into a novel volume representation, named Voxel-based Gaussian Splatting (VoxGS), which enables efficient CT volume extraction and differentiable X-ray rendering. This combination of a high-capacity model and flexible volume representation, empowers our model to produce high-quality reconstructions from various testing inputs, including in-domain and out-domain X-ray projections. Our codes are available at: https://github.com/CUHK-AIM-Group/X-GRM.",
    "arxiv_url": "http://arxiv.org/abs/2505.15235v2",
    "pdf_url": "http://arxiv.org/pdf/2505.15235v2",
    "published_date": "2025-05-21",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "https://github.com/CUHK-AIM-Group/X-GRM",
    "keywords": [
      "sparse-view",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting",
    "authors": [
      "Wenjie Liu",
      "Zhongliang Liu",
      "Junwei Shu",
      "Changbo Wang",
      "Yang Li"
    ],
    "abstract": "Transferring 2D textures to 3D modalities is of great significance for improving the efficiency of multimedia content creation. Existing approaches have rarely focused on transferring image textures onto 3D representations. 3D style transfer methods are capable of transferring abstract artistic styles to 3D scenes. However, these methods often overlook the geometric information of the scene, which makes it challenging to achieve high-quality 3D texture transfer results. In this paper, we present GT^2-GS, a geometry-aware texture transfer framework for gaussian splitting. From the perspective of matching texture features with geometric information in rendered views, we identify the issue of insufficient texture features and propose a geometry-aware texture augmentation module to expand the texture feature set. Moreover, a geometry-consistent texture loss is proposed to optimize texture features into the scene representation. This loss function incorporates both camera pose and 3D geometric information of the scene, enabling controllable texture-oriented appearance editing. Finally, a geometry preservation strategy is introduced. By alternating between the texture transfer and geometry correction stages over multiple iterations, this strategy achieves a balance between learning texture features and preserving geometric integrity. Extensive experiments demonstrate the effectiveness and controllability of our method. Through geometric awareness, our approach achieves texture transfer results that better align with human visual perception. Our homepage is available at https://vpx-ecnu.github.io/GT2-GS-website.",
    "arxiv_url": "http://arxiv.org/abs/2505.15208v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15208v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "human",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ],
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at https://github.com/CUHK-AIM-Group/MonoSplat.",
    "arxiv_url": "http://arxiv.org/abs/2505.15185v1",
    "pdf_url": "http://arxiv.org/pdf/2505.15185v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/CUHK-AIM-Group/MonoSplat",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression",
    "authors": [
      "Kangli Wang",
      "Shihao Li",
      "Qianxi Yi",
      "Wei Gao"
    ],
    "abstract": "Recently, immersive media and autonomous driving applications have significantly advanced through 3D Gaussian Splatting (3DGS), which offers high-fidelity rendering and computational efficiency. Despite these advantages, 3DGS as a display-oriented representation requires substantial storage due to its numerous Gaussian attributes. Current compression methods have shown promising results but typically neglect the compression of Gaussian spatial positions, creating unnecessary bitstream overhead. We conceptualize Gaussian primitives as point clouds and propose leveraging point cloud compression techniques for more effective storage. AI-based point cloud compression demonstrates superior performance and faster inference compared to MPEG Geometry-based Point Cloud Compression (G-PCC). However, direct application of existing models to Gaussian compression may yield suboptimal results, as Gaussian point clouds tend to exhibit globally sparse yet locally dense geometric distributions that differ from conventional point cloud characteristics. To address these challenges, we introduce GausPcgc for Gaussian point cloud geometry compression along with a specialized training dataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud compression into Gaussian compression pipelines, achieving superior compression ratios. The framework complements existing Gaussian compression methods while delivering significant performance improvements. All code, data, and pre-trained models will be publicly released to facilitate further research advances in this field.",
    "arxiv_url": "http://arxiv.org/abs/2505.18197v1",
    "pdf_url": "http://arxiv.org/pdf/2505.18197v1",
    "published_date": "2025-05-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "autonomous driving",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning",
    "authors": [
      "Amine Elhafsi",
      "Daniel Morton",
      "Marco Pavone"
    ],
    "abstract": "Autonomous robots must reason about the physical consequences of their actions to operate effectively in unstructured, real-world environments. We present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D Gaussian Splatting for accurate scene reconstruction, visual foundation models for semantic segmentation, vision-language models for material property inference, and physics simulation for reliable prediction of action outcomes. By integrating these components, SMS enables generalizable physical reasoning and object-centric planning without the need to re-learn foundational physical dynamics. We empirically validate SMS in a billiards-inspired manipulation task and a challenging quadrotor landing scenario, demonstrating robust performance on both simulated domain transfer and real-world experiments. Our results highlight the potential of bridging differentiable rendering for scene reconstruction, foundation models for semantic understanding, and physics-based simulation to achieve physically grounded robot planning across diverse settings.",
    "arxiv_url": "http://arxiv.org/abs/2505.14938v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14938v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image",
    "authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Qingshan Xu",
      "Yuan Zhou",
      "Long Chen",
      "Hanwang Zhang"
    ],
    "abstract": "Personalizing 3D scenes from a single reference image enables intuitive user-guided editing, which requires achieving both multi-view consistency across perspectives and referential consistency with the input image. However, these goals are particularly challenging due to the viewpoint bias caused by the limited perspective provided in a single image. Lacking the mechanisms to effectively expand reference information beyond the original view, existing methods of image-conditioned 3DGS personalization often suffer from this viewpoint bias and struggle to produce consistent results. Therefore, in this paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS), a framework that progressively propagates the single-view reference appearance to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D generation and iterative LoRA fine-tuning to extract and extend the reference appearance, and finally produces faithful multi-view guidance images and the personalized 3DGS outputs through a view-consistent generation process guided by geometric cues. Extensive experiments on real-world scenes show that our CP-GS effectively mitigates the viewpoint bias, achieving high-quality personalization that significantly outperforms existing methods. The code will be released at https://github.com/Yuxuan-W/CP-GS.",
    "arxiv_url": "http://arxiv.org/abs/2505.14537v1",
    "pdf_url": "http://arxiv.org/pdf/2505.14537v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Yuxuan-W/CP-GS",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction",
    "authors": [
      "Zhenyu Bao",
      "Qing Li",
      "Guibiao Liao",
      "Zhongyuan Zhao",
      "Kanglin Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention in streamable dynamic novel view synthesis (DNVS) for its photorealistic rendering capability and computational efficiency. Despite much progress in improving rendering quality and optimization strategies, 3DGS-based streamable dynamic scene reconstruction still suffers from flickering artifacts and storage inefficiency, and struggles to model the emerging objects. To tackle this, we introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related 3DGs are implemented according to the motion mask and the clustering-based convex hull algorithm. The rigid deformation is applied to the motion-related 3DGs for modeling the dynamic, and the attention-based optimization on the motion-related 3DGs enables the reconstruction of the emerging objects. As the deformation and optimization are only conducted on the motion-related 3DGs, MGStream avoids flickering artifacts and improves the storage efficiency. Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that MGStream surpasses existing streaming 3DGS-based approaches in terms of rendering quality, training/storage efficiency and temporal consistency. Our code is available at: https://github.com/pcl3dv/MGStream.",
    "arxiv_url": "http://arxiv.org/abs/2505.13839v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13839v1",
    "published_date": "2025-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/pcl3dv/MGStream",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos",
    "authors": [
      "Ruoyu Wang",
      "Yi Ma",
      "Shenghua Gao"
    ],
    "abstract": "Currently almost all state-of-the-art novel view synthesis and reconstruction models rely on calibrated cameras or additional geometric priors for training. These prerequisites significantly limit their applicability to massive uncalibrated data. To alleviate this requirement and unlock the potential for self-supervised training on large-scale uncalibrated videos, we propose a novel two-stage strategy to train a view synthesis model from only raw video frames or multi-view images, without providing camera parameters or other priors. In the first stage, we learn to reconstruct the scene implicitly in a latent space without relying on any explicit 3D representation. Specifically, we predict per-frame latent camera and scene context features, and employ a view synthesis model as a proxy for explicit rendering. This pretraining stage substantially reduces the optimization complexity and encourages the network to learn the underlying 3D consistency in a self-supervised manner. The learned latent camera and implicit scene representation have a large gap compared with the real 3D world. To reduce this gap, we introduce the second stage training by explicitly predicting 3D Gaussian primitives. We additionally apply explicit Gaussian Splatting rendering loss and depth projection loss to align the learned latent representations with physically grounded 3D geometry. In this way, Stage 1 provides a strong initialization and Stage 2 enforces 3D consistency - the two stages are complementary and mutually beneficial. Extensive experiments demonstrate the effectiveness of our approach, achieving high-quality novel view synthesis and accurate camera pose estimation, compared to methods that employ supervision with calibration, pose, or depth information. The code is available at https://github.com/Dwawayu/Pensieve.",
    "arxiv_url": "http://arxiv.org/abs/2505.13440v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13440v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Dwawayu/Pensieve",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
    "authors": [
      "Seungjun Oh",
      "Younggeun Lee",
      "Hyejin Jeon",
      "Eunbyung Park"
    ],
    "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.13215v1",
    "pdf_url": "http://arxiv.org/pdf/2505.13215v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "motion",
      "fast",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy",
    "authors": [
      "Chenyu Xu",
      "Zhouyu Jin",
      "Chengkang Shen",
      "Hao Zhu",
      "Zhan Ma",
      "Bo Xiong",
      "You Zhou",
      "Xun Cao",
      "Ning Gu"
    ],
    "abstract": "Compared to light-field microscopy (LFM), which enables high-speed volumetric imaging but suffers from non-uniform spatial sampling, Fourier light-field microscopy (FLFM) introduces sub-aperture division at the pupil plane, thereby ensuring spatially invariant sampling and enhancing spatial resolution. Conventional FLFM reconstruction methods, such as Richardson-Lucy (RL) deconvolution, exhibit poor axial resolution and signal degradation due to the ill-posed nature of the inverse problem. While data-driven approaches enhance spatial resolution by leveraging high-quality paired datasets or imposing structural priors, Neural Radiance Fields (NeRF)-based methods employ physics-informed self-supervised learning to overcome these limitations, yet they are hindered by substantial computational costs and memory demands. Therefore, we propose 3D Gaussian Adaptive Tomography (3DGAT) for FLFM, a 3D gaussian splatting based self-supervised learning framework that significantly improves the volumetric reconstruction quality of FLFM while maintaining computational efficiency. Experimental results indicate that our approach achieves higher resolution and improved reconstruction accuracy, highlighting its potential to advance FLFM imaging and broaden its applications in 3D optical microscopy.",
    "arxiv_url": "http://arxiv.org/abs/2505.12875v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12875v1",
    "published_date": "2025-05-19",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy",
    "authors": [
      "Luyao Lei",
      "Shuo Xu",
      "Yifan Bai",
      "Xing Wei"
    ],
    "abstract": "The performance of multi-modal 3D occupancy prediction is limited by ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion strategies and surface detail loss caused by sparse, noisy annotations. The mismatch stems from the heterogeneous scale and distribution of point cloud and image features, leading to biased matching under fixed neighborhood fusion. To address this, we propose a target-scale adaptive, bidirectional symmetric retrieval mechanism. It expands the neighborhood for large targets to enhance context awareness and shrinks it for small ones to improve efficiency and suppress noise, enabling accurate cross-modal feature alignment. This mechanism explicitly establishes spatial correspondences and improves fusion accuracy. For surface detail loss, sparse labels provide limited supervision, resulting in poor predictions for small objects. We introduce an improved volume rendering pipeline based on 3D Gaussian Splatting, which takes fused features as input to render images, applies photometric consistency supervision, and jointly optimizes 2D-3D consistency. This enhances surface detail reconstruction while suppressing noise propagation. In summary, we propose TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy prediction, enhanced by volume rendering supervision. Experiments on the nuScenes and SemanticKITTI benchmarks validate its effectiveness.",
    "arxiv_url": "http://arxiv.org/abs/2505.12693v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12693v1",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey",
    "authors": [
      "Calvin Galagain",
      "Martyna Poreba",
      "Fran√ßois Goulette"
    ],
    "abstract": "In embedded systems, robots must perceive and interpret their environment efficiently to operate reliably in real-world conditions. Visual Semantic SLAM (Simultaneous Localization and Mapping) enhances standard SLAM by incorporating semantic information into the map, enabling more informed decision-making. However, implementing such systems on resource-limited hardware involves trade-offs between accuracy, computing efficiency, and power usage.   This paper provides a comparative review of recent Semantic Visual SLAM methods with a focus on their applicability to embedded platforms. We analyze three main types of architectures - Geometric SLAM, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting - and evaluate their performance on constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their accuracy, segmentation quality, memory usage, and energy consumption.   Our results show that methods based on NeRF and Gaussian Splatting achieve high semantic detail but demand substantial computing resources, limiting their use on embedded devices. In contrast, Semantic Geometric SLAM offers a more practical balance between computational cost and accuracy. The review highlights a need for SLAM algorithms that are better adapted to embedded environments, and it discusses key directions for improving their efficiency through algorithm-hardware co-design.",
    "arxiv_url": "http://arxiv.org/abs/2505.12384v1",
    "pdf_url": "http://arxiv.org/pdf/2505.12384v1",
    "published_date": "2025-05-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "survey",
      "semantic",
      "mapping",
      "segmentation",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations",
    "authors": [
      "Songchun Zhang",
      "Huiyao Xu",
      "Sitong Guo",
      "Zhongwei Xie",
      "Pengwei Liu",
      "Hujun Bao",
      "Weiwei Xu",
      "Changqing Zou"
    ],
    "abstract": "Novel view synthesis (NVS) boosts immersive experiences in computer vision and graphics. Existing techniques, though progressed, rely on dense multi-view observations, restricting their application. This work takes on the challenge of reconstructing photorealistic 3D scenes from sparse or single-view inputs. We introduce SpatialCrafter, a framework that leverages the rich knowledge in video diffusion models to generate plausible additional observations, thereby alleviating reconstruction ambiguity. Through a trainable camera encoder and an epipolar attention mechanism for explicit geometric constraints, we achieve precise camera control and 3D consistency, further reinforced by a unified scale estimation strategy to handle scale discrepancies across datasets. Furthermore, by integrating monocular depth priors with semantic features in the video latent space, our framework directly regresses 3D Gaussian primitives and efficiently processes long-sequence features using a hybrid network structure. Extensive experiments show our method enhances sparse view reconstruction and restores the realistic appearance of 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2505.11992v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11992v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "semantic",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians",
    "authors": [
      "Yian Zhao",
      "Wanshi Xu",
      "Ruochong Zheng",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ],
    "abstract": "The efficient rendering and explicit nature of 3DGS promote the advancement of 3D scene manipulation. However, existing methods typically encounter challenges in controlling the manipulation region and are unable to furnish the user with interactive feedback, which inevitably leads to unexpected results. Intuitively, incorporating interactive 3D segmentation tools can compensate for this deficiency. Nevertheless, existing segmentation frameworks impose a pre-processing step of scene-specific parameter training, which limits the efficiency and flexibility of scene manipulation. To deliver a 3D region control module that is well-suited for scene manipulation with reliable efficiency, we propose interactive Segment-and-Manipulate 3D Gaussians (iSegMan), an interactive segmentation and manipulation framework that only requires simple 2D user interactions in any view. To propagate user interactions to other views, we propose Epipolar-guided Interaction Propagation (EIP), which innovatively exploits epipolar constraint for efficient and robust interaction matching. To avoid scene-specific training to maintain efficiency, we further propose the novel Visibility-based Gaussian Voting (VGV), which obtains 2D segmentations from SAM and models the region extraction as a voting game between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking advantage of the efficient and precise region control of EIP and VGV, we put forth a Manipulation Toolbox to implement various functions on selected regions, enhancing the controllability, flexibility and practicality of scene manipulation. Extensive results on 3D scene manipulation and segmentation tasks fully demonstrate the significant advantages of iSegMan. Project page is available at https://zhao-yian.github.io/iSegMan.",
    "arxiv_url": "http://arxiv.org/abs/2505.11934v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11934v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity",
    "authors": [
      "Takuya Ikeda",
      "Sergey Zakharov",
      "Muhammad Zubair Irshad",
      "Istvan Balazs Opra",
      "Shun Iwase",
      "Dian Chen",
      "Mark Tjersland",
      "Robert Lee",
      "Alexandre Dilly",
      "Rares Ambrus",
      "Koichi Nishiwaki"
    ],
    "abstract": "We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2505.11905v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11905v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos",
    "authors": [
      "Hongyi Zhou",
      "Xiaogang Wang",
      "Yulan Guo",
      "Kai Xu"
    ],
    "abstract": "Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.",
    "arxiv_url": "http://arxiv.org/abs/2505.11868v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11868v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "In this work, we argue that Gaussian splatting is a suitable unified representation for autonomous robot navigation in large-scale unstructured outdoor environments. Such environments require representations that can capture complex structures while remaining computationally tractable for real-time navigation. We demonstrate that the dense geometric and photometric information provided by a Gaussian splatting representation is useful for navigation in unstructured environments. Additionally, semantic information can be embedded in the Gaussian map to enable large-scale task-driven navigation. From the lessons learned through our experiments, we highlight several challenges and opportunities arising from the use of such a representation for robot autonomy.",
    "arxiv_url": "http://arxiv.org/abs/2505.11794v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11794v1",
    "published_date": "2025-05-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views",
    "authors": [
      "Abhishek Kashyap",
      "Henrik Andreasson",
      "Todor Stoyanov"
    ],
    "abstract": "Vision based robot manipulation uses cameras to capture one or more images of a scene containing the objects to be manipulated. Taking multiple images can help if any object is occluded from one viewpoint but more visible from another viewpoint. However, the camera has to be moved to a sequence of suitable positions for capturing multiple images, which requires time and may not always be possible, due to reachability constraints. So while additional images can produce more accurate grasp poses due to the extra information available, the time-cost goes up with the number of additional views sampled. Scene representations like Gaussian Splatting are capable of rendering accurate photorealistic virtual images from user-specified novel viewpoints. In this work, we show initial results which indicate that novel view synthesis can provide additional context in generating grasp poses. Our experiments on the Graspnet-1billion dataset show that novel views contributed force-closure grasps in addition to the force-closure grasps obtained from sparsely sampled real views while also improving grasp coverage. In the future we hope this work can be extended to improve grasp extraction from radiance fields constructed with a single input image, using for example diffusion models or generalizable radiance fields.",
    "arxiv_url": "http://arxiv.org/abs/2505.11467v1",
    "pdf_url": "http://arxiv.org/pdf/2505.11467v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Shuangyu Xie",
      "Chung Min Kim",
      "Justin Kerr",
      "Bart M. van Marrewijk",
      "Mieke van Vlaardingen",
      "Tim van Daalen",
      "E. N. van Loo",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Rick van de Zedde",
      "Ken Goldberg"
    ],
    "abstract": "Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/",
    "arxiv_url": "http://arxiv.org/abs/2505.10923v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10923v2",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "deformation",
      "4d",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes",
    "authors": [
      "Jianlin Guo",
      "Haihong Xiao",
      "Wenxiong Kang"
    ],
    "abstract": "Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent performance with its high-quality rendering and real-time speed, especially for objects and small-scale scenes. However, in outdoor scenes, its point-based explicit representation lacks an effective adjustment mechanism, and the millions of Gaussian points required often lead to memory constraints during training. To address these challenges, we propose EA-3DGS, a high-quality real-time rendering method designed for outdoor scenes. First, we introduce a mesh structure to regulate the initialization of Gaussian components by leveraging an adaptive tetrahedral mesh that partitions the grid and initializes Gaussian components on each face, effectively capturing geometric structures in low-texture regions. Second, we propose an efficient Gaussian pruning strategy that evaluates each 3D Gaussian's contribution to the view and prunes accordingly. To retain geometry-critical Gaussian points, we also present a structure-aware densification strategy that densifies Gaussian points in low-curvature regions. Additionally, we employ vector quantization for parameter quantization of Gaussian components, significantly reducing disk space requirements with only a minimal impact on rendering quality. Extensive experiments on 13 scenes, including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \\& Temples, WHU) and five self-collected scenes acquired through UAV photogrammetry measurement from SCUT-CA and plateau regions, further demonstrate the superiority of our method.",
    "arxiv_url": "http://arxiv.org/abs/2505.10787v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10787v1",
    "published_date": "2025-05-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention",
    "authors": [
      "Lingjun Zhao",
      "Sizhe Wei",
      "James Hays",
      "Lu Gan"
    ],
    "abstract": "3D semantic occupancy prediction is critical for achieving safe and reliable autonomous driving. Compared to camera-only perception systems, multi-modal pipelines, especially LiDAR-camera fusion methods, can produce more accurate and detailed predictions. Although most existing works utilize a dense grid-based representation, in which the entire 3D space is uniformly divided into discrete voxels, the emergence of 3D Gaussians provides a compact and continuous object-centric representation. In this work, we propose a multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D deformable attention, named as GaussianFormer3D. We introduce a voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry priors from LiDAR data, and design a LiDAR-guided 3D deformable attention mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a lifted 3D space. We conducted extensive experiments on both on-road and off-road datasets, demonstrating that our GaussianFormer3D achieves high prediction accuracy that is comparable to state-of-the-art multi-modal fusion-based methods with reduced memory consumption and improved efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2505.10685v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10685v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting",
    "authors": [
      "Fengdi Zhang",
      "Hongkun Cao",
      "Ruqi Huang"
    ],
    "abstract": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks to minimize the number of Gaussians used while preserving high rendering quality, introducing an inherent trade-off between Gaussian quantity and rendering quality. Existing methods strive for better quantity-quality performance, but lack the ability for users to intuitively adjust this trade-off to suit practical needs such as model deployment under diverse hardware and communication constraints. Here, we present ControlGS, a 3DGS optimization method that achieves semantically meaningful and cross-scene consistent quantity-quality control. Through a single training run using a fixed setup and a user-specified hyperparameter reflecting quantity-quality preference, ControlGS can automatically find desirable quantity-quality trade-off points across diverse scenes, from compact objects to large outdoor scenes. It also outperforms baselines by achieving higher rendering quality with fewer Gaussians, and supports a broad adjustment range with stepless control over the trade-off. Project page: https://zhang-fengdi.github.io/ControlGS/",
    "arxiv_url": "http://arxiv.org/abs/2505.10473v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10473v2",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "semantic",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
    "authors": [
      "Xuechang Tu",
      "Lukas Radl",
      "Michael Steiner",
      "Markus Steinberger",
      "Bernhard Kerbl",
      "Fernando de la Torre"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.",
    "arxiv_url": "http://arxiv.org/abs/2505.10144v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10144v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "face",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars",
    "authors": [
      "Rui-Yang Ju",
      "Sheng-Yen Huang",
      "Yi-Ping Hung"
    ],
    "abstract": "The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based framework, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we employ an improved StyleGAN to generate the stylized video from the input video frames, which addresses the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, and efficiently generate high-quality animation in the next stage. In Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head model and a set of expression blendshapes from the generated video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane and Pixar.",
    "arxiv_url": "http://arxiv.org/abs/2505.10072v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10072v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "face",
      "3d gaussian",
      "ar",
      "animation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field",
    "authors": [
      "Jinlong Fan",
      "Xuepu Zeng",
      "Jing Zhang",
      "Mingming Gong",
      "Yuxiang Yang",
      "Dacheng Tao"
    ],
    "abstract": "Dynamic scene representation and reconstruction have undergone transformative advances in recent years, catalyzed by breakthroughs in neural radiance fields and 3D Gaussian splatting techniques. While initially developed for static environments, these methodologies have rapidly evolved to address the complexities inherent in 4D dynamic scenes through an expansive body of research. Coupled with innovations in differentiable volumetric rendering, these approaches have significantly enhanced the quality of motion representation and dynamic scene reconstruction, thereby garnering substantial attention from the computer vision and graphics communities. This survey presents a systematic analysis of over 200 papers focused on dynamic scene representation using radiance field, spanning the spectrum from implicit neural representations to explicit Gaussian primitives. We categorize and evaluate these works through multiple critical lenses: motion representation paradigms, reconstruction techniques for varied scene dynamics, auxiliary information integration strategies, and regularization approaches that ensure temporal consistency and physical plausibility. We organize diverse methodological approaches under a unified representational framework, concluding with a critical examination of persistent challenges and promising research directions. By providing this comprehensive overview, we aim to establish a definitive reference for researchers entering this rapidly evolving field while offering experienced practitioners a systematic understanding of both conceptual principles and practical frontiers in dynamic scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2505.10049v2",
    "pdf_url": "http://arxiv.org/pdf/2505.10049v2",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "motion",
      "body",
      "understanding",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large-Scale Gaussian Splatting SLAM",
    "authors": [
      "Zhe Xin",
      "Chenyang Wu",
      "Penghui Huang",
      "Yanyong Zhang",
      "Yinian Mao",
      "Guoquan Huang"
    ],
    "abstract": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2505.09915v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09915v1",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "tracking",
      "outdoor",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
    "authors": [
      "Justin Yu",
      "Letian Fu",
      "Huang Huang",
      "Karim El-Refai",
      "Rares Andrei Ambrus",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "abstract": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com",
    "arxiv_url": "http://arxiv.org/abs/2505.09601v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09601v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians",
    "authors": [
      "Ma Changfeng",
      "Bi Ran",
      "Guo Jie",
      "Wang Chongjun",
      "Guo Yanwen"
    ],
    "abstract": "Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.",
    "arxiv_url": "http://arxiv.org/abs/2505.09413v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09413v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/murcherful/GauPCRender",
    "keywords": [
      "nerf",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Video Compression using 2D Gaussian Splatting",
    "authors": [
      "Lakshya Gupta",
      "Imran N. Junejo"
    ],
    "abstract": "The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.",
    "arxiv_url": "http://arxiv.org/abs/2505.09324v1",
    "pdf_url": "http://arxiv.org/pdf/2505.09324v1",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: a vision-based low overhead framework for 3D scene reconstruction",
    "authors": [
      "Yunji Feng",
      "Chengpu Yu",
      "Fengrui Ran",
      "Zhi Yang",
      "Yinni Liu"
    ],
    "abstract": "This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.10578v1",
    "pdf_url": "http://arxiv.org/pdf/2505.10578v1",
    "published_date": "2025-05-14",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance",
    "authors": [
      "Wenzhe Cai",
      "Jiaqi Peng",
      "Yuqiang Yang",
      "Yujian Zhang",
      "Meng Wei",
      "Hanqing Wang",
      "Yilun Chen",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "abstract": "Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20$\\times$ more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.",
    "arxiv_url": "http://arxiv.org/abs/2505.08712v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08712v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "outdoor",
      "mapping",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting",
    "authors": [
      "Holly Dinkel",
      "Marcel B√ºsching",
      "Alberta Longhini",
      "Brian Coltin",
      "Trey Smith",
      "Danica Kragic",
      "M√•rten Bj√∂rkman",
      "Timothy Bretl"
    ],
    "abstract": "This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.08644v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08644v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FOCI: Trajectory Optimization on Gaussian Splats",
    "authors": [
      "Mario Gomez Andreu",
      "Maximum Wilder-Smith",
      "Victor Klemm",
      "Vaishakh Patil",
      "Jesus Tordesillas",
      "Marco Hutter"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at https://rffr.leggedrobotics.com/works/foci/",
    "arxiv_url": "http://arxiv.org/abs/2505.08510v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08510v1",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey of 3D Reconstruction with Event Cameras",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Langyi Chen",
      "Haodong Chen",
      "Ying Zhou",
      "Vera Chung",
      "Qiang Qu",
      "Weidong Cai"
    ],
    "abstract": "Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2505.08438v2",
    "pdf_url": "http://arxiv.org/pdf/2505.08438v2",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "robotics",
      "autonomous driving",
      "survey",
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction",
    "authors": [
      "He Huang",
      "Qi Yang",
      "Mufan Liu",
      "Yiling Xu",
      "Zhu Li"
    ],
    "abstract": "Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.",
    "arxiv_url": "http://arxiv.org/abs/2505.08196v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08196v1",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/H-Huang774/ADC-GS.git",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "4d",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAG: Scalable Language-Augmented Gaussian Splatting",
    "authors": [
      "Laszlo Szilagyi",
      "Francis Engelmann",
      "Jeannette Bohg"
    ],
    "abstract": "Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2505.08124v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08124v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "3d gaussian",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ],
    "abstract": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream",
    "arxiv_url": "http://arxiv.org/abs/2505.07539v1",
    "pdf_url": "http://arxiv.org/pdf/2505.07539v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "fast",
      "deformation",
      "4d",
      "real-time rendering",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
    "authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias Nie√üner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win",
    "arxiv_url": "http://arxiv.org/abs/2505.07396v2",
    "pdf_url": "http://arxiv.org/pdf/2505.07396v2",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "semantic",
      "segmentation",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian",
    "authors": [
      "Shijie Lian",
      "Ziyi Zhang",
      "Laurence Tianruo Yang and",
      "Mengyu Ren",
      "Debin Liu",
      "Hua Li"
    ],
    "abstract": "Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications",
    "arxiv_url": "http://arxiv.org/abs/2505.08811v1",
    "pdf_url": "http://arxiv.org/pdf/2505.08811v1",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "lightweight",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular Online Reconstruction with Enhanced Detail Preservation",
    "authors": [
      "Songyin Wu",
      "Zhaoyang Lv",
      "Yufeng Zhu",
      "Duncan Frost",
      "Zhengqin Li",
      "Ling-Qi Yan",
      "Carl Ren",
      "Richard Newcombe",
      "Zhao Dong"
    ],
    "abstract": "We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2505.07887v2",
    "pdf_url": "http://arxiv.org/pdf/2505.07887v2",
    "published_date": "2025-05-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "tracking",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes",
    "authors": [
      "Xijie Yang",
      "Linning Xu",
      "Lihan Jiang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2505.06523v1",
    "pdf_url": "http://arxiv.org/pdf/2505.06523v1",
    "published_date": "2025-05-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling",
    "authors": [
      "Gengyan Li",
      "Paulo Gotardo",
      "Timo Bolkart",
      "Stephan Garbin",
      "Kripasindhu Sarkar",
      "Abhimitra Meka",
      "Alexandros Lattas",
      "Thabo Beeler"
    ],
    "abstract": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.",
    "arxiv_url": "http://arxiv.org/abs/2505.05672v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05672v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV",
      "I.3.7; I.3.5"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes",
    "authors": [
      "Mark C. Eid",
      "Ana I. L. Namburete",
      "Jo√£o F. Henriques"
    ],
    "abstract": "Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2505.05643v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05643v1",
    "published_date": "2025-05-08",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "fast",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization",
    "authors": [
      "Yueh-Cheng Liu",
      "Lukas H√∂llein",
      "Matthias Nie√üner",
      "Angela Dai"
    ],
    "abstract": "Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.",
    "arxiv_url": "http://arxiv.org/abs/2505.05591v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05591v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "face",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2505.05587v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05587v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation",
    "authors": [
      "Yonwoo Choi"
    ],
    "abstract": "Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.",
    "arxiv_url": "http://arxiv.org/abs/2505.05475v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05475v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Scene Generation: A Survey",
    "authors": [
      "Beichen Wen",
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ],
    "abstract": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "arxiv_url": "http://arxiv.org/abs/2505.05474v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05474v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
    "keywords": [
      "robotics",
      "autonomous driving",
      "survey",
      "semantic",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ],
    "abstract": "We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf",
    "arxiv_url": "http://arxiv.org/abs/2505.05356v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05356v1",
    "published_date": "2025-05-08",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "fast",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation",
    "authors": [
      "Tengya Peng",
      "Ruyi Zha",
      "Qing Zou"
    ],
    "abstract": "This study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI), utilizing a three-dimensional Gaussian representation (3DGS). The proposed method leverages 3DGS to address the challenges of motion-resolved 3D isotropic pulmonary MRI reconstruction by enabling data smoothing between voxels for continuous spatial representation. Pulmonary MRI data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. Based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. A 3DGS framework is then applied to reconstruct a reference image volume from the first motion state. Subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (DVFs), which are used to generate the remaining motion states through spatial transformation of the reference volume. The proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. The experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary MR images. Compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. The proposed unsupervised 3DGS-based reconstruction method enables accurate motion-resolved pulmonary MRI with isotropic spatial resolution. Its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary MR imaging.",
    "arxiv_url": "http://arxiv.org/abs/2505.04959v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04959v1",
    "published_date": "2025-05-08",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting",
    "authors": [
      "Feng Yang",
      "Wenliang Qian",
      "Wangmeng Zuo",
      "Hui Li"
    ],
    "abstract": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2505.04262v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04262v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ],
    "abstract": "Neural rendering techniques have made substantial progress in generating photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has achieved high quality novel view synthesis as well as fast rendering speed. However, 3D Gaussians lack proficiency in defining accurate 3D geometric structures despite their explicit primitive representations. This is due to the fact that Gaussian's attributes are primarily tailored and fine-tuned for rendering diverse 2D images by their anisotropic nature. To pave the way for efficient 3D reconstruction, we present Spherical Gaussians, a simple and effective representation for 3D geometric boundaries, from which we can directly reconstruct 3D feature curves from a set of calibrated multi-view images. Spherical Gaussians is optimized from grid initialization with a view-based rendering loss, where a 2D edge map is rendered at a specific view and then compared to the ground-truth edge map extracted from the corresponding image, without the need for any 3D guidance or supervision. Given Spherical Gaussians serve as intermedia for the robust edge representation, we further introduce a novel optimization-based algorithm called SGCR to directly extract accurate parametric curves from aligned Spherical Gaussians. We demonstrate that SGCR outperforms existing state-of-the-art methods in 3D edge reconstruction while enjoying great efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2505.04668v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04668v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation",
    "authors": [
      "Yiming Qin",
      "Zhu Xu",
      "Yang Liu"
    ],
    "abstract": "Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at https://github.com/Wakals/GASCOL .",
    "arxiv_url": "http://arxiv.org/abs/2505.05505v1",
    "pdf_url": "http://arxiv.org/pdf/2505.05505v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/Wakals/GASCOL",
    "keywords": [
      "3d gaussian",
      "localization",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes",
    "authors": [
      "Feng Xiao",
      "Hongbin Xu",
      "Wanlin Liang",
      "Wenxiong Kang"
    ],
    "abstract": "The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.",
    "arxiv_url": "http://arxiv.org/abs/2505.04659v1",
    "pdf_url": "http://arxiv.org/pdf/2505.04659v1",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "understanding",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar",
    "authors": [
      "Dongbin Zhang",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Yang Li",
      "Minghan Qin",
      "Yu Li",
      "Haoqian Wang"
    ],
    "abstract": "Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.",
    "arxiv_url": "http://arxiv.org/abs/2505.03351v1",
    "pdf_url": "http://arxiv.org/pdf/2505.03351v1",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "tracking",
      "motion",
      "fast",
      "mapping",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting Data Compression with Mixture of Priors",
    "authors": [
      "Lei Liu",
      "Zhenghao Chen",
      "Dong Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.",
    "arxiv_url": "http://arxiv.org/abs/2505.03310v1",
    "pdf_url": "http://arxiv.org/pdf/2505.03310v1",
    "published_date": "2025-05-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "3d gaussian",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
    "authors": [
      "Shubhendu Jena",
      "Amine Ouasfi",
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",
    "arxiv_url": "http://arxiv.org/abs/2505.02178v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02178v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "sparse view",
      "efficient",
      "head",
      "fast",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting",
    "authors": [
      "Shubhendu Jena",
      "Shishir Reddy Vutukur",
      "Adnane Boukhayma"
    ],
    "abstract": "Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",
    "arxiv_url": "http://arxiv.org/abs/2505.02175v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02175v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "sparse view",
      "sparse-view",
      "fast",
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction",
    "authors": [
      "Zhihao Tang",
      "Shenghao Yang",
      "Hongtao Zhang",
      "Mingbo Zhao"
    ],
    "abstract": "Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.02126v2",
    "pdf_url": "http://arxiv.org/pdf/2505.02126v2",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2505.02108v1",
    "pdf_url": "http://arxiv.org/pdf/2505.02108v1",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "body",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder",
    "authors": [
      "Qi Yang",
      "Le Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "abstract": "Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS.",
    "arxiv_url": "http://arxiv.org/abs/2505.01938v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01938v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/Qi-Yangsjtu/HybridGS",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
    "authors": [
      "Anushka Agarwal",
      "Muhammad Yusuf Hassan",
      "Talha Chafekar"
    ],
    "abstract": "We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2505.01928v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01928v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual enhancement and 3D representation for underwater scenes: a review",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Brett Seymour",
      "Evan Kovacs",
      "John Ellerbrock",
      "Dave Blackham",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in   computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an   in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and   3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian   Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple   benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.",
    "arxiv_url": "http://arxiv.org/abs/2505.01869v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01869v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "lighting",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting",
    "authors": [
      "Junhao Shi",
      "Jisheng Xu",
      "Jianping He",
      "Zhiliang Lin"
    ],
    "abstract": "Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.",
    "arxiv_url": "http://arxiv.org/abs/2505.01799v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01799v1",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research",
    "authors": [
      "Yan Miao",
      "Will Shen",
      "Hang Cui",
      "Sayan Mitra"
    ],
    "abstract": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing platform for autonomy research. The hardware platform integrates a small camera, a standard airframe, offboard computation, and radio communication for manual overrides. We demonstrate FalconWing's capabilities by developing and deploying a purely vision-based control policy for autonomous landing (without IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our learning approach: (1) constructs a photorealistic simulation environment via 3D Gaussian splatting trained on real-world images; (2) identifies nonlinear dynamics from vision-estimated real-flight data; and (3) trains a multi-modal Vision Transformer (ViT) policy through simulation-only imitation learning. The ViT architecture fuses single RGB image with the history of control actions via self-attention, preserving temporal context while maintaining real-time 20 Hz inference. When deployed zero-shot on the hardware platform, this policy achieves an 80% success rate in vision-based autonomous landings. Together with the hardware specifications, we also open-source the system dynamics, the software for photorealistic simulator and the learning approach.",
    "arxiv_url": "http://arxiv.org/abs/2505.01383v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01383v1",
    "published_date": "2025-05-02",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors",
    "authors": [
      "Chenxi Li",
      "Weijie Wang",
      "Qiang Li",
      "Bruno Lepri",
      "Nicu Sebe",
      "Weizhi Nie"
    ],
    "abstract": "Text-driven object insertion in 3D scenes is an emerging task that enables intuitive scene editing through natural language. However, existing 2D editing-based methods often rely on spatial priors such as 2D masks or 3D bounding boxes, and they struggle to ensure consistency of the inserted object. These limitations hinder flexibility and scalability in real-world applications. In this paper, we propose FreeInsert, a novel framework that leverages foundation models including MLLMs, LGMs, and diffusion models to disentangle object generation from spatial placement. This enables unsupervised and flexible object insertion in 3D scenes without spatial priors. FreeInsert starts with an MLLM-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. These semantics guide both the reconstruction of the inserted object for 3D consistency and the learning of its degrees of freedom. We leverage the spatial reasoning capabilities of MLLMs to initialize object pose and scale. A hierarchical, spatially aware refinement stage further integrates spatial semantics and MLLM-inferred priors to enhance placement. Finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. Experimental results demonstrate that FreeInsert achieves semantically coherent, spatially precise, and visually realistic 3D insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.",
    "arxiv_url": "http://arxiv.org/abs/2505.01322v2",
    "pdf_url": "http://arxiv.org/pdf/2505.01322v2",
    "published_date": "2025-05-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting",
    "authors": [
      "Youngsik Yun",
      "Jeongmin Bae",
      "Hyunseung Son",
      "Seoha Kim",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh"
    ],
    "abstract": "Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at https://bbangsik13.github.io/OR2.",
    "arxiv_url": "http://arxiv.org/abs/2505.01235v1",
    "pdf_url": "http://arxiv.org/pdf/2505.01235v1",
    "published_date": "2025-05-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos",
    "authors": [
      "Xia Yuan",
      "Hai Yuan",
      "Wenyi Ge",
      "Ying Fu",
      "Xi Wu",
      "Guanyu Xing"
    ],
    "abstract": "High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2505.00421v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00421v1",
    "published_date": "2025-05-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "face",
      "deformation",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation",
    "authors": [
      "Haiyang Zhou",
      "Wangbo Yu",
      "Jiawen Guan",
      "Xinhua Cheng",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.21650v2",
    "pdf_url": "http://arxiv.org/pdf/2504.21650v2",
    "published_date": "2025-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "vr",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond",
    "authors": [
      "Jiajia Li",
      "Xinda Qi",
      "Seyed Hamidreza Nabaei",
      "Meiqi Liu",
      "Dong Chen",
      "Xin Zhang",
      "Xunyuan Yin",
      "Zhaojian Li"
    ],
    "abstract": "Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.",
    "arxiv_url": "http://arxiv.org/abs/2505.00737v1",
    "pdf_url": "http://arxiv.org/pdf/2505.00737v1",
    "published_date": "2025-04-30",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "https://github.com/JiajiaLi04/3D-Reconstruction-Plants",
    "keywords": [
      "sparse view",
      "outdoor",
      "survey",
      "face",
      "understanding",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
    "authors": [
      "Jiaxin Hong",
      "Sixu Chen",
      "Shuoyang Sun",
      "Hongyao Yu",
      "Hao Fang",
      "Yuqi Tan",
      "Bin Chen",
      "Shuhan Qi",
      "Jiawei Li"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.",
    "arxiv_url": "http://arxiv.org/abs/2504.20829v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20829v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "vr",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction",
    "authors": [
      "Yuhan Xie",
      "Yixi Cai",
      "Yinqiang Zhang",
      "Lei Yang",
      "Jia Pan"
    ],
    "abstract": "This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",
    "arxiv_url": "http://arxiv.org/abs/2504.21067v1",
    "pdf_url": "http://arxiv.org/pdf/2504.21067v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian",
    "authors": [
      "Hao Tian",
      "Rui Liu",
      "Wen Shen",
      "Yilong Hu",
      "Zhihao Zheng",
      "Xiaolin Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2504.20607v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20607v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ],
    "abstract": "Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.",
    "arxiv_url": "http://arxiv.org/abs/2504.20403v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20403v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "vr",
      "geometry",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting",
    "authors": [
      "Jongwon Lee",
      "Timothy Bretl"
    ],
    "abstract": "In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.",
    "arxiv_url": "http://arxiv.org/abs/2504.20379v2",
    "pdf_url": "http://arxiv.org/pdf/2504.20379v2",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ],
    "abstract": "We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach.",
    "arxiv_url": "http://arxiv.org/abs/2504.20378v1",
    "pdf_url": "http://arxiv.org/pdf/2504.20378v1",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "motion",
      "fast",
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics",
    "authors": [
      "Yunfei Wan",
      "Jianheng Liu",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "abstract": "In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.",
    "arxiv_url": "http://arxiv.org/abs/2504.19938v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19938v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/hku-mars/Mesh-Learner",
    "keywords": [
      "robotics",
      "efficient",
      "fast",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes",
    "authors": [
      "Mohammad Altillawi",
      "Fengyi Shen",
      "Liudi Yang",
      "Sai Manoj Prakhya",
      "Ziyuan Liu"
    ],
    "abstract": "Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2504.19557v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19557v1",
    "published_date": "2025-04-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field",
    "authors": [
      "Zuxing Lu",
      "Xin Yuan",
      "Shaowen Yang",
      "Jingyu Liu",
      "Changyin Sun"
    ],
    "abstract": "Semantic-aware 3D scene reconstruction is essential for autonomous robots to perform complex interactions. Semantic SLAM, an online approach, integrates pose tracking, geometric reconstruction, and semantic mapping into a unified framework, shows significant potential. However, existing systems, which rely on 2D ground truth priors for supervision, are often limited by the sparsity and noise of these signals in real-world environments. To address this challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D Gaussian Splatting that leverages feature fields to achieve joint rendering of appearance, geometry, and N-dimensional semantic features. By independently optimizing feature gradients, our method supports semantic reconstruction using various forms of 2D priors, particularly sparse and noisy signals. Experimental results demonstrate that our approach outperforms previous methods in both tracking accuracy and photorealistic rendering quality. When utilizing 2D ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation performance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with only marginal performance degradation.",
    "arxiv_url": "http://arxiv.org/abs/2504.19409v2",
    "pdf_url": "http://arxiv.org/pdf/2504.19409v2",
    "published_date": "2025-04-28",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "semantic",
      "mapping",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "slam",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting",
    "authors": [
      "Xiaofeng Jin",
      "Yan Fang",
      "Matteo Frosi",
      "Jianfei Ge",
      "Jiangjian Xiao",
      "Matteo Matteucci"
    ],
    "abstract": "Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360{\\deg} views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.",
    "arxiv_url": "http://arxiv.org/abs/2504.19261v1",
    "pdf_url": "http://arxiv.org/pdf/2504.19261v1",
    "published_date": "2025-04-27",
    "categories": [
      "cs.CV",
      "65D18, 68U05",
      "I.3.7; I.4.8"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos",
    "authors": [
      "Yuan Li",
      "Ziqian Bai",
      "Feitong Tan",
      "Zhaopeng Cui",
      "Sean Fanello",
      "Yinda Zhang"
    ],
    "abstract": "We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.",
    "arxiv_url": "http://arxiv.org/abs/2504.19165v2",
    "pdf_url": "http://arxiv.org/pdf/2504.19165v2",
    "published_date": "2025-04-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "nerf",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression",
    "authors": [
      "Zicong Chen",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Lei Liu",
      "Dong Xu"
    ],
    "abstract": "Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints. Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively. Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding. Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook. By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.",
    "arxiv_url": "http://arxiv.org/abs/2504.18925v2",
    "pdf_url": "http://arxiv.org/pdf/2504.18925v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians",
    "authors": [
      "Letian Huang",
      "Dongwei Ye",
      "Jialin Dan",
      "Chengzhi Tao",
      "Huiwen Liu",
      "Kun Zhou",
      "Bo Ren",
      "Yuanqi Li",
      "Yanwen Guo",
      "Jie Guo"
    ],
    "abstract": "The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",
    "arxiv_url": "http://arxiv.org/abs/2504.18768v2",
    "pdf_url": "http://arxiv.org/pdf/2504.18768v2",
    "published_date": "2025-04-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "reflection",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.",
    "arxiv_url": "http://arxiv.org/abs/2504.18468v3",
    "pdf_url": "http://arxiv.org/pdf/2504.18468v3",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "relighting",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting",
    "authors": [
      "Yunze Deng",
      "Haijun Xiong",
      "Bin Feng",
      "Xinggang Wang",
      "Wenyu Liu"
    ],
    "abstract": "Text-to-4D generation is rapidly developing and widely applied in various scenarios. However, existing methods often fail to incorporate adequate spatio-temporal modeling and prompt alignment within a unified framework, resulting in temporal inconsistencies, geometric distortions, or low-quality 4D content that deviates from the provided texts. Therefore, we propose STP4D, a novel approach that aims to integrate comprehensive spatio-temporal-prompt consistency modeling for high-quality text-to-4D generation. Specifically, STP4D employs three carefully designed modules: Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation, which collaborate to accomplish this goal. Furthermore, STP4D is among the first methods to exploit the Diffusion model to generate 4D Gaussians, combining the fine-grained modeling capabilities and the real-time rendering process of 4DGS with the rapid inference speed of the Diffusion model. Extensive experiments demonstrate that STP4D excels in generating high-fidelity 4D content with exceptional efficiency (approximately 4.6s per asset), surpassing existing methods in both quality and speed.",
    "arxiv_url": "http://arxiv.org/abs/2504.18318v1",
    "pdf_url": "http://arxiv.org/pdf/2504.18318v1",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "4d",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models",
    "authors": [
      "Michel Gokan Khan",
      "Renan Guarese",
      "Fabian Johnson",
      "Xi Vincent Wang",
      "Anders Bergman",
      "Benjamin Edvinsson",
      "Mario Romero",
      "J√©r√©my Vachier",
      "Jan Kronqvist"
    ],
    "abstract": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.",
    "arxiv_url": "http://arxiv.org/abs/2504.18165v1",
    "pdf_url": "http://arxiv.org/pdf/2504.18165v1",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting",
    "authors": [
      "Kaiyuan Tang",
      "Siyuan Yao",
      "Chaoli Wang"
    ],
    "abstract": "In volume visualization, users can interactively explore the three-dimensional data by specifying color and opacity mappings in the transfer function (TF) or adjusting lighting parameters, facilitating meaningful interpretation of the underlying structure. However, rendering large-scale volumes demands powerful GPUs and high-speed memory access for real-time performance. While existing novel view synthesis (NVS) methods offer faster rendering speeds with lower hardware requirements, the visible parts of a reconstructed scene are fixed and constrained by preset TF settings, significantly limiting user exploration. This paper introduces inverse volume rendering via Gaussian splatting (iVR-GS), an innovative NVS method that reduces the rendering cost while enabling scene editing for interactive volume exploration. Specifically, we compose multiple iVR-GS models associated with basic TFs covering disjoint visible parts to make the entire volumetric scene visible. Each basic model contains a collection of 3D editable Gaussians, where each Gaussian is a 3D spatial point that supports real-time scene rendering and editing. We demonstrate the superior reconstruction quality and composability of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on various volume datasets. The code is available at https://github.com/TouKaienn/iVR-GS.",
    "arxiv_url": "http://arxiv.org/abs/2504.17954v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17954v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/TouKaienn/iVR-GS",
    "keywords": [
      "vr",
      "lighting",
      "fast",
      "mapping",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos",
    "authors": [
      "Shucheng Gong",
      "Lingzhe Zhao",
      "Wenpu Li",
      "Hong Xie",
      "Yin Zhang",
      "Shiyu Zhao",
      "Peidong Liu"
    ],
    "abstract": "Recently, photo-realistic novel view synthesis from multi-view images, such as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered widespread attention due to their superior performance. However, most works rely on low dynamic range (LDR) images, which limits the capturing of richer scene details. Some prior works have focused on high dynamic range (HDR) scene reconstruction, typically require capturing of multi-view sharp images with different exposure times at fixed camera positions during exposure times, which is time-consuming and challenging in practice. For a more flexible data acquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily and robustly reconstruct the 3D HDR scene from casually captured videos with auto-exposure enabled, even in the presence of severe motion blur and varying unknown exposure time. \\textbf{CasualHDRSplat} contains a unified differentiable physical imaging model which first applies continuous-time trajectory constraint to imaging process so that we can jointly optimize exposure time, camera response function (CRF), camera poses, and sharp 3D HDR scene. Extensive experiments demonstrate that our approach outperforms existing methods in terms of robustness and rendering quality. Our source code will be available at https://github.com/WU-CVGL/CasualHDRSplat",
    "arxiv_url": "http://arxiv.org/abs/2504.17728v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17728v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "https://github.com/WU-CVGL/CasualHDRSplat",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering",
    "authors": [
      "Keyang Ye",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes -- surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve anti-aliasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.",
    "arxiv_url": "http://arxiv.org/abs/2504.17545v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17545v1",
    "published_date": "2025-04-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting is an Effective Data Generator for 3D Object Detection",
    "authors": [
      "Farhad G. Zanjani",
      "Davide Abati",
      "Auke Wiggers",
      "Dimitris Kalatzis",
      "Jens Petersen",
      "Hong Cai",
      "Amirhossein Habibian"
    ],
    "abstract": "We investigate data augmentation for 3D object detection in autonomous driving. We utilize recent advancements in 3D reconstruction based on Gaussian Splatting for 3D object placement in driving scenes. Unlike existing diffusion-based methods that synthesize images conditioned on BEV layouts, our approach places 3D objects directly in the reconstructed 3D space with explicitly imposed geometric transformations. This ensures both the physical plausibility of object placement and highly accurate 3D pose and position annotations.   Our experiments demonstrate that even by integrating a limited number of external 3D objects into real scenes, the augmented data significantly enhances 3D object detection performance and outperforms existing diffusion-based 3D augmentation for object detection. Extensive testing on the nuScenes dataset reveals that imposing high geometric diversity in object placement has a greater impact compared to the appearance diversity of objects. Additionally, we show that generating hard examples, either by maximizing detection loss or imposing high visual occlusion in camera images, does not lead to more efficient 3D data augmentation for camera-based 3D object detection in autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2504.16740v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16740v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation",
    "authors": [
      "Wenxuan Li",
      "Hang Zhao",
      "Zhiyuan Yu",
      "Yu Du",
      "Qin Zou",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "abstract": "While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.",
    "arxiv_url": "http://arxiv.org/abs/2504.16693v2",
    "pdf_url": "http://arxiv.org/pdf/2504.16693v2",
    "published_date": "2025-04-23",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "few-shot",
      "body",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction",
    "authors": [
      "Zhongtao Wang",
      "Mai Su",
      "Huishan Au",
      "Yilong Li",
      "Xizhe Cao",
      "Chengwei Pan",
      "Yisong Chen",
      "Guoping Wang"
    ],
    "abstract": "As urban 3D scenes become increasingly complex and the demand for high-quality rendering grows, efficient scene reconstruction and rendering techniques become crucial. We present HUG, a novel approach to address inefficiencies in handling large-scale urban environments and intricate details based on 3D Gaussian splatting. Our method optimizes data partitioning and the reconstruction pipeline by incorporating a hierarchical neural Gaussian representation. We employ an enhanced block-based reconstruction pipeline focusing on improving reconstruction quality within each block and reducing the need for redundant training regions around block boundaries. By integrating neural Gaussian representation with a hierarchical architecture, we achieve high-quality scene rendering at a low computational cost. This is demonstrated by our state-of-the-art results on public benchmarks, which prove the effectiveness and advantages in large-scale urban scene representation.",
    "arxiv_url": "http://arxiv.org/abs/2504.16606v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16606v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration",
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Martin R. Oswald",
      "Stefano Mattoccia"
    ],
    "abstract": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.",
    "arxiv_url": "http://arxiv.org/abs/2504.16545v1",
    "pdf_url": "http://arxiv.org/pdf/2504.16545v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "tracking",
      "mapping",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning",
    "authors": [
      "Mingxuan Cui",
      "Qing Guo",
      "Yuyi Wang",
      "Hongkai Yu",
      "Di Lin",
      "Qin Zou",
      "Ming-Ming Cheng",
      "Xi Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.",
    "arxiv_url": "http://arxiv.org/abs/2504.17815v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17815v1",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos",
    "authors": [
      "Yuxin Yao",
      "Yan Zhang",
      "Zhening Huang",
      "Joan Lasenby"
    ],
    "abstract": "Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS",
    "arxiv_url": "http://arxiv.org/abs/2504.17810v1",
    "pdf_url": "http://arxiv.org/pdf/2504.17810v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment",
    "authors": [
      "Yiqian Wu",
      "Malte Prinzler",
      "Xiaogang Jin",
      "Siyu Tang"
    ],
    "abstract": "The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar's appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.15835v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15835v1",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "head",
      "semantic",
      "geometry",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians",
    "authors": [
      "Cailin Zhuang",
      "Yaoqi Hu",
      "Xuanyang Zhang",
      "Wei Cheng",
      "Jiacheng Bao",
      "Shengqi Liu",
      "Yiying Yang",
      "Xianfang Zeng",
      "Gang Yu",
      "Ming Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.",
    "arxiv_url": "http://arxiv.org/abs/2504.15281v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15281v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "quality enhancement",
      "lighting",
      "semantic",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Immersive Teleoperation Framework for Locomanipulation Tasks",
    "authors": [
      "Takuya Boehringer",
      "Jonathan Embley-Riches",
      "Karim Hammoud",
      "Valerio Modugno",
      "Dimitrios Kanoulas"
    ],
    "abstract": "Recent advancements in robotic loco-manipulation have leveraged Virtual Reality (VR) to enhance the precision and immersiveness of teleoperation systems, significantly outperforming traditional methods reliant on 2D camera feeds and joystick controls. Despite these advancements, challenges remain, particularly concerning user experience across different setups. This paper introduces a novel VR-based teleoperation framework designed for a robotic manipulator integrated onto a mobile platform. Central to our approach is the application of Gaussian splatting, a technique that abstracts the manipulable scene into a VR environment, thereby enabling more intuitive and immersive interactions. Users can navigate and manipulate within the virtual scene as if interacting with a real robot, enhancing both the engagement and efficacy of teleoperation tasks. An extensive user study validates our approach, demonstrating significant usability and efficiency improvements. Two-thirds (66%) of participants completed tasks faster, achieving an average time reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface overall, with unanimous (100%) recommendations for future use, highlighting improvements in precision, responsiveness, and situational awareness. Finally, we demonstrate the effectiveness of our framework through real-world experiments in two distinct application scenarios, showcasing the practical capabilities and versatility of the Splat-based VR interface.",
    "arxiv_url": "http://arxiv.org/abs/2504.15229v1",
    "pdf_url": "http://arxiv.org/pdf/2504.15229v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "lighting",
      "face",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video",
    "authors": [
      "Minh-Quan Viet Bui",
      "Jongmin Park",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.",
    "arxiv_url": "http://arxiv.org/abs/2504.15122v3",
    "pdf_url": "http://arxiv.org/pdf/2504.15122v3",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations",
    "authors": [
      "Yating Wang",
      "Xuan Wang",
      "Ran Yi",
      "Yanbo Fan",
      "Jichen Hu",
      "Jingcheng Zhu",
      "Lizhuang Ma"
    ],
    "abstract": "Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to construct high-quality 3D head avatars. In this line of research, existing methods either fail to capture the dynamic textures or incur significant overhead in terms of runtime speed or storage space. To this end, we propose a novel method that addresses all the aforementioned demands. In specific, we introduce an expressive and compact representation that encodes texture-related attributes of the 3D Gaussians in the tensorial format. We store appearance of neutral expression in static tri-planes, and represents dynamic texture details for different expressions using lightweight 1D feature lines, which are then decoded into opacity offset relative to the neutral face. We further propose adaptive truncated opacity penalty and class-balanced sampling to improve generalization across different expressions. Experiments show this design enables accurate face dynamic details capturing while maintains real-time rendering and significantly reduces storage costs, thus broadening the applicability to more scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2504.14967v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14967v1",
    "published_date": "2025-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "avatar",
      "compact",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays",
    "authors": [
      "Sascha Jecklin",
      "Aidana Massalimova",
      "Ruyi Zha",
      "Lilian Calvet",
      "Christoph J. Laux",
      "Mazda Farshad",
      "Philipp F√ºrnstahl"
    ],
    "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems. Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems. However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions. Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements. While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored. This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions. We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality. Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies. We evaluated our approach using an ex-vivo dataset. Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs. This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation.",
    "arxiv_url": "http://arxiv.org/abs/2504.14699v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14699v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation",
    "authors": [
      "Junyuan Fang",
      "Zihan Wang",
      "Yejun Zhang",
      "Shuzhe Wang",
      "Iaroslav Melekhov",
      "Juho Kannala"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive zero-shot transfer capabilities in image-level visual perception tasks. However, they fall short in 3D instance-level segmentation tasks that require accurate localization and recognition of individual objects. To bridge this gap, we introduce a novel 3D Gaussian Splatting based hard visual prompting approach that leverages camera interpolation to generate diverse viewpoints around target objects without any 2D-3D optimization or fine-tuning. Our method simulates realistic 3D perspectives, effectively augmenting existing hard visual prompts by enforcing geometric consistency across viewpoints. This training-free strategy seamlessly integrates with prior hard visual prompts, enriching object-descriptive features and enabling VLMs to achieve more robust and accurate 3D instance segmentation in diverse 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2504.14638v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14638v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "recognition",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboOcc: Enhancing the Geometric and Semantic Scene Understanding for Robots",
    "authors": [
      "Zhang Zhang",
      "Qiang Zhang",
      "Wei Cui",
      "Shuai Shi",
      "Yijie Guo",
      "Gang Han",
      "Wen Zhao",
      "Hengle Ren",
      "Renjing Xu",
      "Jian Tang"
    ],
    "abstract": "3D occupancy prediction enables the robots to obtain spatial fine-grained geometry and semantics of the surrounding scene, and has become an essential task for embodied perception. Existing methods based on 3D Gaussians instead of dense voxels do not effectively exploit the geometry and opacity properties of Gaussians, which limits the network's estimation of complex environments and also limits the description of the scene by 3D Gaussians. In this paper, we propose a 3D occupancy prediction method which enhances the geometric and semantic scene understanding for robots, dubbed RoboOcc. It utilizes the Opacity-guided Self-Encoder (OSE) to alleviate the semantic ambiguity of overlapping Gaussians and the Geometry-aware Cross-Encoder (GCE) to accomplish the fine-grained geometric modeling of the surrounding scene. We conduct extensive experiments on Occ-ScanNet and EmbodiedOcc-ScanNet datasets, and our RoboOcc achieves state-of the-art performance in both local and global camera settings. Further, in ablation studies of Gaussian parameters, the proposed RoboOcc outperforms the state-of-the-art methods by a large margin of (8.47, 6.27) in IoU and mIoU metric, respectively. The codes will be released soon.",
    "arxiv_url": "http://arxiv.org/abs/2504.14604v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14604v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
    "authors": [
      "Lifeng Lin",
      "Rongfeng Lu",
      "Quan Chen",
      "Haofan Ren",
      "Ming Lu",
      "Yaoqi Sun",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in practical 3D reconstruction applications. Recently, many methods based on the 3D Gaussian Splatting (3DGS) framework have been proposed to address sparse-view 3D reconstruction. Although these methods have made considerable advancements, they still show significant issues with overfitting. To reduce the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number Control (VGNC) approach based on generative novel view synthesis (NVS) models. To the best of our knowledge, this is the first attempt to alleviate the overfitting issue of sparse-view 3DGS with generative validation images. Specifically, we first introduce a validation image generation method based on a generative NVS model. We then propose a Gaussian number control strategy that utilizes generated validation images to determine the optimal Gaussian numbers, thereby reducing the issue of overfitting. We conducted detailed experiments on various sparse-view 3DGS baselines and datasets to evaluate the effectiveness of VGNC. Extensive experiments show that our approach not only reduces overfitting but also improves rendering quality on the test set while decreasing the number of Gaussian points. This reduction lowers storage demands and accelerates both training and rendering. The code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2504.14548v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14548v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding",
    "authors": [
      "Junyan Su",
      "Baozhu Zhao",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point features with anchor embeddings has significantly enhanced the performance of newer 3DGS variants. While significant advances have been made, it is still challenging to boost rendering performance. Feature embeddings have difficulty accurately representing colors from different perspectives under varying lighting conditions, which leads to a washed-out appearance. Another reason is the lack of a proper densification strategy that prevents Gaussian point growth in thinly initialized areas, resulting in blurriness and needle-shaped artifacts. To address them, we propose Metamon-GS, from innovative viewpoints of variance-guided densification strategy and multi-level hash grid. The densification strategy guided by variance specifically targets Gaussians with high gradient variance in pixels and compensates for the importance of regions with extra Gaussians to improve reconstruction. The latter studies implicit global lighting conditions and accurately interprets color from different perspectives and feature embeddings. Our thorough experiments on publicly available datasets show that Metamon-GS surpasses its baseline model and previous versions, delivering superior quality in rendering novel views.",
    "arxiv_url": "http://arxiv.org/abs/2504.14460v1",
    "pdf_url": "http://arxiv.org/pdf/2504.14460v1",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image",
    "authors": [
      "Chen Guo",
      "Zhuo Su",
      "Jian Wang",
      "Shuang Li",
      "Xu Chang",
      "Zhaohu Li",
      "Yang Zhao",
      "Guidong Wang",
      "Ruqi Huang"
    ],
    "abstract": "Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.14373v2",
    "pdf_url": "http://arxiv.org/pdf/2504.14373v2",
    "published_date": "2025-04-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "head",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM",
    "authors": [
      "Samuel Cerezo",
      "Gaetano Meli",
      "Tom√°s Berriel Martins",
      "Kirill Safronov",
      "Javier Civera"
    ],
    "abstract": "Models and methods originally developed for novel view synthesis and scene rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as multimodality and sequentiality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. To bridge this gap, we introduce SLAM&Render, a novel dataset designed to benchmark methods in the intersection between SLAM and novel view rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of novel SLAM strategies when applied to robot manipulators. The dataset sequences span five different setups featuring consumer and industrial objects under four different lighting conditions, with separate training and test trajectories per scene, as well as object rearrangements. Our experimental results, obtained with several baselines from the literature, validate SLAM&Render as a relevant benchmark for this emerging research area.",
    "arxiv_url": "http://arxiv.org/abs/2504.13713v2",
    "pdf_url": "http://arxiv.org/pdf/2504.13713v2",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "nerf",
      "lighting",
      "mapping",
      "slam",
      "ar",
      "illumination",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Green Robotic Mixed Reality with Gaussian Splatting",
    "authors": [
      "Chenxuan Liu",
      "He Li",
      "Zongze Li",
      "Shuai Wang",
      "Wei Xu",
      "Kejiang Ye",
      "Derrick Wing Kwan Ng",
      "Chengzhong Xu"
    ],
    "abstract": "Realizing green communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images at high frequencies through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS model which enables the simulator to opportunistically render a photo-realistic view from the robot's pose, thereby reducing the need for excessive image uploads. Since the GS model may involve discrepancies compared to the actual environments, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation across different frames. The GSCLO problem is solved by an accelerated penalty optimization (APO) algorithm. Experiments demonstrate that the proposed GSRMR reduces the communication energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with APO outperforms extensive baseline schemes, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM).",
    "arxiv_url": "http://arxiv.org/abs/2504.13697v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13697v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framework EG-Gaussian, which utilizes epipolar geometry and graph networks for 3D scene reconstruction. Initially, we integrate epipolar geometry into the 3DGS initialization phase to enhance initial 3DGS point construction. Then, we specifically design a graph learning module to refine 3DGS spatial features, in which we incorporate both spatial coordinates and angular relationships among neighboring points. Experiments on indoor and outdoor benchmark datasets demonstrate that our approach significantly improves reconstruction accuracy compared to 3DGS-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2504.13540v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13540v1",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "outdoor",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for Volume Rendering",
    "authors": [
      "Landon Dyken",
      "Andres Sewell",
      "Will Usher",
      "Steve Petruzza",
      "Sidharth Kumar"
    ],
    "abstract": "While HPC resources are increasingly being used to produce adaptively refined or unstructured volume datasets, current research in applying machine learning-based representation to visualization has largely ignored this type of data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D Gaussian-based representation for scientific volume visualization focused on unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that store view-dependent color and opacity for each Gaussian, VEG decouple the visual appearance from the data representation by encoding only scalar values, enabling transfer-function-agnostic rendering of 3DGS models for interactive scientific visualization. VEG are directly initialized from volume datasets, eliminating the need for structure-from-motion pipelines like COLMAP. To ensure complete scalar field coverage, we introduce an opacity-guided training strategy, using differentiable rendering with multiple transfer functions to optimize our data representation. This allows VEG to preserve fine features across the full scalar range of a dataset while remaining independent of any specific transfer function. Each Gaussian is scaled and rotated to adapt to local geometry, allowing for efficient representation of unstructured meshes without storing mesh connectivity and while using far fewer primitives. Across a diverse set of data, VEG achieve high reconstruction quality, compress large volume datasets by up to 3600x, and support lightning-fast rendering on commodity GPUs, enabling interactive visualization of large-scale structured and unstructured volumes.",
    "arxiv_url": "http://arxiv.org/abs/2504.13339v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13339v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation",
    "authors": [
      "Sizhe Yang",
      "Wenye Yu",
      "Jia Zeng",
      "Jun Lv",
      "Kerui Ren",
      "Cewu Lu",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world.",
    "arxiv_url": "http://arxiv.org/abs/2504.13175v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13175v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos",
    "authors": [
      "Zetong Zhang",
      "Manuel Kaufmann",
      "Lixin Xue",
      "Jie Song",
      "Martin R. Oswald"
    ],
    "abstract": "Creating a photorealistic scene and human reconstruction from a single monocular in-the-wild video figures prominently in the perception of a human-centric 3D world. Recent neural rendering advances have enabled holistic human-scene reconstruction but require pre-calibrated camera and human poses, and days of training time. In this work, we introduce a novel unified framework that simultaneously performs camera tracking, human pose estimation and human-scene reconstruction in an online fashion. 3D Gaussian Splatting is utilized to learn Gaussian primitives for humans and scenes efficiently, and reconstruction-based camera tracking and human pose estimation modules are designed to enable holistic understanding and effective disentanglement of pose and appearance. Specifically, we design a human deformation module to reconstruct the details and enhance generalizability to out-of-distribution poses faithfully. Aiming to learn the spatial correlation between human and scene accurately, we introduce occlusion-aware human silhouette rendering and monocular geometric priors, which further improve reconstruction quality. Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par performance with existing methods in camera tracking, human pose estimation, novel view synthesis and runtime. Our project page is at https://eth-ait.github.io/ODHSR.",
    "arxiv_url": "http://arxiv.org/abs/2504.13167v2",
    "pdf_url": "http://arxiv.org/pdf/2504.13167v2",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV",
      "I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "deformation",
      "understanding",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital Twin Generation from Visual Data: A Survey",
    "authors": [
      "Andrew Melnik",
      "Benjamin Alt",
      "Giang Nguyen",
      "Artur Wilkowski",
      "Maciej Stefa≈Ñczyk",
      "Qirui Wu",
      "Sinan Harms",
      "Helge Rhodin",
      "Manolis Savva",
      "Michael Beetz"
    ],
    "abstract": "This survey explores recent developments in generating digital twins from videos. Such digital twins can be used for robotics application, media content creation, or design and construction works. We analyze various approaches, including 3D Gaussian Splatting, generative in-painting, semantic segmentation, and foundation models highlighting their advantages and limitations. Additionally, we discuss challenges such as occlusions, lighting variations, and scalability, as well as potential future research directions. This survey aims to provide a comprehensive overview of state-of-the-art methodologies and their implications for real-world applications. Awesome list: https://github.com/ndrwmlnk/awesome-digital-twins",
    "arxiv_url": "http://arxiv.org/abs/2504.13159v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13159v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ndrwmlnk/awesome-digital-twins",
    "keywords": [
      "robotics",
      "survey",
      "lighting",
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting with Superpoint Graphs",
    "authors": [
      "Shaohui Dai",
      "Yansong Qu",
      "Zheyan Li",
      "Xinyang Li",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "abstract": "Bridging natural language and 3D geometry is a crucial step toward flexible, language-driven scene understanding. While recent advances in 3D Gaussian Splatting (3DGS) have enabled fast and high-quality scene reconstruction, research has also explored incorporating open-vocabulary understanding into 3DGS. However, most existing methods require iterative optimization over per-view 2D semantic feature maps, which not only results in inefficiencies but also leads to inconsistent 3D semantics across views. To address these limitations, we introduce a training-free framework that constructs a superpoint graph directly from Gaussian primitives. The superpoint graph partitions the scene into spatially compact and semantically coherent regions, forming view-consistent 3D entities and providing a structured foundation for open-vocabulary understanding. Based on the graph structure, we design an efficient reprojection strategy that lifts 2D semantic features onto the superpoints, avoiding costly multi-view iterative training. The resulting representation ensures strong 3D semantic coherence and naturally supports hierarchical understanding, enabling both coarse- and fine-grained open-vocabulary perception within a unified semantic field. Extensive experiments demonstrate that our method achieves state-of-the-art open-vocabulary segmentation performance, with semantic field reconstruction completed over $30\\times$ faster. Our code will be available at https://github.com/Atrovast/THGS.",
    "arxiv_url": "http://arxiv.org/abs/2504.13153v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13153v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Atrovast/THGS",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "understanding",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation",
    "authors": [
      "Xiangrui Liu",
      "Xinju Wu",
      "Shiqi Wang",
      "Zhu Li",
      "Sam Kwong"
    ],
    "abstract": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers from substantial data volume due to inherent primitive redundancy. To enable future photorealistic 3D immersive visual communication applications, significant compression is essential for transmission over the existing Internet infrastructure. Hence, we propose Compressed Gaussian Splatting (CompGS++), a novel framework that leverages compact Gaussian primitives to achieve accurate 3D modeling with substantial size reduction for both static and dynamic scenes. Our design is based on the principle of eliminating redundancy both between and within primitives. Specifically, we develop a comprehensive prediction paradigm to address inter-primitive redundancy through spatial and temporal primitive prediction modules. The spatial primitive prediction module establishes predictive relationships for scene primitives and enables most primitives to be encoded as compact residuals, substantially reducing the spatial redundancy. We further devise a temporal primitive prediction module to handle dynamic scenes, which exploits primitive correlations across timestamps to effectively reduce temporal redundancy. Moreover, we devise a rate-constrained optimization module that jointly minimizes reconstruction error and rate consumption. This module effectively eliminates parameter redundancy within primitives and enhances the overall compactness of scene representations. Comprehensive evaluations across multiple benchmark datasets demonstrate that CompGS++ significantly outperforms existing methods, achieving superior compression performance while preserving accurate scene modeling. Our implementation will be made publicly available on GitHub to facilitate further research.",
    "arxiv_url": "http://arxiv.org/abs/2504.13022v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13022v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration",
    "authors": [
      "Rendong Zhang",
      "Alexandra Watkins",
      "Nilanjan Sarkar"
    ],
    "abstract": "Photorealistic avatars have become essential for immersive applications in virtual reality (VR) and augmented reality (AR), enabling lifelike interactions in areas such as training simulations, telemedicine, and virtual collaboration. These avatars bridge the gap between the physical and digital worlds, improving the user experience through realistic human representation. However, existing avatar creation techniques face significant challenges, including high costs, long creation times, and limited utility in virtual applications. Manual methods, such as MetaHuman, require extensive time and expertise, while automatic approaches, such as NeRF-based pipelines often lack efficiency, detailed facial expression fidelity, and are unable to be rendered at a speed sufficent for real-time applications. By involving several cutting-edge modern techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar creation pipeline that leverages monocular video input to create a scalable and efficient photorealistic avatar directly compatible with the Unity game engine. Our pipeline incorporates a novel Gaussian splatting technique with customized preprocessing that enables the user of \"in the wild\" monocular video capture, detailed facial expression reconstruction and embedding within a fully rigged avatar model. Additionally, we present a Unity-integrated Gaussian Splatting Avatar Editor, offering a user-friendly environment for VR/AR application development. Experimental results validate the effectiveness of our preprocessing pipeline in standardizing custom data for 3DGS training and demonstrate the versatility of Gaussian avatars in Unity, highlighting the scalability and practicality of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2504.12999v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12999v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "lighting",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Second-order Optimization of Gaussian Splats with Importance Sampling",
    "authors": [
      "Hamza Pehlivan",
      "Andrea Boscolo Camiletto",
      "Lin Geng Foo",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to its high rendering quality and fast inference time. However, 3DGS predominantly relies on first-order optimizers such as Adam, which leads to long training times. To address this limitation, we propose a novel second-order optimization strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which we specifically tailor towards Gaussian Splatting. Our key insight is that the Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only a limited number of pixels. We exploit this sparsity by proposing a matrix-free and GPU-parallelized LM optimization. To further improve its efficiency, we propose sampling strategies for both the camera views and loss function and, consequently, the normal equation, significantly reducing the computational complexity. In addition, we increase the convergence rate of the second-order approximation by introducing an effective heuristic to determine the learning rate that avoids the expensive computation cost of line search methods. As a result, our method achieves a $3\\times$ speedup over standard LM and outperforms Adam by $~6\\times$ when the Gaussian count is low while remaining competitive for moderate counts. Project Page: https://vcai.mpi-inf.mpg.de/projects/LM-IS",
    "arxiv_url": "http://arxiv.org/abs/2504.12905v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12905v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering",
    "authors": [
      "Michael Steiner",
      "Thomas K√∂hler",
      "Lukas Radl",
      "Felix Windisch",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering.",
    "arxiv_url": "http://arxiv.org/abs/2504.12811v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12811v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation",
    "authors": [
      "Yifei Tong",
      "Runze Tian",
      "Xiao Han",
      "Dingyao Liu",
      "Fenggen Yu",
      "Yan Zhang"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of real scenes, enabling user-friendly deformation to create novel scenes while preserving fine details from the original 3DGS has attracted significant research attention. We introduce CAGE-GS, a cage-based 3DGS deformation method that seamlessly aligns a source 3DGS scene with a user-defined target shape. Our approach learns a deformation cage from the target, which guides the geometric transformation of the source scene. While the cages effectively control structural alignment, preserving the textural appearance of 3DGS remains challenging due to the complexity of covariance parameters. To address this, we employ a Jacobian matrix-based strategy to update the covariance parameters of each Gaussian, ensuring texture fidelity post-deformation. Our method is highly flexible, accommodating various target shape representations, including texts, images, point clouds, meshes and 3DGS models. Extensive experiments and ablation studies on both public datasets and newly proposed scenes demonstrate that our method significantly outperforms existing techniques in both efficiency and deformation quality.",
    "arxiv_url": "http://arxiv.org/abs/2504.12800v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12800v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors",
    "authors": [
      "Mingwei Li",
      "Pu Pang",
      "Hehe Fan",
      "Hua Huang",
      "Yi Yang"
    ],
    "abstract": "Reconstructing transparent surfaces is essential for tasks such as robotic manipulation in labs, yet it poses a significant challenge for 3D reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods often encounter a transparency-depth dilemma, where the pursuit of photorealistic rendering through standard $\\alpha$-blending undermines geometric precision, resulting in considerable depth estimation errors for transparent materials. To address this issue, we introduce Transparent Surface Gaussian Splatting (TSGS), a new framework that separates geometry learning from appearance refinement. In the geometry learning stage, TSGS focuses on geometry by using specular-suppressed inputs to accurately represent surfaces. In the second stage, TSGS improves visual fidelity through anisotropic specular modeling, crucially maintaining the established opacity to ensure geometric accuracy. To enhance depth inference, TSGS employs a first-surface depth extraction method. This technique uses a sliding window over $\\alpha$-blending weights to pinpoint the most likely surface location and calculates a robust weighted average depth. To evaluate the transparent surface reconstruction task under realistic conditions, we collect a TransLab dataset that includes complex transparent laboratory glassware. Extensive experiments on TransLab show that TSGS achieves accurate geometric reconstruction and realistic rendering of transparent objects simultaneously within the efficient 3DGS framework. Specifically, TSGS significantly surpasses current leading methods, achieving a 37.3% reduction in chamfer distance and an 8.0% improvement in F1 score compared to the top baseline. The code and dataset will be released at https://longxiang-ai.github.io/TSGS/.",
    "arxiv_url": "http://arxiv.org/abs/2504.12799v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12799v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior",
    "authors": [
      "Xiao Han",
      "Runze Tian",
      "Yifei Tong",
      "Fenggen Yu",
      "Dingyao Liu",
      "Yan Zhang"
    ],
    "abstract": "Drag-driven editing has become popular among designers for its ability to modify complex geometric structures through simple and intuitive manipulation, allowing users to adjust and reshape content with minimal technical skill. This drag operation has been incorporated into numerous methods to facilitate the editing of 2D images and 3D meshes in design. However, few studies have explored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS) representation, as deforming 3DGS while preserving shape coherence and visual continuity remains challenging. In this paper, we introduce ARAP-GS, a drag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP) deformation. Unlike previous 3DGS editing methods, we are the first to apply ARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven geometric transformations. To preserve scene appearance after deformation, we incorporate an advanced diffusion prior for image super-resolution within our iterative optimization process. This approach enhances visual quality while maintaining multi-view consistency in the edited results. Experiments show that ARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its effectiveness and superiority for drag-driven 3DGS editing. Additionally, our method is highly efficient, requiring only 10 to 20 minutes to edit a scene on a single RTX 3090 GPU.",
    "arxiv_url": "http://arxiv.org/abs/2504.12788v1",
    "pdf_url": "http://arxiv.org/pdf/2504.12788v1",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mind2Matter: Creating 3D Models from EEG Signals",
    "authors": [
      "Xia Deng",
      "Shen Chen",
      "Jiale Zhou",
      "Lei Li"
    ],
    "abstract": "The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in https://github.com/sddwwww/Mind2Matter.",
    "arxiv_url": "http://arxiv.org/abs/2504.11936v3",
    "pdf_url": "http://arxiv.org/pdf/2504.11936v3",
    "published_date": "2025-04-16",
    "categories": [
      "cs.GR",
      "cs.HC",
      "eess.SP"
    ],
    "github_url": "https://github.com/sddwwww/Mind2Matter",
    "keywords": [
      "face",
      "semantic",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting",
    "authors": [
      "Wei Sun",
      "Yanzhao Zhou",
      "Jianbin Jiao",
      "Yuan Li"
    ],
    "abstract": "Open-vocabulary 3D scene understanding is crucial for applications requiring natural language-driven spatial interpretation, such as robotics and augmented reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation for scene reconstruction, integrating it with open-vocabulary frameworks reveals a key challenge: cross-view granularity inconsistency. This issue, stemming from 2D segmentation methods like SAM, results in inconsistent object segmentations across views (e.g., a \"coffee set\" segmented as a single entity in one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based methods often rely on isolated per-Gaussian feature learning, neglecting the spatial context needed for cohesive object reasoning, leading to fragmented representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel framework that addresses this challenge by incorporating spatial context into 3DGS. CAGS constructs local graphs to propagate contextual features across Gaussians, reducing noise from inconsistent granularity, employs mask-centric contrastive learning to smooth SAM-derived features across views, and leverages a precomputation strategy to reduce computational cost by precomputing neighborhood relationships, enabling efficient training in large-scale scenes. By integrating spatial context, CAGS significantly improves 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet, enabling robust language-guided 3D scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2504.11893v1",
    "pdf_url": "http://arxiv.org/pdf/2504.11893v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction",
    "authors": [
      "Wenhua Wu",
      "Tong Zhao",
      "Chensheng Peng",
      "Lei Yang",
      "Yintao Wei",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typically rely on multi-view image inputs and require prolonged optimization times. In this paper, we propose BEV-GS, a real-time single-frame road surface reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists of a prediction module and a rendering module. The prediction module introduces separate geometry and texture networks following Bird's-Eye-View paradigm. Geometric and texture parameters are directly estimated from a single frame, avoiding per-scene optimization. In the rendering module, we utilize grid Gaussian for road surface representation and novel view synthesis, which better aligns with road surface characteristics. Our method achieves state-of-the-art performance on the real-world dataset RSRD. The road elevation error reduces to 1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and real-time applications. The code will be available at: \\href{https://github.com/cat-wwh/BEV-GS}{\\texttt{https://github.com/cat-wwh/BEV-GS}}",
    "arxiv_url": "http://arxiv.org/abs/2504.13207v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13207v1",
    "published_date": "2025-04-16",
    "categories": [
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "https://github.com/cat-wwh/BEV-GS",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDGS: Eliminating Densification for Efficient Convergence of 3DGS",
    "authors": [
      "Dmytro Kotovenko",
      "Olga Grebenkova",
      "Bj√∂rn Ommer"
    ],
    "abstract": "3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a lengthy optimization path. Moreover, this incremental approach often leads to suboptimal renderings, particularly in high-frequency regions where detail is critical.   We propose a fundamentally different approach: we eliminate densification process with a one-step approximation of scene geometry using triangulated pixels from dense image correspondences. This dense initialization allows us to estimate rough geometry of the scene while preserving rich details from input RGB images, providing each Gaussian with well-informed colors, scales, and positions. As a result, we dramatically shorten the optimization path and remove the need for densification. Unlike traditional methods that rely on sparse keypoints, our dense initialization ensures uniform detail across the scene, even in high-frequency regions where 3DGS and other methods struggle. Moreover, since all splats are initialized in parallel at the start of optimization, we eliminate the need to wait for densification to adjust new Gaussians.   Our method not only outperforms speed-optimized models in training efficiency but also achieves higher rendering quality than state-of-the-art approaches, all while using only half the splats of standard 3DGS. It is fully compatible with other 3DGS acceleration techniques, making it a versatile and efficient solution that can be integrated with existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2504.13204v1",
    "pdf_url": "http://arxiv.org/pdf/2504.13204v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "geometry",
      "3d gaussian",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians",
    "authors": [
      "Zeming Wei",
      "Junyi Lin",
      "Yang Liu",
      "Weixing Chen",
      "Jingzhou Luo",
      "Guanbin Li",
      "Liang Lin"
    ],
    "abstract": "3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2504.11218v2",
    "pdf_url": "http://arxiv.org/pdf/2504.11218v2",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "efficient",
      "high-fidelity",
      "head",
      "lighting",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation",
    "authors": [
      "Andrea Simonelli",
      "Norman M√ºller",
      "Peter Kontschieder"
    ],
    "abstract": "The increasing availability of digital 3D environments, whether through image-based 3D reconstruction, generation, or scans obtained by robots, is driving innovation across various applications. These come with a significant demand for 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and performing well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a 3D interactive segmentation method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as the ones obtained by Gaussian Splatting. The project web-page is available at https://simonelli-andrea.github.io/easy3d.",
    "arxiv_url": "http://arxiv.org/abs/2504.11024v1",
    "pdf_url": "http://arxiv.org/pdf/2504.11024v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gabor Splatting: Reconstruction of High-frequency Surface Texture using Gabor Noise",
    "authors": [
      "Haato Watanabe",
      "Kenji Tojo",
      "Nobuyuki Umetani"
    ],
    "abstract": "3D Gaussian splatting has experienced explosive popularity in the past few years in the field of novel view synthesis. The lightweight and differentiable representation of the radiance field using the Gaussian enables rapid and high-quality reconstruction and fast rendering. However, reconstructing objects with high-frequency surface textures (e.g., fine stripes) requires many skinny Gaussian kernels because each Gaussian represents only one color if viewed from one direction. Thus, reconstructing the stripes pattern, for example, requires Gaussians for at least the number of stripes. We present 3D Gabor splatting, which augments the Gaussian kernel to represent spatially high-frequency signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term and spatially fluctuating wave functions, making it suitable for representing spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can reconstruct various high-frequency textures on the objects.",
    "arxiv_url": "http://arxiv.org/abs/2504.11003v1",
    "pdf_url": "http://arxiv.org/pdf/2504.11003v1",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR",
    "authors": [
      "Christophe Bolduc",
      "Yannick Hold-Geoffroy",
      "Zhixin Shu",
      "Jean-Fran√ßois Lalonde"
    ],
    "abstract": "We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: https://lvsn.github.io/gaslight/",
    "arxiv_url": "http://arxiv.org/abs/2504.10809v2",
    "pdf_url": "http://arxiv.org/pdf/2504.10809v2",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting",
    "authors": [
      "Zeren Jiang",
      "Shaofei Wang",
      "Siyu Tang"
    ],
    "abstract": "Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2504.10486v1",
    "pdf_url": "http://arxiv.org/pdf/2504.10486v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "ray tracing",
      "relighting",
      "lighting",
      "fast",
      "shadow",
      "geometry",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis",
    "authors": [
      "Hao Sun",
      "Fenggen Yu",
      "Huiyao Xu",
      "Tao Zhang",
      "Changqing Zou"
    ],
    "abstract": "Novel view synthesis (NVS) in low-light scenes remains a significant challenge due to degraded inputs characterized by severe noise, low dynamic range (LDR) and unreliable initialization. While recent NeRF-based approaches have shown promising results, most suffer from high computational costs, and some rely on carefully captured or pre-processed data--such as RAW sensor inputs or multi-exposure sequences--which severely limits their practicality. In contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with competitive visual fidelity; however, existing 3DGS-based methods struggle with low-light sRGB inputs, resulting in unstable Gaussian initialization and ineffective noise suppression. To address these challenges, we propose LL-Gaussian, a novel framework for 3D reconstruction and enhancement from low-light sRGB images, enabling pseudo normal-light novel view synthesis. Our method introduces three key innovations: 1) an end-to-end Low-Light Gaussian Initialization Module (LLGIM) that leverages dense priors from learning-based MVS approach to generate high-quality initial point clouds; 2) a dual-branch Gaussian decomposition model that disentangles intrinsic scene properties (reflectance and illumination) from transient interference, enabling stable and interpretable optimization; 3) an unsupervised optimization strategy guided by both physical constrains and diffusion prior to jointly steer decomposition and enhancement. Additionally, we contribute a challenging dataset collected in extreme low-light environments and demonstrate the effectiveness of LL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian achieves up to 2,000 times faster inference and reduces training time to just 2%, while delivering superior reconstruction and rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2504.10331v3",
    "pdf_url": "http://arxiv.org/pdf/2504.10331v3",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting",
    "authors": [
      "Huiqi Wu",
      "Jianbo Mei",
      "Yingjie Huang",
      "Yining Xu",
      "Jingjiao You",
      "Yilong Liu",
      "Li Yao"
    ],
    "abstract": "In recent years, significant advancements have been made in text-driven 3D content generation. However, several challenges remain. In practical applications, users often provide extremely simple text inputs while expecting high-quality 3D content. Generating optimal results from such minimal text is a difficult task due to the strong dependency of text-to-3D models on the quality of input prompts. Moreover, the generation process exhibits high variability, making it difficult to control. Consequently, multiple iterations are typically required to produce content that meets user expectations, reducing generation efficiency. To address this issue, we propose GPT-4V for self-optimization, which significantly enhances the efficiency of generating satisfactory content in a single attempt. Furthermore, the controllability of text-to-3D generation methods has not been fully explored. Our approach enables users to not only provide textual descriptions but also specify additional conditions, such as style, edges, scribbles, poses, or combinations of multiple conditions, allowing for more precise control over the generated 3D content. Additionally, during training, we effectively integrate multi-view information, including multi-view depth, masks, features, and images, to address the common Janus problem in 3D content generation. Extensive experiments demonstrate that our method achieves robust generalization, facilitating the efficient and controllable generation of high-quality 3D content.",
    "arxiv_url": "http://arxiv.org/abs/2504.10316v1",
    "pdf_url": "http://arxiv.org/pdf/2504.10316v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting",
    "authors": [
      "Yufei Deng",
      "Yuanjian Wang",
      "Rong Xiao",
      "Chenwei Tang",
      "Jizhe Zhou",
      "Jiahao Fan",
      "Deng Xiong",
      "Jiancheng Lv",
      "Huajin Tang"
    ],
    "abstract": "While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view synthesis, its performance degrades with motion blur. In scenarios with rapid motion or low-light conditions, existing RGB-based deblurring methods struggle to model camera pose and radiance changes during exposure, reducing reconstruction accuracy. Event cameras, capturing continuous brightness changes during exposure, can effectively assist in modeling motion blur and improving reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D Gaussians from event streams and severely blurred images. This method jointly learns the parameters of these Gaussians while recovering camera motion trajectories during exposure time. Specifically, we first construct a blur loss function by synthesizing multiple latent sharp images during the exposure time, minimizing the difference between real and synthesized blurred images. Then we use event stream to supervise the light intensity changes between latent sharp images at any time within the exposure period, supplementing the light intensity dynamic changes lost in RGB images. Furthermore, we optimize the latent sharp images at intermediate exposure times based on the event-based double integral (EDI) prior, applying consistency constraints to enhance the details and texture information of the reconstructed images. Extensive experiments on synthetic and real-world datasets show that EBAD-Gaussian can achieve high-quality 3D scene reconstruction under the condition of blurred images and event stream inputs.",
    "arxiv_url": "http://arxiv.org/abs/2504.10012v1",
    "pdf_url": "http://arxiv.org/pdf/2504.10012v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting",
    "authors": [
      "Junlin Hao",
      "Peiheng Wang",
      "Haoyang Wang",
      "Xinggong Zhang",
      "Zongming Guo"
    ],
    "abstract": "Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.",
    "arxiv_url": "http://arxiv.org/abs/2504.10001v3",
    "pdf_url": "http://arxiv.org/pdf/2504.10001v3",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling",
    "authors": [
      "Yunpeng Tan",
      "Junlin Hao",
      "Jiangkai Wu",
      "Liming Liu",
      "Qingyang Li",
      "Xinggong Zhang"
    ],
    "abstract": "Neural Radiance Field (NeRF) is widely known for high-fidelity novel view synthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting, requires minutes for training, far from the real-time performance required by multimedia scenarios like telemedicine. One of the obstacles is its inefficient sampling, which is only partially addressed by existing works. Existing point-sampling algorithms uniformly sample simple-texture regions (easy to fit) and complex-texture regions (hard to fit), while existing ray-sampling algorithms sample these regions all in the finest granularity (i.e. the pixel level), both wasting GPU training resources. Actually, regions with different texture intensities require different sampling granularities. To this end, we propose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which employs Monte Carlo Tree Search (MCTS) to partition each training image into pixel blocks with different sizes for active block-wise training. Specifically, the trees are initialized according to the texture of training images to boost the initialization speed, and an expansion/pruning module dynamically optimizes the block partition. MCBlock is implemented in Nerfstudio, an open-source toolset, and achieves a training acceleration of up to 2.33x, surpassing other ray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF model and contribute to the multimedia community.",
    "arxiv_url": "http://arxiv.org/abs/2504.09878v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09878v1",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "acceleration",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting",
    "authors": [
      "Zhicong Wu",
      "Hongbin Xu",
      "Gang Xu",
      "Ping Nie",
      "Zhixin Yan",
      "Jinkai Zheng",
      "Liangqiong Qu",
      "Ming Li",
      "Liqiang Nie"
    ],
    "abstract": "Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework. The code will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2504.09588v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09588v1",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering",
    "authors": [
      "Yexing Xu",
      "Longguang Wang",
      "Minglin Chen",
      "Sheng Ao",
      "Li Li",
      "Yulan Guo"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in novel view synthesis, its performance degrades dramatically with sparse inputs and generates undesirable artifacts. As the number of training views decreases, the novel view synthesis task degrades to a highly under-determined problem such that existing methods suffer from the notorious overfitting issue. Interestingly, we observe that models with fewer Gaussian primitives exhibit less overfitting under sparse inputs. Inspired by this observation, we propose a Random Dropout Regularization (RDR) to exploit the advantages of low-complexity models to alleviate overfitting. In addition, to remedy the lack of high-frequency details for these models, an Edge-guided Splitting Strategy (ESS) is developed. With these two techniques, our method (termed DropoutGS) provides a simple yet effective plug-in approach to improve the generalization performance of existing 3DGS methods. Extensive experiments show that our DropoutGS produces state-of-the-art performance under sparse views on benchmark datasets including Blender, LLFF, and DTU. The project page is at: https://xuyx55.github.io/DropoutGS/.",
    "arxiv_url": "http://arxiv.org/abs/2504.09491v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09491v1",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds",
    "authors": [
      "Jizong Peng",
      "Tze Ho Elden Tse",
      "Kai Xu",
      "Wenchao Gao",
      "Angela Yao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it needs to be initialized from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2504.09129v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09129v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Junuk Cha",
      "Soohyun Hwang",
      "Hyein Hwang",
      "Seungryul Baek"
    ],
    "abstract": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem that can find numerous applications. Despite recent advances, there is no comprehensive pipeline yet for bimanual class-agnostic interaction reconstruction from a monocular RGB video, where two hands and an unknown object are interacting with each other. Previous works tackled the limited hand-object interaction case, where object templates are pre-known or only one hand is involved in the interaction. The bimanual interaction reconstruction exhibits severe occlusions introduced by complex interactions between two hands and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an unknown object from a monocular video. To robustly obtain object Gaussians avoiding severe occlusions, we leverage prior knowledge of pre-trained diffusion model with score distillation sampling (SDS) loss, to reconstruct unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model (i.e., MANO) and share a single Gaussian for two hands to effectively accumulate hand 3D information, given limited views. To further consider the 3D alignment between hands and objects, we include the interacting-subjects optimization step during Gaussian optimization. Our method achieves the state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering quality (PSNR, SSIM, LPIPS), respectively.",
    "arxiv_url": "http://arxiv.org/abs/2504.09097v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09097v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting",
    "authors": [
      "Zhijie Shen",
      "Chunyu Lin",
      "Shujuan Huang",
      "Lang Nie",
      "Kang Liao",
      "Yao Zhao"
    ],
    "abstract": "Recently, reconstructing scenes from a single panoramic image using advanced 3D Gaussian Splatting (3DGS) techniques has attracted growing interest. Panoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the entire scene in a single shot. However, panoramic images introduce severe distortion, making it challenging to render 3D Gaussians into 2D distorted equirectangular space directly. Converting equirectangular images to cubemap projections partially alleviates this problem but introduces new challenges, such as projection distortion and discontinuities across cube-face boundaries. To address these limitations, we present a novel framework, named TPGS, to bridge continuous panoramic 3D scene reconstruction with perspective Gaussian splatting. Firstly, we introduce a Transition Plane between adjacent cube faces to enable smoother transitions in splatting directions and mitigate optimization ambiguity in the boundary region. Moreover, an intra-to-inter face optimization strategy is proposed to enhance local details and restore visual consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians within individual cube faces and then fine-tune them in the stitched panoramic space. Additionally, we introduce a spherical sampling technique to eliminate visible stitching seams. Extensive experiments on indoor and outdoor, egocentric, and roaming benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods. Code and models will be available at https://github.com/zhijieshen-bjtu/TPGS.",
    "arxiv_url": "http://arxiv.org/abs/2504.09062v1",
    "pdf_url": "http://arxiv.org/pdf/2504.09062v1",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zhijieshen-bjtu/TPGS",
    "keywords": [
      "outdoor",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting",
    "authors": [
      "Yongchang Wu",
      "Zipeng Qi",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ],
    "abstract": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian",
    "arxiv_url": "http://arxiv.org/abs/2504.09048v2",
    "pdf_url": "http://arxiv.org/pdf/2504.09048v2",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/SunshineWYC/BlockGaussian",
    "keywords": [
      "efficient",
      "vr",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents",
    "authors": [
      "Xin Tan",
      "Yuzhou Ji",
      "He Zhu",
      "Yuan Xie"
    ],
    "abstract": "The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.",
    "arxiv_url": "http://arxiv.org/abs/2504.08581v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08581v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "fast",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation",
    "authors": [
      "Bram Vanherle",
      "Brent Zoomers",
      "Jeroen Put",
      "Frank Van Reeth",
      "Nick Michiels"
    ],
    "abstract": "Generating synthetic images is a useful method for cheaply obtaining labeled data for training computer vision models. However, obtaining accurate 3D models of relevant objects is necessary, and the resulting images often have a gap in realism due to challenges in simulating lighting effects and camera artifacts. We propose using the novel view synthesis method called Gaussian Splatting to address these challenges. We have developed a synthetic data pipeline for generating high-quality context-aware instance segmentation training data for specific objects. This process is fully automated, requiring only a video of the target object. We train a Gaussian Splatting model of the target object and automatically extract the object from the video. Leveraging Gaussian Splatting, we then render the object on a random background image, and monocular depth estimation is employed to place the object in a believable pose. We introduce a novel dataset to validate our approach and show superior performance over other data generation approaches, such as Cut-and-Paste and Diffusion model-based generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.08473v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08473v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "lighting",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "authors": [
      "Sauradip Nag",
      "Daniel Cohen-Or",
      "Hao Zhang",
      "Ali Mahdavi-Amiri"
    ],
    "abstract": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2504.08366v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08366v1",
    "published_date": "2025-04-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting",
    "authors": [
      "Junbang Liu",
      "Enpei Huang",
      "Dongxing Mao",
      "Hui Zhang",
      "Xinyuan Song",
      "Yongxin Ni"
    ],
    "abstract": "Creating 3D content from single-view images is a challenging problem that has attracted considerable attention in recent years. Current approaches typically utilize score distillation sampling (SDS) from pre-trained 2D diffusion models to generate multi-view 3D representations. Although some methods have made notable progress by balancing generation speed and model quality, their performance is often limited by the visual inconsistencies of the diffusion model outputs. In this work, we propose ContrastiveGaussian, which integrates contrastive learning into the generative process. By using a perceptual loss, we effectively differentiate between positive and negative samples, leveraging the visual inconsistencies to improve 3D generation quality. To further enhance sample differentiation and improve contrastive learning, we incorporate a super-resolution model and introduce another Quantity-Aware Triplet Loss to address varying sample distributions during training. Our experiments demonstrate that our approach achieves superior texture fidelity and improved geometric consistency.",
    "arxiv_url": "http://arxiv.org/abs/2504.08100v1",
    "pdf_url": "http://arxiv.org/pdf/2504.08100v1",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians",
    "authors": [
      "Kefan Chen",
      "Sergiu Oprea",
      "Justin Theiss",
      "Sreyas Mohan",
      "Srinath Sridhar",
      "Aayush Prakash"
    ],
    "abstract": "With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses.",
    "arxiv_url": "http://arxiv.org/abs/2504.07949v1",
    "pdf_url": "http://arxiv.org/pdf/2504.07949v1",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "vr",
      "shadow",
      "face",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting",
    "authors": [
      "Chenyu Han",
      "Corentin Dumery"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene reconstruction for its high visual accuracy. However, uncertainty estimation of 3DGS scenes remains underexplored and is crucial to downstream tasks such as asset extraction and scene completion. Since the appearance of 3D gaussians is view-dependent, the color of a gaussian can thus be certain from an angle and uncertain from another. We thus propose to model uncertainty in 3DGS as an additional view-dependent per-gaussian feature that can be modeled with spherical harmonics. This simple yet effective modeling is easily interpretable and can be integrated into the traditional 3DGS pipeline. It is also significantly faster than ensemble methods while maintaining high accuracy, as demonstrated in our experiments.",
    "arxiv_url": "http://arxiv.org/abs/2504.07370v1",
    "pdf_url": "http://arxiv.org/pdf/2504.07370v1",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets",
    "authors": [
      "Yuhang Yang",
      "Fengqi Liu",
      "Yixing Lu",
      "Qin Zhao",
      "Pingyu Wu",
      "Wei Zhai",
      "Ran Yi",
      "Yang Cao",
      "Lizhuang Ma",
      "Zheng-Jun Zha",
      "Junting Dong"
    ],
    "abstract": "3D human digitization has long been a highly pursued yet challenging task. Existing methods aim to generate high-quality 3D digital humans from single or multiple views, but remain primarily constrained by current paradigms and the scarcity of 3D human assets. Specifically, recent approaches fall into several paradigms: optimization-based and feed-forward (both single-view regression and multi-view generation with reconstruction). However, they are limited by slow speed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional planes to high-dimensional space due to occlusion and invisibility, respectively. Furthermore, existing 3D human assets remain small-scale, insufficient for large-scale training. To address these challenges, we propose a latent space generation paradigm for 3D human digitization, which involves compressing multi-view images into Gaussians via a UV-structured VAE, along with DiT-based conditional generation, we transform the ill-posed low-to-high-dimensional mapping problem into a learnable distribution shift, which also supports end-to-end inference. In addition, we employ the multi-view optimization approach combined with synthetic data to construct the HGS-1M dataset, which contains $1$ million 3D Gaussian assets to support the large-scale training. Experimental results demonstrate that our paradigm, powered by large-scale training, produces high-quality 3D human Gaussians with intricate textures, facial details, and loose clothing deformation.",
    "arxiv_url": "http://arxiv.org/abs/2504.06982v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06982v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "deformation",
      "3d gaussian",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting",
    "authors": [
      "Daiwei Zhang",
      "Joaquin Gajardo",
      "Tomislav Medic",
      "Isinsu Katircioglu",
      "Mike Boss",
      "Norbert Kirchgessner",
      "Achim Walter",
      "Lukas Roth"
    ],
    "abstract": "Automated extraction of plant morphological traits is crucial for supporting crop breeding and agricultural management through high-throughput field phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due to their scalability and affordability, enabling volumetric measurements that 2D approaches cannot directly capture. While advanced methods like Neural Radiance Fields (NeRFs) have shown promise, their application has been limited to counting or extracting traits from only a few plants or organs. Furthermore, accurately measuring complex structures like individual wheat heads-essential for studying crop yields-remains particularly challenging due to occlusions and the dense arrangement of crop canopies in field conditions. The recent development of 3D Gaussian Splatting (3DGS) offers a promising alternative for HTFP due to its high-quality reconstructions and explicit point-based representation. In this paper, we present Wheat3DGS, a novel approach that leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance segmentation and morphological measurement of hundreds of wheat heads automatically, representing the first application of 3DGS to HTFP. We validate the accuracy of wheat head extraction against high-resolution laser scan data, obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and 40.2% for length, width, and volume. We provide additional comparisons to NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating superior results. Our approach enables rapid, non-destructive measurements of key yield-related traits at scale, with significant implications for accelerating crop breeding and improving our understanding of wheat development.",
    "arxiv_url": "http://arxiv.org/abs/2504.06978v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06978v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "understanding",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "abstract": "This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2504.06827v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06827v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Hanxiao Sun",
      "YuPeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameters and normal, alongside the absence of physical constraints for indirect lighting. In this paper, we present a novel framework called Spatially-vayring Gaussian Inverse Rendering (SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we propose a new representation-Spatially-varying Gaussian (SVG)-that allows per-Gaussian spatially varying parameters. This enhanced representation is complemented by a SVG splatting scheme akin to vertex/fragment shading in traditional graphics pipelines. Furthermore, we integrate a physically-based indirect lighting model, enabling more realistic relighting. The proposed SVG-IR framework significantly improves rendering quality, outperforming state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio (PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in relighting tasks, all while maintaining a real-time rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2504.06815v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06815v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "relighting",
      "lighting",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction",
    "authors": [
      "Anil Armagan",
      "Albert Sa√†-Garriga",
      "Bruno Manganelli",
      "Kyuwon Kim",
      "M. Kerim Yucel"
    ],
    "abstract": "Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly due to its ability to converge reasonably fast, faithfully represent the scene and render (novel) views in a fast fashion. However, it suffers from large storage and memory requirements, and its training speed still lags behind the hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it especially difficult to deploy them in robotics scenarios, where 3D reconstruction is crucial for accurate operation. In this paper, we propose GSta that dynamically identifies Gaussians that have converged well during training, based on their positional and color gradient norms. By forcing such Gaussians into a siesta and stopping their updates (freezing) during training, we improve training speed with competitive accuracy compared to state of the art. We also propose an early stopping mechanism based on the PSNR values computed on a subset of training images. Combined with other improvements, such as integrating a learning rate scheduler, GSta achieves an improved Pareto front in convergence speed, memory and storage requirements, while preserving quality. We also show that GSta can improve other methods and complement orthogonal approaches in efficiency improvement; once combined with Trick-GS, GSta achieves up to 5x faster training, 16x smaller disk size compared to vanilla GS, while having comparable accuracy and consuming only half the peak memory. More visualisations are available at https://anilarmagan.github.io/SRUK-GSta.",
    "arxiv_url": "http://arxiv.org/abs/2504.06716v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06716v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "fast",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Collision avoidance from monocular vision trained with novel view synthesis",
    "authors": [
      "Valentin Tordjman--Levavasseur",
      "St√©phane Caron"
    ],
    "abstract": "Collision avoidance can be checked in explicit environment models such as elevation maps or occupancy grids, yet integrating such models with a locomotion policy requires accurate state estimation. In this work, we consider the question of collision avoidance from an implicit environment model. We use monocular RGB images as inputs and train a collisionavoidance policy from photorealistic images generated by 2D Gaussian splatting. We evaluate the resulting pipeline in realworld experiments under velocity commands that bring the robot on an intercept course with obstacles. Our results suggest that RGB images can be enough to make collision-avoidance decisions, both in the room where training data was collected and in out-of-distribution environments.",
    "arxiv_url": "http://arxiv.org/abs/2504.06651v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06651v1",
    "published_date": "2025-04-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stochastic Ray Tracing of Transparent 3D Gaussians",
    "authors": [
      "Xin Sun",
      "Iliyan Georgiev",
      "Yun Fei",
      "Milo≈° Ha≈°an"
    ],
    "abstract": "3D Gaussian splatting has been widely adopted as a 3D representation for novel-view synthesis, relighting, and 3D generation tasks. It delivers realistic and detailed results through a collection of explicit 3D Gaussian primitives, each carrying opacity and view-dependent color. However, efficient rendering of many transparent primitives remains a significant challenge. Existing approaches either rasterize the Gaussians with approximate per-view sorting or rely on high-end RTX GPUs. This paper proposes a stochastic ray-tracing method to render 3D clouds of transparent primitives. Instead of processing all ray-Gaussian intersections in sequential order, each ray traverses the acceleration structure only once, randomly accepting and shading a single intersection (or $N$ intersections, using a simple extension). This approach minimizes shading time and avoids primitive sorting along the ray, thereby minimizing register usage and maximizing parallelism even on low-end GPUs. The cost of rays through the Gaussian asset is comparable to that of standard mesh-intersection rays. The shading is unbiased and has low variance, as our stochastic acceptance achieves importance sampling based on accumulated weight. The alignment with Monte Carlo philosophy simplifies implementation and integration into a conventional path-tracing framework.",
    "arxiv_url": "http://arxiv.org/abs/2504.06598v3",
    "pdf_url": "http://arxiv.org/pdf/2504.06598v3",
    "published_date": "2025-04-09",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "efficient rendering",
      "relighting",
      "lighting",
      "3d gaussian",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIGA: Generalizable Sparse Image-driven Gaussian Avatars",
    "authors": [
      "Anton Zubekhin",
      "Heming Zhu",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "Driving a high-quality and photorealistic full-body human avatar, from only a few RGB cameras, is a challenging problem that has become increasingly relevant with emerging virtual reality technologies. To democratize such technology, a promising solution may be a generalizable method that takes sparse multi-view images of an unseen person and then generates photoreal free-view renderings of such identity. However, the current state of the art is not scalable to very large datasets and, thus, lacks in diversity and photorealism. To address this problem, we propose a novel, generalizable full-body model for rendering photoreal humans in free viewpoint, as driven by sparse multi-view video. For the first time in literature, our model can scale up training to thousands of subjects while maintaining high photorealism. At the core, we introduce a MultiHeadUNet architecture, which takes sparse multi-view images in texture space as input and predicts Gaussian primitives represented as 2D texels on top of a human body mesh. Importantly, we represent sparse-view image information, body shape, and the Gaussian parameters in 2D so that we can design a deep and scalable architecture entirely based on 2D convolutions and attention mechanisms. At test time, our method synthesizes an articulated 3D Gaussian-based avatar from as few as four input views and a tracked body template for unseen identities. Our method excels over prior works by a significant margin in terms of cross-subject generalization capability as well as photorealism.",
    "arxiv_url": "http://arxiv.org/abs/2504.07144v1",
    "pdf_url": "http://arxiv.org/pdf/2504.07144v1",
    "published_date": "2025-04-08",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "head",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation",
    "authors": [
      "Yiming Liang",
      "Tianhan Xu",
      "Yuta Kikuchi"
    ],
    "abstract": "We present Hierarchical Motion Representation (HiMoR), a novel deformation representation for 3D Gaussian primitives capable of achieving high-quality monocular dynamic 3D reconstruction. The insight behind HiMoR is that motions in everyday scenes can be decomposed into coarser motions that serve as the foundation for finer details. Using a tree structure, HiMoR's nodes represent different levels of motion detail, with shallower nodes modeling coarse motion for temporal smoothness and deeper nodes capturing finer motion. Additionally, our model uses a few shared motion bases to represent motions of different sets of nodes, aligning with the assumption that motion tends to be smooth and simple. This motion representation design provides Gaussians with a more structured deformation, maximizing the use of temporal relationships to tackle the challenging task of monocular dynamic 3D reconstruction. We also propose using a more reliable perceptual metric as an alternative, given that pixel-level metrics for evaluating monocular dynamic 3D reconstruction can sometimes fail to accurately reflect the true quality of reconstruction. Extensive experiments demonstrate our method's efficacy in achieving superior novel view synthesis from challenging monocular videos with complex motions.",
    "arxiv_url": "http://arxiv.org/abs/2504.06210v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06210v1",
    "published_date": "2025-04-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "abstract": "The primary focus of most recent works on open-vocabulary neural fields is extracting precise semantic features from the VLMs and then consolidating them efficiently into a multi-view consistent 3D neural fields representation. However, most existing works over-trusted SAM to regularize image-level CLIP without any further refinement. Moreover, several existing works improved efficiency by dimensionality reduction of semantic features from 2D VLMs before fusing with 3DGS semantic fields, which inevitably leads to multi-view inconsistency. In this work, we propose econSG for open-vocabulary semantic segmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided Regularization (CRR) that mutually refines SAM and CLIP to get the best of both worlds for precise semantic features with complete and precise boundaries. 2) A low dimensional contextual space to enforce 3D multi-view consistency while improving computational efficiency by fusing backprojected multi-view 2D features and follow by dimensional reduction directly on the fused 3D features instead of operating on each 2D view separately. Our econSG shows state-of-the-art performance on four benchmark datasets compared to the existing methods. Furthermore, we are also the most efficient training among all the methods.",
    "arxiv_url": "http://arxiv.org/abs/2504.06003v1",
    "pdf_url": "http://arxiv.org/pdf/2504.06003v1",
    "published_date": "2025-04-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve Phoneme-Viseme Alignment Ambiguity",
    "authors": [
      "Yihuan Huang",
      "Jiajun Liu",
      "Yanzhen Ren",
      "Wuyang Liu",
      "Juhua Tang"
    ],
    "abstract": "Speech-driven talking head synthesis tasks commonly use general acoustic features (such as HuBERT and DeepSpeech) as guided speech features. However, we discovered that these features suffer from phoneme-viseme alignment ambiguity, which refers to the uncertainty and imprecision in matching phonemes (speech) with visemes (lip). To address this issue, we propose the Speech Encoder for Lip (SE4Lip) to encode lip features from speech directly, aligning speech and lip features in the joint embedding space by a cross-modal alignment framework. The STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve the fine-grained speech features. Experimental results show that SE4Lip achieves state-of-the-art performance in both NeRF and 3DGS rendering models. Its lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline and produces results close to the ground truth videos.",
    "arxiv_url": "http://arxiv.org/abs/2504.05803v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05803v1",
    "published_date": "2025-04-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "head",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-splatting: Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting",
    "authors": [
      "Jee Won Lee",
      "Hansol Lim",
      "Sooyeun Yang",
      "Jongseong Choi"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have achieved impressive scalability and real-time rendering for large-scale scenes but often fall short in capturing fine-grained details. Conventional approaches that rely on relatively large covariance parameters tend to produce blurred representations, while directly reducing covariance sizes leads to sparsity. In this work, we introduce Micro-splatting (Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting), a novel framework designed to overcome these limitations. Our approach leverages a covariance regularization term to penalize excessively large Gaussians to ensure each splat remains compact and isotropic. This work implements an adaptive densification strategy that dynamically refines regions with high image gradients by lowering the splitting threshold, followed by loss function enhancement. This strategy results in a denser and more detailed gaussian means where needed, without sacrificing rendering efficiency. Quantitative evaluations using metrics such as L1, L2, PSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our method significantly enhances fine-details in 3D reconstructions.",
    "arxiv_url": "http://arxiv.org/abs/2504.05740v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05740v1",
    "published_date": "2025-04-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Deformation Fields for 2D Editing of 3D Models",
    "authors": [
      "Martin El Mqirmi",
      "Noam Aigerman"
    ],
    "abstract": "We propose a method for authoring non-realistic 3D objects (represented as either 3D Gaussian Splats or meshes), that comply with 2D edits from specific viewpoints. Namely, given a 3D object, a user chooses different viewpoints and interactively deforms the object in the 2D image plane of each view. The method then produces a \"deformation field\" - an interpolation between those 2D deformations in a smooth manner as the viewpoint changes. Our core observation is that the 2D deformations do not need to be tied to an underlying object, nor share the same deformation space. We use this observation to devise a method for authoring view-dependent deformations, holding several technical contributions: first, a novel way to compositionality-blend between the 2D deformations after lifting them to 3D - this enables the user to \"stack\" the deformations similarly to layers in an editing software, each deformation operating on the results of the previous; second, a novel method to apply the 3D deformation to 3D Gaussian Splats; third, an approach to author the 2D deformations, by deforming a 2D mesh encapsulating a rendered image of the object. We show the versatility and efficacy of our method by adding cartoonish effects to objects, providing means to modify human characters, fitting 3D models to given 2D sketches and caricatures, resolving occlusions, and recreating classic non-realistic paintings as 3D models.",
    "arxiv_url": "http://arxiv.org/abs/2504.05544v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05544v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery",
    "authors": [
      "Yi-Zhen Tsai",
      "Xuechen Zhang",
      "Zheng Li",
      "Jiasi Chen"
    ],
    "abstract": "Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations.",
    "arxiv_url": "http://arxiv.org/abs/2504.05517v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05517v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "high quality",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather Effects",
    "authors": [
      "Gal Fiebelman",
      "Hadar Averbuch-Elor",
      "Sagie Benaim"
    ],
    "abstract": "3D Gaussian Splatting has recently enabled fast and photorealistic reconstruction of static 3D scenes. However, introducing dynamic elements that interact naturally with such static scenes remains challenging. Accordingly, we present a novel hybrid framework that combines Gaussian-particle representations for incorporating physically-based global weather effects into static 3D Gaussian Splatting scenes, correctly handling the interactions of dynamic elements with the static scene. We follow a three-stage process: we first map static 3D Gaussians to a particle-based representation. We then introduce dynamic particles and simulate their motion using the Material Point Method (MPM). Finally, we map the simulated particles back to the Gaussian domain while introducing appearance parameters tailored for specific effects. To correctly handle the interactions of dynamic elements with the static scene, we introduce specialized collision handling techniques. Our approach supports a variety of weather effects, including snowfall, rainfall, fog, and sandstorms, and can also support falling objects, all with physically plausible motion and appearance. Experiments demonstrate that our method significantly outperforms existing approaches in both visual quality and physical realism.",
    "arxiv_url": "http://arxiv.org/abs/2504.05296v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05296v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanoDreamer: Consistent Text to 360-Degree Scene Generation",
    "authors": [
      "Zhexiao Xiong",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu",
      "Nathan Jacobs"
    ],
    "abstract": "Automatically generating a complete 3D scene from a text description, a reference image, or both has significant applications in fields like virtual reality and gaming. However, current methods often generate low-quality textures and inconsistent 3D structures. This is especially true when extrapolating significantly beyond the field of view of the reference image. To address these challenges, we propose PanoDreamer, a novel framework for consistent, 3D scene generation with flexible text and image control. Our approach employs a large language model and a warp-refine pipeline, first generating an initial set of images and then compositing them into a 360-degree panorama. This panorama is then lifted into 3D to form an initial point cloud. We then use several approaches to generate additional images, from different viewpoints, that are consistent with the initial point cloud and expand/refine the initial point cloud. Given the resulting set of images, we utilize 3D Gaussian Splatting to create the final 3D scene, which can then be rendered from different viewpoints. Experiments demonstrate the effectiveness of PanoDreamer in generating high-quality, geometrically consistent 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2504.05152v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05152v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Particle Approximation of VDB Datasets: A Study for Scientific Visualization",
    "authors": [
      "Isha Sharma",
      "Dieter Schmalstieg"
    ],
    "abstract": "The complexity and scale of Volumetric and Simulation datasets for Scientific Visualization(SciVis) continue to grow. And the approaches and advantages of memory-efficient data formats and storage techniques for such datasets vary. OpenVDB library and its VDB data format excels in memory efficiency through its hierarchical and dynamic tree structure, with active and inactive sub-trees for data storage. It is heavily used in current production renderers for both animation and rendering stages in VFX pipelines and photorealistic rendering of volumes and fluids. However, it still remains to be fully leveraged in SciVis where domains dealing with sparse scalar fields like porous media, time varying volumes such as tornado and weather simulation or high resolution simulation of Computational Fluid Dynamics present ample number of large challenging data sets. The goal of this paper hence is not only to explore the use of OpenVDB in SciVis but also to explore a level of detail(LOD) technique using 3D Gaussian particles approximating voxel regions. For rendering, we utilize NVIDIA OptiX library for ray marching through the Gaussians particles. Data modeling using 3D Gaussians has been very popular lately due to success in stereoscopic image to 3D scene conversion using Gaussian Splatting and Gaussian approximation and mixture models aren't entirely new in SciVis as well. Our work explores the integration with rendering software libraries like OpenVDB and OptiX to take advantage of their built-in memory compaction and hardware acceleration features, while also leveraging the performance capabilities of modern GPUs. Thus, we present a SciVis rendering approach that uses 3D Gaussians at varying LOD in a lossy scheme derived from VDB datasets, rather than focusing on photorealistic volume rendering.",
    "arxiv_url": "http://arxiv.org/abs/2504.04857v2",
    "pdf_url": "http://arxiv.org/pdf/2504.04857v2",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray marching",
      "efficient",
      "3d gaussian",
      "acceleration",
      "ar",
      "animation",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM",
    "authors": [
      "Zhicong Sun",
      "Jacqueline Lo",
      "Jinxing Hu"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) technology now has photorealistic mapping capabilities thanks to the real-time high-fidelity rendering capability of 3D Gaussian splatting (3DGS). However, due to the static representation of scenes, current 3DGS-based SLAM encounters issues with pose drift and failure to reconstruct accurate maps in dynamic environments. To address this problem, we present D4DGS-SLAM, the first SLAM method based on 4DGS map representation for dynamic environments. By incorporating the temporal dimension into scene representation, D4DGS-SLAM enables high-quality reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we can obtain the dynamics, visibility, and reliability of scene points, and filter stable static points for tracking accordingly. When optimizing Gaussian points, we apply different isotropic regularization terms to Gaussians with varying dynamic characteristics. Experimental results on real-world dynamic scene datasets demonstrate that our method outperforms state-of-the-art approaches in both camera pose tracking and map quality.",
    "arxiv_url": "http://arxiv.org/abs/2504.04844v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04844v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "high-fidelity",
      "tracking",
      "mapping",
      "4d",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal",
    "authors": [
      "Wanzhou Liu",
      "Zhexiao Xiong",
      "Xinyu Li",
      "Nathan Jacobs"
    ],
    "abstract": "Recent novel view synthesis (NVS) techniques, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene reconstruction with high-quality rendering and realistic detail recovery. Effectively removing occlusions while preserving scene details can further enhance the robustness and applicability of these techniques. However, existing approaches for object and occlusion removal predominantly rely on generative priors, which, despite filling the resulting holes, introduce new artifacts and blurriness. Moreover, existing benchmark datasets for evaluating occlusion removal methods lack realistic complexity and viewpoint variations. To address these issues, we introduce DeclutterSet, a novel dataset featuring diverse scenes with pronounced occlusions distributed across foreground, midground, and background, exhibiting substantial relative motion across viewpoints. We further introduce DeclutterNeRF, an occlusion removal method free from generative priors. DeclutterNeRF introduces joint multi-view optimization of learnable camera parameters, occlusion annealing regularization, and employs an explainable stochastic structural similarity loss, ensuring high-quality, artifact-free reconstructions from incomplete images. Experiments demonstrate that DeclutterNeRF significantly outperforms state-of-the-art methods on our proposed DeclutterSet, establishing a strong baseline for future research.",
    "arxiv_url": "http://arxiv.org/abs/2504.04679v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04679v1",
    "published_date": "2025-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning",
    "authors": [
      "Haonan Chen",
      "Cheng Zhu",
      "Yunzhu Li",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "Tool use is critical for enabling robots to perform complex real-world tasks, and leveraging human tool-use data can be instrumental for teaching robots. However, existing data collection methods like teleoperation are slow, prone to control delays, and unsuitable for dynamic tasks. In contrast, human natural data, where humans directly perform tasks with tools, offers natural, unstructured interactions that are both efficient and easy to collect. Building on the insight that humans and robots can share the same tools, we propose a framework to transfer tool-use knowledge from human data to robots. Using two RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting for novel view augmentation, employs segmentation models to extract embodiment-agnostic observations, and leverages task-space tool-action representations to train visuomotor policies. We validate our approach on diverse real-world tasks, including meatball scooping, pan flipping, wine bottle balancing, and other complex tasks. Our method achieves a 71\\% higher average success rate compared to diffusion policies trained with teleoperation data and reduces data collection time by 77\\%, with some tasks solvable only by our framework. Compared to hand-held gripper, our method cuts data collection time by 41\\%. Additionally, our method bridges the embodiment gap, improves robustness to variations in camera viewpoints and robot configurations, and generalizes effectively across objects and spatial setups.",
    "arxiv_url": "http://arxiv.org/abs/2504.04612v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04612v1",
    "published_date": "2025-04-06",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "segmentation",
      "human",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians",
    "authors": [
      "Haebeom Jung",
      "Namtae Kim",
      "Jungwoo Kim",
      "Jaesik Park"
    ],
    "abstract": "We present a targetless LiDAR-camera calibration method that jointly optimizes sensor poses and scene geometry from arbitrary scenes, without relying on traditional calibration targets such as checkerboards or spherical reflectors. Our approach leverages a 3D Gaussian-based scene representation. We first freeze reliable LiDAR points as anchors, then jointly optimize the poses and auxiliary Gaussian parameters in a fully differentiable manner using a photometric loss. This joint optimization significantly reduces sensor misalignment, resulting in higher rendering quality and consistently improved PSNR compared to the carefully calibrated poses provided in popular datasets. We validate our method through extensive experiments on two real-world autonomous driving datasets, KITTI-360 and Waymo, each featuring distinct sensor configurations. Additionally, we demonstrate the robustness of our approach using a custom LiDAR-camera setup, confirming strong performance across diverse hardware configurations.",
    "arxiv_url": "http://arxiv.org/abs/2504.04597v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04597v1",
    "published_date": "2025-04-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models",
    "authors": [
      "Etienne Chassaing",
      "Florent Forest",
      "Olga Fink",
      "Malcolm Mielle"
    ],
    "abstract": "In the European Union, buildings account for 42% of energy use and 35% of greenhouse gas emissions. Since most existing buildings will still be in use by 2050, retrofitting is crucial for emissions reduction. However, current building assessment methods rely mainly on qualitative thermal imaging, which limits data-driven decisions for energy savings. On the other hand, quantitative assessments using finite element analysis (FEA) offer precise insights but require manual CAD design, which is tedious and error-prone. Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, enable precise 3D modeling from sparse images but lack clearly defined volumes and the interfaces between them needed for FEA. We propose Thermoxels, a novel voxel-based method able to generate FEA-compatible models, including both geometry and temperature, from a sparse set of RGB and thermal images. Using pairs of RGB and thermal images as input, Thermoxels represents a scene's geometry as a set of voxels comprising color and temperature information. After optimization, a simple process is used to transform Thermoxels' models into tetrahedral meshes compatible with FEA. We demonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes, surpassing other state-of-the-art methods. To showcase the practical applications of Thermoxels' models, we conduct a simple heat conduction simulation using FEA, achieving convergence from an initial state defined by Thermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image synthesis abilities with current state-of-the-art methods, showing competitive results, and discuss the limitations of existing metrics in assessing mesh quality.",
    "arxiv_url": "http://arxiv.org/abs/2504.04448v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04448v1",
    "published_date": "2025-04-06",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS",
    "authors": [
      "Zhisheng Huang",
      "Peng Wang",
      "Jingdong Zhang",
      "Yuan Liu",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its efficiency and quality, but like many novel view synthesis methods, it heavily depends on accurate camera poses from Structure-from-Motion (SfM) systems. Although recent SfM pipelines have made impressive progress, questions remain about how to further improve both their robust performance in challenging conditions (e.g., textureless scenes) and the precision of camera parameter estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework that bridges this gap by jointly optimizing 3D Gaussians and camera parameters from large reconstruction priors MASt3R-SfM. We note that naively performing joint 3D Gaussian and camera optimization faces two challenges: the sensitivity to the quality of SfM initialization, and its limited capacity for global optimization, leading to suboptimal reconstruction results. Our 3R-GS, overcomes these issues by incorporating optimized practices, enabling robust scene reconstruction even with imperfect camera registration. Extensive experiments demonstrate that 3R-GS delivers high-quality novel view synthesis and precise camera pose estimation while remaining computationally efficient. Project page: https://zsh523.github.io/3R-GS/",
    "arxiv_url": "http://arxiv.org/abs/2504.04294v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04294v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning",
    "authors": [
      "Yuyang Zhang",
      "Baao Xie",
      "Hu Zhu",
      "Qi Wang",
      "Huanting Guo",
      "Xin Jin",
      "Wenjun Zeng"
    ],
    "abstract": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D reconstruction, delivering both rapid rendering and high-quality results. However, existing 3DGS methods pose challenges in understanding underlying 3D semantics, which hinders model controllability and interpretability. To address it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to discover both coarse- and fine-grained 3D semantics via hierarchical disentangled representation learning (DRL). Specifically, the model employs a dual-branch architecture, consisting of a point cloud initialization branch and a triplane-Gaussian generation branch, to achieve coarse-grained disentanglement by separating 3D geometry and visual appearance features. Subsequently, fine-grained semantic representations within each modality are further discovered through DRL-based encoder-adapters. To our knowledge, this is the first work to achieve unsupervised interpretable 3DGS. Evaluations indicate that our model achieves 3D disentanglement while preserving high-quality and rapid reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2504.04190v1",
    "pdf_url": "http://arxiv.org/pdf/2504.04190v1",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
    "authors": [
      "Jianhao Zheng",
      "Zihan Zhu",
      "Valentin Bieri",
      "Marc Pollefeys",
      "Songyou Peng",
      "Iro Armeni"
    ],
    "abstract": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2504.03886v1",
    "pdf_url": "http://arxiv.org/pdf/2504.03886v1",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "mapping",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration",
    "authors": [
      "Boyuan Wang",
      "Runqi Ouyang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Guosheng Zhao",
      "Chaojun Ni",
      "Guan Huang",
      "Lihong Liu",
      "Xingang Wang"
    ],
    "abstract": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \\textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.",
    "arxiv_url": "http://arxiv.org/abs/2504.03536v1",
    "pdf_url": "http://arxiv.org/pdf/2504.03536v1",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images",
    "authors": [
      "In-Hwan Jin",
      "Haesoo Choo",
      "Seong-Hun Jeong",
      "Heemoon Park",
      "Junghwan Kim",
      "Oh-joon Kwon",
      "Kyeongbo Kong"
    ],
    "abstract": "To achieve realistic immersion in landscape images, fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. These methods use pseudo 3D space, implicitly represented with Layered Depth Images (LDIs). LDIs separate a single image into depth-based layers, which enables elements like water and clouds to move within the image while revealing new scenes from different camera perspectives. However, as landscapes typically consist of continuous elements, including fluids, the representation of a 3D space separates a landscape image into discrete layers, and it can lead to diminished depth perception and potential distortions depending on camera movement. Furthermore, due to its implicit modeling of 3D space, the output may be limited to videos in the 2D domain, potentially reducing their versatility. In this paper, we propose representing a complete 3D space for dynamic scene video by modeling explicit representations, specifically 4D Gaussians, from a single image. The framework is focused on optimizing 3D Gaussians by generating multi-view images from a single image and creating 3D motion to optimize 4D Gaussians. The most important part of proposed framework is consistent 3D motion estimation, which estimates common motion among multi-view images to bring the motion in 3D space closer to actual motions. As far as we know, this is the first attempt that considers animation while representing a complete 3D space from a single landscape image. Our model demonstrates the ability to provide realistic immersion in various landscape images through diverse experiments and metrics. Extensive experimental results are https://cvsp-lab.github.io/ICLR2025_3D-MOM/.",
    "arxiv_url": "http://arxiv.org/abs/2504.05458v1",
    "pdf_url": "http://arxiv.org/pdf/2504.05458v1",
    "published_date": "2025-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.03059v2",
    "pdf_url": "http://arxiv.org/pdf/2504.03059v2",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM",
    "authors": [
      "Renwu Li",
      "Wenjing Ke",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "We present MonoGS++, a novel fast and accurate Simultaneous Localization and Mapping (SLAM) method that leverages 3D Gaussian representations and operates solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods largely depended on depth sensors, our approach reduces the hardware dependency and only requires RGB input, leveraging online visual odometry (VO) to generate sparse point clouds in real-time. To reduce redundancy and enhance the quality of 3D scene reconstruction, we implemented a series of methodological enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian insertion to avoid adding redundant Gaussians in previously well-reconstructed areas. Secondly, we introduced clarity-enhancing Gaussian densification module and planar regularization to handle texture-less areas and flat surfaces better. We achieved precise camera tracking results both on the synthetic Replica and real-world TUM-RGBD datasets, comparable to those of the state-of-the-art. Additionally, our method realized a significant 5.57x improvement in frames per second (fps) over the previous state-of-the-art, MonoGS.",
    "arxiv_url": "http://arxiv.org/abs/2504.02437v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02437v1",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "face",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation",
    "authors": [
      "Yuan Zhou",
      "Shilong Jin",
      "Litao Hua",
      "Wanjun Lv",
      "Haoran Duan",
      "Jungong Han"
    ],
    "abstract": "Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel framework that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise camera parameters; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer effectively mitigates the multi-face Janus problem in text-to-3D generation, outperforming existing methods in both visual quality and consistency.",
    "arxiv_url": "http://arxiv.org/abs/2504.02316v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02316v1",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital-twin imaging based on descattering Gaussian splatting",
    "authors": [
      "Suguru Shimomura",
      "Kazuki Yamanouchi",
      "Jun Tanida"
    ],
    "abstract": "Three-dimensional imaging through scattering media is important in medical science and astronomy. We propose a digital-twin imaging method based on Gaussian splatting to observe an object behind a scattering medium. A digital twin model built through data assimilation, emulates the behavior of objects and environmental changes in a virtual space. By constructing a digital twin using point clouds composed of Gaussians and simulating the scattering process through the convolution of a point spread function, three-dimensional objects behind a scattering medium can be reproduced as a digital twin. In this study, a high-contrast digital twin reproducing a three-dimensional object was successfully constructed from degraded images, assuming that data were acquired from wavefronts disturbed by a scattering medium. This technique reproduces objects by integrating data processing with image measurements.",
    "arxiv_url": "http://arxiv.org/abs/2504.02278v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02278v1",
    "published_date": "2025-04-03",
    "categories": [
      "physics.optics"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "We present UAVTwin, a method for creating digital twins from real-world environments and facilitating data augmentation for training downstream models embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses on synthesizing foreground components, such as various human instances in motion within complex scene backgrounds, from UAV perspectives. This is achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing backgrounds along with controllable synthetic human models that display diverse appearances and actions in multiple poses. To the best of our knowledge, UAVTwin is the first approach for UAV-based perception that is capable of generating high-fidelity digital twins based on 3DGS. The proposed work significantly enhances downstream models through data augmentation for real-world environments with multiple dynamic objects and significant appearance variations-both of which typically introduce artifacts in 3DGS-based modeling. To tackle these challenges, we propose a novel appearance modeling strategy and a mask refinement module to enhance the training of 3D Gaussian Splatting. We demonstrate the high quality of neural rendering by achieving a 1.23 dB improvement in PSNR compared to recent methods. Furthermore, we validate the effectiveness of data augmentation by showing a 2.5% to 13.7% improvement in mAP for the human detection task.",
    "arxiv_url": "http://arxiv.org/abs/2504.02158v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02158v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "high quality",
      "3d gaussian",
      "human",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WorldPrompter: Traversable Text-to-Scene Generation",
    "authors": [
      "Zhaoyang Zhang",
      "Yannick Hold-Geoffroy",
      "Milo≈° Ha≈°an",
      "Chen Ziwen",
      "Fujun Luan",
      "Julie Dorsey",
      "Yiwei Hu"
    ],
    "abstract": "Scene-level 3D generation is a challenging research topic, with most existing methods generating only partial scenes and offering limited navigational freedom. We introduce WorldPrompter, a novel generative pipeline for synthesizing traversable 3D scenes from text prompts. We leverage panoramic videos as an intermediate representation to model the 360{\\deg} details of a scene. WorldPrompter incorporates a conditional 360{\\deg} panoramic video generator, capable of producing a 128-frame video that simulates a person walking through and capturing a virtual environment. The resulting video is then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor, enabling a true walkable experience within the 3D scene. Experiments demonstrate that our panoramic video generation model achieves convincing view consistency across frames, enabling high-quality panoramic Gaussian splat reconstruction and facilitating traversal over an area of the scene. Qualitative and quantitative results also show it outperforms the state-of-the-art 360{\\deg} video generators and 3D scene generation models.",
    "arxiv_url": "http://arxiv.org/abs/2504.02045v1",
    "pdf_url": "http://arxiv.org/pdf/2504.02045v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis",
    "authors": [
      "Niluthpol Chowdhury Mithun",
      "Tuan Pham",
      "Qiao Wang",
      "Ben Southall",
      "Kshitij Minhas",
      "Bogdan Matei",
      "Stephan Mandt",
      "Supun Samarasekera",
      "Rakesh Kumar"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins.",
    "arxiv_url": "http://arxiv.org/abs/2504.01960v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01960v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting",
    "authors": [
      "Shu-Wei Lu",
      "Yi-Hsuan Tsai",
      "Yi-Ting Chen"
    ],
    "abstract": "Bird's-eye view (BEV) perception has gained significant attention because it provides a unified representation to fuse multiple view images and enables a wide range of down-stream autonomous driving tasks, such as forecasting and planning. Recent state-of-the-art models utilize projection-based methods which formulate BEV perception as query learning to bypass explicit depth estimation. While we observe promising advancements in this paradigm, they still fall short of real-world applications because of the lack of uncertainty modeling and expensive computational requirement. In this work, we introduce GaussianLSS, a novel uncertainty-aware BEV perception framework that revisits unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm, and enhances them with depth un-certainty modeling. GaussianLSS represents spatial dispersion by learning a soft depth mean and computing the variance of the depth distribution, which implicitly captures object extents. We then transform the depth distribution into 3D Gaussians and rasterize them to construct uncertainty-aware BEV features. We evaluate GaussianLSS on the nuScenes dataset, achieving state-of-the-art performance compared to unprojection-based methods. In particular, it provides significant advantages in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory compared to projection-based methods, while achieving competitive performance with only a 0.4% IoU difference.",
    "arxiv_url": "http://arxiv.org/abs/2504.01957v2",
    "pdf_url": "http://arxiv.org/pdf/2504.01957v2",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BOGausS: Better Optimized Gaussian Splatting",
    "authors": [
      "St√©phane Pateux",
      "Matthieu Gendrin",
      "Luce Morin",
      "Th√©o Ladune",
      "Xiaoran Jiang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2504.01844v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01844v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking",
    "authors": [
      "Ulas Gunes",
      "Matias Turkulainen",
      "Xuqian Ren",
      "Arno Solin",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "abstract": "The development of large-scale 3D scene reconstruction and novel view synthesis methods mostly rely on datasets comprising perspective images with narrow fields of view (FoV). While effective for small-scale scenes, these datasets require large image sets and extensive structure-from-motion (SfM) processing, limiting scalability. To address this, we introduce a fisheye image dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense point clouds that can be used as geometric ground-truth, enabling robust benchmarking under challenging conditions such as occlusions and reflections. While the baseline experiments focus on vanilla Gaussian Splatting and NeRF based Nerfacto methods, the dataset supports diverse approaches for scene reconstruction, novel view synthesis, and image-based rendering.",
    "arxiv_url": "http://arxiv.org/abs/2504.01732v2",
    "pdf_url": "http://arxiv.org/pdf/2504.01732v2",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "outdoor",
      "motion",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlowR: Flowing from Sparse to Dense 3D Reconstructions",
    "authors": [
      "Tobias Fischer",
      "Samuel Rota Bul√≤",
      "Yung-Hsu Yang",
      "Nikhil Varma Keetha",
      "Lorenzo Porzi",
      "Norman M√ºller",
      "Katja Schwarz",
      "Jonathon Luiten",
      "Marc Pollefeys",
      "Peter Kontschieder"
    ],
    "abstract": "3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of some applications, e.g. Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These methods are often conditioned only on a handful of reference input views and thus do not fully exploit the available 3D information, leading to inconsistent generation results and reconstruction artifacts. To tackle this problem, we propose a multi-view, flow matching model that learns a flow to connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with novel, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2504.01647v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01647v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting",
    "authors": [
      "Hao Wu",
      "Hao Wang",
      "Ruochong Li",
      "Xuran Ma",
      "Hui Xiong"
    ],
    "abstract": "Recent advancements in text-to-3D generation have shown remarkable results by leveraging 3D priors in combination with 2D diffusion. However, previous methods utilize 3D priors that lack detailed and complex structural information, limiting them to generating simple objects and presenting challenges for creating intricate structures such as bonsai. In this paper, we propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with complex structures. Technically, we first design a trainable 3D space colonization algorithm to produce bonsai structures, which are then enhanced through random sampling and point cloud augmentation to serve as the 3D Gaussian priors. We introduce two bonsai generation pipelines with distinct structural levels: fine structure conditioned generation, which initializes 3D Gaussians using a 3D structure prior to produce detailed and complex bonsai, and coarse structure conditioned generation, which employs a multi-view structure consistency module to align 2D and 3D structures. Moreover, we have compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental results demonstrate that 3DBonsai significantly outperforms existing methods, providing a new benchmark for structure-aware 3D bonsai generation.",
    "arxiv_url": "http://arxiv.org/abs/2504.01619v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01619v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars",
    "authors": [
      "Yahui Li",
      "Zhi Zeng",
      "Liming Pang",
      "Guixuan Zhang",
      "Shuwu Zhang"
    ],
    "abstract": "Modeling animatable human avatars from monocular or multi-view videos has been widely studied, with recent approaches leveraging neural radiance fields (NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in novel-view and novel-pose synthesis. However, existing methods often struggle to accurately capture the dynamics of loose clothing, as they primarily rely on global pose conditioning or static per-frame representations, leading to oversmoothing and temporal inconsistencies in non-rigid regions. To address this, We propose RealityAvatar, an efficient framework for high-fidelity digital human modeling, specifically targeting loosely dressed avatars. Our method leverages 3D Gaussian Splatting to capture complex clothing deformations and motion dynamics while ensuring geometric consistency. By incorporating a motion trend module and a latentbone encoder, we explicitly model pose-dependent deformations and temporal variations in clothing behavior. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach in capturing fine-grained clothing deformations and motion-driven shape variations. Our method significantly enhances structural fidelity and perceptual quality in dynamic human reconstruction, particularly in non-rigid regions, while achieving better consistency across temporal frames.",
    "arxiv_url": "http://arxiv.org/abs/2504.01559v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01559v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "high-fidelity",
      "motion",
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model",
    "authors": [
      "Yiyang Shen",
      "Kun Zhou",
      "He Wang",
      "Yin Yang",
      "Tianjia Shao"
    ],
    "abstract": "Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2504.01512v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01512v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment",
    "authors": [
      "Ziteng Cui",
      "Xuangeng Chu",
      "Tatsuya Harada"
    ],
    "abstract": "Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions -- including low-light, overexposure, and varying exposure -- while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality.",
    "arxiv_url": "http://arxiv.org/abs/2504.01503v2",
    "pdf_url": "http://arxiv.org/pdf/2504.01503v2",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Inverse Rendering with Approximated Global Illumination",
    "authors": [
      "Zirui Wu",
      "Jianteng Chen",
      "Laijian Li",
      "Shaoteng Wu",
      "Zhikai Zhu",
      "Kang Xu",
      "Martin R. Oswald",
      "Jie Song"
    ],
    "abstract": "3D Gaussian Splatting shows great potential in reconstructing photo-realistic 3D scenes. However, these methods typically bake illumination into their representations, limiting their use for physically-based rendering and scene editing. Although recent inverse rendering approaches aim to decompose scenes into material and lighting components, they often rely on simplifying assumptions that fail when editing. We present a novel approach that enables efficient global illumination for 3D Gaussians Splatting through screen-space ray tracing. Our key insight is that a substantial amount of indirect light can be traced back to surfaces visible within the current view frustum. Leveraging this observation, we augment the direct shading computed by 3D Gaussians with Monte-Carlo screen-space ray-tracing to capture one-bounce indirect illumination. In this way, our method enables realistic global illumination without sacrificing the computational efficiency and editability benefits of 3D Gaussians. Through experiments, we show that the screen-space approximation we utilize allows for indirect illumination and supports real-time rendering and editing. Code, data, and models will be made available at our project page: https://wuzirui.github.io/gs-ssr.",
    "arxiv_url": "http://arxiv.org/abs/2504.01358v1",
    "pdf_url": "http://arxiv.org/pdf/2504.01358v1",
    "published_date": "2025-04-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "lighting",
      "global illumination",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting",
    "authors": [
      "Hyunwoo Park",
      "Gun Ryu",
      "Wonjun Kim"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in the field of novel view synthesis due to its fast performance while yielding the excellent image quality. However, 3DGS in sparse-view settings (e.g., three-view inputs) often faces with the problem of overfitting to training views, which significantly drops the visual quality of novel view images. Many existing approaches have tackled this issue by using strong priors, such as 2D generative contextual information and external depth signals. In contrast, this paper introduces a prior-free method, so-called DropGaussian, with simple changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians during the training process in a similar way of dropout, which allows non-excluded Gaussians to have larger gradients while improving their visibility. This makes the remaining Gaussians to contribute more to the optimization process for rendering with sparse input views. Such simple operation effectively alleviates the overfitting problem and enhances the quality of novel view synthesis. By simply applying DropGaussian to the original 3DGS framework, we can achieve the competitive performance with existing prior-based 3DGS methods in sparse-view settings of benchmark datasets without any additional complexity. The code and model are publicly available at: https://github.com/DCVL-3D/DropGaussian release.",
    "arxiv_url": "http://arxiv.org/abs/2504.00773v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00773v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/DCVL-3D/DropGaussian",
    "keywords": [
      "sparse-view",
      "face",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene Reconstruction",
    "authors": [
      "Yunxuan Mao",
      "Rong Xiong",
      "Yue Wang",
      "Yiyi Liao"
    ],
    "abstract": "Reconstructing and decomposing dynamic urban scenes is crucial for autonomous driving, urban planning, and scene editing. However, existing methods fail to perform instance-aware decomposition without manual annotations, which is crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian Splatting (3DGS) based approach that decomposes a scene into a static background and individual dynamic instances using only RGB images and LiDAR point clouds. At its core, we introduce 4D superpoints, a novel representation that clusters multi-frame LiDAR points in 4D space, enabling unsupervised instance separation based on spatiotemporal correlations. These 4D superpoints serve as the foundation for our decomposed 4D initialization, i.e., providing spatial and temporal initialization to train a dynamic 3DGS for arbitrary dynamic classes without requiring bounding boxes or object templates.Furthermore, we introduce a smoothness regularization strategy in both 2D and 3D space, further improving the temporal stability.Experiments on benchmark datasets show that our method outperforms existing methods in decomposed dynamic scene reconstruction while enabling accurate and flexible instance-level editing, making it a practical solution for real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2504.00763v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00763v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "4d",
      "3d gaussian",
      "ar",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular and Generalizable Gaussian Talking Head Animation",
    "authors": [
      "Shengjie Gong",
      "Haojie Li",
      "Jiapeng Tang",
      "Dongming Hu",
      "Shuangping Huang",
      "Hao Chen",
      "Tianshui Chen",
      "Zhuoman Liu"
    ],
    "abstract": "In this work, we introduce Monocular and Generalizable Gaussian Talking Head Animation (MGGTalk), which requires monocular datasets and generalizes to unseen identities without personalized re-training. Compared with previous 3D Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or tedious personalized learning/inference, MGGtalk enables more practical and broader applications. However, in the absence of multi-view and personalized training data, the incompleteness of geometric and appearance information poses a significant challenge. To address these challenges, MGGTalk explores depth information to enhance geometric and facial symmetry characteristics to supplement both geometric and appearance features. Initially, based on the pixel-wise geometric information obtained from depth estimation, we incorporate symmetry operations and point cloud filtering techniques to ensure a complete and precise position parameter for 3DGS. Subsequently, we adopt a two-stage strategy with symmetric priors for predicting the remaining 3DGS parameters. We begin by predicting Gaussian parameters for the visible facial regions of the source image. These parameters are subsequently utilized to improve the prediction of Gaussian parameters for the non-visible regions. Extensive experiments demonstrate that MGGTalk surpasses previous state-of-the-art methods, achieving superior performance across various metrics.",
    "arxiv_url": "http://arxiv.org/abs/2504.00665v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00665v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Coca-Splat: Collaborative Optimization for Camera Parameters and 3D Gaussians",
    "authors": [
      "Jiamin Wu",
      "Hongyang Li",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ],
    "abstract": "In this work, we introduce Coca-Splat, a novel approach to addressing the challenges of sparse view pose-free scene reconstruction and novel view synthesis (NVS) by jointly optimizing camera parameters with 3D Gaussians. Inspired by deformable DEtection TRansformer, we design separate queries for 3D Gaussians and camera parameters and update them layer by layer through deformable Transformer layers, enabling joint optimization in a single network. This design demonstrates better performance because to accurately render views that closely approximate ground-truth images relies on precise estimation of both 3D Gaussians and camera parameters. In such a design, the centers of 3D Gaussians are projected onto each view by camera parameters to get projected points, which are regarded as 2D reference points in deformable cross-attention. With camera-aware multi-view deformable cross-attention (CaMDFA), 3D Gaussians and camera parameters are intrinsically connected by sharing the 2D reference points. Additionally, 2D reference point determined rays (RayRef) defined from camera centers to the reference points assist in modeling relationship between 3D Gaussians and camera parameters through RQ-decomposition on an overdetermined system of equations derived from the rays, enhancing the relationship between 3D Gaussians and camera parameters. Extensive evaluation shows that our approach outperforms previous methods, both pose-required and pose-free, on RealEstate10K and ACID within the same pose-free setting.",
    "arxiv_url": "http://arxiv.org/abs/2504.00639v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00639v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust LiDAR-Camera Calibration with 2D Gaussian Splatting",
    "authors": [
      "Shuyi Zhou",
      "Shuxiang Xie",
      "Ryoichi Ishikawa",
      "Takeshi Oishi"
    ],
    "abstract": "LiDAR-camera systems have become increasingly popular in robotics recently. A critical and initial step in integrating the LiDAR and camera data is the calibration of the LiDAR-camera system. Most existing calibration methods rely on auxiliary target objects, which often involve complex manual operations, whereas targetless methods have yet to achieve practical effectiveness. Recognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric information from camera image sequences, we propose a calibration method that estimates LiDAR-camera extrinsic parameters using geometric constraints. The proposed method begins by reconstructing colorless 2DGS using LiDAR point clouds. Subsequently, we update the colors of the Gaussian splats by minimizing the photometric loss. The extrinsic parameters are optimized during this process. Additionally, we address the limitations of the photometric loss by incorporating the reprojection and triangulation losses, thereby enhancing the calibration robustness and accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2504.00525v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00525v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distilling Multi-view Diffusion Models into 3D Generators",
    "authors": [
      "Hao Qin",
      "Luyuan Chen",
      "Ming Kong",
      "Mengxu Lu",
      "Qiang Zhu"
    ],
    "abstract": "We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: https://qinbaigao.github.io/DD3G_project/",
    "arxiv_url": "http://arxiv.org/abs/2504.00457v3",
    "pdf_url": "http://arxiv.org/pdf/2504.00457v3",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with Multi-modal Inputs",
    "authors": [
      "Qi Song",
      "Chenghong Li",
      "Haotong Lin",
      "Sida Peng",
      "Rui Huang"
    ],
    "abstract": "We present a novel approach, termed ADGaussian, for generalizable street scene reconstruction. The proposed method enables high-quality rendering from single-view input. Unlike prior Gaussian Splatting methods that primarily focus on geometry refinement, we emphasize the importance of joint optimization of image and depth features for accurate Gaussian prediction. To this end, we first incorporate sparse LiDAR depth as an additional input modality, formulating the Gaussian prediction process as a joint learning framework of visual information and geometric clue. Furthermore, we propose a multi-modal feature matching strategy coupled with a multi-scale Gaussian decoding model to enhance the joint refinement of multi-modal features, thereby enabling efficient multi-modal Gaussian learning. Extensive experiments on two large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our ADGaussian achieves state-of-the-art performance and exhibits superior zero-shot generalization capabilities in novel-view shifting.",
    "arxiv_url": "http://arxiv.org/abs/2504.00437v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00437v1",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single Panoramic Image for Your Immerse Exploration",
    "authors": [
      "Zilong Huang",
      "Jun He",
      "Junyan Ye",
      "Lihan Jiang",
      "Weijia Li",
      "Yiping Chen",
      "Ting Han"
    ],
    "abstract": "The reconstruction of immersive and realistic 3D scenes holds significant practical importance in various fields of computer vision and computer graphics. Typically, immersive and realistic scenes should be free from obstructions by dynamic objects, maintain global texture consistency, and allow for unrestricted exploration. The current mainstream methods for image-driven scene construction involves iteratively refining the initial image using a moving virtual camera to generate the scene. However, previous methods struggle with visual discontinuities due to global texture inconsistencies under varying camera poses, and they frequently exhibit scene voids caused by foreground-background occlusions. To this end, we propose a novel layered 3D scene reconstruction framework from panoramic image, named Scene4U. Specifically, Scene4U integrates an open-vocabulary segmentation model with a large language model to decompose a real panorama into multiple layers. Then, we employs a layered repair module based on diffusion model to restore occluded regions using visual cues and depth information, generating a hierarchical representation of the scene. The multi-layer panorama is then initialized as a 3D Gaussian Splatting representation, followed by layered optimization, which ultimately produces an immersive 3D scene with semantic and structural consistency that supports free exploration. Scene4U outperforms state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE, while also achieving the fastest training speed. Additionally, to demonstrate the robustness of Scene4U and allow users to experience immersive scenes from various landmarks, we build WorldVista3D dataset for 3D scene reconstruction, which contains panoramic images of globally renowned sites. The implementation code and dataset will be released at https://github.com/LongHZ140516/Scene4U .",
    "arxiv_url": "http://arxiv.org/abs/2504.00387v2",
    "pdf_url": "http://arxiv.org/pdf/2504.00387v2",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/LongHZ140516/Scene4U",
    "keywords": [
      "fast",
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free 3D Gaussian Splatting and Physical Priors",
    "authors": [
      "Han Zhou",
      "Wei Dong",
      "Jun Chen"
    ],
    "abstract": "Directly employing 3D Gaussian Splatting (3DGS) on images with adverse illumination conditions exhibits considerable difficulty in achieving high-quality, normally-exposed representations due to: (1) The limited Structure from Motion (SfM) points estimated in adverse illumination scenarios fail to capture sufficient scene details; (2) Without ground-truth references, the intensive information loss, significant noise, and color distortion pose substantial challenges for 3DGS to produce high-quality results; (3) Combining existing exposure correction methods with 3DGS does not achieve satisfactory performance due to their individual enhancement processes, which lead to the illumination inconsistency between enhanced images from different viewpoints. To address these issues, we propose LITA-GS, a novel illumination-agnostic novel view synthesis method via reference-free 3DGS and physical priors. Firstly, we introduce an illumination-invariant physical prior extraction pipeline. Secondly, based on the extracted robust spatial structure prior, we develop the lighting-agnostic structure rendering strategy, which facilitates the optimization of the scene structure and object appearance. Moreover, a progressive denoising module is introduced to effectively mitigate the noise within the light-invariant representation. We adopt the unsupervised strategy for the training of LITA-GS and extensive experiments demonstrate that LITA-GS surpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster inference speed and costing reduced training time. The code is released at https://github.com/LowLevelAI/LITA-GS.",
    "arxiv_url": "http://arxiv.org/abs/2504.00219v1",
    "pdf_url": "http://arxiv.org/pdf/2504.00219v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/LowLevelAI/LITA-GS",
    "keywords": [
      "illumination",
      "lighting",
      "fast",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting",
    "authors": [
      "Advaith V. Sethuraman",
      "Max Rucker",
      "Onur Bagoren",
      "Pou-Chun Kung",
      "Nibarkavi N. B. Amutha",
      "Katherine A. Skinner"
    ],
    "abstract": "In this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal.",
    "arxiv_url": "http://arxiv.org/abs/2504.00159v2",
    "pdf_url": "http://arxiv.org/pdf/2504.00159v2",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
    "authors": [
      "Chong Bao",
      "Xiyu Zhang",
      "Zehao Yu",
      "Jiale Shi",
      "Guofeng Zhang",
      "Songyou Peng",
      "Zhaopeng Cui"
    ],
    "abstract": "Neural rendering has demonstrated remarkable success in high-quality 3D neural reconstruction and novel view synthesis with dense input views and accurate poses. However, applying it to extremely sparse, unposed views in unbounded 360{\\deg} scenes remains a challenging problem. In this paper, we propose a novel neural rendering framework to accomplish the unposed and extremely sparse-view 3D reconstruction in unbounded 360{\\deg} scenes. To resolve the spatial ambiguity inherent in unbounded scenes with sparse input views, we propose a layered Gaussian-based representation to effectively model the scene with distinct spatial layers. By employing a dense stereo reconstruction model to recover coarse geometry, we introduce a layer-specific bootstrap optimization to refine the noise and fill occluded regions in the reconstruction. Furthermore, we propose an iterative fusion of reconstruction and generation alongside an uncertainty-aware training approach to facilitate mutual conditioning and enhancement between these two processes. Comprehensive experiments show that our approach outperforms existing state-of-the-art methods in terms of rendering quality and surface reconstruction accuracy. Project page: https://zju3dv.github.io/free360/",
    "arxiv_url": "http://arxiv.org/abs/2503.24382v1",
    "pdf_url": "http://arxiv.org/pdf/2503.24382v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "face",
      "geometry",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer",
    "authors": [
      "Maxim V. Shugaev",
      "Vincent Chen",
      "Maxim Karrenbach",
      "Kyle Ashley",
      "Bridget Kennedy",
      "Naresh P. Cuntoor"
    ],
    "abstract": "This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95\\% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.24374v1",
    "pdf_url": "http://arxiv.org/pdf/2503.24374v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting",
    "authors": [
      "Shakiba Kheradmand",
      "Delio Vicini",
      "George Kopanas",
      "Dmitry Lagun",
      "Kwang Moo Yi",
      "Mark Matthews",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a popular radiance field method, with many application-specific extensions. Most variants rely on the same core algorithm: depth-sorting of Gaussian splats then rasterizing in primitive order. This ensures correct alpha compositing, but can cause rendering artifacts due to built-in approximations. Moreover, for a fixed representation, sorted rendering offers little control over render cost and visual fidelity. For example, and counter-intuitively, rendering a lower-resolution image is not necessarily faster. In this work, we address the above limitations by combining 3D Gaussian splatting with stochastic rasterization. Concretely, we leverage an unbiased Monte Carlo estimator of the volume rendering equation. This removes the need for sorting, and allows for accurate 3D blending of overlapping Gaussians. The number of Monte Carlo samples further imbues 3DGS with a way to trade off computation time and quality. We implement our method using OpenGL shaders, enabling efficient rendering on modern GPU hardware. At a reasonable visual quality, our method renders more than four times faster than sorted rasterization.",
    "arxiv_url": "http://arxiv.org/abs/2503.24366v1",
    "pdf_url": "http://arxiv.org/pdf/2503.24366v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual Acoustic Fields",
    "authors": [
      "Yuelei Li",
      "Hyunjin Kim",
      "Fangneng Zhan",
      "Ri-Zhao Qiu",
      "Mazeyu Ji",
      "Xiaojun Shan",
      "Xueyan Zou",
      "Paul Liang",
      "Hanspeter Pfister",
      "Xiaolong Wang"
    ],
    "abstract": "Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources. Our project page is at https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
    "arxiv_url": "http://arxiv.org/abs/2503.24270v2",
    "pdf_url": "http://arxiv.org/pdf/2503.24270v2",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
    "authors": [
      "Seungjun Lee",
      "Gim Hee Lee"
    ],
    "abstract": "Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io",
    "arxiv_url": "http://arxiv.org/abs/2503.24210v1",
    "pdf_url": "http://arxiv.org/pdf/2503.24210v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning 3D-Gaussian Simulators from RGB Videos",
    "authors": [
      "Mikel Zhobro",
      "Andreas Ren√© Geist",
      "Georg Martius"
    ],
    "abstract": "Learning physics simulations from video data requires maintaining spatial and temporal consistency, a challenge often addressed with strong inductive biases or ground-truth 3D information -- limiting scalability and generalization. We introduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end from multi-view RGB videos. It encodes images into a 3D Gaussian particle representation, propagates dynamics via a transformer, and renders frames using 3D Gaussian splatting. By jointly training inverse rendering with a dynamics transformer using a temporal encoding and merging layer, 3DGSimembeds physical properties into point-wise latent vectors without enforcing explicit connectivity constraints. This enables the model to capture diverse physical behaviors, from rigid to elastic and cloth-like interactions, along with realistic lighting effects that also generalize to unseen multi-body interactions and novel scene edits.",
    "arxiv_url": "http://arxiv.org/abs/2503.24009v1",
    "pdf_url": "http://arxiv.org/pdf/2503.24009v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "body",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image",
    "authors": [
      "Tianyi Gong",
      "Boyan Li",
      "Yifei Zhong",
      "Fangxin Wang"
    ],
    "abstract": "The increasing demand for augmented and virtual reality applications has highlighted the importance of crafting immersive 3D scenes from a simple single-view image. However, due to the partial priors provided by single-view input, existing methods are often limited to reconstruct low-consistency 3D scenes with narrow fields of view from single-view input. These limitations make them less capable of generalizing to reconstruct immersive scenes. To address this problem, we propose ExScene, a two-stage pipeline to reconstruct an immersive 3D scene from any given single-view image. ExScene designs a novel multimodal diffusion model to generate a high-fidelity and globally consistent panoramic image. We then develop a panoramic depth estimation approach to calculate geometric information from panorama, and we combine geometric information with high-fidelity panoramic image to train an initial 3D Gaussian Splatting (3DGS) model. Following this, we introduce a GS refinement technique with 2D stable video diffusion priors. We add camera trajectory consistency and color-geometric priors into the denoising process of diffusion to improve color and spatial consistency across image sequences. These refined sequences are then used to fine-tune the initial 3DGS model, leading to better reconstruction quality. Experimental results demonstrate that our ExScene achieves consistent and immersive scene reconstruction using only single-view input, significantly surpassing state-of-the-art baselines.",
    "arxiv_url": "http://arxiv.org/abs/2503.23881v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23881v1",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Blending Unit: An Edge GPU Plug-in for Real-Time Gaussian-Based Rendering in AR/VR",
    "authors": [
      "Zhifan Ye",
      "Yonggan Fu",
      "Jingqun Zhang",
      "Leshu Li",
      "Yongan Zhang",
      "Sixu Li",
      "Cheng Wan",
      "Chenxi Wan",
      "Chaojian Li",
      "Sreemanth Prathipati",
      "Yingyan Celine Lin"
    ],
    "abstract": "The rapidly advancing field of Augmented and Virtual Reality (AR/VR) demands real-time, photorealistic rendering on resource-constrained platforms. 3D Gaussian Splatting, delivering state-of-the-art (SOTA) performance in rendering efficiency and quality, has emerged as a promising solution across a broad spectrum of AR/VR applications. However, despite its effectiveness on high-end GPUs, it struggles on edge systems like the Jetson Orin NX Edge GPU, achieving only 7-17 FPS -- well below the over 60 FPS standard required for truly immersive AR/VR experiences. Addressing this challenge, we perform a comprehensive analysis of Gaussian-based AR/VR applications and identify the Gaussian Blending Stage, which intensively calculates each Gaussian's contribution at every pixel, as the primary bottleneck. In response, we propose a Gaussian Blending Unit (GBU), an edge GPU plug-in module for real-time rendering in AR/VR applications. Notably, our GBU can be seamlessly integrated into conventional edge GPUs and collaboratively supports a wide range of AR/VR applications. Specifically, GBU incorporates an intra-row sequential shading (IRSS) dataflow that shades each row of pixels sequentially from left to right, utilizing a two-step coordinate transformation. When directly deployed on a GPU, the proposed dataflow achieved a non-trivial 1.72x speedup on real-world static scenes, though still falls short of real-time rendering performance. Recognizing the limited compute utilization in the GPU-based implementation, GBU enhances rendering speed with a dedicated rendering engine that balances the workload across rows by aggregating computations from multiple Gaussians. Experiments across representative AR/VR applications demonstrate that our GBU provides a unified solution for on-device real-time rendering while maintaining SOTA rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.23625v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23625v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.GR",
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction",
    "authors": [
      "Jingui Ma",
      "Yang Hu",
      "Luyang Tang",
      "Jiayu Yang",
      "Yongqi Zhai",
      "Ronggang Wang"
    ],
    "abstract": "Recently, 3D Gaussian Spatting (3DGS) has gained widespread attention in Novel View Synthesis (NVS) due to the remarkable real-time rendering performance. However, the substantial cost of storage and transmission of vanilla 3DGS hinders its further application (hundreds of megabytes or even gigabytes for a single scene). Motivated by the achievements of prediction in video compression, we introduce the prediction technique into the anchor-based Gaussian representation to effectively reduce the bit rate. Specifically, we propose a spatial condition-based prediction module to utilize the grid-captured scene information for prediction, with a residual compensation strategy designed to learn the missing fine-grained information. Besides, to further compress the residual, we propose an instance-aware hyper prior, developing a structure-aware and instance-aware entropy model. Extensive experiments demonstrate the effectiveness of our prediction-based compression framework and each technical component. Even compared with SOTA compression method, our framework still achieves a bit rate savings of 24.42 percent. Code is to be released!",
    "arxiv_url": "http://arxiv.org/abs/2503.23337v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23337v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning",
    "authors": [
      "Zhenyang Liu",
      "Yikai Wang",
      "Sixiao Zheng",
      "Tongying Pan",
      "Longfei Liang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ],
    "abstract": "Open-vocabulary 3D visual grounding and reasoning aim to localize objects in a scene based on implicit language descriptions, even when they are occluded. This ability is crucial for tasks such as vision-language navigation and autonomous robotics. However, current methods struggle because they rely heavily on fine-tuning with 3D annotations and mask proposals, which limits their ability to handle diverse semantics and common knowledge required for effective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided framework that uses hierarchical 3D feature Gaussian fields for adaptive grouping based on physical scale, enabling open-vocabulary 3D grounding and reasoning. ReasonGrounder interprets implicit instructions using large vision-language models (LVLM) and localizes occluded objects through 3D Gaussian splatting. By incorporating 2D segmentation masks from the SAM and multi-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on object scale, enabling accurate localization through both explicit and implicit language understanding, even in novel, occluded views. We also contribute ReasoningGD, a new dataset containing over 10K scenes and 2 million annotations for evaluating open-vocabulary 3D grounding and amodal perception under occlusion. Experiments show that ReasonGrounder significantly improves 3D grounding accuracy in real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2503.23297v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23297v1",
    "published_date": "2025-03-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations",
    "authors": [
      "Zhenyu Tang",
      "Chaoran Feng",
      "Xinhua Cheng",
      "Wangbo Yu",
      "Junwu Zhang",
      "Yuan Liu",
      "Xiaoxiao Long",
      "Wenping Wang",
      "Li Yuan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.",
    "arxiv_url": "http://arxiv.org/abs/2503.23162v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23162v1",
    "published_date": "2025-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction",
    "authors": [
      "Yuanyuan Gao",
      "Hao Li",
      "Jiaqi Chen",
      "Zhengyu Zou",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun",
      "Junwei Han"
    ],
    "abstract": "Despite its significant achievements in large-scale scene reconstruction, 3D Gaussian Splatting still faces substantial challenges, including slow processing, high computational costs, and limited geometric accuracy. These core issues arise from its inherently unstructured design and the absence of efficient parallelization. To overcome these challenges simultaneously, we introduce CityGS-X, a scalable architecture built on a novel parallelized hybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X abandons the cumbersome merge-and-partition process and instead adopts a newly-designed batch-level multi-task rendering process. This architecture enables efficient multi-GPU rendering through dynamic Level-of-Detail voxel allocations, significantly improving scalability and performance. Through extensive experiments, CityGS-X consistently outperforms existing methods in terms of faster training times, larger rendering capacities, and more accurate geometric details in large-scale scenes. Notably, CityGS-X can train and render a scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task that would make other alternative methods encounter Out-Of-Memory (OOM) issues and fail completely. This implies that CityGS-X is far beyond the capacity of other existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.23044v1",
    "pdf_url": "http://arxiv.org/pdf/2503.23044v1",
    "published_date": "2025-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction",
    "authors": [
      "Yunsong Wang",
      "Tianxin Huang",
      "Hanlin Chen",
      "Gim Hee Lee"
    ],
    "abstract": "Recently, the integration of the efficient feed-forward scheme into 3D Gaussian Splatting (3DGS) has been actively explored. However, most existing methods focus on sparse view reconstruction of small regions and cannot produce eligible whole-scene reconstruction results in terms of either quality or efficiency. In this paper, we propose FreeSplat++, which focuses on extending the generalizable 3DGS to become an alternative approach to large-scale indoor whole-scene reconstruction, which has the potential of significantly accelerating the reconstruction speed and improving the geometric accuracy. To facilitate whole-scene reconstruction, we initially propose the Low-cost Cross-View Aggregation framework to efficiently process extremely long input sequences. Subsequently, we introduce a carefully designed pixel-wise triplet fusion method to incrementally aggregate the overlapping 3D Gaussian primitives from multiple views, adaptively reducing their redundancy. Furthermore, we propose a weighted floater removal strategy that can effectively reduce floaters, which serves as an explicit depth fusion approach that is crucial in whole-scene reconstruction. After the feed-forward reconstruction of 3DGS primitives, we investigate a depth-regularized per-scene fine-tuning process. Leveraging the dense, multi-view consistent depth maps obtained during the feed-forward prediction phase for an extra constraint, we refine the entire scene's 3DGS primitive to enhance rendering quality while preserving geometric accuracy. Extensive experiments confirm that our FreeSplat++ significantly outperforms existing generalizable 3DGS methods, especially in whole-scene reconstructions. Compared to conventional per-scene optimized 3DGS approaches, our method with depth-regularized per-scene fine-tuning demonstrates substantial improvements in reconstruction accuracy and a notable reduction in training time.",
    "arxiv_url": "http://arxiv.org/abs/2503.22986v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22986v1",
    "published_date": "2025-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial Robots",
    "authors": [
      "Kushagra Srivastava",
      "Rutwik Kulkarni",
      "Manoj Velmurugan",
      "Nitin J. Sanket"
    ],
    "abstract": "Autonomous aerial robots are becoming commonplace in our lives. Hands-on aerial robotics courses are pivotal in training the next-generation workforce to meet the growing market demands. Such an efficient and compelling course depends on a reliable testbed. In this paper, we present VizFlyt, an open-source perception-centric Hardware-In-The-Loop (HITL) photorealistic testing framework for aerial robotics courses. We utilize pose from an external localization system to hallucinate real-time and photorealistic visual sensors using 3D Gaussian Splatting. This enables stress-free testing of autonomy algorithms on aerial robots without the risk of crashing into obstacles. We achieve over 100Hz of system update rate. Lastly, we build upon our past experiences of offering hands-on aerial robotics courses and propose a new open-source and open-hardware curriculum based on VizFlyt for the future. We test our framework on various course projects in real-world HITL experiments and present the results showing the efficacy of such a system and its large potential use cases. Code, datasets, hardware guides and demo videos are available at https://pear.wpi.edu/research/vizflyt.html",
    "arxiv_url": "http://arxiv.org/abs/2503.22876v2",
    "pdf_url": "http://arxiv.org/pdf/2503.22876v2",
    "published_date": "2025-03-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "localization",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting",
    "authors": [
      "Tony Yu",
      "Yanlin Jin",
      "Ashok Veeraraghavan",
      "Akshat Dave",
      "Guha Balakrishnan"
    ],
    "abstract": "We present TranSplat, a 3D scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the Gaussian Splatting framework. Our approach addresses two critical challenges: (1) precise 3D object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. TranSplat fits a splatting model to the source scene, using 2D object masks to drive fine-grained 3D segmentation. Following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, TranSplat derives per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. This relighting strategy does not require explicitly estimating physical scene properties such as BRDFs. Evaluated on several synthetic and real-world scenes and objects, TranSplat yields excellent 3D object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. We conclude by discussing the limitations of the approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.22676v2",
    "pdf_url": "http://arxiv.org/pdf/2503.22676v2",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time Talking Head Synthesis",
    "authors": [
      "Shuai Shen",
      "Wanhua Li",
      "Yunpeng Zhang",
      "Weipeng Hu",
      "Yap-Peng Tan"
    ],
    "abstract": "Talking head synthesis has become a key research area in computer graphics and multimedia, yet most existing methods often struggle to balance generation quality with computational efficiency. In this paper, we present a novel approach that leverages an Audio Factorization Plane (Audio-Plane) based Gaussian Splatting for high-quality and real-time talking head generation. For modeling a dynamic talking head, 4D volume representation is needed. However, directly storing a dense 4D grid is impractical due to the high cost and lack of scalability for longer durations. We overcome this challenge with the proposed Audio-Plane, where the 4D volume representation is decomposed into audio-independent space planes and audio-dependent planes. This provides a compact and interpretable feature representation for talking head, facilitating more precise audio-aware spatial encoding and enhanced audio-driven lip dynamic modeling. To further improve speech dynamics, we develop a dynamic splatting method that helps the network more effectively focus on modeling the dynamics of the mouth region. Extensive experiments demonstrate that by integrating these innovations with the powerful Gaussian Splatting, our method is capable of synthesizing highly realistic talking videos in real time while ensuring precise audio-lip synchronization. Synthesized results are available in https://sstzal.github.io/Audio-Plane/.",
    "arxiv_url": "http://arxiv.org/abs/2503.22605v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22605v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "4d",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting",
    "authors": [
      "Xu Wang",
      "Shuai Zhang",
      "Baoru Huang",
      "Danail Stoyanov",
      "Evangelos B. Mazomenos"
    ],
    "abstract": "Complete reconstruction of surgical scenes is crucial for robot-assisted surgery (RAS). Deep depth estimation is promising but existing works struggle with depth discontinuities, resulting in noisy predictions at object boundaries and do not achieve complete reconstruction omitting occluded surfaces. To address these issues we propose EndoLRMGS, that combines Large Reconstruction Modelling (LRM) and Gaussian Splatting (GS), for complete surgical scene reconstruction. GS reconstructs deformable tissues and LRM generates 3D models for surgical tools while position and scale are subsequently optimized by introducing orthogonal perspective joint projection optimization (OPjPO) to enhance accuracy. In experiments on four surgical videos from three public datasets, our method improves the Intersection-over-union (IoU) of tool 3D models in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR of the tools projection from 3.82% to 11.07%. Tissue rendering quality also improves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to 29.21% across all test videos.",
    "arxiv_url": "http://arxiv.org/abs/2503.22437v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22437v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation",
    "authors": [
      "Chenyang Xu",
      "XingGuo Deng",
      "Rui Zhong"
    ],
    "abstract": "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation and view synthesis. Although Scaffold-GS achieves higher quality real-time rendering compared to the original 3D-GS, its fine-grained rendering of the scene is extremely dependent on adequate viewing angles. The spectral bias of neural network learning results in Scaffold-GS's poor ability to perceive and learn high-frequency information in the scene. In this work, we propose enhancing the manifold complexity of input features and using network-based feature map loss to improve the image reconstruction quality of 3D-GS models. We introduce AH-GS, which enables 3D Gaussians in structurally complex regions to obtain higher-frequency encodings, allowing the model to more effectively learn the high-frequency information of the scene. Additionally, we incorporate high-frequency reinforce loss to further enhance the model's ability to capture detailed frequency information. Our result demonstrates that our model significantly improves rendering fidelity, and in specific scenarios (e.g., MipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in just 15K iterations.",
    "arxiv_url": "http://arxiv.org/abs/2503.22324v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22324v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance",
    "authors": [
      "Haijie Yang",
      "Zhenyu Zhang",
      "Hao Tang",
      "Jianjun Qian",
      "Jian Yang"
    ],
    "abstract": "Pre-trained conditional diffusion models have demonstrated remarkable potential in image editing. However, they often face challenges with temporal consistency, particularly in the talking head domain, where continuous changes in facial expressions intensify the level of difficulty. These issues stem from the independent editing of individual images and the inherent loss of temporal continuity during the editing process. In this paper, we introduce Follow Your Motion (FYM), a generic framework for maintaining temporal consistency in portrait editing. Specifically, given portrait images rendered by a pre-trained 3D Gaussian Splatting model, we first develop a diffusion model that intuitively and inherently learns motion trajectory changes at different scales and pixel coordinates, from the first frame to each subsequent frame. This approach ensures that temporally inconsistent edited avatars inherit the motion information from the rendered avatars. Secondly, to maintain fine-grained expression temporal consistency in talking head editing, we propose a dynamic re-weighted attention mechanism. This mechanism assigns higher weight coefficients to landmark points in space and dynamically updates these weights based on landmark loss, achieving more consistent and refined facial expressions. Extensive experiments demonstrate that our method outperforms existing approaches in terms of temporal consistency and can be used to optimize and compensate for temporally inconsistent outputs in a range of applications, such as text-driven editing, relighting, and various other applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.22225v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22225v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "relighting",
      "lighting",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting",
    "authors": [
      "Wenjie Liu",
      "Zhongliang Liu",
      "Xiaoyan Yang",
      "Man Sha",
      "Yang Li"
    ],
    "abstract": "3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.",
    "arxiv_url": "http://arxiv.org/abs/2503.22218v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22218v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting",
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Yiran Qiao",
      "Chaoda Song",
      "Tuo Liang",
      "Jing Ma",
      "Yu Yin"
    ],
    "abstract": "Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentation approach for both static and dynamic scenes based on Gaussian Splatting. Segment then Splat reverses the long established approach of \"segmentation after reconstruction\" by dividing Gaussians into distinct object sets before reconstruction. Once the reconstruction is complete, the scene is naturally segmented into individual objects, achieving true 3D segmentation. This approach not only eliminates Gaussian-object misalignment issues in dynamic scenes but also accelerates the optimization process, as it eliminates the need for learning a separate language field. After optimization, a CLIP embedding is assigned to each object to enable open-vocabulary querying. Extensive experiments on various datasets demonstrate the effectiveness of our proposed method in both static and dynamic scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2503.22204v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22204v1",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient Dynamic Scene Rendering",
    "authors": [
      "Hao Feng",
      "Hao Sun",
      "Wei Xie"
    ],
    "abstract": "Novel-view synthesis (NVS) for dynamic scenes from 2D images presents significant challenges due to the spatial complexity and temporal variability of such scenes. Recently, inspired by the remarkable success of NVS using 3D Gaussian Splatting (3DGS), researchers have sought to extend 3D Gaussian models to four dimensions (4D) for dynamic novel-view synthesis. However, methods based on 4D rotation and scaling introduce spatiotemporal deformation into the 4D covariance matrix, necessitating the slicing of 4D Gaussians into 3D Gaussians. This process increases redundant computations as timestamps change-an inherent characteristic of dynamic scene rendering. Additionally, performing calculations on a four-dimensional matrix is computationally intensive. In this paper, we introduce Disentangled 4D Gaussian Splatting (Disentangled4DGS), a novel representation and rendering approach that disentangles temporal and spatial deformations, thereby eliminating the reliance on 4D matrix computations. We extend the 3DGS rendering process to 4D, enabling the projection of temporal and spatial deformations into dynamic 2D Gaussians in ray space. Consequently, our method facilitates faster dynamic scene synthesis. Moreover, it reduces storage requirements by at least 4.5\\% due to our efficient presentation method. Our approach achieves an unprecedented average rendering speed of 343 FPS at a resolution of $1352\\times1014$ on an RTX 3090 GPU, with experiments across multiple benchmarks demonstrating its competitive performance in both monocular and multi-view scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2503.22159v2",
    "pdf_url": "http://arxiv.org/pdf/2503.22159v2",
    "published_date": "2025-03-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "deformation",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Time-resolved dynamic CBCT reconstruction using prior-model-free spatiotemporal Gaussian representation (PMF-STGR)",
    "authors": [
      "Jiacheng Xie",
      "Hua-Chieh Shao",
      "You Zhang"
    ],
    "abstract": "Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a 'one-shot' training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation.",
    "arxiv_url": "http://arxiv.org/abs/2503.22139v1",
    "pdf_url": "http://arxiv.org/pdf/2503.22139v1",
    "published_date": "2025-03-28",
    "categories": [
      "physics.med-ph",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "motion",
      "fast",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction",
    "authors": [
      "Weihao Yu",
      "Yuanhao Cai",
      "Ruyi Zha",
      "Zhiwen Fan",
      "Chenxin Li",
      "Yixuan Yuan"
    ],
    "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.21779v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21779v1",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "face",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying",
    "authors": [
      "Hairong Yin",
      "Huangying Zhan",
      "Yi Xu",
      "Raymond A. Yeh"
    ],
    "abstract": "Open-vocabulary querying in 3D Gaussian Splatting aims to identify semantically relevant regions within a 3D Gaussian representation based on a given text query. Prior work, such as LangSplat, addressed this task by retrieving these regions in the form of segmentation masks on 2D renderings. More recently, OpenGaussian introduced point-level querying, which directly selects a subset of 3D Gaussians. In this work, we propose a point-level querying method that builds upon LangSplat's framework. Our approach improves the framework in two key ways: (a) we leverage masklets from the Segment Anything Model 2 (SAM2) to establish semantic consistent ground-truth for distilling the language Gaussians; (b) we introduces a novel two-step querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians. Experimental evaluations on three benchmark datasets demonstrate that the proposed method achieves better performance compared to state-of-the-art approaches. For instance, our method achieves an mIoU improvement of +20.42 on the 3D-OVS dataset.",
    "arxiv_url": "http://arxiv.org/abs/2503.21767v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21767v1",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting",
    "authors": [
      "Qiyu Dai",
      "Xingyu Ni",
      "Qianfan Shen",
      "Wenzheng Chen",
      "Baoquan Chen",
      "Mengyu Chu"
    ],
    "abstract": "We consider the problem of adding dynamic rain effects to in-the-wild scenes in a physically-correct manner. Recent advances in scene modeling have made significant progress, with NeRF and 3DGS techniques emerging as powerful tools for reconstructing complex scenes. However, while effective for novel view synthesis, these methods typically struggle with challenging scene editing tasks, such as physics-based rain simulation. In contrast, traditional physics-based simulations can generate realistic rain effects, such as raindrops and splashes, but they often rely on skilled artists to carefully set up high-fidelity scenes. This process lacks flexibility and scalability, limiting its applicability to broader, open-world environments. In this work, we introduce RainyGS, a novel approach that leverages the strengths of both physics-based modeling and 3DGS to generate photorealistic, dynamic rain effects in open-world scenes with physical accuracy. At the core of our method is the integration of physically-based raindrop and shallow water simulation techniques within the fast 3DGS rendering framework, enabling realistic and efficient simulations of raindrop behavior, splashes, and reflections. Our method supports synthesizing rain effects at over 30 fps, offering users flexible control over rain intensity -- from light drizzles to heavy downpours. We demonstrate that RainyGS performs effectively for both real-world outdoor scenes and large-scale driving scenarios, delivering more photorealistic and physically-accurate rain effects compared to state-of-the-art methods. Project page can be found at https://pku-vcl-geometry.github.io/RainyGS/",
    "arxiv_url": "http://arxiv.org/abs/2503.21442v2",
    "pdf_url": "http://arxiv.org/pdf/2503.21442v2",
    "published_date": "2025-03-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "reflection",
      "outdoor",
      "fast",
      "geometry",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STAMICS: Splat, Track And Map with Integrated Consistency and Semantics for Dense RGB-D SLAM",
    "authors": [
      "Yongxu Wang",
      "Xu Cao",
      "Weiyun Yi",
      "Zhaoxin Fan"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a critical task in robotics, enabling systems to autonomously navigate and understand complex environments. Current SLAM approaches predominantly rely on geometric cues for mapping and localization, but they often fail to ensure semantic consistency, particularly in dynamic or densely populated scenes. To address this limitation, we introduce STAMICS, a novel method that integrates semantic information with 3D Gaussian representations to enhance both localization and mapping accuracy. STAMICS consists of three key components: a 3D Gaussian-based scene representation for high-fidelity reconstruction, a graph-based clustering technique that enforces temporal semantic consistency, and an open-vocabulary system that allows for the classification of unseen objects. Extensive experiments show that STAMICS significantly improves camera pose estimation and map quality, outperforming state-of-the-art methods while reducing reconstruction errors. Code will be public available.",
    "arxiv_url": "http://arxiv.org/abs/2503.21425v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21425v1",
    "published_date": "2025-03-27",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "high-fidelity",
      "semantic",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LandMarkSystem Technical Report",
    "authors": [
      "Zhenxiang Ma",
      "Zhenyu Yang",
      "Miao Tao",
      "Yuanzhen Zhou",
      "Zeyu He",
      "Yuchang Zhang",
      "Rong Fu",
      "Hengjie Li"
    ],
    "abstract": "3D reconstruction is vital for applications in autonomous driving, virtual reality, augmented reality, and the metaverse. Recent advancements such as Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed the field, yet traditional deep learning frameworks struggle to meet the increasing demands for scene quality and scale. This paper introduces LandMarkSystem, a novel computing framework designed to enhance multi-scale scene reconstruction and rendering. By leveraging a componentized model adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures while optimizing computational efficiency through distributed parallel computing and model parameter offloading. Our system addresses the limitations of existing frameworks, providing dedicated operators for complex 3D sparse computations, thus facilitating efficient training and rapid inference over extensive scenes. Key contributions include a modular architecture, a dynamic loading strategy for limited resources, and proven capabilities across multiple representative algorithms.This comprehensive solution aims to advance the efficiency and effectiveness of 3D reconstruction tasks.To facilitate further research and collaboration, the source code and documentation for the LandMarkSystem project are publicly available in an open-source repository, accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.",
    "arxiv_url": "http://arxiv.org/abs/2503.21364v2",
    "pdf_url": "http://arxiv.org/pdf/2503.21364v2",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/InternLandMark/LandMarkSystem",
    "keywords": [
      "efficient",
      "autonomous driving",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-Aware Gaussian Splatting Decomposition",
    "authors": [
      "Yishai Lavi",
      "Leo Segre",
      "Shai Avidan"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis with its efficient, explicit representation. However, it lacks frequency interpretability, making it difficult to separate low-frequency structures from fine details. We introduce a frequency-decomposed 3D-GS framework that groups 3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the input images. Our approach enforces coherence within each subband (i.e., group of 3D Gaussians) through dedicated regularization, ensuring well-separated frequency components. We extend color values to both positive and negative ranges, allowing higher-frequency layers to add or subtract residual details. To stabilize optimization, we employ a progressive training scheme that refines details in a coarse-to-fine manner. Beyond interpretability, this frequency-aware design unlocks a range of practical benefits. Explicit frequency separation enables advanced 3D editing and stylization, allowing precise manipulation of specific frequency bands. It also supports dynamic level-of-detail control for progressive rendering, streaming, foveated rendering and fast geometry interaction. Through extensive experiments, we demonstrate that our method provides improved control and flexibility for emerging applications in scene editing and interactive rendering. Our code will be made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2503.21226v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21226v1",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency",
    "authors": [
      "Yuyin Chen",
      "Yida Wang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Peng Jia",
      "Yifei Zhan",
      "Xianpeng Lang"
    ],
    "abstract": "Urban scene reconstruction requires modeling both static infrastructure and dynamic elements while supporting diverse environmental conditions. We present \\textbf{StyledStreets}, a multi-style street simulator that achieves instruction-driven scene editing with guaranteed spatial and temporal consistency. Building on a state-of-the-art Gaussian Splatting framework for street scenarios enhanced by our proposed pose optimization and multi-view training, our method enables photorealistic style transfers across seasons, weather conditions, and camera setups through three key innovations: First, a hybrid embedding scheme disentangles persistent scene geometry from transient style attributes, allowing realistic environmental edits while preserving structural integrity. Second, uncertainty-aware rendering mitigates supervision noise from diffusion priors, enabling robust training across extreme style variations. Third, a unified parametric model prevents geometric drift through regularized updates, maintaining multi-view consistency across seven vehicle-mounted cameras.   Our framework preserves the original scene's motion patterns and geometric relationships. Qualitative results demonstrate plausible transitions between diverse conditions (snow, sandstorm, night), while quantitative evaluations show state-of-the-art geometric accuracy under style transfers. The approach establishes new capabilities for urban simulation, with applications in autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Codes will be publicly available upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2503.21104v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21104v1",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "geometry",
      "ar",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGC: Physics-Based Gaussian Cloth from a Single Pose",
    "authors": [
      "Michelle Guo",
      "Matt Jen-Yuan Chiang",
      "Igor Santesteban",
      "Nikolaos Sarafianos",
      "Hsiao-yu Chen",
      "Oshri Halimi",
      "Alja≈æ Bo≈æiƒç",
      "Shunsuke Saito",
      "Jiajun Wu",
      "C. Karen Liu",
      "Tuur Stuyck",
      "Egor Larionov"
    ],
    "abstract": "We introduce a novel approach to reconstruct simulation-ready garments with intricate appearance. Despite recent advancements, existing methods often struggle to balance the need for accurate garment reconstruction with the ability to generalize to new poses and body shapes or require large amounts of data to achieve this. In contrast, our method only requires a multi-view capture of a single static frame. We represent garments as hybrid mesh-embedded 3D Gaussian splats, where the Gaussians capture near-field shading and high-frequency details, while the mesh encodes far-field albedo and optimized reflectance parameters. We achieve novel pose generalization by exploiting the mesh from our hybrid approach, enabling physics-based simulation and surface rendering techniques, while also capturing fine details with Gaussians that accurately reconstruct garment details. Our optimized garments can be used for simulating garments on novel poses, and garment relighting. Project page: https://phys-gaussian-cloth.github.io .",
    "arxiv_url": "http://arxiv.org/abs/2503.20779v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20779v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.GR",
      "I.3.6; I.3.7"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "face",
      "body",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields",
    "authors": [
      "Shijie Zhou",
      "Hui Ren",
      "Yijia Weng",
      "Shuwang Zhang",
      "Zhen Wang",
      "Dejia Xu",
      "Zhiwen Fan",
      "Suya You",
      "Zhangyang Wang",
      "Leonidas Guibas",
      "Achuta Kadambi"
    ],
    "abstract": "Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The \"X\" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.",
    "arxiv_url": "http://arxiv.org/abs/2503.20776v2",
    "pdf_url": "http://arxiv.org/pdf/2503.20776v2",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "4d",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: Tri-plane based compression for 3D Gaussian Splatting",
    "authors": [
      "Taorui Wang",
      "Zitong Yu",
      "Yong Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a prominent framework for novel view synthesis, providing high fidelity and rapid rendering speed. However, the substantial data volume of 3DGS and its attributes impede its practical utility, requiring compression techniques for reducing memory cost. Nevertheless, the unorganized shape of 3DGS leads to difficulties in compression. To formulate unstructured attributes into normative distribution, we propose a well-structured tri-plane to encode Gaussian attributes, leveraging the distribution of attributes for compression. To exploit the correlations among adjacent Gaussians, K-Nearest Neighbors (KNN) is used when decoding Gaussian distribution from the Tri-plane. We also introduce Gaussian position information as a prior of the position-sensitive decoder. Additionally, we incorporate an adaptive wavelet loss, aiming to focus on the high-frequency details as iterations increase. Our approach has achieved results that are comparable to or surpass that of SOTA 3D Gaussians Splatting compression work in extensive experiments across multiple datasets. The codes are released at https://github.com/timwang2001/TC-GS.",
    "arxiv_url": "http://arxiv.org/abs/2503.20221v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20221v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/timwang2001/TC-GS",
    "keywords": [
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis",
    "authors": [
      "Sheng Miao",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Xu Yan",
      "Hongyu Zhou",
      "Yue Wang",
      "Bingbing Liu",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "Novel view synthesis of urban scenes is essential for autonomous driving-related applications.Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixel-aligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feed-forward 3DGS- and NeRF-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.20168v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20168v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis",
    "authors": [
      "Jiahe Li",
      "Feiyu Wang",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Ting Liu"
    ],
    "abstract": "Gaussian Splatting (GS)-based methods rely on sufficient training view coverage and perform synthesis on interpolated views. In this work, we tackle the more challenging and underexplored Extrapolated View Synthesis (EVS) task. Here we enable GS-based models trained with limited view coverage to generalize well to extrapolated views. To achieve our goal, we propose a view augmentation framework to guide training through a coarse-to-fine process. At the coarse stage, we reduce rendering artifacts due to insufficient view coverage by introducing a regularization strategy at both appearance and geometry levels. At the fine stage, we generate reliable view priors to provide further training guidance. To this end, we incorporate an occlusion awareness into the view prior generation process, and refine the view priors with the aid of coarse stage output. We call our framework Enhanced View Prior Guidance for Splatting (EVPGS). To comprehensively evaluate EVPGS on the EVS task, we collect a real-world dataset called Merchandise3D dedicated to the EVS scenario. Experiments on three datasets including both real and synthetic demonstrate EVPGS achieves state-of-the-art performance, while improving synthesis quality at extrapolated views for GS-based methods both qualitatively and quantitatively. We will make our code, dataset, and models public.",
    "arxiv_url": "http://arxiv.org/abs/2503.21816v1",
    "pdf_url": "http://arxiv.org/pdf/2503.21816v1",
    "published_date": "2025-03-26",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields",
    "authors": [
      "Navami Kairanda",
      "Marc Habermann",
      "Shanthika Naik",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ],
    "abstract": "3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular RGB videos is a challenging problem, and no solution provides a consistent and accurate recovery of fine-grained surface details. To account for the ill-posed nature of the setting, existing methods use deformation models with statistical, neural, or physical priors. They also predominantly rely on nonadaptive discrete surface representations (e.g. polygonal meshes), perform frame-by-frame optimisation leading to error propagation, and suffer from poor gradients of the mesh-based differentiable renderers. Consequently, fine surface details such as cloth wrinkles are often not recovered with the desired accuracy. In response to these limitations, we propose ThinShell-SfT, a new method for non-rigid 3D tracking that represents a surface as an implicit and continuous spatiotemporal neural field. We incorporate continuous thin shell physics prior based on the Kirchhoff-Love model for spatial regularisation, which starkly contrasts the discretised alternatives of earlier works. Lastly, we leverage 3D Gaussian splatting to differentiably render the surface into image space and optimise the deformations based on analysis-bysynthesis principles. Our Thin-Shell-SfT outperforms prior works qualitatively and quantitatively thanks to our continuous surface formulation in conjunction with a specially tailored simulation prior and surface-induced 3D Gaussians. See our project page at https://4dqv.mpiinf.mpg.de/ThinShellSfT.",
    "arxiv_url": "http://arxiv.org/abs/2503.19976v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19976v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "face",
      "deformation",
      "4d",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model",
    "authors": [
      "Mingju Gao",
      "Yike Pan",
      "Huan-ang Gao",
      "Zongzheng Zhang",
      "Wenyi Li",
      "Hao Dong",
      "Hao Tang",
      "Li Yi",
      "Hao Zhao"
    ],
    "abstract": "As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the model's understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research.",
    "arxiv_url": "http://arxiv.org/abs/2503.19913v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19913v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "understanding",
      "geometry",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Haodong Chen",
      "Vera Chung",
      "Qiang Qu"
    ],
    "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to their high temporal resolution, low latency, and high dynamic range. They capture per-pixel brightness changes asynchronously, allowing accurate reconstruction under fast motion and challenging lighting conditions. In this survey, we provide a comprehensive review of event-driven 3D reconstruction methods, including stereo, monocular, and multimodal systems. We further categorize recent developments based on geometric, learning-based, and hybrid approaches. Emerging trends, such as neural radiance fields and 3D Gaussian splatting with event data, are also covered. The related works are structured chronologically to illustrate the innovations and progression within the field. To support future research, we also highlight key research gaps and future research directions in dataset, experiment, evaluation, event representation, etc.",
    "arxiv_url": "http://arxiv.org/abs/2503.19753v3",
    "pdf_url": "http://arxiv.org/pdf/2503.19753v3",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "lighting",
      "fast",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting",
    "authors": [
      "Qian Wang",
      "Zhihao Zhan",
      "Jialei He",
      "Zhituo Tu",
      "Xiang Zhu",
      "Jie Yuan"
    ],
    "abstract": "Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring.Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors.This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.",
    "arxiv_url": "http://arxiv.org/abs/2503.19703v2",
    "pdf_url": "http://arxiv.org/pdf/2503.19703v2",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis",
    "authors": [
      "Youngkyoon Jang",
      "Eduardo P√©rez-Pellitero"
    ],
    "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision using a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with varying sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF.",
    "arxiv_url": "http://arxiv.org/abs/2503.20998v1",
    "pdf_url": "http://arxiv.org/pdf/2503.20998v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "few-shot",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting",
    "authors": [
      "Shujuan Li",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Reconstructing open surfaces from multi-view images is vital in digitalizing complex objects in daily life. A widely used strategy is to learn unsigned distance functions (UDFs) by checking if their appearance conforms to the image observations through neural rendering. However, it is still hard to learn continuous and implicit UDF representations through 3D Gaussians splatting (3DGS) due to the discrete and explicit scene representation, i.e., 3D Gaussians. To resolve this issue, we propose a novel approach to bridge the gap between 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D Gaussian planes on surfaces, and then, leverage the self-supervision and gradient-based inference to supervise unsigned distances in both near and far area to surfaces. To this end, we introduce novel constraints and strategies to constrain the learning of 2D Gaussians to pursue more stable optimization and more reliable self-supervision, addressing the challenges brought by complicated gradient field on or near the zero level set of UDFs. We report numerical and visual comparisons with the state-of-the-art on widely used benchmarks and real data to show our advantages in terms of accuracy, efficiency, completeness, and sharpness of reconstructed open surfaces with boundaries.",
    "arxiv_url": "http://arxiv.org/abs/2503.19458v2",
    "pdf_url": "http://arxiv.org/pdf/2503.19458v2",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with Generative Priors",
    "authors": [
      "Yiqing Li",
      "Xuan Wang",
      "Jiawei Wu",
      "Yikun Ma",
      "Zhi Jin"
    ],
    "abstract": "Synthesizing novel views of large-scale scenes from unconstrained in-the-wild images is an important but challenging task in computer vision. Existing methods, which optimize per-image appearance and transient occlusion through implicit neural networks from dense training views (approximately 1000 images), struggle to perform effectively under sparse input conditions, resulting in noticeable artifacts. To this end, we propose SparseGS-W, a novel framework based on 3D Gaussian Splatting that enables the reconstruction of complex outdoor scenes and handles occlusions and appearance changes with as few as five training images. We leverage geometric priors and constrained diffusion priors to compensate for the lack of multi-view information from extremely sparse input. Specifically, we propose a plug-and-play Constrained Novel-View Enhancement module to iteratively improve the quality of rendered novel views during the Gaussian optimization process. Furthermore, we propose an Occlusion Handling module, which flexibly removes occlusions utilizing the inherent high-quality inpainting capability of constrained diffusion priors. Both modules are capable of extracting appearance features from any user-provided reference image, enabling flexible modeling of illumination-consistent scenes. Extensive experiments on the PhotoTourism and Tanks and Temples datasets demonstrate that SparseGS-W achieves state-of-the-art performance not only in full-reference metrics, but also in commonly used non-reference metrics such as FID, ClipIQA, and MUSIQ.",
    "arxiv_url": "http://arxiv.org/abs/2503.19452v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19452v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "outdoor",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting",
    "authors": [
      "Jiaxin Zhang",
      "Junjun Jiang",
      "Youyu Chen",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "abstract": "Accurate object segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D segmentation based on 3D Gaussian Splatting (3DGS) struggles with accurately delineating object boundaries, as Gaussian primitives often span across object edges due to their inherent volume and the lack of semantic guidance during training. In order to tackle these challenges, we introduce Clear Object Boundaries for 3DGS Segmentation (COB-GS), which aims to improve segmentation accuracy by clearly delineating blurry boundaries of interwoven Gaussian primitives within the scene. Unlike existing approaches that remove ambiguous Gaussians and sacrifice visual quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and visual information, allowing the two different levels to cooperate with each other effectively. Specifically, for the semantic guidance, we introduce a boundary-adaptive Gaussian splitting technique that leverages semantic gradient statistics to identify and split ambiguous Gaussians, aligning them closely with object boundaries. For the visual optimization, we rectify the degraded suboptimal texture of the 3DGS scene, particularly along the refined boundary structures. Experimental results show that COB-GS substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained model, yielding clear boundaries while preserving high visual quality. Code is available at https://github.com/ZestfulJX/COB-GS.",
    "arxiv_url": "http://arxiv.org/abs/2503.19443v2",
    "pdf_url": "http://arxiv.org/pdf/2503.19443v2",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ZestfulJX/COB-GS",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting",
    "authors": [
      "Zhiwei Huang",
      "Hailin Yu",
      "Yichun Shentu",
      "Jin Yuan",
      "Guofeng Zhang"
    ],
    "abstract": "This paper presents a novel camera relocalization method, STDLoc, which leverages Feature Gaussian as scene representation. STDLoc is a full relocalization pipeline that can achieve accurate relocalization without relying on any pose prior. Unlike previous coarse-to-fine localization methods that require image retrieval first and then feature matching, we propose a novel sparse-to-dense localization paradigm. Based on this scene representation, we introduce a novel matching-oriented Gaussian sampling strategy and a scene-specific detector to achieve efficient and robust initial pose estimation. Furthermore, based on the initial localization results, we align the query feature map to the Gaussian feature field by dense feature matching to enable accurate localization. The experiments on indoor and outdoor datasets show that STDLoc outperforms current state-of-the-art localization methods in terms of localization accuracy and recall.",
    "arxiv_url": "http://arxiv.org/abs/2503.19358v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19358v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "outdoor",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D Gaussian Spatting",
    "authors": [
      "Zhiying Yan",
      "Yiyuan Liang",
      "Shilv Cai",
      "Tao Zhang",
      "Sheng Zhong",
      "Luxin Yan",
      "Xu Zou"
    ],
    "abstract": "Semantic 4D Gaussians can be used for reconstructing and understanding dynamic scenes, with temporal variations than static scenes. Directly applying static methods to understand dynamic scenes will fail to capture the temporal features. Few works focus on dynamic scene understanding based on Gaussian Splatting, since once the same update strategy is employed for both dynamic and static parts, regardless of the distinction and interaction between Gaussians, significant artifacts and noise appear. We propose Dual-Hierarchical Optimization (DHO), which consists of Hierarchical Gaussian Flow and Hierarchical Gaussian Guidance in a divide-and-conquer manner. The former implements effective division of static and dynamic rendering and features. The latter helps to mitigate the issue of dynamic foreground rendering distortion in textured complex scenes. Extensive experiments show that our method consistently outperforms the baselines on both synthetic and real-world datasets, and supports various downstream tasks. Project Page: https://sweety-yan.github.io/DHO.",
    "arxiv_url": "http://arxiv.org/abs/2503.19332v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19332v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MATT-GS: Masked Attention-based 3DGS for Robot Perception and Object Detection",
    "authors": [
      "Jee Won Lee",
      "Hansol Lim",
      "SooYeun Yang",
      "Jongseong Brad Choi"
    ],
    "abstract": "This paper presents a novel masked attention-based 3D Gaussian Splatting (3DGS) approach to enhance robotic perception and object detection in industrial and smart factory environments. U2-Net is employed for background removal to isolate target objects from raw images, thereby minimizing clutter and ensuring that the model processes only relevant data. Additionally, a Sobel filter-based attention mechanism is integrated into the 3DGS framework to enhance fine details - capturing critical features such as screws, wires, and intricate textures essential for high-precision tasks. We validate our approach using quantitative metrics, including L1 loss, SSIM, PSNR, comparing the performance of the background-removed and attention-incorporated 3DGS model against the ground truth images and the original 3DGS training baseline. The results demonstrate significant improves in visual fidelity and detail preservation, highlighting the effectiveness of our method in enhancing robotic vision for object recognition and manipulation in complex industrial settings.",
    "arxiv_url": "http://arxiv.org/abs/2503.19330v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19330v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "lighting",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting",
    "authors": [
      "Xinpeng Liu",
      "Zeyi Huang",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ],
    "abstract": "Novel view synthesis has demonstrated impressive progress recently, with 3D Gaussian splatting (3DGS) offering efficient training time and photorealistic real-time rendering. However, reliance on Cartesian coordinates limits 3DGS's performance on distant objects, which is important for reconstructing unbounded outdoor environments. We found that, despite its ultimate simplicity, using homogeneous coordinates, a concept on the projective geometry, for the 3DGS pipeline remarkably improves the rendering accuracies of distant objects. We therefore propose Homogeneous Gaussian Splatting (HoGS) incorporating homogeneous coordinates into the 3DGS framework, providing a unified representation for enhancing near and distant objects. HoGS effectively manages both expansive spatial positions and scales particularly in outdoor unbounded environments by adopting projective geometry principles. Experiments show that HoGS significantly enhances accuracy in reconstructing distant objects while maintaining high-quality rendering of nearby objects, along with fast training speed and real-time rendering capability. Our implementations are available on our project page https://kh129.github.io/hogs/.",
    "arxiv_url": "http://arxiv.org/abs/2503.19232v1",
    "pdf_url": "http://arxiv.org/pdf/2503.19232v1",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting",
    "authors": [
      "Yulong Zheng",
      "Zicheng Jiang",
      "Shengfeng He",
      "Yandu Sun",
      "Junyu Dong",
      "Huaidong Zhang",
      "Yong Du"
    ],
    "abstract": "Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably advanced photo-realistic novel view synthesis using images from densely spaced camera viewpoints. However, these methods struggle in few-shot scenarios due to limited supervision. In this paper, we present NexusGS, a 3DGS-based approach that enhances novel view synthesis from sparse-view images by directly embedding depth information into point clouds, without relying on complex manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our method introduces a novel point cloud densification strategy that initializes 3DGS with a dense point cloud, reducing randomness in point placement while preventing over-smoothing and overfitting. Specifically, NexusGS comprises three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses to compute accurate depth maps, while mitigating the inaccuracies often associated with optical flow. By incorporating epipolar depth priors, NexusGS ensures reliable dense point cloud coverage and supports stable 3DGS training under sparse-view conditions. Experiments demonstrate that NexusGS significantly enhances depth accuracy and rendering quality, surpassing state-of-the-art methods by a considerable margin. Furthermore, we validate the superiority of our generated point clouds by substantially boosting the performance of competing methods. Project page: https://usmizuki.github.io/NexusGS/.",
    "arxiv_url": "http://arxiv.org/abs/2503.18794v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18794v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "few-shot",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting",
    "authors": [
      "Lijiang Li",
      "Jinglu Wang",
      "Xiang Ming",
      "Yan Lu"
    ],
    "abstract": "In the Generative AI era, safeguarding 3D models has become increasingly urgent. While invisible watermarking is well-established for 2D images with encoder-decoder frameworks, generalizable and robust solutions for 3D remain elusive. The main difficulty arises from the renderer between the 3D encoder and 2D decoder, which disrupts direct gradient flow and complicates training. Existing 3D methods typically rely on per-scene iterative optimization, resulting in time inefficiency and limited generalization. In this work, we propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a well-known yet underexplored representation for watermarking. We identify two major challenges: (1) ensuring effective training generalized across diverse 3D models, and (2) reliably extracting watermarks from free-view renderings, even under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to embed messages, distortion layers to enhance resilience against various distortions, and a 2D decoder to extract watermarks from renderings. A key innovation is the Adaptive Marker Control mechanism that adaptively perturbs the initially optimized 3DGS, escaping local minima and improving both training stability and convergence. Extensive experiments show that GS-Marker outperforms per-scene training approaches in terms of decoding accuracy and model fidelity, while also significantly reducing computation time.",
    "arxiv_url": "http://arxiv.org/abs/2503.18718v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18718v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hardware-Rasterized Ray-Based Gaussian Splatting",
    "authors": [
      "Samuel Rota Bul√≤",
      "Nemanja Bartolovic",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ],
    "abstract": "We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing MIP-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS.",
    "arxiv_url": "http://arxiv.org/abs/2503.18682v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18682v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment",
    "authors": [
      "Haoran Wang",
      "Jingwei Huang",
      "Lu Yang",
      "Tianchen Deng",
      "Gaojing Zhang",
      "Mingrui Li"
    ],
    "abstract": "3D Gaussian Splatting has shown remarkable capabilities in novel view rendering tasks and exhibits significant potential for multi-view optimization.However, the original 3D Gaussian Splatting lacks color representation for inputs in low-light environments. Simply using enhanced images as inputs would lead to issues with multi-view consistency, and current single-view enhancement systems rely on pre-trained data, lacking scene generalization. These problems limit the application of 3D Gaussian Splatting in low-light conditions in the field of robotics, including high-fidelity modeling and feature matching. To address these challenges, we propose an unsupervised multi-view stereoscopic system based on Gaussian Splatting, called Low-Light Gaussian Splatting (LLGS). This system aims to enhance images in low-light environments while reconstructing the scene. Our method introduces a decomposable Gaussian representation called M-Color, which separately characterizes color information for targeted enhancement. Furthermore, we propose an unsupervised optimization method with zero-knowledge priors, using direction-based enhancement to ensure multi-view consistency. Experiments conducted on real-world datasets demonstrate that our system outperforms state-of-the-art methods in both low-light enhancement and 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2503.18640v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18640v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting",
    "authors": [
      "Luchao Wang",
      "Qian Ren",
      "Kaimin Liao",
      "Hua Wang",
      "Zhi Chen",
      "Yaohua Tang"
    ],
    "abstract": "Recent years have witnessed remarkable success of 3D Gaussian Splatting (3DGS) in novel view synthesis, surpassing prior differentiable rendering methods in both quality and efficiency. However, its training process suffers from coupled opacity-color optimization that frequently converges to local minima, producing floater artifacts that degrade visual fidelity. We present StableGS, a framework that eliminates floaters through cross-view depth consistency constraints while introducing a dual-opacity GS model to decouple geometry and material properties of translucent objects. To further enhance reconstruction quality in weakly-textured regions, we integrate DUSt3R depth estimation, significantly improving geometric stability. Our method fundamentally addresses 3DGS training instabilities, outperforming existing state-of-the-art methods across open-source datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.18458v2",
    "pdf_url": "http://arxiv.org/pdf/2503.18458v2",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation",
    "authors": [
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Chaojun Ni",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Xingang Wang"
    ],
    "abstract": "Combining reconstruction models with generative models has emerged as a promising paradigm for closed-loop simulation in autonomous driving. For example, ReconDreamer has demonstrated remarkable success in rendering large-scale maneuvers. However, a significant gap remains between the generated data and real-world sensor observations, particularly in terms of fidelity for structured elements, such as the ground surface. To address these challenges, we propose ReconDreamer++, an enhanced framework that significantly improves the overall rendering quality by mitigating the domain gap and refining the representation of the ground surface. Specifically, ReconDreamer++ introduces the Novel Trajectory Deformable Network (NTDNet), which leverages learnable spatial deformation mechanisms to bridge the domain gap between synthesized novel views and original sensor observations. Moreover, for structured elements such as the ground surface, we preserve geometric prior knowledge in 3D Gaussians, and the optimization process focuses on refining appearance attributes while preserving the underlying geometric structure. Experimental evaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and EUVS) confirm the superior performance of ReconDreamer++. Specifically, on Waymo, ReconDreamer++ achieves performance comparable to Street Gaussians for the original trajectory while significantly outperforming ReconDreamer on novel trajectories. In particular, it achieves substantial improvements, including a 6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5% gain in the ground surface metric NTL-IoU, highlighting its effectiveness in accurately reconstructing structured elements such as the road surface.",
    "arxiv_url": "http://arxiv.org/abs/2503.18438v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18438v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "lighting",
      "face",
      "deformation",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video",
    "authors": [
      "Qiang Hu",
      "Zihan Zheng",
      "Houqiang Zhong",
      "Sihua Fu",
      "Li Song",
      "XiaoyunZhang",
      "Guangtao Zhai",
      "Yanfeng Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has substantial potential for enabling photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number of Gaussians and their associated attributes poses significant challenges for storage and transmission. Existing methods typically handle dynamic 3DGS representation and compression separately, neglecting motion information and the rate-distortion (RD) trade-off during training, leading to performance degradation and increased model redundancy. To address this gap, we propose 4DGC, a novel rate-aware 4D Gaussian compression framework that significantly reduces storage size while maintaining superior RD performance for FVV. Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation that utilizes a compact motion grid combined with sparse compensated Gaussians to exploit inter-frame similarities. This representation effectively handles large motions, preserving quality and reducing temporal redundancy. Furthermore, we present an end-to-end compression scheme that employs differentiable quantization and a tiny implicit entropy model to compress the motion grid and compensated Gaussians efficiently. The entire framework is jointly optimized using a rate-distortion trade-off. Extensive experiments demonstrate that 4DGC supports variable bitrates and consistently outperforms existing methods in RD performance across multiple datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.18421v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18421v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "efficient",
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds",
    "authors": [
      "Youyu Chen",
      "Junjun Jiang",
      "Kui Jiang",
      "Xiao Tang",
      "Zhihao Li",
      "Xianming Liu",
      "Yinyu Nie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where the rendering resolution and the primitive number, concluded as the optimization complexity, dominate the time cost in primitive optimization. In this paper, we propose DashGaussian, a scheduling scheme over the optimization complexity of 3DGS that strips redundant complexity to accelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as progressively fitting 3DGS to higher levels of frequency components in the training views, and propose a dynamic rendering resolution scheme that largely reduces the optimization complexity based on this formulation. Besides, we argue that a specific rendering resolution should cooperate with a proper primitive number for a better balance between computing redundancy and fitting quality, where we schedule the growth of the primitives to synchronize with the rendering resolution. Extensive experiments show that our method accelerates the optimization of various 3DGS backbones by 45.7% on average while preserving the rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.18402v2",
    "pdf_url": "http://arxiv.org/pdf/2503.18402v2",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GI-SLAM: Gaussian-Inertial SLAM",
    "authors": [
      "Xulang Liu",
      "Ning Tan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful representation of geometry and appearance for dense Simultaneous Localization and Mapping (SLAM). Through rapid, differentiable rasterization of 3D Gaussians, many 3DGS SLAM methods achieve near real-time rendering and accelerated training. However, these methods largely overlook inertial data, witch is a critical piece of information collected from the inertial measurement unit (IMU). In this paper, we present GI-SLAM, a novel gaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking module and a realistic 3D Gaussian-based scene representation for mapping. Our method introduces an IMU loss that seamlessly integrates into the deep learning framework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the accuracy, robustness and efficiency of camera tracking. Moreover, our SLAM system supports a wide range of sensor configurations, including monocular, stereo, and RGBD cameras, both with and without IMU integration. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the EuRoC and TUM-RGBD datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.18275v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18275v1",
    "published_date": "2025-03-24",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "mapping",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving",
    "authors": [
      "Junhao Ge",
      "Zuhong Liu",
      "Longteng Fan",
      "Yifan Jiang",
      "Jiaqi Su",
      "Yiming Li",
      "Zhejun Zhang",
      "Siheng Chen"
    ],
    "abstract": "End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality data to perform well across various driving scenarios. However, collecting large-scale real-world data is expensive and time-consuming, making high-fidelity synthetic data essential for enhancing data diversity and model robustness. Existing driving simulators for synthetic data generation have significant limitations: game-engine-based simulators struggle to produce realistic sensor data, while NeRF-based and diffusion-based methods face efficiency challenges. Additionally, recent simulators designed for closed-loop evaluation provide limited interaction with other vehicles, failing to simulate complex real-world traffic dynamics. To address these issues, we introduce SceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D Gaussian Splatting (3DGS). SceneCrafter not only efficiently generates realistic driving logs across diverse traffic scenarios but also enables robust closed-loop evaluation of end-to-end models. Experimental results demonstrate that SceneCrafter serves as both a reliable evaluation platform and a efficient data generator that significantly improves end-to-end model generalization.",
    "arxiv_url": "http://arxiv.org/abs/2503.18108v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18108v1",
    "published_date": "2025-03-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "autonomous driving",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding",
    "authors": [
      "Hongjia Zhai",
      "Hai Li",
      "Zhenzhe Li",
      "Xiaokun Pan",
      "Yijia He",
      "Guofeng Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for open vocabulary scene understanding tasks. However, previous methods cannot distinguish 3D instance-level information, which usually predicts a heatmap between the scene feature and text query. In this paper, we propose PanoGS, a novel and effective 3D panoptic open vocabulary scene understanding approach. Technically, to learn accurate 3D language features that can scale to large indoor scenarios, we adopt the pyramid tri-plane to model the latent continuous parametric feature space and use a 3D feature decoder to regress the multi-view fused 2D feature cloud. Besides, we propose language-guided graph cuts that synergistically leverage reconstructed geometry and learned language cues to group 3D Gaussian primitives into a set of super-primitives. To obtain 3D consistent instance, we perform graph clustering based segmentation with SAM-guided edge affinity computation between different super-primitives. Extensive experiments on widely used datasets show better or more competitive performance on 3D panoptic open vocabulary scene understanding. Project page: \\href{https://zju3dv.github.io/panogs}{https://zju3dv.github.io/panogs}.",
    "arxiv_url": "http://arxiv.org/abs/2503.18107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18107v1",
    "published_date": "2025-03-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
    "authors": [
      "Yuxuan Xie",
      "Xuan Yu",
      "Changjian Jiang",
      "Sitong Mao",
      "Shunbo Zhou",
      "Rui Fan",
      "Rong Xiong",
      "Yue Wang"
    ],
    "abstract": "Open-vocabulary panoptic reconstruction is a challenging task for simultaneous scene reconstruction and understanding. Recently, methods have been proposed for 3D scene understanding based on Gaussian splatting. However, these methods are multi-staged, suffering from the accumulated errors and the dependence of hand-designed components. To streamline the pipeline and achieve global optimization, we propose PanopticSplatting, an end-to-end system for open-vocabulary panoptic reconstruction. Our method introduces query-guided Gaussian segmentation with local cross attention, lifting 2D instance masks without cross-frame association in an end-to-end way. The local cross attention within view frustum effectively reduces the training memory, making our model more accessible to large scenes with more Gaussians and objects. In addition, to address the challenge of noisy labels in 2D pseudo masks, we propose label blending to promote consistent 3D segmentation with less noisy floaters, as well as label warping on 2D predictions which enhances multi-view coherence and segmentation accuracy. Our method demonstrates strong performances in 3D scene panoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with both NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover, PanopticSplatting can be easily generalized to numerous variants of Gaussian splatting, and we demonstrate its robustness on different Gaussian base models.",
    "arxiv_url": "http://arxiv.org/abs/2503.18073v1",
    "pdf_url": "http://arxiv.org/pdf/2503.18073v1",
    "published_date": "2025-03-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "gaussian splatting",
      "ar",
      "large scene",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining",
    "authors": [
      "Yue Li",
      "Qi Ma",
      "Runyi Yang",
      "Huapeng Li",
      "Mengjiao Ma",
      "Bin Ren",
      "Nikola Popovic",
      "Nicu Sebe",
      "Ender Konukoglu",
      "Theo Gevers",
      "Luc Van Gool",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "Recognizing arbitrary or previously unseen categories is essential for comprehensive real-world 3D scene understanding. Currently, all existing methods rely on 2D or textual modalities during training or together at inference. This highlights the clear absence of a model capable of processing 3D data alone for learning semantics end-to-end, along with the necessary data to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the de facto standard for 3D scene representation across various vision tasks. However, effectively integrating semantic reasoning into 3DGS in a generalizable manner remains an open challenge. To address these limitations, we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene understanding approach that operates natively on 3DGS. Furthermore, we propose a self-supervised learning scheme that unlocks rich 3D feature learning from unlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes derived from seven established datasets, such as ScanNet and Matterport3D. Generating SceneSplat-7K required computational resources equivalent to 150 GPU days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the significant benefit of the proposed method over the established baselines.",
    "arxiv_url": "http://arxiv.org/abs/2503.18052v2",
    "pdf_url": "http://arxiv.org/pdf/2503.18052v2",
    "published_date": "2025-03-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos",
    "authors": [
      "Hanxiao Jiang",
      "Hao-Yu Hsu",
      "Kaifeng Zhang",
      "Hsin-Ni Yu",
      "Shenlong Wang",
      "Yunzhu Li"
    ],
    "abstract": "Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning.",
    "arxiv_url": "http://arxiv.org/abs/2503.17973v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17973v1",
    "published_date": "2025-03-23",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Global Illumination for Dynamic 3D Gaussian Scenes",
    "authors": [
      "Chenxiao Hu",
      "Meng Gai",
      "Guoping Wang",
      "Sheng Li"
    ],
    "abstract": "We present a real-time global illumination approach along with a pipeline for dynamic 3D Gaussian models and meshes. Building on a formulated surface light transport model for 3D Gaussians, we address key performance challenges with a fast compound stochastic ray-tracing algorithm and an optimized 3D Gaussian rasterizer. Our pipeline integrates multiple real-time techniques to accelerate performance and achieve high-quality lighting effects. Our approach enables real-time rendering of dynamic scenes with interactively editable materials and dynamic lighting of diverse multi-lights settings, capturing mutual multi-bounce light transport (indirect illumination) between 3D Gaussians and mesh. Additionally, we present a real-time renderer with an interactive user interface, validating our approach and demonstrating its practicality and high efficiency with over 40 fps in scenes including both 3D Gaussians and mesh. Furthermore, our work highlights the potential of 3D Gaussians in real-time applications with dynamic lighting, offering insights into performance and optimization.",
    "arxiv_url": "http://arxiv.org/abs/2503.17897v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17897v1",
    "published_date": "2025-03-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "fast",
      "global illumination",
      "face",
      "light transport",
      "3d gaussian",
      "real-time rendering",
      "illumination",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "Recent developments in 3D reconstruction and neural rendering have significantly propelled the capabilities of photo-realistic 3D scene rendering across various academic and industrial fields. The 3D Gaussian Splatting technique, alongside its derivatives, integrates the advantages of primitive-based and volumetric representations to deliver top-tier rendering quality and efficiency. Despite these advancements, the method tends to generate excessive redundant noisy Gaussians overfitted to every training view, which degrades the rendering quality. Additionally, while 3D Gaussian Splatting excels in small-scale and object-centric scenes, its application to larger scenes is hindered by constraints such as limited video memory, excessive optimization duration, and variable appearance across views. To address these challenges, we introduce GaussianFocus, an innovative approach that incorporates a patch attention algorithm to refine rendering quality and implements a Gaussian constraints strategy to minimize redundancy. Moreover, we propose a subdivision reconstruction strategy for large-scale scenes, dividing them into smaller, manageable blocks for individual training. Our results indicate that GaussianFocus significantly reduces unnecessary Gaussians and enhances rendering quality, surpassing existing State-of-The-Art (SoTA) methods. Furthermore, we demonstrate the capability of our approach to effectively manage and render large scenes, such as urban environments, whilst maintaining high fidelity in the visual output.",
    "arxiv_url": "http://arxiv.org/abs/2503.17798v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17798v1",
    "published_date": "2025-03-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "3d reconstruction",
      "large scene",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term Service Robots",
    "authors": [
      "Bin Fu",
      "Jialin Li",
      "Bin Zhang",
      "Ruiping Wang",
      "Xilin Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has garnered significant attention in robotics for its explicit, high fidelity dense scene representation, demonstrating strong potential for robotic applications. However, 3DGS-based methods in robotics primarily focus on static scenes, with limited attention to the dynamic scene changes essential for long-term service robots. These robots demand sustained task execution and efficient scene updates-challenges current approaches fail to meet. To address these limitations, we propose GS-LTS (Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor robots to manage diverse tasks in dynamic environments over time. GS-LTS detects scene changes (e.g., object addition or removal) via single-image change detection, employs a rule-based policy to autonomously collect multi-view observations, and efficiently updates the scene representation through Gaussian editing. Additionally, we propose a simulation-based benchmark that automatically generates scene change data as compact configuration scripts, providing a standardized, user-friendly evaluation benchmark. Experimental results demonstrate GS-LTS's advantages in reconstruction, navigation, and superior scene updates-faster and higher quality than the image training baseline-advancing 3DGS for long-term robotic operations. Code and benchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.",
    "arxiv_url": "http://arxiv.org/abs/2503.17733v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17733v1",
    "published_date": "2025-03-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Is there anything left? Measuring semantic residuals of objects removed from 3D Gaussian Splatting",
    "authors": [
      "Simona Kocour",
      "Assia Benbihi",
      "Aikaterini Adam",
      "Torsten Sattler"
    ],
    "abstract": "Searching in and editing 3D scenes has become extremely intuitive with trainable scene representations that allow linking human concepts to elements in the scene. These operations are often evaluated on the basis of how accurately the searched element is segmented or extracted from the scene. In this paper, we address the inverse problem, that is, how much of the searched element remains in the scene after it is removed. This question is particularly important in the context of privacy-preserving mapping when a user reconstructs a 3D scene and wants to remove private elements before sharing the map. To the best of our knowledge, this is the first work to address this question. To answer this, we propose a quantitative evaluation that measures whether a removal operation leaves object residuals that can be reasoned over. The scene is not private when such residuals are present. Experiments on state-of-the-art scene representations show that the proposed metrics are meaningful and consistent with the user study that we also present. We also propose a method to refine the removal based on spatial and semantic consistency.",
    "arxiv_url": "http://arxiv.org/abs/2503.17574v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17574v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "mapping",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping",
    "authors": [
      "Emanuele Giacomini",
      "Luca Di Giammarino",
      "Lorenzo De Rebotti",
      "Giorgio Grisetti",
      "Martin R. Oswald"
    ],
    "abstract": "LiDARs provide accurate geometric measurements, making them valuable for ego-motion estimation and reconstruction tasks. Although its success, managing an accurate and lightweight representation of the environment still poses challenges. Both classic and NeRF-based solutions have to trade off accuracy over memory and processing times. In this work, we build on recent advancements in Gaussian Splatting methods to develop a novel LiDAR odometry and mapping pipeline that exclusively relies on Gaussian primitives for its scene representation. Leveraging spherical projection, we drive the refinement of the primitives uniquely from LiDAR measurements. Experiments show that our approach matches the current registration performance, while achieving SOTA results for mapping tasks with minimal GPU requirements. This efficiency makes it a strong candidate for further exploration and potential adoption in real-time robotics estimation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2503.17491v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17491v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "mapping",
      "lightweight",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
    "authors": [
      "Zhengqing Gao",
      "Dongting Hu",
      "Jia-Wang Bian",
      "Huan Fu",
      "Yan Li",
      "Tongliang Liu",
      "Mingming Gong",
      "Kun Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2503.17486v3",
    "pdf_url": "http://arxiv.org/pdf/2503.17486v3",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "motion",
      "lightweight",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting",
    "authors": [
      "Jianchuan Chen",
      "Jingchuan Hu",
      "Gaige Wang",
      "Zhonghua Jiang",
      "Tiansong Zhou",
      "Zhiwen Chen",
      "Chengfei Lv"
    ],
    "abstract": "Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we \"bake\" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.",
    "arxiv_url": "http://arxiv.org/abs/2503.17032v1",
    "pdf_url": "http://arxiv.org/pdf/2503.17032v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "body",
      "3d gaussian",
      "gaussian splatting",
      "human",
      "ar",
      "avatar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting",
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Zhiyan Wang",
      "Luyang Tang",
      "Jiayu Yang",
      "Jie Liang",
      "Jiahao Wu",
      "Ronggang Wang"
    ],
    "abstract": "Building Free-Viewpoint Videos in a streaming manner offers the advantage of rapid responsiveness compared to offline training methods, greatly enhancing user experience. However, current streaming approaches face challenges of high per-frame reconstruction time (10s+) and error accumulation, limiting their broader application. In this paper, we propose Instant Gaussian Stream (IGS), a fast and generalizable streaming framework, to address these issues. First, we introduce a generalized Anchor-driven Gaussian Motion Network, which projects multi-view 2D motion features into 3D space, using anchor points to drive the motion of all Gaussians. This generalized Network generates the motion of Gaussians for each target frame in the time required for a single inference. Second, we propose a Key-frame-guided Streaming Strategy that refines each key frame, enabling accurate reconstruction of temporally complex scenes while mitigating error accumulation. We conducted extensive in-domain and cross-domain evaluations, demonstrating that our approach can achieve streaming with a average per-frame reconstruction time of 2s+, alongside a enhancement in view synthesis quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.16979v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16979v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "fast",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
    "authors": [
      "Jiadong Tang",
      "Yu Gao",
      "Dianyi Yang",
      "Liqi Yan",
      "Yufeng Yue",
      "Yi Yang"
    ],
    "abstract": "Drones have become essential tools for reconstructing wild scenes due to their outstanding maneuverability. Recent advances in radiance field methods have achieved remarkable rendering quality, providing a new avenue for 3D reconstruction from drone imagery. However, dynamic distractors in wild environments challenge the static scene assumption in radiance fields, while limited view constraints hinder the accurate capture of underlying scene geometry. To address these challenges, we introduce DroneSplat, a novel framework designed for robust 3D reconstruction from in-the-wild drone imagery. Our method adaptively adjusts masking thresholds by integrating local-global segmentation heuristics with statistical approaches, enabling precise identification and elimination of dynamic distractors in static scenes. We enhance 3D Gaussian Splatting with multi-view stereo predictions and a voxel-guided optimization strategy, supporting high-quality rendering under limited view constraints. For comprehensive evaluation, we provide a drone-captured 3D reconstruction dataset encompassing both dynamic and static scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS and NeRF baselines in handling in-the-wild drone imagery.",
    "arxiv_url": "http://arxiv.org/abs/2503.16964v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16964v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "authors": [
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/.",
    "arxiv_url": "http://arxiv.org/abs/2503.16924v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16924v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos",
    "authors": [
      "Yuxin Yao",
      "Zhi Deng",
      "Junhui Hou"
    ],
    "abstract": "This paper considers the problem of modeling articulated objects captured in 2D videos to enable novel view synthesis, while also being easily editable, drivable, and re-posable. To tackle this challenging problem, we propose RigGS, a new paradigm that leverages 3D Gaussian representation and skeleton-based motion representation to model dynamic objects without utilizing additional template priors. Specifically, we first propose skeleton-aware node-controlled deformation, which deforms a canonical 3D Gaussian representation over time to initialize the modeling process, producing candidate skeleton nodes that are further simplified into a sparse 3D skeleton according to their motion and semantic information. Subsequently, based on the resulting skeleton, we design learnable skin deformations and pose-dependent detailed deformations, thereby easily deforming the 3D Gaussian representation to generate new actions and render further high-quality images from novel views. Extensive experiments demonstrate that our method can generate realistic new actions easily for objects and achieve high-quality rendering.",
    "arxiv_url": "http://arxiv.org/abs/2503.16822v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16822v1",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality",
    "authors": [
      "Chiara Schiavo",
      "Elena Camuffo",
      "Leonardo Badia",
      "Simone Milani"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has significantly improved the efficiency and realism of three-dimensional scene visualization in several applications, ranging from robotics to eXtended Reality (XR). This work presents SAGE (Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel framework designed to enhance the user experience by dynamically adapting the Level of Detail (LOD) of different 3DGS objects identified via a semantic segmentation. Experimental results demonstrate how SAGE effectively reduces memory and computational overhead while keeping a desired target visual quality, thus providing a powerful optimization for interactive XR applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.16747v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16747v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "head",
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting SLAM",
    "authors": [
      "Yanyan Li",
      "Youxu Fang",
      "Zunjie Zhu",
      "Kunyi Li",
      "Yong Ding",
      "Federico Tombari"
    ],
    "abstract": "Simultaneously localizing camera poses and constructing Gaussian radiance fields in dynamic scenes establish a crucial bridge between 2D images and the 4D real world. Instead of removing dynamic objects as distractors and reconstructing only static environments, this paper proposes an efficient architecture that incrementally tracks camera poses and establishes the 4D Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D images. First, by generating motion masks, we obtain static and dynamic priors for each pixel. To eliminate the influence of static scenes and improve the efficiency on learning the motion of dynamic objects, we classify the Gaussian primitives into static and dynamic Gaussian sets, while the sparse control points along with an MLP is utilized to model the transformation fields of the dynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a novel 2D optical flow map reconstruction algorithm is designed to render optical flows of dynamic objects between neighbor images, which are further used to supervise the 4D Gaussian radiance fields along with traditional photometric and geometric constraints. In experiments, qualitative and quantitative evaluation results show that the proposed method achieves robust tracking and high-quality view synthesis performance in real-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2503.16710v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16710v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "motion",
      "4d",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting",
    "authors": [
      "Sixu Li",
      "Ben Keller",
      "Yingyan Celine Lin",
      "Brucek Khailany"
    ],
    "abstract": "3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely on dedicated accelerators that require substantial integration overhead and hardware costs. This work proposes an acceleration strategy that leverages the similarities between the 3DGS pipeline and the highly optimized conventional graphics pipeline in modern GPUs. Instead of developing a dedicated accelerator, we enhance existing GPU rasterizer hardware to efficiently support 3DGS operations. Our results demonstrate a 23$\\times$ increase in processing speed and a 24$\\times$ reduction in energy consumption, with improvements yielding 6$\\times$ faster end-to-end runtime for the original 3DGS algorithm and 4$\\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and 46 FPS respectively. These enhancements incur only a minimal area overhead of 0.2\\% relative to the entire SoC chip area, underscoring the practicality and efficiency of our approach for enabling 3DGS rendering on resource-constrained platforms.",
    "arxiv_url": "http://arxiv.org/abs/2503.16681v2",
    "pdf_url": "http://arxiv.org/pdf/2503.16681v2",
    "published_date": "2025-03-20",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "3d gaussian",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "authors": [
      "Yuheng Yuan",
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2503.16422v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16422v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "fast",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "M3: 3D-Spatial MultiModal Memory",
    "authors": [
      "Xueyan Zou",
      "Yuchen Song",
      "Ri-Zhao Qiu",
      "Xuanbin Peng",
      "Jianglong Ye",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.",
    "arxiv_url": "http://arxiv.org/abs/2503.16413v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16413v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images",
    "authors": [
      "Shengjun Zhang",
      "Xin Fei",
      "Fangfu Liu",
      "Haixu Song",
      "Yueqi Duan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2503.16338v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16338v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering",
    "authors": [
      "Shiyong Liu",
      "Xiao Tang",
      "Zhihao Li",
      "Yingfan He",
      "Chongjie Ye",
      "Jianzhuang Liu",
      "Binxiao Huang",
      "Shunbo Zhou",
      "Xiaofei Wu"
    ],
    "abstract": "In large-scale scene reconstruction using 3D Gaussian splatting, it is common to partition the scene into multiple smaller regions and reconstruct them individually. However, existing division methods are occlusion-agnostic, meaning that each region may contain areas with severe occlusions. As a result, the cameras within those regions are less correlated, leading to a low average contribution to the overall reconstruction. In this paper, we propose an occlusion-aware scene division strategy that clusters training cameras based on their positions and co-visibilities to acquire multiple regions. Cameras in such regions exhibit stronger correlations and a higher average contribution, facilitating high-quality scene reconstruction. We further propose a region-based rendering technique to accelerate large scene rendering, which culls Gaussians invisible to the region where the viewpoint is located. Such a technique significantly speeds up the rendering without compromising quality. Extensive experiments on multiple large scenes show that our method achieves superior reconstruction results with faster rendering speed compared to existing state-of-the-art approaches. Project page: https://occlugaussian.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2503.16177v1",
    "pdf_url": "http://arxiv.org/pdf/2503.16177v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "large scene",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Close-up Novel View Synthesis via Pseudo-labeling",
    "authors": [
      "Jiatong Xia",
      "Libo Sun",
      "Lingqiao Liu"
    ],
    "abstract": "Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated remarkable capabilities in novel view synthesis. However, despite their success in producing high-quality images for viewpoints similar to those seen during training, they struggle when generating detailed images from viewpoints that significantly deviate from the training set, particularly in close-up views. The primary challenge stems from the lack of specific training data for close-up views, leading to the inability of current methods to render these views accurately. To address this issue, we introduce a novel pseudo-label-based learning strategy. This approach leverages pseudo-labels derived from existing training data to provide targeted supervision across a wide range of close-up viewpoints. Recognizing the absence of benchmarks for this specific challenge, we also present a new dataset designed to assess the effectiveness of both current and future methods in this area. Our extensive experiments demonstrate the efficacy of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2503.15908v1",
    "pdf_url": "http://arxiv.org/pdf/2503.15908v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
    "authors": [
      "Tiange Xiang",
      "Kai Li",
      "Chengjiang Long",
      "Christian H√§ne",
      "Peihong Guo",
      "Scott Delp",
      "Ehsan Adeli",
      "Li Fei-Fei"
    ],
    "abstract": "Recent advances in text-to-image diffusion models have been driven by the increasing availability of paired 2D data. However, the development of 3D diffusion models has been hindered by the scarcity of high-quality 3D data, resulting in less competitive performance compared to their 2D counterparts. To address this challenge, we propose repurposing pre-trained 2D diffusion models for 3D object generation. We introduce Gaussian Atlas, a novel representation that utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models to generate 3D Gaussians. Our approach demonstrates successful transfer learning from a pre-trained 2D diffusion model to a 2D manifold flattened from 3D structures. To support model training, we compile GaussianVerse, a large-scale dataset comprising 205K high-quality 3D Gaussian fittings of various 3D objects. Our experimental results show that text-to-image diffusion models can be effectively adapted for 3D content generation, bridging the gap between 2D and 3D modeling.",
    "arxiv_url": "http://arxiv.org/abs/2503.15877v1",
    "pdf_url": "http://arxiv.org/pdf/2503.15877v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling",
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Hyelin Nam",
      "Byung-Hoon Kim",
      "Hyungjin Chung",
      "Changick Kim"
    ],
    "abstract": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.",
    "arxiv_url": "http://arxiv.org/abs/2503.15855v1",
    "pdf_url": "http://arxiv.org/pdf/2503.15855v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Disheng Liu",
      "Tuo Liang",
      "Yu Yin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.15835v1",
    "pdf_url": "http://arxiv.org/pdf/2503.15835v1",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image",
    "authors": [
      "Arindam Dutta",
      "Meng Zheng",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Anwesha Choudhuri",
      "Terrence Chen",
      "Amit K. Roy-Chowdhury",
      "Ziyan Wu"
    ],
    "abstract": "Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions.",
    "arxiv_url": "http://arxiv.org/abs/2503.15671v1",
    "pdf_url": "http://arxiv.org/pdf/2503.15671v1",
    "published_date": "2025-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer",
    "authors": [
      "Yuezhen Xie",
      "Meiying Zhang",
      "Qi Hao"
    ],
    "abstract": "Adverse climate conditions pose significant challenges for autonomous systems, demanding reliable perception and decision-making across diverse environments. To better simulate these conditions, physically-based NeRF rendering methods have been explored for their ability to generate realistic scene representations. However, these methods suffer from slow rendering speeds and long preprocessing times, making them impractical for real-time testing and user interaction. This paper presents ClimateGS, a novel framework integrating 3D Gaussian representations with physical simulation to enable real-time climate effects rendering. The novelty of this work is threefold: 1) developing a linear transformation for 3D Gaussian photorealistic style transfer, enabling direct modification of spherical harmonics across bands for efficient and consistent style adaptation; 2) developing a joint training strategy for 3D style transfer, combining supervised and self-supervised learning to accelerate convergence while preserving original scene details; 3) developing a real-time rendering method for climate simulation, integrating physics-based effects with 3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS on MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with comparable or superior visual quality to SOTA 2D/3D methods, making it suitable for interactive applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.14845v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14845v1",
    "published_date": "2025-03-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering",
    "authors": [
      "Yilan Dong",
      "Haohe Liu",
      "Qing Wang",
      "Jiahao Yang",
      "Wenqing Wang",
      "Gregory Slabaugh",
      "Shanxin Yuan"
    ],
    "abstract": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on rigid skeletal motion with an oversimplified non-rigid motion model, which fails to capture fine geometric and appearance details. Additionally, they perform densification based solely on per-point gradients and process poses independently, ignoring spatial and temporal correlations. These limitations lead to geometric detail loss, temporal instability, and inefficient point distribution. To address these issues, we propose HandSplat, a novel Gaussian Splatting-based framework that enhances both fidelity and stability for hand rendering. To improve fidelity, we extend standard 3DGS attributes with implicit geometry and appearance embeddings for finer non-rigid motion modeling while preserving the static hand characteristic modeled by original 3DGS attributes. Additionally, we introduce a local gradient-aware densification strategy that dynamically refines Gaussian density in high-variation regions. To improve stability, we incorporate pose-conditioned attribute regularization to encourage attribute consistency across similar poses, mitigating temporal artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat surpasses existing methods in fidelity and stability while achieving real-time performance. We will release the code and pre-trained models upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2503.14736v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14736v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training",
    "authors": [
      "Yiming Wang",
      "Lucy Chai",
      "Xuan Luo",
      "Michael Niemeyer",
      "Manuel Lagunas",
      "Stephen Lombardi",
      "Siyu Tang",
      "Tiancheng Sun"
    ],
    "abstract": "We study the problem of novel view streaming from sparse-view videos, which aims to generate a continuous sequence of high-quality, temporally consistent novel views as new input frames arrive. However, existing novel view synthesis methods struggle with temporal coherence and visual fidelity, leading to flickering and inconsistency. To address these challenges, we introduce history-awareness, leveraging previous frames to reconstruct the scene and improve quality and stability. We propose a hybrid splat-voxel feed-forward scene reconstruction approach that combines Gaussian Splatting to propagate information over time, with a hierarchical voxel grid for temporal fusion. Gaussian primitives are efficiently warped over time using a motion graph that extends 2D tracking models to 3D motion, while a sparse voxel transformer integrates new temporal observations in an error-aware manner. Crucially, our method does not require training on multi-view video datasets, which are currently limited in size and diversity, and can be directly applied to sparse-view video streams in a history-aware manner at inference time. Our approach achieves state-of-the-art performance in both static and streaming scene reconstruction, effectively reducing temporal artifacts and visual artifacts while running at interactive rates (15 fps with 350ms delay) on a single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/",
    "arxiv_url": "http://arxiv.org/abs/2503.14698v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14698v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "tracking",
      "motion",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation",
    "authors": [
      "Umar Farooq",
      "Jean-Yves Guillemaut",
      "Adrian Hilton",
      "Marco Volino"
    ],
    "abstract": "The field of Novel View Synthesis has been revolutionized by 3D Gaussian Splatting (3DGS), which enables high-quality scene reconstruction that can be rendered in real-time. 3DGS-based techniques typically suffer from high GPU memory and disk storage requirements which limits their practical application on consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated coarse-to-fine optimization framework that aims to minimize the number of Gaussian primitives used to represent a scene, thus reducing memory and storage demands. Opti3DGS leverages image frequency modulation, initially enforcing a coarse scene representation and progressively refining it by modulating frequency details in the training images. On the baseline 3DGS, we demonstrate an average reduction of 62% in Gaussians, a 40% reduction in the training GPU memory requirements and a 20% reduction in optimization time without sacrificing the visual quality. Furthermore, we show that our method integrates seamlessly with many 3DGS-based techniques, consistently reducing the number of Gaussian primitives while maintaining, and often improving, visual quality. Additionally, Opti3DGS inherently produces a level-of-detail scene representation at no extra cost, a natural byproduct of the optimization pipeline. Results and code will be made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2503.14475v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14475v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Adaptive Density Control for 3D Gaussian Splatting",
    "authors": [
      "Glenn Grubert",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become one of the most influential works in the past year. Due to its efficient and high-quality novel view synthesis capabilities, it has been widely adopted in many research fields and applications. Nevertheless, 3DGS still faces challenges to properly manage the number of Gaussian primitives that are used during scene reconstruction. Following the adaptive density control (ADC) mechanism of 3D Gaussian Splatting, new Gaussians in under-reconstructed regions are created, while Gaussians that do not contribute to the rendering quality are pruned. We observe that those criteria for densifying and pruning Gaussians can sometimes lead to worse rendering by introducing artifacts. We especially observe under-reconstructed background or overfitted foreground regions. To encounter both problems, we propose three new improvements to the adaptive density control mechanism. Those include a correction for the scene extent calculation that does not only rely on camera positions, an exponentially ascending gradient threshold to improve training convergence, and significance-aware pruning strategy to avoid background artifacts. With these adaptions, we show that the rendering quality improves while using the same number of Gaussians primitives. Furthermore, with our improvements, the training converges considerably faster, allowing for more than twice as fast training times while yielding better quality than 3DGS. Finally, our contributions are easily compatible with most existing derivative works of 3DGS making them relevant for future works.",
    "arxiv_url": "http://arxiv.org/abs/2503.14274v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14274v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
    "authors": [
      "Junjin Xiao",
      "Qing Zhang",
      "Yonewei Nie",
      "Lei Zhu",
      "Wei-Shi Zheng"
    ],
    "abstract": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity novel views of unseen human from sparse multi-view images, while requiring no cumbersome per-subject optimization. Unlike previous methods that typically struggle with sparse views with few overlappings and are less effective in reconstructing complex human geometry, the proposed method enables robust reconstruction in such challenging conditions. Our key idea is to lift SMPL vertices to dense and reliable 3D prior points representing accurate human body geometry, and then regress human Gaussian parameters based on the points. To account for possible misalignment between SMPL model and images, we propose to predict image-aligned 3D prior points by leveraging both pixel-level features and voxel-level features, from which we regress the coarse Gaussians. To enhance the ability to capture high-frequency details, we further render depth maps from the coarse 3D Gaussians to help regress fine-grained pixel-wise Gaussians. Experiments on several benchmark datasets demonstrate that our method outperforms state-of-the-art methods in novel view synthesis and cross-dataset generalization. Our code is available at https://github.com/iSEE-Laboratory/RoGSplat.",
    "arxiv_url": "http://arxiv.org/abs/2503.14198v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14198v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/iSEE-Laboratory/RoGSplat",
    "keywords": [
      "sparse view",
      "high-fidelity",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images",
    "authors": [
      "Simon Niedermayr",
      "Christoph Neuhauser R√ºdiger Westermann"
    ],
    "abstract": "We introduce an image upscaling technique tailored for 3D Gaussian Splatting (3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher rendering speeds and reduces artifacts commonly observed in 3DGS reconstructions. Our technique upscales low-resolution 3DGS renderings with a marginal increase in cost by directly leveraging the analytical image gradients of Gaussians for gradient-based bicubic spline interpolation. The technique is agnostic to the specific 3DGS implementation, achieving novel view synthesis at rates 3x-4x higher than the baseline implementation. Through extensive experiments on multiple datasets, we showcase the performance improvements and high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS images. We further demonstrate the integration of gradient-aware upscaling into the gradient-based optimization of a 3DGS model and analyze its effects on reconstruction quality and performance.",
    "arxiv_url": "http://arxiv.org/abs/2503.14171v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14171v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting",
    "authors": [
      "Runsong Zhu",
      "Shi Qiu",
      "Zhengzhe Liu",
      "Ka-Hei Hui",
      "Qianyi Wu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Lifting multi-view 2D instance segmentation to a radiance field has proven to be effective to enhance 3D understanding. Existing methods rely on direct matching for end-to-end lifting, yielding inferior results; or employ a two-stage solution constrained by complex pre- or post-processing. In this work, we design a new end-to-end object-aware lifting approach, named Unified-Lift that provides accurate 3D segmentation based on the 3D Gaussian representation. To start, we augment each Gaussian point with an additional Gaussian-level feature learned using a contrastive loss to encode instance information. Importantly, we introduce a learnable object-level codebook to account for individual objects in the scene for an explicit object-level understanding and associate the encoded object-level features with the Gaussian-level point features for segmentation predictions. While promising, achieving effective codebook learning is non-trivial and a naive solution leads to degraded performance. Therefore, we formulate the association learning module and the noisy label filtering module for effective and robust codebook learning. We conduct experiments on three benchmarks: LERF-Masked, Replica, and Messy Rooms datasets. Both qualitative and quantitative results manifest that our Unified-Lift clearly outperforms existing methods in terms of segmentation quality and time efficiency. The code is publicly available at \\href{https://github.com/Runsong123/Unified-Lift}{https://github.com/Runsong123/Unified-Lift}.",
    "arxiv_url": "http://arxiv.org/abs/2503.14029v1",
    "pdf_url": "http://arxiv.org/pdf/2503.14029v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Runsong123/Unified-Lift",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BG-Triangle: B√©zier Gaussian Triangle for 3D Vectorization and Rendering",
    "authors": [
      "Minye Wu",
      "Haizhao Dai",
      "Kaixin Yao",
      "Tinne Tuytelaars",
      "Jingyi Yu"
    ],
    "abstract": "Differentiable rendering enables efficient optimization by allowing gradients to be computed through the rendering process, facilitating 3D reconstruction, inverse rendering and neural scene representation learning. To ensure differentiability, existing solutions approximate or re-formulate traditional rendering operations using smooth, probabilistic proxies such as volumes or Gaussian primitives. Consequently, they struggle to preserve sharp edges due to the lack of explicit boundary definitions. We present a novel hybrid representation, B\\'ezier Gaussian Triangle (BG-Triangle), that combines B\\'ezier triangle-based vector graphics primitives with Gaussian-based probabilistic models, to maintain accurate shape modeling while conducting resolution-independent differentiable rendering. We present a robust and effective discontinuity-aware rendering technique to reduce uncertainties at object boundaries. We also employ an adaptive densification and pruning scheme for efficient training while reliably handling level-of-detail (LoD) variations. Experiments show that BG-Triangle achieves comparable rendering quality as 3DGS but with superior boundary preservation. More importantly, BG-Triangle uses a much smaller number of primitives than its alternatives, showcasing the benefits of vectorized graphics primitives and the potential to bridge the gap between classic and emerging representations.",
    "arxiv_url": "http://arxiv.org/abs/2503.13961v1",
    "pdf_url": "http://arxiv.org/pdf/2503.13961v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via Context Model",
    "authors": [
      "Mufan Liu",
      "Qi Yang",
      "He Huang",
      "Wenjie Huang",
      "Zhenlong Yuan",
      "Zhu Li",
      "Yiling Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity paradigm for novel view synthesis. To adapt 3DGS for dynamic content, deformable 3DGS incorporates temporally deformable primitives with learnable latent embeddings to capture complex motions. Despite its impressive performance, the high-dimensional embeddings and vast number of primitives lead to substantial storage requirements. In this paper, we introduce a \\textbf{Light}weight \\textbf{4}D\\textbf{GS} framework, called Light4GS, that employs significance pruning with a deep context model to provide a lightweight storage-efficient dynamic 3DGS representation. The proposed Light4GS is based on 4DGS that is a typical representation of deformable 3DGS. Specifically, our framework is built upon two core components: (1) a spatio-temporal significance pruning strategy that eliminates over 64\\% of the deformable primitives, followed by an entropy-constrained spherical harmonics compression applied to the remainder; and (2) a deep context model that integrates intra- and inter-prediction with hyperprior into a coarse-to-fine context structure to enable efficient multiscale latent embedding compression. Our approach achieves over 120x compression and increases rendering FPS up to 20\\% compared to the baseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS compression methods, revealing the effectiveness of our Light4GS in terms of both intra- and inter-prediction methods without sacrificing rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.13948v1",
    "pdf_url": "http://arxiv.org/pdf/2503.13948v1",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "efficient",
      "high-fidelity",
      "motion",
      "lightweight",
      "4d",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors",
    "authors": [
      "Katja Schwarz",
      "Norman Mueller",
      "Peter Kontschieder"
    ],
    "abstract": "Synthesizing consistent and photorealistic 3D scenes is an open problem in computer vision. Video diffusion models generate impressive videos but cannot directly synthesize 3D representations, i.e., lack 3D consistency in the generated sequences. In addition, directly training generative 3D models is challenging due to a lack of 3D training data at scale. In this work, we present Generative Gaussian Splatting (GGS) -- a novel approach that integrates a 3D representation with a pre-trained latent video diffusion model. Specifically, our model synthesizes a feature field parameterized via 3D Gaussian primitives. The feature field is then either rendered to feature maps and decoded into multi-view images, or directly upsampled into a 3D radiance field. We evaluate our approach on two common benchmark datasets for scene synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model significantly improves both the 3D consistency of the generated multi-view images, and the quality of the generated 3D scenes over all relevant baselines. Compared to a similar model without 3D representation, GGS improves FID on the generated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page: https://katjaschwarz.github.io/ggs/",
    "arxiv_url": "http://arxiv.org/abs/2503.13272v1",
    "pdf_url": "http://arxiv.org/pdf/2503.13272v1",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction",
    "authors": [
      "Rui Wang",
      "Quentin Lohmeyer",
      "Mirko Meboldt",
      "Siyu Tang"
    ],
    "abstract": "Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstructionin highly dynamic, interaction-rich environments.",
    "arxiv_url": "http://arxiv.org/abs/2503.13176v1",
    "pdf_url": "http://arxiv.org/pdf/2503.13176v1",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
    "authors": [
      "Yiwei Xu",
      "Yifei Yu",
      "Wentian Gan",
      "Tengfei Wang",
      "Zongqian Zhan",
      "Hao Cheng",
      "Xin Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast real-time performance, but existing methods rely on offline training after full Structure-from-Motion (SfM) processing. In contrast, this work introduces On-the-Fly GS, a progressive framework enabling near real-time 3DGS optimization during image capture. As each image arrives, its pose and sparse points are updated via on-the-fly SfM, and newly optimized Gaussians are immediately integrated into the 3DGS field. We propose a progressive local optimization strategy to prioritize new images and their neighbors by their corresponding overlapping relationship, allowing the new image and its overlapping images to get more training. To further stabilize training across old and new images, an adaptive learning rate schedule balances the iterations and the learning rate. Moreover, to maintain overall quality of the 3DGS field, an efficient global optimization scheme prevents overfitting to the newly added images. Experiments on multiple benchmark datasets show that our On-the-Fly GS reduces training time significantly, optimizing each new image in seconds with minimal rendering loss, offering the first practical step toward rapid, progressive 3DGS reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2503.13086v1",
    "pdf_url": "http://arxiv.org/pdf/2503.13086v1",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression",
    "authors": [
      "Yu-Ting Zhan",
      "He-bi Yang",
      "Cheng-Yuan Ho",
      "Jui-Chiu Chiang",
      "Wen-Hsiao Peng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown immense potential for novel view synthesis. However, achieving rate-distortion-optimized compression of 3DGS representations for transmission and/or storage applications remains a challenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for end-to-end optimized compression, delivering state-of-the-art coding performance. Despite this, it requires prolonged training and decoding time. To address these limitations, we propose CAT-3DGS Pro, an enhanced version of CAT-3DGS that improves both compression performance and computational efficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which replaces the triplane-based hyperprior to reduce redundant parameters. To achieve a more balanced rate-distortion trade-off and faster encoding, we propose an alternate optimization strategy (A-RDO). Additionally, we refine the sampling rate optimization method in CAT-3DGS, leading to significant improvements in rate-distortion performance. These enhancements result in a 46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while achieving 5x acceleration in decoding speed for the Amsterdam scene compared to CAT-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2503.12862v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12862v1",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "acceleration",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting",
    "authors": [
      "Sumin In",
      "Youngdong Jang",
      "Utae Jeong",
      "MinHyuk Jang",
      "Hyeongcheol Park",
      "Eunbyung Park",
      "Sangpil Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly adopted in various academic and commercial applications due to its real-time and high-quality rendering capabilities, emphasizing the growing need for copyright protection technologies for 3DGS. However, the large model size of 3DGS requires developing efficient compression techniques. This highlights the necessity of an integrated framework that addresses copyright protection and data compression for 3D content. Nevertheless, existing 3DGS watermarking methods significantly degrade watermark performance under 3DGS compression methods, particularly quantization-based approaches that achieve superior compression performance. To ensure reliable watermark detection under compression, we propose a compression-tolerant anchor-based 3DGS watermarking, which preserves watermark integrity and rendering quality. This is achieved by introducing anchor-based 3DGS watermarking. We embed the watermark into the anchor attributes, particularly the anchor feature, to enhance security and rendering quality. We also propose a quantization distortion layer that injects quantization noise during training, preserving the watermark after quantization-based compression. Moreover, we employ a frequency-aware anchor growing strategy that improves rendering quality and watermark performance by effectively identifying Gaussians in high-frequency regions. Extensive experiments demonstrate that our proposed method preserves the watermark even under compression and maintains high rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.12836v5",
    "pdf_url": "http://arxiv.org/pdf/2503.12836v5",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis",
    "authors": [
      "Hadam Baek",
      "Hannie Shin",
      "Jiyoung Seo",
      "Chanwoo Kim",
      "Saerom Kim",
      "Hyeongbok Kim",
      "Sangpil Kim"
    ],
    "abstract": "Accurately modeling sound propagation with complex real-world environments is essential for Novel View Acoustic Synthesis (NVAS). While previous studies have leveraged visual perception to estimate spatial acoustics, the combined use of surface normal and structural details from 3D representations in acoustic modeling has been underexplored. Given their direct impact on sound wave reflections and propagation, surface normals should be jointly modeled with structural details to achieve accurate spatial acoustics. In this paper, we propose a surface-enhanced geometry-aware approach for NVAS to improve spatial acoustic modeling. To achieve this, we exploit geometric priors, such as image, depth map, surface normals, and point clouds obtained using a 3D Gaussian Splatting (3DGS) based framework. We introduce a dual cross-attention-based transformer integrating geometrical constraints into frequency query to understand the surroundings of the emitter. Additionally, we design a ConvNeXt-based spectral features processing network called Spectral Refinement Network (SRN) to synthesize realistic binaural audio. Experimental results on the RWAVS and SoundSpace datasets highlight the necessity of our approach, as it surpasses existing methods in novel view acoustic synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2503.12806v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12806v1",
    "published_date": "2025-03-17",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deblur Gaussian Splatting SLAM",
    "authors": [
      "Francesco Girlanda",
      "Denys Rozumnyi",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp reconstructions from motion-blurred inputs. The proposed method bridges the strengths of both frame-to-frame and frame-to-model approaches to model sub-frame camera trajectories that lead to high-fidelity reconstructions in motion-blurred settings. Moreover, our pipeline incorporates techniques such as online loop closure and global bundle adjustment to achieve a dense and precise global trajectory. We model the physical image formation process of motion-blurred images and minimize the error between the observed blurry images and rendered blurry images obtained by averaging sharp virtual sub-frame images. Additionally, by utilizing a monocular depth estimator alongside the online deformation of Gaussians, we ensure precise mapping and enhanced image deblurring. The proposed SLAM pipeline integrates all these components to improve the results. We achieve state-of-the-art results for sharp map estimation and sub-frame trajectory recovery both on synthetic and real-world blurry input data.",
    "arxiv_url": "http://arxiv.org/abs/2503.12572v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12572v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "deformation",
      "mapping",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View",
    "authors": [
      "Xianzu Wu",
      "Zhenxin Ai",
      "Harry Yang",
      "Ser-Nam Lim",
      "Jun Liu",
      "Huan Wang"
    ],
    "abstract": "Recent advances in single-view 3D scene reconstruction have highlighted the challenges in capturing fine geometric details and ensuring structural consistency, particularly in high-fidelity outdoor scene modeling. This paper presents Niagara, a new single-view 3D scene reconstruction framework that can faithfully reconstruct challenging outdoor scenes from a single input image for the first time.   Our approach integrates monocular depth and normal estimation as input, which substantially improves its ability to capture fine details, mitigating common issues like geometric detail loss and deformation.   Additionally, we introduce a geometric affine field (GAF) and 3D self-attention as geometry-constraint, which combines the structural properties of explicit geometry with the adaptability of implicit feature fields, striking a balance between efficient rendering and high-fidelity reconstruction.   Our framework finally proposes a specialized encoder-decoder architecture, where a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian parameters, which can be used for novel view synthesis. Extensive results and analyses suggest that our Niagara surpasses prior SoTA approaches such as Flash3D in both single-view and dual-view settings, significantly enhancing the geometric accuracy and visual fidelity, especially in outdoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.12553v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12553v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "efficient rendering",
      "outdoor",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MTGS: Multi-Traversal Gaussian Splatting",
    "authors": [
      "Tianyu Li",
      "Yihang Qiu",
      "Zhenhua Wu",
      "Carl Lindstr√∂m",
      "Peng Su",
      "Matthias Nie√üner",
      "Hongyang Li"
    ],
    "abstract": "Multi-traversal data, commonly collected through daily commutes or by self-driving fleets, provides multiple viewpoints for scene reconstruction within a road block. This data offers significant potential for high-quality novel view synthesis, which is crucial for applications such as autonomous vehicle simulators. However, inherent challenges in multi-traversal data often result in suboptimal reconstruction quality, including variations in appearance and the presence of dynamic objects. To address these issues, we propose Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs high-quality driving scenes from arbitrarily collected multi-traversal data by modeling a shared static geometry while separately handling dynamic elements and appearance variations. Our method employs a multi-traversal dynamic scene graph with a shared static node and traversal-specific dynamic nodes, complemented by color correction nodes with learnable spherical harmonics coefficient residuals. This approach enables high-fidelity novel view synthesis and provides flexibility to navigate any viewpoint. We conduct extensive experiments on a large-scale driving dataset, nuPlan, with multi-traversal data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry accuracy by 46.3% compared to single-traversal baselines. The code and data would be available to the public.",
    "arxiv_url": "http://arxiv.org/abs/2503.12552v3",
    "pdf_url": "http://arxiv.org/pdf/2503.12552v3",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs",
    "authors": [
      "Guibiao Liao",
      "Qing Li",
      "Zhenyu Bao",
      "Guoping Qiu",
      "Kanglin Liu"
    ],
    "abstract": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches have shown significant performance with dense input images. However, they exhibit poor performance when confronted with sparse inputs, primarily due to the sparse distribution of Gaussian points and insufficient view supervision. To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC) Regularization for open-world free view synthesis with sparse inputs. Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by utilizing view-changed images generated from the video generation model and view-constraint Gaussian points densification. Additionally, SPC mitigates limited view supervision by employing semantic-prompt-based consistency constraints developed by SAM2. This approach leverages available semantics from training views, serving as instructive prompts, to optimize visually overlapping regions in novel views with 2D and 3D consistency constraints. Extensive experiments demonstrate the superior performance of SPC-GS across Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world semantic segmentation.",
    "arxiv_url": "http://arxiv.org/abs/2503.12535v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12535v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting",
    "authors": [
      "Songen Gu",
      "Haoxuan Song",
      "Binjie Liu",
      "Qian Yu",
      "Sanyi Zhang",
      "Haiyong Jiang",
      "Jin Huang",
      "Feng Tian"
    ],
    "abstract": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native 3D object generation framework that incorporates a 3D Gaussian Splatting representation. As part of our work, we introduce VRSS, the first large-scale paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap in multi-modal VR sketch-based generation. Our approach features the following key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage alignment strategy that bridges the domain gap between sparse VR sketch embeddings and rich CLIP embeddings, facilitating both VR sketch-based retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We disentangle the 3D generation process by using explicit VR sketches for geometric conditioning and text descriptions for appearance control. To facilitate this, we propose a generalizable VR sketch encoder that effectively aligns different modalities. 3) Efficient and high-fidelity 3D native generation. Our method leverages a 3D-native generation approach that enables fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS dataset demonstrate that our method achieves high-quality, multi-modal VR sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian method will be beneficial for the 3D generation community.",
    "arxiv_url": "http://arxiv.org/abs/2503.12383v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12383v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "vr",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues",
    "authors": [
      "Xiaoyu Xiong",
      "Changyu Hu",
      "Chunru Lin",
      "Pingchuan Ma",
      "Chuang Gan",
      "Tao Du"
    ],
    "abstract": "We present TopoGaussian, a holistic, particle-based pipeline for inferring the interior structure of an opaque object from easily accessible photos and videos as input. Traditional mesh-based approaches require tedious and error-prone mesh filling and fixing process, while typically output rough boundary surface. Our pipeline combines Gaussian Splatting with a novel, versatile particle-based differentiable simulator that simultaneously accommodates constitutive model, actuator, and collision, without interference with mesh. Based on the gradients from this simulator, we provide flexible choice of topology representation for optimization, including particle, neural implicit surface, and quadratic surface. The resultant pipeline takes easily accessible photos and videos as input and outputs the topology that matches the physical characteristics of the input. We demonstrate the efficacy of our pipeline on a synthetic dataset and four real-world tasks with 3D-printed prototypes. Compared with existing mesh-based method, our pipeline is 5.26x faster on average with improved shape quality. These results highlight the potential of our pipeline in 3D vision, soft robotics, and manufacturing applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.12343v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12343v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "face",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images",
    "authors": [
      "Tengfei Wang",
      "Yongmao Hou",
      "Zhaoning Zhang",
      "Yiwei Xu",
      "Zongqian Zhan",
      "Xin Wang"
    ],
    "abstract": "Accurate geometric surface reconstruction, providing essential environmental information for navigation and manipulation tasks, is critical for enabling robotic self-exploration and interaction. Recently, 3D Gaussian Splatting (3DGS) has gained significant attention in the field of surface reconstruction due to its impressive geometric quality and computational efficiency. While recent relevant advancements in novel view synthesis under inconsistent illumination using 3DGS have shown promise, the challenge of robust surface reconstruction under such conditions is still being explored. To address this challenge, we propose a method called GS-3I. Specifically, to mitigate 3D Gaussian optimization bias caused by underexposed regions in single-view images, based on Convolutional Neural Network (CNN), a tone mapping correction framework is introduced. Furthermore, inconsistent lighting across multi-view images, resulting from variations in camera settings and complex scene illumination, often leads to geometric constraint mismatches and deviations in the reconstructed surface. To overcome this, we propose a normal compensation mechanism that integrates reference normals extracted from single-view image with normals computed from multi-view observations to effectively constrain geometric inconsistencies. Extensive experimental evaluations demonstrate that GS-3I can achieve robust and accurate surface reconstruction across complex illumination scenarios, highlighting its effectiveness and versatility in this critical challenge. https://github.com/TFwang-9527/GS-3I",
    "arxiv_url": "http://arxiv.org/abs/2503.12335v2",
    "pdf_url": "http://arxiv.org/pdf/2503.12335v2",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/TFwang-9527/GS-3I",
    "keywords": [
      "lighting",
      "face",
      "mapping",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Zhiyan Wang",
      "Lu Xiao",
      "Luyang Tang",
      "Jinbo Yan",
      "Kaiqiang Xiong",
      "Ronggang Wang"
    ],
    "abstract": "Novel view synthesis has long been a practical but challenging task, although the introduction of numerous methods to solve this problem, even combining advanced representations like 3D Gaussian Splatting, they still struggle to recover high-quality results and often consume too much storage memory and training time. In this paper we propose Swift4D, a divide-and-conquer 3D Gaussian Splatting method that can handle static and dynamic primitives separately, achieving a good trade-off between rendering quality and efficiency, motivated by the fact that most of the scene is the static primitive and does not require additional dynamic properties. Concretely, we focus on modeling dynamic transformations only for the dynamic primitives which benefits both efficiency and quality. We first employ a learnable decomposition strategy to separate the primitives, which relies on an additional parameter to classify primitives as static or dynamic. For the dynamic primitives, we employ a compact multi-resolution 4D Hash mapper to transform these primitives from canonical space into deformation space at each timestamp, and then mix the static and dynamic primitives to produce the final output. This divide-and-conquer method facilitates efficient training and reduces storage redundancy. Our method not only achieves state-of-the-art rendering quality while being 20X faster in training than previous SOTA methods with a minimum storage requirement of only 30MB on real-world datasets. Code is available at https://github.com/WuJH2001/swift4d.",
    "arxiv_url": "http://arxiv.org/abs/2503.12307v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12307v1",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/WuJH2001/swift4d",
    "keywords": [
      "efficient",
      "fast",
      "deformation",
      "4d",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REdiSplats: Ray Tracing for Editable Gaussian Splatting",
    "authors": [
      "Krzysztof Byrski",
      "Grzegorz Wilczy≈Ñski",
      "Weronika Smolak-Dy≈ºewska",
      "Piotr Borycki",
      "Dawid Baran",
      "S≈Çawomir Tadeja",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) has become one of the most important neural rendering algorithms. GS represents 3D scenes using Gaussian components with trainable color and opacity. This representation achieves high-quality renderings with fast inference. Regrettably, it is challenging to integrate such a solution with varying light conditions, including shadows and light reflections, manual adjustments, and a physical engine. Recently, a few approaches have appeared that incorporate ray-tracing or mesh primitives into GS to address some of these caveats. However, no such solution can simultaneously solve all the existing limitations of the classical GS. Consequently, we introduce REdiSplats, which employs ray tracing and a mesh-based representation of flat 3D Gaussians. In practice, we model the scene using flat Gaussian distributions parameterized by the mesh. We can leverage fast ray tracing and control Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats allows modeling of light conditions, manual adjustments, and physical simulation. Furthermore, we can render our models using 3D tools such as Blender or Nvdiffrast, which opens the possibility of integrating them with all existing 3D graphics techniques dedicated to mesh representations.",
    "arxiv_url": "http://arxiv.org/abs/2503.12284v1",
    "pdf_url": "http://arxiv.org/pdf/2503.12284v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "reflection",
      "shadow",
      "fast",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction",
    "authors": [
      "Peizhen Zheng",
      "Longfei Wei",
      "Dongjing Jiang",
      "Jianfei Zhang"
    ],
    "abstract": "The accurate reconstruction of dynamic street scenes is critical for applications in autonomous driving, augmented reality, and virtual reality. Traditional methods relying on dense point clouds and triangular meshes struggle with moving objects, occlusions, and real-time processing constraints, limiting their effectiveness in complex urban environments. While multi-view stereo and neural radiance fields have advanced 3D reconstruction, they face challenges in computational efficiency and handling scene dynamics. This paper proposes a novel 3D Gaussian point distribution method for dynamic street scene reconstruction. Our approach introduces an adaptive transparency mechanism that eliminates moving objects while preserving high-fidelity static scene details. Additionally, iterative refinement of Gaussian point distribution enhances geometric accuracy and texture representation. We integrate directional encoding with spatial position optimization to optimize storage and rendering efficiency, reducing redundancy while maintaining scene integrity. Experimental results demonstrate that our method achieves high reconstruction quality, improved rendering performance, and adaptability in large-scale dynamic environments. These contributions establish a robust framework for real-time, high-precision 3D reconstruction, advancing the practicality of dynamic scene modeling across multiple applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.12001v3",
    "pdf_url": "http://arxiv.org/pdf/2503.12001v3",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "autonomous driving",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting",
    "authors": [
      "Utkarsh Nath",
      "Rajeev Goel",
      "Rahul Khurana",
      "Kyle Min",
      "Mark Ollila",
      "Pavan Turaga",
      "Varun Jampani",
      "Tejaswi Gowda"
    ],
    "abstract": "Text-to-3D generation saw dramatic advances in recent years by leveraging Text-to-Image models. However, most existing techniques struggle with compositional prompts, which describe multiple objects and their spatial relationships. They often fail to capture fine-grained inter-object interactions. We introduce DecompDreamer, a Gaussian splatting-based training routine designed to generate high-quality 3D compositions from such complex prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose scenes into structured components and their relationships. We propose a progressive optimization strategy that first prioritizes joint relationship modeling before gradually shifting toward targeted object refinement. Our qualitative and quantitative evaluations against state-of-the-art text-to-3D models demonstrate that DecompDreamer effectively generates intricate 3D compositions with superior object disentanglement, offering enhanced control and flexibility in 3D generation. Project page : https://decompdreamer3d.github.io",
    "arxiv_url": "http://arxiv.org/abs/2503.11981v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11981v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes",
    "authors": [
      "Runfa Blark Li",
      "Mahdi Shaghaghi",
      "Keito Suzuki",
      "Xinshuang Liu",
      "Varun Moparthi",
      "Bang Du",
      "Walker Curtis",
      "Martin Renschler",
      "Ki Myung Brian Lee",
      "Nikolay Atanasov",
      "Truong Nguyen"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is one of the most important environment-perception and navigation algorithms for computer vision, robotics, and autonomous cars/drones. Hence, high quality and fast mapping becomes a fundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an explicit representation with excellent rendering quality and speed, state-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical pointcloud-SLAM, GS-SLAM generates photometric information by learning from input camera views and synthesize unseen views with high-quality textures. However, these GS-SLAM fail when moving objects occupy the scene that violate the static assumption of bundle adjustment. The failed updates of moving GS affects the static GS and contaminates the full map over long frames. Although some efforts have been made by concurrent works to consider moving objects for GS-SLAM, they simply detect and remove the moving regions from GS rendering (\"anti'' dynamic GS-SLAM), where only the static background could benefit from GS. To this end, we propose the first real-time GS-SLAM, \"DynaGSLAM'', that achieves high-quality online GS rendering, tracking, motion predictions of moving objects in dynamic scenes while jointly estimating accurate ego motion. Our DynaGSLAM outperforms SOTA static & \"Anti'' dynamic GS-SLAM on three dynamic real datasets, while keeping speed and memory efficiency in practice.",
    "arxiv_url": "http://arxiv.org/abs/2503.11979v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11979v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "tracking",
      "motion",
      "fast",
      "mapping",
      "high quality",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars",
    "authors": [
      "Eric M. Chen",
      "Di Liu",
      "Sizhuo Ma",
      "Michael Vasilkovsky",
      "Bing Zhou",
      "Qiang Gao",
      "Wenzhou Wang",
      "Jiahao Luo",
      "Dimitris N. Metaxas",
      "Vincent Sitzmann",
      "Jian Wang"
    ],
    "abstract": "The increasing popularity of personalized avatar systems, such as Snapchat Bitmojis and Apple Memojis, highlights the growing demand for digital self-representation. Despite their widespread use, existing avatar platforms face significant limitations, including restricted expressivity due to predefined assets, tedious customization processes, or inefficient rendering requirements. Addressing these shortcomings, we introduce Snapmoji, an avatar generation system that instantly creates animatable, dual-stylized avatars from a selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on large-scale Gaussian models using 3D data from sources such as Objaverse and fine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This enables Snapmoji to transform a selfie into a primary stylized avatar, like the Bitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all while preserving the user's identity and the primary style's integrity. Our system is capable of producing 3D Gaussian avatars that support dynamic animation, including accurate facial expression transfer. Designed for efficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds and supports real-time interactions on mobile devices at 30 to 40 frames per second. Extensive testing confirms that Snapmoji outperforms existing methods in versatility and speed, making it a convenient tool for automatic avatar creation in various styles.",
    "arxiv_url": "http://arxiv.org/abs/2503.11978v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11978v1",
    "published_date": "2025-03-15",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "efficient rendering",
      "face",
      "3d gaussian",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advancing 3D Gaussian Splatting Editing with Complementary and Consensus Information",
    "authors": [
      "Xuanqi Zhang",
      "Jieun Lee",
      "Chris Joslin",
      "Wonsook Lee"
    ],
    "abstract": "We present a novel framework for enhancing the visual fidelity and consistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing editing approaches face two critical challenges: inconsistent geometric reconstructions across multiple viewpoints, particularly in challenging camera positions, and ineffective utilization of depth information during image manipulation, resulting in over-texture artifacts and degraded object boundaries. To address these limitations, we introduce: 1) A complementary information mutual learning network that enhances depth map estimation from 3DGS, enabling precise depth-conditioned 3D editing while preserving geometric structures. 2) A wavelet consensus attention mechanism that effectively aligns latent codes during the diffusion denoising process, ensuring multi-view consistency in the edited results. Through extensive experimentation, our method demonstrates superior performance in rendering quality and view consistency compared to state-of-the-art approaches. The results validate our framework as an effective solution for text-guided editing of 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.11601v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11601v1",
    "published_date": "2025-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Neural Network Architecture Based on Attention Gate Mechanism for 3D Magnetotelluric Forward Modeling",
    "authors": [
      "Xin Zhong",
      "Weiwei Ling",
      "Kejia Pan",
      "Pinxia Wu",
      "Jiajing Zhang",
      "Zhiliang Zhan",
      "Wenbo Xiao"
    ],
    "abstract": "Traditional three-dimensional magnetotelluric (MT) numerical forward modeling methods, such as the finite element method (FEM) and finite volume method (FVM), suffer from high computational costs and low efficiency due to limitations in mesh refinement and computational resources. We propose a novel neural network architecture named MTAGU-Net, which integrates an attention gating mechanism for 3D MT forward modeling. Specifically, a dual-path attention gating module is designed based on forward response data images and embedded in the skip connections between the encoder and decoder. This module enables the fusion of critical anomaly information from shallow feature maps during the decoding of deep feature maps, significantly enhancing the network's capability to extract features from anomalous regions. Furthermore, we introduce a synthetic model generation method utilizing 3D Gaussian random field (GRF), which accurately replicates the electrical structures of real-world geological scenarios with high fidelity. Numerical experiments demonstrate that MTAGU-Net outperforms conventional 3D U-Net in terms of convergence stability and prediction accuracy, with the structural similarity index (SSIM) of the forward response data consistently exceeding 0.98. Moreover, the network can accurately predict forward response data on previously unseen datasets models, demonstrating its strong generalization ability and validating the feasibility and effectiveness of this method in practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.11408v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11408v1",
    "published_date": "2025-03-14",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language Embedded 3D Gaussian Splatting",
    "authors": [
      "Di Li",
      "Jie Feng",
      "Jiahao Chen",
      "Weisheng Dong",
      "Guanbin Li",
      "Guangming Shi",
      "Licheng Jiao"
    ],
    "abstract": "Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic interactions compared to typical scene understanding tasks. Occlusions and varied viewpoints can lead to multi-view semantic inconsistencies, while dynamic objects may act as transient distractors, introducing artifacts into semantic feature modeling. To address these challenges, we propose EgoSplat, a language-embedded 3D Gaussian Splatting framework for open-vocabulary egocentric scene understanding. A multi-view consistent instance feature aggregation method is designed to leverage the segmentation and tracking capabilities of SAM2 to selectively aggregate complementary features across views for each instance, ensuring precise semantic representation of scenes. Additionally, an instance-aware spatial-temporal transient prediction module is constructed to improve spatial integrity and temporal continuity in predictions by incorporating spatial-temporal associations across multi-view instances, effectively reducing artifacts in the semantic reconstruction of egocentric scenes. EgoSplat achieves state-of-the-art performance in both localization and segmentation tasks on two datasets, outperforming existing methods with a 8.2% improvement in localization accuracy and a 3.7% improvement in segmentation mIoU on the ADT dataset, and setting a new benchmark in open-vocabulary egocentric scene understanding. The code will be made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2503.11345v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11345v1",
    "published_date": "2025-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation",
    "authors": [
      "Xianming Zeng",
      "Sicong Du",
      "Qifeng Chen",
      "Lizhe Liu",
      "Haoyu Shu",
      "Jiaxuan Gao",
      "Jiarun Liu",
      "Jiulong Xu",
      "Jianyun Xu",
      "Mingxia Chen",
      "Yiru Zhao",
      "Peng Chen",
      "Yapeng Xue",
      "Chunming Zhao",
      "Sheng Yang",
      "Qiang Li"
    ],
    "abstract": "Sensor simulation is pivotal for scalable validation of autonomous driving systems, yet existing Neural Radiance Fields (NeRF) based methods face applicability and efficiency challenges in industrial workflows. This paper introduces a Gaussian Splatting (GS) based system to address these challenges: We first break down sensor simulator components and analyze the possible advantages of GS over NeRF. Then in practice, we refactor three crucial components through GS, to leverage its explicit scene representation and real-time rendering: (1) choosing the 2D neural Gaussian representation for physics-compliant scene and sensor modeling, (2) proposing a scene editing pipeline to leverage Gaussian primitives library for data augmentation, and (3) coupling a controllable diffusion model for scene expansion and harmonization. We implement this framework on a proprietary autonomous driving dataset supporting cameras and LiDAR sensors. We demonstrate through ablation studies that our approach reduces frame-wise simulation latency, achieves better geometric and photometric consistency, and enables interpretable explicit scene editing and expansion. Furthermore, we showcase how integrating such a GS-based sensor simulator with traffic and dynamic simulators enables full-stack testing of end-to-end autonomy algorithms. Our work provides both algorithmic insights and practical validation, establishing GS as a cornerstone for industrial-grade sensor simulation.",
    "arxiv_url": "http://arxiv.org/abs/2503.11731v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11731v1",
    "published_date": "2025-03-14",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "face",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface Reconstruction from Sparse Image Sequences",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Jinpu Zhang",
      "Lei Feng",
      "Dewen Hu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in novel view synthesis. However, its efficacy diminishes considerably in sparse image sequences, where inherent data sparsity amplifies geometric uncertainty during optimization. This often leads to convergence at suboptimal local minima, resulting in noticeable structural artifacts in the reconstructed scenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided Gaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial Uncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS pipeline. UNG-GS enables high-fidelity rendering and achieves high-precision reconstruction without relying on priors. Specifically, we first integrate Gaussian-based probabilistic modeling into the training of 3DGS to optimize the SUF, providing the model with adaptive error tolerance. An uncertainty-aware depth rendering strategy is then employed to weight depth contributions based on the SUF, effectively reducing noise while preserving fine details. Furthermore, an uncertainty-guided normal refinement method adjusts the influence of neighboring depth values in normal estimation, promoting robust results. Extensive experiments demonstrate that UNG-GS significantly outperforms state-of-the-art methods in both sparse and dense sequences. The code will be open-source.",
    "arxiv_url": "http://arxiv.org/abs/2503.11172v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11172v1",
    "published_date": "2025-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion Priors",
    "authors": [
      "Avinash Paliwal",
      "Xilong Zhou",
      "Wei Ye",
      "Jinhui Xiong",
      "Rakesh Ranjan",
      "Nima Khademi Kalantari"
    ],
    "abstract": "In this paper, we propose RI3D, a novel 3DGS-based approach that harnesses the power of diffusion models to reconstruct high-quality novel views given a sparse set of input images. Our key contribution is separating the view synthesis process into two tasks of reconstructing visible regions and hallucinating missing regions, and introducing two personalized diffusion models, each tailored to one of these tasks. Specifically, one model ('repair') takes a rendered image as input and predicts the corresponding high-quality image, which in turn is used as a pseudo ground truth image to constrain the optimization. The other model ('inpainting') primarily focuses on hallucinating details in unobserved areas. To integrate these models effectively, we introduce a two-stage optimization strategy: the first stage reconstructs visible areas using the repair model, and the second stage reconstructs missing regions with the inpainting model while ensuring coherence through further optimization. Moreover, we augment the optimization with a novel Gaussian initialization method that obtains per-image depth by combining 3D-consistent and smooth depth with highly detailed relative depth. We demonstrate that by separating the process into two tasks and addressing them with the repair and inpainting models, we produce results with detailed textures in both visible and missing regions that outperform state-of-the-art approaches on a diverse set of scenes with extremely sparse inputs.",
    "arxiv_url": "http://arxiv.org/abs/2503.10860v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10860v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds",
    "authors": [
      "Lingteng Qiu",
      "Xiaodong Gu",
      "Peihao Li",
      "Qi Zuo",
      "Weichao Shen",
      "Junfei Zhang",
      "Kejie Qiu",
      "Weihao Yuan",
      "Guanying Chen",
      "Zilong Dong",
      "Liefeng Bo"
    ],
    "abstract": "Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.",
    "arxiv_url": "http://arxiv.org/abs/2503.10625v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10625v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "face",
      "deformation",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction",
    "authors": [
      "Yingshuang Zou",
      "Yikang Ding",
      "Chuanrui Zhang",
      "Jiazhe Guo",
      "Bohan Li",
      "Xiaoyang Lyu",
      "Feiyang Tan",
      "Xiaojuan Qi",
      "Haoqian Wang"
    ],
    "abstract": "Recent breakthroughs in radiance fields have significantly advanced 3D scene reconstruction and novel view synthesis (NVS) in autonomous driving. Nevertheless, critical limitations persist: reconstruction-based methods exhibit substantial performance deterioration under significant viewpoint deviations from training trajectories, while generation-based techniques struggle with temporal coherence and precise scene controllability. To overcome these challenges, we present MuDG, an innovative framework that integrates Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and geometric priors to condition a multi-modal video diffusion model, synthesizing photorealistic RGB, depth, and semantic outputs for novel viewpoints. This synthesis pipeline enables feed-forward NVS without computationally intensive per-scene optimization, providing comprehensive supervision signals to refine 3DGS representations for rendering robustness enhancement under extreme viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG outperforms existing methods in both reconstruction and synthesis quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.10604v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10604v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models",
    "authors": [
      "Wanhua Li",
      "Renping Zhou",
      "Jiawei Zhou",
      "Yingwei Song",
      "Johannes Herter",
      "Minghan Qin",
      "Gao Huang",
      "Hanspeter Pfister"
    ],
    "abstract": "Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.",
    "arxiv_url": "http://arxiv.org/abs/2503.10437v2",
    "pdf_url": "http://arxiv.org/pdf/2503.10437v2",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and Camera Estimation from Unposed Video Frames",
    "authors": [
      "Zhiqi Li",
      "Chengrui Dong",
      "Yiming Chen",
      "Zhangchi Huang",
      "Peidong Liu"
    ],
    "abstract": "We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction and camera pose estimation from a sequence of unposed video frames, which is a critical yet underexplored task in real-world 3D applications. The core of our method lies in a novel transformer-based network architecture. In particular, our model starts with an image encoder that maps each image to a list of visual tokens. All visual tokens are concatenated with additional inserted learnable camera tokens. The obtained tokens then fully communicate with each other within a tailored transformer decoder. The camera tokens causally aggregate features from visual tokens of different views, and further modulate them frame-wisely to inject view-dependent features. 3D Gaussian splats and camera pose parameters can then be estimated via different prediction heads. Experiments show that VicaSplat surpasses baseline methods for multi-view inputs, and achieves comparable performance to prior two-view approaches. Remarkably, VicaSplat also demonstrates exceptional cross-dataset generalization capability on the ScanNet benchmark, achieving superior performance without any fine-tuning. Project page: https://lizhiqi49.github.io/VicaSplat.",
    "arxiv_url": "http://arxiv.org/abs/2503.10286v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10286v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROODI: Reconstructing Occluded Objects with Denoising Inpainters",
    "authors": [
      "Yeonjin Chang",
      "Erqun Dong",
      "Seunghyeon Seo",
      "Nojun Kwak",
      "Kwang Moo Yi"
    ],
    "abstract": "While the quality of novel-view images has improved dramatically with 3D Gaussian Splatting, extracting specific objects from scenes remains challenging. Isolating individual 3D Gaussian primitives for each object and handling occlusions in scenes remain far from being solved. We propose a novel object extraction method based on two key principles: (1) being object-centric by pruning irrelevant primitives; and (2) leveraging generative inpainting to compensate for missing observations caused by occlusions. For pruning, we analyze the local structure of primitives using K-nearest neighbors, and retain only relevant ones. For inpainting, we employ an off-the-shelf diffusion-based inpainter combined with occlusion reasoning, utilizing the 3D representation of the entire scene. Our findings highlight the crucial synergy between pruning and inpainting, both of which significantly enhance extraction performance. We evaluate our method on a standard real-world dataset and introduce a synthetic dataset for quantitative analysis. Our approach outperforms the state-of-the-art, demonstrating its effectiveness in object extraction from complex scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.10256v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10256v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for Geometrically Consistent Rendering and Reconstruction",
    "authors": [
      "Jianheng Liu",
      "Yunfei Wan",
      "Bowen Wang",
      "Chunran Zheng",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "abstract": "Digital twins are fundamental to the development of autonomous driving and embodied artificial intelligence. However, achieving high-granularity surface reconstruction and high-fidelity rendering remains a challenge. Gaussian splatting offers efficient photorealistic rendering but struggles with geometric inconsistencies due to fragmented primitives and sparse observational data in robotics applications. Existing regularization methods, which rely on render-derived constraints, often fail in complex environments. Moreover, effectively integrating sparse LiDAR data with Gaussian splatting remains challenging. We propose a unified LiDAR-visual system that synergizes Gaussian splatting with a neural signed distance field. The accurate LiDAR point clouds enable a trained neural signed distance field to offer a manifold geometry field, This motivates us to offer an SDF-based Gaussian initialization for physically grounded primitive placement and a comprehensive geometric regularization for geometrically consistent rendering and reconstruction. Experiments demonstrate superior reconstruction accuracy and rendering quality across diverse trajectories. To benefit the community, the codes will be released at https://github.com/hku-mars/GS-SDF.",
    "arxiv_url": "http://arxiv.org/abs/2503.10170v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10170v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "https://github.com/hku-mars/GS-SDF",
    "keywords": [
      "robotics",
      "efficient",
      "high-fidelity",
      "autonomous driving",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Student Splatting and Scooping",
    "authors": [
      "Jialin Zhu",
      "Jiangbei Yue",
      "Feixiang He",
      "He Wang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%.",
    "arxiv_url": "http://arxiv.org/abs/2503.10148v4",
    "pdf_url": "http://arxiv.org/pdf/2503.10148v4",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping",
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Bo Li",
      "Dan Xu"
    ],
    "abstract": "High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR scenes by leveraging multi-view low dynamic range (LDR) images captured at different exposure levels. Current training paradigms with 3D tone mapping often result in unstable HDR reconstruction, while training with 2D tone mapping reduces the model's capacity to fit LDR images. Additionally, the global tone mapper used in existing methods can impede the learning of both HDR and LDR representations. To address these challenges, we present GaussHDR, which unifies 3D and 2D local tone mapping through 3D Gaussian splatting. Specifically, we design a residual local tone mapper for both 3D and 2D tone mapping that accepts an additional context feature as input. We then propose combining the dual LDR rendering results from both 3D and 2D local tone mapping at the loss level. Finally, recognizing that different scenes may exhibit varying balances between the dual results, we introduce uncertainty learning and use the uncertainties for adaptive modulation. Extensive experiments demonstrate that GaussHDR significantly outperforms state-of-the-art methods in both synthetic and real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2503.10143v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10143v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AI-assisted 3D Preservation and Reconstruction of Temple Arts",
    "authors": [
      "Naai-Jung Shih"
    ],
    "abstract": "How does AI connect to the past in conservation? What can 17 years old photos be helpful in a renewed effort of preservation? This research aims to use AI to connect both in a seamless 3D reconstruction of heritage from imagery data taken from Gongfan Palace, Yunlin Taiwan. AI-assisted 3D modeling was used to reconstruct correspondent details across different 3D platforms of 3DGS or NeRF models generated by Postshot or KIRI Engine. Polygon or point models by Zephyr were referred to and assessed in two sets. The results also include AI-assist modeling outcomes in Stable Diffusion and Postshot-based animation. The evolved documenta-tion and interpretation in AI presents a novel arrangement of working processes contributed by new structure and management of resources, formats, and interfaces, as a continuous preservation effort.",
    "arxiv_url": "http://arxiv.org/abs/2503.10031v1",
    "pdf_url": "http://arxiv.org/pdf/2503.10031v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.GR",
      "F.2.2; I.2.7"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "face",
      "3d reconstruction",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TGP: Two-modal occupancy prediction with 3D Gaussian and sparse points for 3D Environment Awareness",
    "authors": [
      "Mu Chen",
      "Wenyu Chen",
      "Mingchuan Yang",
      "Yuan Zhang",
      "Tao Han",
      "Xinchi Li",
      "Yunlong Li",
      "Huaici Zhao"
    ],
    "abstract": "3D semantic occupancy has rapidly become a research focus in the fields of robotics and autonomous driving environment perception due to its ability to provide more realistic geometric perception and its closer integration with downstream tasks. By performing occupancy prediction of the 3D space in the environment, the ability and robustness of scene understanding can be effectively improved. However, existing occupancy prediction tasks are primarily modeled using voxel or point cloud-based approaches: voxel-based network structures often suffer from the loss of spatial information due to the voxelization process, while point cloud-based methods, although better at retaining spatial location information, face limitations in representing volumetric structural details. To address this issue, we propose a dual-modal prediction method based on 3D Gaussian sets and sparse points, which balances both spatial location and volumetric structural information, achieving higher accuracy in semantic occupancy prediction. Specifically, our method adopts a Transformer-based architecture, taking 3D Gaussian sets, sparse points, and queries as inputs. Through the multi-layer structure of the Transformer, the enhanced queries and 3D Gaussian sets jointly contribute to the semantic occupancy prediction, and an adaptive fusion mechanism integrates the semantic outputs of both modalities to generate the final prediction results. Additionally, to further improve accuracy, we dynamically refine the point cloud at each layer, allowing for more precise location information during occupancy prediction. We conducted experiments on the Occ3DnuScenes dataset, and the experimental results demonstrate superior performance of the proposed method on IoU based metrics.",
    "arxiv_url": "http://arxiv.org/abs/2503.09941v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09941v1",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "autonomous driving",
      "face",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation",
    "authors": [
      "M√°t√© T√≥th",
      "P√©ter Kov√°cs",
      "Zolt√°n Bendefy",
      "Zolt√°n Hortsin",
      "Bal√°zs Ter√©ki",
      "Tam√°s Matuszka"
    ],
    "abstract": "Neural reconstruction models for autonomous driving simulation have made significant strides in recent years, with dynamic models becoming increasingly prevalent. However, these models are typically limited to handling in-domain objects closely following their original trajectories. We introduce a hybrid approach that combines the strengths of neural reconstruction with physics-based rendering. This method enables the virtual placement of traditional mesh-based dynamic agents at arbitrary locations, adjustments to environmental conditions, and rendering from novel camera viewpoints. Our approach significantly enhances novel view synthesis quality -- especially for road surfaces and lane markings -- while maintaining interactive frame rates through our novel training method, NeRF2GS. This technique leverages the superior generalization capabilities of NeRF-based methods and the real-time rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a customized NeRF model on the original images with depth regularization derived from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS training. This process ensures accurate depth, surface normals, and camera appearance modeling as supervision. With our block-based training parallelization, the method can handle large-scale reconstructions (greater than or equal to 100,000 square meters) and predict segmentation masks, surface normals, and depth maps. During simulation, it supports a rasterization-based rendering backend with depth-based composition and multiple camera models for real-time camera simulation, as well as a ray-traced backend for precise LiDAR simulation.",
    "arxiv_url": "http://arxiv.org/abs/2503.09464v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09464v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "face",
      "segmentation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Online Language Splatting",
    "authors": [
      "Saimouli Katragadda",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Guoquan Huang",
      "Liu Ren"
    ],
    "abstract": "To enable AI agents to interact seamlessly with both humans and 3D environments, they must not only perceive the 3D world accurately but also align human language with 3D spatial representations. While prior work has made significant progress by integrating language features into geometrically detailed 3D scene representations using 3D Gaussian Splatting (GS), these approaches rely on computationally intensive offline preprocessing of language features for each input image, limiting adaptability to new environments. In this work, we introduce Online Language Splatting, the first framework to achieve online, near real-time, open-vocabulary language mapping within a 3DGS-SLAM system without requiring pre-generated language features. The key challenge lies in efficiently fusing high-dimensional language features into 3D representations while balancing the computation speed, memory usage, rendering quality and open-vocabulary capability. To this end, we innovatively design: (1) a high-resolution CLIP embedding module capable of generating detailed language feature maps in 18ms per frame, (2) a two-stage online auto-encoder that compresses 768-dimensional CLIP features to 15 dimensions while preserving open-vocabulary capabilities, and (3) a color-language disentangled optimization approach to improve rendering quality. Experimental results show that our online method not only surpasses the state-of-the-art offline methods in accuracy but also achieves more than 40x efficiency boost, demonstrating the potential for dynamic and interactive AI applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.09447v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09447v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "mapping",
      "3d gaussian",
      "human",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
    "authors": [
      "Jiatong Xia",
      "Lingqiao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in synthesizing novel views after training on a given set of viewpoints. However, its rendering quality deteriorates when the synthesized view deviates significantly from the training views. This decline occurs due to (1) the model's difficulty in generalizing to out-of-distribution scenarios and (2) challenges in interpolating fine details caused by substantial resolution changes and occlusions. A notable case of this limitation is close-up view generation--producing views that are significantly closer to the object than those in the training set. To tackle this issue, we propose a novel approach for close-up view generation based by progressively training the 3DGS model with self-generated data. Our solution is based on three key ideas. First, we leverage the See3D model, a recently introduced 3D-aware generative model, to enhance the details of rendered views. Second, we propose a strategy to progressively expand the ``trust regions'' of the 3DGS model and update a set of reference views for See3D. Finally, we introduce a fine-tuning strategy to carefully update the 3DGS model with training data generated from the above schemes. We further define metrics for close-up views evaluation to facilitate better research on this problem. By conducting evaluations on specifically selected scenarios for close-up views, our proposed approach demonstrates a clear advantage over competitive solutions.",
    "arxiv_url": "http://arxiv.org/abs/2503.09396v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09396v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GASPACHO: Gaussian Splatting for Controllable Humans and Objects",
    "authors": [
      "Aymen Mir",
      "Arthur Moreau",
      "Helisa Dhamo",
      "Zhensong Zhang",
      "Eduardo P√©rez-Pellitero"
    ],
    "abstract": "We present GASPACHO: a method for generating photorealistic controllable renderings of human-object interactions. Given a set of multi-view RGB images of human-object interactions, our method reconstructs animatable templates of the human and object as separate sets of Gaussians simultaneously. Different from existing work, which focuses on human reconstruction and ignores objects as background, our method explicitly reconstructs both humans and objects, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. During reconstruction, we constrain the Gaussians that generate rendered images to be a linear function of a set of canonical Gaussians. By simply changing the parameters of the linear deformation functions after training, our method can generate renderings of novel human-object interaction in novel poses from novel camera viewpoints. We learn the 3D Gaussian properties of the canonical Gaussians on the underlying 2D manifold of the canonical human and object templates. This in turns requires a canonical object template with a fixed UV unwrapping. To define such an object template, we use a feature based representation to track the object across the multi-view sequence. We further propose an occlusion aware photometric loss that allows for reconstructions under significant occlusions. Several experiments on two human-object datasets - BEHAVE and DNA-Rendering - demonstrate that our method allows for high-quality reconstruction of human and object templates under significant occlusion and the synthesis of controllable renderings of novel human-object interactions in novel human poses from novel camera views.",
    "arxiv_url": "http://arxiv.org/abs/2503.09342v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09342v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
    "authors": [
      "Dai Sun",
      "Huhao Guan",
      "Kun Zhang",
      "Xike Xie",
      "S. Kevin Zhou"
    ],
    "abstract": "Dynamic and static components in scenes often exhibit distinct properties, yet most 4D reconstruction methods treat them indiscriminately, leading to suboptimal performance in both cases. This work introduces SDD-4DGS, the first framework for static-dynamic decoupled 4D scene reconstruction based on Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic perception coefficient that is naturally integrated into the Gaussian reconstruction pipeline, enabling adaptive separation of static and dynamic components. With carefully designed implementation strategies to realize this theoretical framework, our method effectively facilitates explicit learning of motion patterns for dynamic elements while maintaining geometric stability for static structures. Extensive experiments on five benchmark datasets demonstrate that SDD-4DGS consistently outperforms state-of-the-art methods in reconstruction fidelity, with enhanced detail restoration for static structures and precise modeling of dynamic motions. The code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2503.09332v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09332v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Better Together: Unified Motion Capture and 3D Avatar Reconstruction",
    "authors": [
      "Arthur Moreau",
      "Mohammed Brahimi",
      "Richard Shaw",
      "Athanasios Papaioannou",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Eduardo P√©rez-Pellitero"
    ],
    "abstract": "We present Better Together, a method that simultaneously solves the human pose estimation problem while reconstructing a photorealistic 3D human avatar from multi-view videos. While prior art usually solves these problems separately, we argue that joint optimization of skeletal motion with a 3D renderable body model brings synergistic effects, i.e. yields more precise motion capture and improved visual quality of real-time rendering of avatars. To achieve this, we introduce a novel animatable avatar with 3D Gaussians rigged on a personalized mesh and propose to optimize the motion sequence with time-dependent MLPs that provide accurate and temporally consistent pose estimates. We first evaluate our method on highly challenging yoga poses and demonstrate state-of-the-art accuracy on multi-view human pose estimation, reducing error by 35% on body joints and 45% on hand joints compared to keypoint-based methods. At the same time, our method significantly boosts the visual quality of animatable avatars (+2dB PSNR on novel view synthesis) on diverse challenging subjects.",
    "arxiv_url": "http://arxiv.org/abs/2503.09293v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09293v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "body",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-Aware Human-Object Rendering from Sparse Views via 3D Gaussian Splatting",
    "authors": [
      "Weiquan Wang",
      "Jun Xiao",
      "Yueting Zhuang",
      "Long Chen"
    ],
    "abstract": "Rendering realistic human-object interactions (HOIs) from sparse-view inputs is challenging due to occlusions and incomplete observations, yet crucial for various real-world applications. Existing methods always struggle with either low rendering qualities (\\eg, visual fidelity and physically plausible HOIs) or high computational costs. To address these limitations, we propose HOGS (Human-Object Rendering via 3D Gaussian Splatting), a novel framework for efficient and physically plausible HOI rendering from sparse views. Specifically, HOGS combines 3D Gaussian Splatting with a physics-aware optimization process. It incorporates a Human Pose Refinement module for accurate pose estimation and a Sparse-View Human-Object Contact Prediction module for efficient contact region identification. This combination enables coherent joint rendering of human and object Gaussians while enforcing physically plausible interactions. Extensive experiments on the HODome dataset demonstrate that HOGS achieves superior rendering quality, efficiency, and physical plausibility compared to existing methods. We further show its extensibility to hand-object grasp rendering tasks, presenting its broader applicability to articulated object interactions.",
    "arxiv_url": "http://arxiv.org/abs/2503.09640v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09640v1",
    "published_date": "2025-03-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "efficient",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction",
    "authors": [
      "Xinyu Zhang",
      "Haonan Chang",
      "Yuhan Liu",
      "Abdeslam Boularias"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.",
    "arxiv_url": "http://arxiv.org/abs/2503.09040v2",
    "pdf_url": "http://arxiv.org/pdf/2503.09040v2",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting",
    "authors": [
      "GeonU Kim",
      "Kim Youwang",
      "Lee Hyoseok",
      "Tae-Hyun Oh"
    ],
    "abstract": "We present FPGS, a feed-forward photorealistic style transfer method of large-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view consistency and real-time rendering speed of 3D Gaussians. Prior arts required tedious per-style optimization or time-consuming per-scene training stage and were limited to small-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D feature field, which inherits AdaIN's feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPGS supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPGS also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPGS achieves favorable photorealistic quality scene stylization for large-scale static and dynamic 3D scenes with diverse reference images. Project page: https://kim-geonu.github.io/FPGS/",
    "arxiv_url": "http://arxiv.org/abs/2503.09635v1",
    "pdf_url": "http://arxiv.org/pdf/2503.09635v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PCGS: Progressive Compression of 3D Gaussian Splatting",
    "authors": [
      "Yihang Chen",
      "Mengyao Li",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves impressive rendering fidelity and speed for novel view synthesis. However, its substantial data size poses a significant challenge for practical applications. While many compression techniques have been proposed, they fail to efficiently utilize existing bitstreams in on-demand applications due to their lack of progressivity, leading to a waste of resource. To address this issue, we propose PCGS (Progressive Compression of 3D Gaussian Splatting), which adaptively controls both the quantity and quality of Gaussians (or anchors) to enable effective progressivity for on-demand applications. Specifically, for quantity, we introduce a progressive masking strategy that incrementally incorporates new anchors while refining existing ones to enhance fidelity. For quality, we propose a progressive quantization approach that gradually reduces quantization step sizes to achieve finer modeling of Gaussian attributes. Furthermore, to compact the incremental bitstreams, we leverage existing quantization results to refine probability prediction, improving entropy coding efficiency across progressive levels. Overall, PCGS achieves progressivity while maintaining compression performance comparable to SoTA non-progressive methods. Code available at: github.com/YihangChen-ee/PCGS.",
    "arxiv_url": "http://arxiv.org/abs/2503.08511v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08511v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YihangChen-ee/PCGS",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TT-Occ: Test-Time Compute for Self-Supervised Occupancy via Spatio-Temporal Gaussian Splatting",
    "authors": [
      "Fengyi Zhang",
      "Huitong Yang",
      "Zheng Zhang",
      "Zi Huang",
      "Yadan Luo"
    ],
    "abstract": "Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VLMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VLMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VLMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VLM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions. Code will be available at https://github.com/Xian-Bei/TT-Occ.",
    "arxiv_url": "http://arxiv.org/abs/2503.08485v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08485v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Xian-Bei/TT-Occ",
    "keywords": [
      "recognition",
      "tracking",
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
    "authors": [
      "Ruiqi Zhang",
      "Hao Zhu",
      "Jingyi Zhao",
      "Qi Zhang",
      "Xun Cao",
      "Zhan Ma"
    ],
    "abstract": "3D classification with point cloud input is a fundamental problem in 3D vision. However, due to the discrete nature and the insufficient material description of point cloud representations, there are ambiguities in distinguishing wire-like and flat surfaces, as well as transparent or reflective objects. To address these issues, we propose Gaussian Splatting (GS) point cloud-based 3D classification. We find that the scale and rotation coefficients in the GS point cloud help characterize surface types. Specifically, wire-like surfaces consist of multiple slender Gaussian ellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids. Additionally, the opacity in the GS point cloud represents the transparency characteristics of objects. As a result, ambiguities in point cloud-based 3D classification can be mitigated utilizing GS point cloud as input. To verify the effectiveness of GS point cloud input, we construct the first real-world GS point cloud dataset in the community, which includes 20 categories with 200 objects in each category. Experiments not only validate the superiority of GS point cloud input, especially in distinguishing ambiguous objects, but also demonstrate the generalization ability across different classification methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.08352v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08352v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios",
    "authors": [
      "Zikang Yuan",
      "Yuechuan Pu",
      "Hongcheng Luo",
      "Fengtian Lang",
      "Cheng Chi",
      "Teng Li",
      "Yingying Shen",
      "Haiyang Sun",
      "Bing Wang",
      "Xin Yang"
    ],
    "abstract": "Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2503.08317v3",
    "pdf_url": "http://arxiv.org/pdf/2503.08317v3",
    "published_date": "2025-03-11",
    "categories": [
      "cs.RO",
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals",
    "authors": [
      "Jonas Elsborg",
      "Luca Thiede",
      "Al√°n Aspuru-Guzik",
      "Tejs Vegge",
      "Arghya Bhowmik"
    ],
    "abstract": "We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2503.08305v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08305v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.LG",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRAvatar: High-Quality and Relightable Gaussian Head Avatar",
    "authors": [
      "Dongbin Zhang",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Kangjie Chen",
      "Minghan Qin",
      "Yu Li",
      "Haoqian Wang"
    ],
    "abstract": "Reconstructing animatable and high-quality 3D head avatars from monocular videos, especially with realistic relighting, is a valuable task. However, the limited information from single-view input, combined with the complex head poses and facial movements, makes this challenging. Previous methods achieve real-time performance by combining 3D Gaussian Splatting with a parametric head model, but the resulting head quality suffers from inaccurate face tracking and limited expressiveness of the deformation model. These methods also fail to produce realistic effects under novel lighting conditions. To address these issues, we propose HRAvatar, a 3DGS-based method that reconstructs high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors through end-to-end optimization and better captures individual facial deformations using learnable blendshapes and learnable linear blend skinning. Additionally, it decomposes head appearance into several physical properties and incorporates physically-based shading to account for environmental lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs superior-quality heads but also achieves realistic visual effects under varying lighting conditions.",
    "arxiv_url": "http://arxiv.org/abs/2503.08224v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08224v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "relightable",
      "tracking",
      "relighting",
      "lighting",
      "face",
      "deformation",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVD-HuGaS: Human Gaussians from a Single Image via 3D Human Multi-view Diffusion Prior",
    "authors": [
      "Kaiqiang Xiong",
      "Ying Feng",
      "Qi Zhang",
      "Jianbo Jiao",
      "Yang Zhao",
      "Zhihao Liang",
      "Huachen Gao",
      "Ronggang Wang"
    ],
    "abstract": "3D human reconstruction from a single image is a challenging problem and has been exclusively studied in the literature. Recently, some methods have resorted to diffusion models for guidance, optimizing a 3D representation via Score Distillation Sampling(SDS) or generating one back-view image for facilitating reconstruction. However, these methods tend to produce unsatisfactory artifacts (\\textit{e.g.} flattened human structure or over-smoothing results caused by inconsistent priors from multiple views) and struggle with real-world generalization in the wild. In this work, we present \\emph{MVD-HuGaS}, enabling free-view 3D human rendering from a single image via a multi-view human diffusion model. We first generate multi-view images from the single reference image with an enhanced multi-view diffusion model, which is well fine-tuned on high-quality 3D human datasets to incorporate 3D geometry priors and human structure priors. To infer accurate camera poses from the sparse generated multi-view images for reconstruction, an alignment module is introduced to facilitate joint optimization of 3D Gaussians and camera poses. Furthermore, we propose a depth-based Facial Distortion Mitigation module to refine the generated facial regions, thereby improving the overall fidelity of the reconstruction.Finally, leveraging the refined multi-view images, along with their accurate camera poses, MVD-HuGaS optimizes the 3D Gaussians of the target human for high-fidelity free-view renderings. Extensive experiments on Thuman2.0 and 2K2K datasets show that the proposed MVD-HuGaS achieves state-of-the-art performance on single-view 3D human rendering.",
    "arxiv_url": "http://arxiv.org/abs/2503.08218v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08218v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction",
    "authors": [
      "Guangting Zheng",
      "Jiajun Deng",
      "Xiaomeng Chu",
      "Yu Yuan",
      "Houqiang Li",
      "Yanyong Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of photorealistic 3D reconstruction, achieving impressive rendering quality and speed. However, when applied to large-scale street scenes, existing methods suffer from rapidly escalating per-viewpoint reconstruction costs as scene size increases, leading to significant computational overhead. After revisiting the conventional pipeline, we identify three key factors accounting for this issue: unnecessary local-to-global transformations, excessive 3D-to-2D projections, and inefficient rendering of distant content. To address these challenges, we propose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scale Street Scene Reconstruction, effectively mitigating these limitations. Moreover, most existing street 3DGS methods rely on ground-truth 3D bounding boxes to separate dynamic and static components, but 3D bounding boxes are difficult to obtain, limiting real-world applicability. To address this, we propose an alternative solution with 2D boxes, which are easier to annotate or can be predicted by off-the-shelf vision foundation models. Such designs together make S3R-GS readily adapt to large, in-the-wild scenarios. Extensive experiments demonstrate that S3R-GS enhances rendering quality and significantly accelerates reconstruction. Remarkably, when applied to videos from the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR and SSIM, reducing reconstruction time to below 50%--and even 20%--of competing methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.08217v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08217v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and Streaming",
    "authors": [
      "Jiaxuan Zhu",
      "Hao Tang"
    ],
    "abstract": "Representing and rendering dynamic scenes from 2D images is a fundamental yet challenging problem in computer vision and graphics. This survey provides a comprehensive review of the evolution and advancements in dynamic scene representation and rendering, with a particular emphasis on recent progress in Neural Radiance Fields based and 3D Gaussian Splatting based reconstruction methods. We systematically summarize existing approaches, categorize them according to their core principles, compile relevant datasets, compare the performance of various methods on these benchmarks, and explore the challenges and future research directions in this rapidly evolving field. In total, we review over 170 relevant papers, offering a broad perspective on the state of the art in this domain.",
    "arxiv_url": "http://arxiv.org/abs/2503.08166v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08166v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
    "authors": [
      "Junfu Guo",
      "Yu Xin",
      "Gaoyi Liu",
      "Kai Xu",
      "Ligang Liu",
      "Ruizhen Hu"
    ],
    "abstract": "We tackle the challenge of concurrent reconstruction at the part level with the RGB appearance and estimation of motion parameters for building digital twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method. With two distinct sets of multi-view imagery, each depicting an object in separate static articulation configurations, we reconstruct the articulated object in 3D Gaussian representations with both appearance and geometry information at the same time. Our approach decoupled multiple highly interdependent parameters through a multi-step optimization process, thereby achieving a stable optimization procedure and high-quality outcomes. We introduce ArticulatedGS, a self-supervised, comprehensive framework that autonomously learns to model shapes and appearances at the part level and synchronizes the optimization of motion parameters, all without reliance on 3D supervision, motion cues, or semantic labels. Our experimental results demonstrate that, among comparable methodologies, our approach has achieved optimal outcomes in terms of part segmentation accuracy, motion estimation accuracy, and visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.08135v1",
    "pdf_url": "http://arxiv.org/pdf/2503.08135v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction",
    "authors": [
      "Chenfeng Hou",
      "Qi Xun Yeo",
      "Mengqi Guo",
      "Yongxin Su",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its high-quality rendering capabilities, ultra-fast training, and inference speeds. However, when we apply 3DGS to surface reconstruction tasks, especially in environments with dynamic objects and distractors, the method suffers from floating artifacts and color errors due to inconsistency from different viewpoints. To address this challenge, we propose Multi-View Consistency Gaussian Splatting for the domain of Robust Surface Reconstruction (\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a {heuristics-guided distractor masking} strategy for robust surface reconstruction in non-static environments. Compared to existing methods that rely on MLPs for distractor segmentation strategies, our approach separates distractors from static scene elements by comparing multi-view feature consistency, allowing us to obtain precise distractor masks early in training. Furthermore, we introduce a pruning measure based on multi-view contributions to reset transmittance, effectively reducing floating artifacts. Finally, a multi-view consistency loss is applied to achieve high-quality performance in surface reconstruction tasks. Experimental results demonstrate that MVGSR achieves competitive geometric accuracy and rendering fidelity compared to the state-of-the-art surface reconstruction algorithms. More information is available on our project page (https://mvgsr.github.io).",
    "arxiv_url": "http://arxiv.org/abs/2503.08093v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08093v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "segmentation",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats",
    "authors": [
      "Kai Deng",
      "Yigong Zhang",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "Tracking and mapping in large-scale, unbounded outdoor environments using only monocular RGB input presents substantial challenges for existing SLAM systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) SLAM methods are typically limited to small, bounded indoor settings. To overcome these challenges, we introduce GigaSLAM, the first RGB NeRF / 3DGS-based SLAM framework for kilometer-scale outdoor environments, as demonstrated on the KITTI, KITTI 360, 4 Seasons and A2D2 datasets. Our approach employs a hierarchical sparse voxel map representation, where Gaussians are decoded by neural networks at multiple levels of detail. This design enables efficient, scalable mapping and high-fidelity viewpoint rendering across expansive, unbounded scenes. For front-end tracking, GigaSLAM utilizes a metric depth model combined with epipolar geometry and PnP algorithms to accurately estimate poses, while incorporating a Bag-of-Words-based loop closure mechanism to maintain robust alignment over long trajectories. Consequently, GigaSLAM delivers high-precision tracking and visually faithful rendering on urban outdoor benchmarks, establishing a robust SLAM solution for large-scale, long-term scenarios, and significantly extending the applicability of Gaussian Splatting SLAM systems to unbounded outdoor environments. GitHub: https://github.com/DengKaiCQ/GigaSLAM.",
    "arxiv_url": "http://arxiv.org/abs/2503.08071v2",
    "pdf_url": "http://arxiv.org/pdf/2503.08071v2",
    "published_date": "2025-03-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "https://github.com/DengKaiCQ/GigaSLAM",
    "keywords": [
      "efficient",
      "high-fidelity",
      "tracking",
      "outdoor",
      "mapping",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a fundamental challenge in computer graphics. While recent advances in Gaussian Splatting have shown promising results separately handling dynamic scenes (4DGS) and view-dependent effects (6DGS), no existing method unifies these capabilities while maintaining real-time performance. We present 7D Gaussian Splatting (7DGS), a unified framework representing scene elements as seven-dimensional Gaussians spanning position (3D), time (1D), and viewing direction (3D). Our key contribution is an efficient conditional slicing mechanism that transforms 7D Gaussians into view- and time-conditioned 3D Gaussians, maintaining compatibility with existing 3D Gaussian Splatting pipelines while enabling joint optimization. Experiments demonstrate that 7DGS outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time rendering (401 FPS) on challenging dynamic scenes with complex view-dependent effects. The project page is: https://gaozhongpai.github.io/7dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2503.07946v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07946v1",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality",
    "authors": [
      "Joey Wilson",
      "Marcelino Almeida",
      "Sachit Mahajan",
      "Martin Labrie",
      "Maani Ghaffari",
      "Omid Ghasemalizadeh",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnab Sen"
    ],
    "abstract": "In this paper, we present a novel algorithm for quantifying uncertainty and information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality. While 3D-GS has proven to be a useful world model with high-quality rasterizations, it does not natively quantify uncertainty or information, posing a challenge for real-world applications such as 3D-GS SLAM. We propose to quantify information gain in 3D-GS by reformulating the problem through the lens of optimal experimental design, which is a classical solution widely used in literature. By restructuring information quantification of 3D-GS through optimal experimental design, we arrive at multiple solutions, of which T-Optimality and D-Optimality perform the best quantitatively and qualitatively as measured on two popular datasets. Additionally, we propose a block diagonal covariance approximation which provides a measure of correlation at the expense of a greater computation cost.",
    "arxiv_url": "http://arxiv.org/abs/2503.07819v2",
    "pdf_url": "http://arxiv.org/pdf/2503.07819v2",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "slam",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting",
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "Anchor-based 3D Gaussian splatting (3D-GS) exploits anchor features in 3D Gaussian prediction, which has achieved impressive 3D rendering quality with reduced Gaussian redundancy. On the other hand, it often encounters the dilemma among anchor features, model size, and rendering quality - large anchor features lead to large 3D models and high-quality rendering whereas reducing anchor features degrades Gaussian attribute prediction which leads to clear artifacts in the rendered textures and geometries. We design SOGS, an anchor-based 3D-GS technique that introduces second-order anchors to achieve superior rendering quality and reduced anchor features and model size simultaneously. Specifically, SOGS incorporates covariance-based second-order statistics and correlation across feature dimensions to augment features within each anchor, compensating for the reduced feature size and improving rendering quality effectively. In addition, it introduces a selective gradient loss to enhance the optimization of scene textures and scene geometries, leading to high-quality rendering with small anchor features. Extensive experiments over multiple widely adopted benchmarks show that SOGS achieves superior rendering quality in novel view synthesis with clearly reduced model size.",
    "arxiv_url": "http://arxiv.org/abs/2503.07476v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07476v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EigenGS Representation: From Eigenspace to Gaussian Image Space",
    "authors": [
      "Lo-Wei Tai",
      "Ching-En Li",
      "Cheng-Lin Chen",
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ],
    "abstract": "Principal Component Analysis (PCA), a classical dimensionality reduction technique, and 2D Gaussian representation, an adaptation of 3D Gaussian Splatting for image representation, offer distinct approaches to modeling visual data. We present EigenGS, a novel method that bridges these paradigms through an efficient transformation pipeline connecting eigenspace and image-space Gaussian representations. Our approach enables instant initialization of Gaussian parameters for new images without requiring per-image optimization from scratch, dramatically accelerating convergence. EigenGS introduces a frequency-aware learning mechanism that encourages Gaussians to adapt to different scales, effectively modeling varied spatial frequencies and preventing artifacts in high-resolution reconstruction. Extensive experiments demonstrate that EigenGS not only achieves superior reconstruction quality compared to direct 2D Gaussian fitting but also reduces necessary parameter count and training time. The results highlight EigenGS's effectiveness and generalization ability across images with varying resolutions and diverse categories, making Gaussian-based image representation both high-quality and viable for real-time applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.07446v2",
    "pdf_url": "http://arxiv.org/pdf/2503.07446v2",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian Splatting",
    "authors": [
      "Yan Ren",
      "Shilin Lu",
      "Adams Wai-Kin Kong"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene reconstruction, opening new possibilities for 3D steganography by hiding 3D secrets within 3D covers. The key challenge in steganography is ensuring imperceptibility while maintaining high-fidelity reconstruction. However, existing methods often suffer from detectability risks and utilize only suboptimal 3DGS features, limiting their full potential. We propose a novel end-to-end key-secured 3D steganography framework (KeySS) that jointly optimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our approach reveals that Gaussian features contribute unequally to secret hiding. The framework incorporates a key-controllable mechanism enabling multi-secret hiding and unauthorized access prevention, while systematically exploring optimal feature update to balance fidelity and security. To rigorously evaluate steganographic imperceptibility beyond conventional 2D metrics, we introduce 3D-Sinkhorn distance analysis, which quantifies distributional differences between original and steganographic Gaussian parameters in the representation space. Extensive experiments demonstrate that our method achieves state-of-the-art performance in both cover and secret reconstruction while maintaining high security levels, advancing the field of 3D steganography. Code is available at https://github.com/RY-Paper/KeySS",
    "arxiv_url": "http://arxiv.org/abs/2503.07191v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07191v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.GR",
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "https://github.com/RY-Paper/KeySS",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Modal 3D Mesh Reconstruction from Images and Text",
    "authors": [
      "Melvin Reka",
      "Tessa Pulli",
      "Markus Vincze"
    ],
    "abstract": "6D object pose estimation for unseen objects is essential in robotics but traditionally relies on trained models that require large datasets, high computational costs, and struggle to generalize. Zero-shot approaches eliminate the need for training but depend on pre-existing 3D object models, which are often impractical to obtain. To address this, we propose a language-guided few-shot 3D reconstruction method, reconstructing a 3D mesh from few input images. In the proposed pipeline, receives a set of input images and a language query. A combination of GroundingDINO and Segment Anything Model outputs segmented masks from which a sparse point cloud is reconstructed with VGGSfM. Subsequently, the mesh is reconstructed with the Gaussian Splatting method SuGAR. In a final cleaning step, artifacts are removed, resulting in the final 3D mesh of the queried object. We evaluate the method in terms of accuracy and quality of the geometry and texture. Furthermore, we study the impact of imaging conditions such as viewing angle, number of input images, and image overlap on 3D object reconstruction quality, efficiency, and computational scalability.",
    "arxiv_url": "http://arxiv.org/abs/2503.07190v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07190v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "few-shot",
      "geometry",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-Aware Density Control via Reparameterization for High-Quality Rendering of 3D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Lili Ju",
      "Tao Guan"
    ],
    "abstract": "By adaptively controlling the density and generating more Gaussians in regions with high-frequency information, 3D Gaussian Splatting (3DGS) can better represent scene details. From the signal processing perspective, representing details usually needs more Gaussians with relatively smaller scales. However, 3DGS currently lacks an explicit constraint linking the density and scale of 3D Gaussians across the domain, leading to 3DGS using improper-scale Gaussians to express frequency information, resulting in the loss of accuracy. In this paper, we propose to establish a direct relation between density and scale through the reparameterization of the scaling parameters and ensure the consistency between them via explicit constraints (i.e., density responds well to changes in frequency). Furthermore, we develop a frequency-aware density control strategy, consisting of densification and deletion, to improve representation quality with fewer Gaussians. A dynamic threshold encourages densification in high-frequency regions, while a scale-based filter deletes Gaussians with improper scale. Experimental results on various datasets demonstrate that our method outperforms existing state-of-the-art methods quantitatively and qualitatively.",
    "arxiv_url": "http://arxiv.org/abs/2503.07000v1",
    "pdf_url": "http://arxiv.org/pdf/2503.07000v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation",
    "authors": [
      "Xiaoliang Ju",
      "Hongsheng Li"
    ],
    "abstract": "We present DirectTriGS, a novel framework designed for 3D object generation with Gaussian Splatting (GS). GS-based rendering for 3D content has gained considerable attention recently. However, there has been limited exploration in directly generating 3D Gaussians compared to traditional generative modeling approaches. The main challenge lies in the complex data structure of GS represented by discrete point clouds with multiple channels. To overcome this challenge, we propose employing the triplane representation, which allows us to represent Gaussian Splatting as an image-like continuous field. This representation effectively encodes both the geometry and texture information, enabling smooth transformation back to Gaussian point clouds and rendering into images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is fully differentiable, so that the rendering loss can supervise both texture and geometry encoding. Furthermore, the triplane representation can be compressed using a Variational Autoencoder (VAE), which can subsequently be utilized in latent diffusion to generate 3D objects. The experiments demonstrate that the proposed generation framework can produce high-quality 3D object geometry and rendering results in the text-to-3D task.",
    "arxiv_url": "http://arxiv.org/abs/2503.06900v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06900v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting",
    "authors": [
      "Konstantinos D. Polyzos",
      "Athanasios Bacharis",
      "Saketh Madhuvarasu",
      "Nikos Papanikolopoulos",
      "Tara Javidi"
    ],
    "abstract": "Gaussian splatting (GS) along with its extensions and variants provides outstanding performance in real-time scene rendering while meeting reduced storage demands and computational efficiency. While the selection of 2D images capturing the scene of interest is crucial for the proper initialization and training of GS, hence markedly affecting the rendering performance, prior works rely on passively and typically densely selected 2D images. In contrast, this paper proposes `ActiveInitSplat', a novel framework for active selection of training images for proper initialization and training of GS. ActiveInitSplat relies on density and occupancy criteria of the resultant 3D scene representation from the selected 2D images, to ensure that the latter are captured from diverse viewpoints leading to better scene coverage and that the initialized Gaussian functions are well aligned with the actual 3D structure. Numerical tests on well-known simulated and real environments demonstrate the merits of ActiveInitSplat resulting in significant GS rendering performance improvement over passive GS baselines, in the widely adopted LPIPS, SSIM, and PSNR metrics.",
    "arxiv_url": "http://arxiv.org/abs/2503.06859v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06859v1",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain Generation by Latent Fractal Diffusion Models",
    "authors": [
      "Tianyi Zhang",
      "Weiming Zhi",
      "Joshua Mangelson",
      "Matthew Johnson-Roberson"
    ],
    "abstract": "This paper tackles the problem of generating representations of underwater 3D terrain. Off-the-shelf generative models, trained on Internet-scale data but not on specialized underwater images, exhibit downgraded realism, as images of the seafloor are relatively uncommon. To this end, we introduce DreamSea, a generative model to generate hyper-realistic underwater scenes. DreamSea is trained on real-world image databases collected from underwater robot surveys. Images from these surveys contain massive real seafloor observations and covering large areas, but are prone to noise and artifacts from the real world. We extract 3D geometry and semantics from the data with visual foundation models, and train a diffusion model that generates realistic seafloor images in RGBD channels, conditioned on novel fractal distribution-based latent embeddings. We then fuse the generated images into a 3D map, building a 3DGS model supervised by 2D diffusion priors which allows photorealistic novel view rendering. DreamSea is rigorously evaluated, demonstrating the ability to robustly generate large-scale underwater scenes that are consistent, diverse, and photorealistic. Our work drives impact in multiple domains, spanning filming, gaming, and robot simulation.",
    "arxiv_url": "http://arxiv.org/abs/2503.06784v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06784v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "survey",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields",
    "authors": [
      "Abdelaziz Bouzidi",
      "Hamid Laga",
      "Hazem Wannous"
    ],
    "abstract": "Neural fields such as DeepSDF and Neural Radiance Fields have recently revolutionized novel-view synthesis and 3D reconstruction from RGB images and videos. However, achieving high-quality representation, reconstruction, and rendering requires deep neural networks, which are slow to train and evaluate. Although several acceleration techniques have been proposed, they often trade off speed for memory. Gaussian splatting-based methods, on the other hand, accelerate the rendering time but remain costly in terms of training speed and memory needed to store the parameters of a large number of Gaussians. In this paper, we introduce a novel neural representation that is fast, both at training and inference times, and lightweight. Our key observation is that the neurons used in traditional MLPs perform simple computations (a dot product followed by ReLU activation) and thus one needs to use either wide and deep MLPs or high-resolution and high-dimensional feature grids to parameterize complex nonlinear functions. We show in this paper that by replacing traditional neurons with Radial Basis Function (RBF) kernels, one can achieve highly accurate representation of 2D (RGB images), 3D (geometry), and 5D (radiance fields) signals with just a single layer of such neurons. The representation is highly parallelizable, operates on low-resolution feature grids, and is compact and memory-efficient. We demonstrate that the proposed novel representation can be trained for 3D geometry representation in less than 15 seconds and for novel view synthesis in less than 15 mins. At runtime, it can synthesize novel views at more than 60 fps without sacrificing quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.06762v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06762v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "geometry",
      "acceleration",
      "3d reconstruction",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving",
    "authors": [
      "Rui Song",
      "Chenwei Liang",
      "Yan Xia",
      "Walter Zimmer",
      "Hu Cao",
      "Holger Caesar",
      "Andreas Festag",
      "Alois Knoll"
    ],
    "abstract": "Dynamic scene rendering opens new avenues in autonomous driving by enabling closed-loop simulations with photorealistic data, which is crucial for validating end-to-end algorithms. However, the complex and highly dynamic nature of traffic environments presents significant challenges in accurately rendering these scenes. In this paper, we introduce a novel 4D Gaussian Splatting (4DGS) approach, which incorporates context and temporal deformation awareness to improve dynamic scene rendering. Specifically, we employ a 2D semantic segmentation foundation model to self-supervise the 4D semantic features of Gaussians, ensuring meaningful contextual embedding. Simultaneously, we track the temporal deformation of each Gaussian across adjacent frames. By aggregating and encoding both semantic and temporal deformation features, each Gaussian is equipped with cues for potential deformation compensation within 3D space, facilitating a more precise representation of dynamic scenes. Experimental results show that our method improves 4DGS's ability to capture fine details in dynamic scene rendering for autonomous driving and outperforms other self-supervised methods in 4D reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms semantic features with each Gaussian, enabling broader applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.06744v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06744v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "deformation",
      "segmentation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D3DR: Lighting-Aware Object Insertion in Gaussian Splatting",
    "authors": [
      "Vsevolod Skorokhodov",
      "Nikita Durasov",
      "Pascal Fua"
    ],
    "abstract": "Gaussian Splatting has become a popular technique for various 3D Computer Vision tasks, including novel view synthesis, scene reconstruction, and dynamic scene rendering. However, the challenge of natural-looking object insertion, where the object's appearance seamlessly matches the scene, remains unsolved. In this work, we propose a method, dubbed D3DR, for inserting a 3DGS-parametrized object into 3DGS scenes while correcting its lighting, shadows, and other visual artifacts to ensure consistency, a problem that has not been successfully addressed before. We leverage advances in diffusion models, which, trained on real-world data, implicitly understand correct scene lighting. After inserting the object, we optimize a diffusion-based Delta Denoising Score (DDS)-inspired objective to adjust its 3D Gaussian parameters for proper lighting correction. Utilizing diffusion model personalization techniques to improve optimization quality, our approach ensures seamless object insertion and natural appearance. Finally, we demonstrate the method's effectiveness by comparing it to existing approaches, achieving 0.5 PSNR and 0.15 SSIM improvements in relighting quality.",
    "arxiv_url": "http://arxiv.org/abs/2503.06740v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06740v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "shadow",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Zhou Linli",
      "Anran Huang",
      "Liangtu Song",
      "Qiaojun Yu",
      "Qi Wu",
      "Cewu Lu"
    ],
    "abstract": "Articulated objects, as prevalent entities in human life, their 3D representations play crucial roles across various applications. However, achieving both high-fidelity textured surface reconstruction and dynamic generation for articulated objects remains challenging for existing methods. In this paper, we present REArtGS, a novel framework that introduces additional geometric and motion constraints to 3D Gaussian primitives, enabling high-quality textured surface reconstruction and generation for articulated objects. Specifically, given multi-view RGB images of arbitrary two states of articulated objects, we first introduce an unbiased Signed Distance Field (SDF) guidance to regularize Gaussian opacity fields, enhancing geometry constraints and improving surface reconstruction quality. Then we establish deformable fields for 3D Gaussians constrained by the kinematic structures of articulated objects, achieving unsupervised generation of surface meshes in unseen states. Extensive experiments on both synthetic and real datasets demonstrate our approach achieves high-quality textured surface reconstruction for given states, and enables high-fidelity surface generation for unseen states. Codes will be released after acceptance and the project website is at https://sites.google.com/view/reartgs/home.",
    "arxiv_url": "http://arxiv.org/abs/2503.06677v3",
    "pdf_url": "http://arxiv.org/pdf/2503.06677v3",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "face",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D Gaussian Modeling",
    "authors": [
      "Long Peng",
      "Anran Wu",
      "Wenbo Li",
      "Peizhe Xia",
      "Xueyuan Dai",
      "Xinjie Zhang",
      "Xin Di",
      "Haoze Sun",
      "Renjing Pei",
      "Yang Wang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "abstract": "Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs with arbitrary upsampling factors using a single model, addressing the limitations of traditional SR methods constrained to fixed-scale factors (\\textit{e.g.}, $\\times$ 2). Recent advances leveraging implicit neural representation (INR) have achieved great progress by modeling coordinate-to-pixel mappings. However, the efficiency of these methods may suffer from repeated upsampling and decoding, while their reconstruction fidelity and quality are constrained by the intrinsic representational limitations of coordinate-based functions. To address these challenges, we propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which explicitly reconstructs 2D continuous HR signals from LR images using Gaussian Splatting. This approach eliminates the need for time-consuming upsampling and decoding, enabling extremely fast arbitrary-scale super-resolution. Once the Gaussian field is built in a single pass, ContinuousSR can perform arbitrary-scale rendering in just 1ms per scale. Our method introduces several key innovations. Through statistical ana",
    "arxiv_url": "http://arxiv.org/abs/2503.06617v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06617v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "fast",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction",
    "authors": [
      "Xiaoming Peng",
      "Yixin Yang",
      "Yang Zhou",
      "Hui Huang"
    ],
    "abstract": "Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We found the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectified the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS.",
    "arxiv_url": "http://arxiv.org/abs/2503.06587v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06587v2",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts.",
    "arxiv_url": "http://arxiv.org/abs/2503.06462v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06462v1",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FloPE: Flower Pose Estimation for Precision Pollination",
    "authors": [
      "Rashik Shrestha",
      "Madhav Rijal",
      "Trevor Smith",
      "Yu Gu"
    ],
    "abstract": "This study presents Flower Pose Estimation (FloPE), a real-time flower pose estimation framework for computationally constrained robotic pollination systems. Robotic pollination has been proposed to supplement natural pollination to ensure global food security due to the decreased population of natural pollinators. However, flower pose estimation for pollination is challenging due to natural variability, flower clusters, and high accuracy demands due to the flowers' fragility when pollinating. This method leverages 3D Gaussian Splatting to generate photorealistic synthetic datasets with precise pose annotations, enabling effective knowledge distillation from a high-capacity teacher model to a lightweight student model for efficient inference. The approach was evaluated on both single and multi-arm robotic platforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees within a low computational cost. Our experiments validate the effectiveness of FloPE, achieving up to 78.75% pollination success rate and outperforming prior robotic pollination techniques.",
    "arxiv_url": "http://arxiv.org/abs/2503.11692v1",
    "pdf_url": "http://arxiv.org/pdf/2503.11692v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatTalk: 3D VQA with Gaussian Splatting",
    "authors": [
      "Anh Thai",
      "Songyou Peng",
      "Kyle Genova",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ],
    "abstract": "Language-guided 3D scene understanding is important for advancing applications in robotics, AR/VR, and human-computer interaction, enabling models to comprehend and interact with 3D environments through natural language. While 2D vision-language models (VLMs) have achieved remarkable success in 2D VQA tasks, progress in the 3D domain has been significantly slower due to the complexity of 3D data and the high cost of manual annotations. In this work, we introduce SplatTalk, a novel method that uses a generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens suitable for direct input into a pretrained LLM, enabling effective zero-shot 3D visual question answering (3D VQA) for scenes with only posed images. During experiments on multiple benchmarks, our approach outperforms both 3D models trained specifically for the task and previous 2D-LMM-based models utilizing only images (our setting), while achieving competitive performance with state-of-the-art 3D LMMs that additionally utilize 3D inputs.",
    "arxiv_url": "http://arxiv.org/abs/2503.06271v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06271v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "vr",
      "understanding",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams",
    "authors": [
      "Yang LI",
      "Jinglu Wang",
      "Lei Chu",
      "Xiao Li",
      "Shiu-hong Kao",
      "Ying-Cong Chen",
      "Yan Lu"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene reconstruction and novel view synthesis. With the growing interest of interactive applications that need immediate feedback, online 3DGS reconstruction in real-time is in high demand. However, none of existing methods yet meet the demand due to three main challenges: the absence of predetermined camera parameters, the need for generalizable 3DGS optimization, and the necessity of reducing redundancy. We propose StreamGS, an online generalizable 3DGS reconstruction method for unposed image streams, which progressively transform image streams to 3D Gaussian streams by predicting and aggregating per-frame Gaussians. Our method overcomes the limitation of the initial point reconstruction \\cite{dust3r} in tackling out-of-domain (OOD) issues by introducing a content adaptive refinement. The refinement enhances cross-frame consistency by establishing reliable pixel correspondences between adjacent frames. Such correspondences further aid in merging redundant Gaussians through cross-frame feature aggregation. The density of Gaussians is thereby reduced, empowering online reconstruction by significantly lowering computational and memory costs. Extensive experiments on diverse datasets have demonstrated that StreamGS achieves quality on par with optimization-based approaches but does so 150 times faster, and exhibits superior generalizability in handling OOD scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.06235v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06235v2",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ForestSplats: Deformable transient field for Gaussian Splatting in the Wild",
    "authors": [
      "Wongi Park",
      "Myeongseok Nam",
      "Siwon Kim",
      "Sangwoo Jo",
      "Soomok Lee"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time rendering speeds and high-quality results in static scenes. Although 3D-GS shows effectiveness in static scenes, their performance significantly degrades in real-world environments due to transient objects, lighting variations, and diverse levels of occlusion. To tackle this, existing methods estimate occluders or transient elements by leveraging pre-trained models or integrating additional transient field pipelines. However, these methods still suffer from two defects: 1) Using semantic features from the Vision Foundation model (VFM) causes additional computational costs. 2) The transient field requires significant memory to handle transient elements with per-view Gaussians and struggles to define clear boundaries for occluders, solely relying on photometric errors. To address these problems, we propose ForestSplats, a novel approach that leverages the deformable transient field and a superpixel-aware mask to efficiently represent transient elements in the 2D scene across unconstrained image collections and effectively decompose static scenes from transient distractors without VFM. We designed the transient field to be deformable, capturing per-view transient elements. Furthermore, we introduce a superpixel-aware mask that clearly defines the boundaries of occluders by considering photometric errors and superpixels. Additionally, we propose uncertainty-aware densification to avoid generating Gaussians within the boundaries of occluders during densification. Through extensive experiments across several benchmark datasets, we demonstrate that ForestSplats outperforms existing methods without VFM and shows significant memory efficiency in representing transient elements.",
    "arxiv_url": "http://arxiv.org/abs/2503.06179v2",
    "pdf_url": "http://arxiv.org/pdf/2503.06179v2",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "semantic",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction",
    "authors": [
      "Kai Li",
      "Junhao Wang",
      "William Han",
      "Ding Zhao"
    ],
    "abstract": "Minimally invasive surgery (MIS) has transformed clinical practice by reducing recovery times, minimizing complications, and enhancing precision. Nonetheless, MIS inherently relies on indirect visualization and precise instrument control, posing unique challenges. Recent advances in artificial intelligence have enabled real-time surgical scene understanding through techniques such as image classification, object detection, and segmentation, with scene reconstruction emerging as a key element for enhanced intraoperative guidance. Although neural radiance fields (NeRFs) have been explored for this purpose, their substantial data requirements and slow rendering inhibit real-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more efficient alternative, achieving state-of-the-art performance in dynamic surgical scene reconstruction. In this work, we introduce Feature-EndoGaussian (FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D rendering to enable real-time semantic and scene reconstruction. By leveraging pretrained segmentation foundation models, FEG incorporates semantic feature distillation within the Gaussian deformation framework, thereby enhancing both reconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG achieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03) compared to leading methods. Additionally, on the EndoVis18 dataset, FEG demonstrates competitive class-wise segmentation metrics while balancing model size and real-time performance.",
    "arxiv_url": "http://arxiv.org/abs/2503.06161v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06161v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "deformation",
      "understanding",
      "segmentation",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
    "authors": [
      "Ye Tao",
      "Jiawei Zhang",
      "Yahao Shi",
      "Dongqing Zou",
      "Bin Zhou"
    ],
    "abstract": "Image-based 3D generation has vast applications in robotics and gaming, where high-quality, diverse outputs and consistent 3D representations are crucial. However, existing methods have limitations: 3D diffusion models are limited by dataset scarcity and the absence of strong pre-trained priors, while 2D diffusion-based approaches struggle with geometric consistency. We propose a method that leverages 2D diffusion models' implicit 3D reasoning ability while ensuring 3D consistency via Gaussian-splatting-based geometric distillation. Specifically, the proposed Gaussian Splatting Decoder enforces 3D consistency by transforming SV3D latent outputs into an explicit 3D representation. Unlike SV3D, which only relies on implicit 2D representations for video generation, Gaussian Splatting explicitly encodes spatial and appearance attributes, enabling multi-view consistency through geometric constraints. These constraints correct view inconsistencies, ensuring robust geometric consistency. As a result, our approach simultaneously generates high-quality, multi-view-consistent images and accurate 3D models, providing a scalable solution for single-image-based 3D generation and bridging the gap between 2D Diffusion diversity and 3D structural coherence. Experimental results demonstrate state-of-the-art multi-view consistency and strong generalization across diverse datasets. The code will be made publicly available upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2503.06136v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06136v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography",
    "authors": [
      "Xuanyu Zhang",
      "Jiarui Meng",
      "Zhipei Xu",
      "Shuzhou Yang",
      "Yanmin Wu",
      "Ronggang Wang",
      "Jian Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D representation due to its real-time rendering and high-quality outputs, underscoring the critical need to protect the privacy of 3D assets. Traditional NeRF steganography methods fail to address the explicit nature of 3DGS since its point cloud files are publicly accessible. Existing GS steganography solutions mitigate some issues but still struggle with reduced rendering fidelity, increased computational demands, and security flaws, especially in the security of the geometric structure of the visualized point cloud. To address these demands, we propose a SecureGS, a secure and efficient 3DGS steganography framework inspired by Scaffold-GS's anchor point design and neural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanism to embed offsets, scales, rotations, and RGB attributes of the hidden 3D Gaussian points in anchor point features, retrievable only by authorized users through privacy-preserving neural networks. To further enhance security, we propose a density region-aware anchor growing and pruning strategy that adaptively locates optimal hiding regions without exposing hidden information. Extensive experiments show that SecureGS significantly surpasses existing GS steganography methods in rendering fidelity, speed, and security.",
    "arxiv_url": "http://arxiv.org/abs/2503.06118v1",
    "pdf_url": "http://arxiv.org/pdf/2503.06118v1",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting",
    "authors": [
      "Dominic Maggio",
      "Luca Carlone"
    ],
    "abstract": "Open-set semantic mapping requires (i) determining the correct granularity to represent the scene (e.g., how should objects be defined), and (ii) fusing semantic knowledge across multiple 2D observations into an overall 3D reconstruction -ideally with a high-fidelity yet low-memory footprint. While most related works bypass the first issue by grouping together primitives with similar semantics (according to some manually tuned threshold), we recognize that the object granularity is task-dependent, and develop a task-driven semantic mapping approach. To address the second issue, current practice is to average visual embedding vectors over multiple views. Instead, we show the benefits of using a probabilistic approach based on the properties of the underlying visual-language foundation model, and leveraging Bayesian updating to aggregate multiple observations of the scene. The result is Bayesian Fields, a task-driven and probabilistic approach for open-set semantic mapping. To enable high-fidelity objects and a dense scene representation, Bayesian Fields uses 3D Gaussians which we cluster into task-relevant objects, allowing for both easy 3D object extraction and reduced memory usage. We release Bayesian Fields open-source at https: //github.com/MIT-SPARK/Bayesian-Fields.",
    "arxiv_url": "http://arxiv.org/abs/2503.05949v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05949v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/MIT-SPARK/Bayesian-Fields",
    "keywords": [
      "high-fidelity",
      "semantic",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D2GV: Deformable 2D Gaussian Splatting for Video Representation in 400FPS",
    "authors": [
      "Mufan Liu",
      "Qi Yang",
      "Miaoran Zhao",
      "He Huang",
      "Le Yang",
      "Zhu Li",
      "Yiling Xu"
    ],
    "abstract": "Implicit Neural Representations (INRs) have emerged as a powerful approach for video representation, offering versatility across tasks such as compression and inpainting. However, their implicit formulation limits both interpretability and efficacy, undermining their practicality as a comprehensive solution. We propose a novel video representation based on deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key objectives: 1) improved efficiency while delivering superior quality; 2) enhanced scalability and interpretability; and 3) increased friendliness for downstream tasks. Specifically, we initially divide the video sequence into fixed-length Groups of Pictures (GoP) to allow parallel training and linear scalability with video length. For each GoP, D2GV represents video frames by applying differentiable rasterization to 2D Gaussians, which are deformed from a canonical space into their corresponding timestamps. Notably, leveraging efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds exceeding 400 FPS, while delivering quality that matches or surpasses state-of-the-art INRs. Moreover, we incorporate a learnable pruning and quantization strategy to streamline D2GV into a more compact representation. We demonstrate D2GV's versatility in tasks including video interpolation, inpainting and denoising, underscoring its potential as a promising solution for video representation. Code is available at: https://github.com/Evan-sudo/D2GV.",
    "arxiv_url": "http://arxiv.org/abs/2503.05600v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05600v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Evan-sudo/D2GV",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Free Your Hands: Lightweight Relightable Turntable Capture Pipeline",
    "authors": [
      "Jiahui Fan",
      "Fujun Luan",
      "Jian Yang",
      "Milo≈° Ha≈°an",
      "Beibei Wang"
    ],
    "abstract": "Novel view synthesis (NVS) from multiple captured photos of an object is a widely studied problem. Achieving high quality typically requires dense sampling of input views, which can lead to frustrating and tedious manual labor. Manually positioning cameras to maintain an optimal desired distribution can be difficult for humans, and if a good distribution is found, it is not easy to replicate. Additionally, the captured data can suffer from motion blur and defocus due to human error. In this paper, we present a lightweight object capture pipeline to reduce the manual workload and standardize the acquisition setup. We use a consumer turntable to carry the object and a tripod to hold the camera. As the turntable rotates, we automatically capture dense samples from various views and lighting conditions; we can repeat this for several camera positions. This way, we can easily capture hundreds of valid images in several minutes without hands-on effort. However, in the object reference frame, the light conditions vary; this is harmful to a standard NVS method like 3D Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural radiance representation conditioned on light rotations, which addresses this issue and allows relightability as an additional benefit. We demonstrate our pipeline using 3DGS as the underlying framework, achieving competitive quality compared to previous methods with exhaustive acquisition and showcasing its potential for relighting and harmonization tasks.",
    "arxiv_url": "http://arxiv.org/abs/2503.05511v3",
    "pdf_url": "http://arxiv.org/pdf/2503.05511v3",
    "published_date": "2025-03-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "motion",
      "lightweight",
      "high quality",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiDAR-enhanced 3D Gaussian Splatting Mapping",
    "authors": [
      "Jian Shen",
      "Huai Yu",
      "Ji Wu",
      "Wen Yang",
      "Gui-Song Xia"
    ],
    "abstract": "This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting (3DGS) mapping framework that improves the accuracy and robustness of 3D scene mapping by integrating LiDAR data. LiGSM constructs joint loss from images and LiDAR point clouds to estimate the poses and optimize their extrinsic parameters, enabling dynamic adaptation to variations in sensor alignment. Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a denser and more reliable starting points compared to sparse SfM points. In scene rendering, the framework augments standard image-based supervision with depth maps generated from LiDAR projections, ensuring an accurate scene representation in both geometry and photometry. Experiments on public and self-collected datasets demonstrate that LiGSM outperforms comparative methods in pose tracking and scene rendering.",
    "arxiv_url": "http://arxiv.org/abs/2503.05425v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05425v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Modeling Robots by Photographing",
    "authors": [
      "Kejun Hu",
      "Peng Yu",
      "Ning Tan"
    ],
    "abstract": "Self-modeling enables robots to build task-agnostic models of their morphology and kinematics based on data that can be automatically collected, with minimal human intervention and prior information, thereby enhancing machine intelligence. Recent research has highlighted the potential of data-driven technology in modeling the morphology and kinematics of robots. However, existing self-modeling methods suffer from either low modeling quality or excessive data acquisition costs. Beyond morphology and kinematics, texture is also a crucial component of robots, which is challenging to model and remains unexplored. In this work, a high-quality, texture-aware, and link-level method is proposed for robot self-modeling. We utilize three-dimensional (3D) Gaussians to represent the static morphology and texture of robots, and cluster the 3D Gaussians to construct neural ellipsoid bones, whose deformations are controlled by the transformation matrices generated by a kinematic neural network. The 3D Gaussians and kinematic neural network are trained using data pairs composed of joint angles, camera parameters and multi-view images without depth information. By feeding the kinematic neural network with joint angles, we can utilize the well-trained model to describe the corresponding morphology, kinematics and texture of robots at the link level, and render robot images from different perspectives with the aid of 3D Gaussian splatting. Furthermore, we demonstrate that the established model can be exploited to perform downstream tasks such as motion planning and inverse kinematics.",
    "arxiv_url": "http://arxiv.org/abs/2503.05398v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05398v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images",
    "authors": [
      "Jungho Lee",
      "Donghyeong Kim",
      "Dogyoon Lee",
      "Suhwan Cho",
      "Minhyeok Lee",
      "Wonjoon Lee",
      "Taeoh Kim",
      "Dongyoon Wee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for their high-quality novel view rendering, motivating research to address real-world challenges. A critical issue is the camera motion blur caused by movement during exposure, which hinders accurate 3D scene reconstruction. In this study, we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that reconstructs precise 3D scenes from motion-blurred images while maintaining real-time rendering speed. Considering the complex motion patterns inherent in real-world camera movements, we predict continuous camera trajectories using neural ordinary differential equations (ODEs). To ensure accurate modeling, we employ rigid body transformations, preserving the shape and size of the object but rely on the discrete integration of sampled frames. To better approximate the continuous nature of motion blur, we introduce a continuous motion refinement (CMR) transformation that refines rigid transformations by incorporating additional learnable parameters. By revisiting fundamental camera theory and leveraging advanced neural ODE techniques, we achieve precise modeling of continuous camera trajectories, leading to improved reconstruction accuracy. Extensive experiments demonstrate state-of-the-art performance both quantitatively and qualitatively on benchmark datasets, which include a wide range of motion blur scenarios, from moderate to extreme blur.",
    "arxiv_url": "http://arxiv.org/abs/2503.05332v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05332v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "body",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STGA: Selective-Training Gaussian Head Avatars",
    "authors": [
      "Hanzhi Guo",
      "Yixiao Chen",
      "Dongye Xiaonuo",
      "Zeyu Tian",
      "Dongdong Weng",
      "Le Luo"
    ],
    "abstract": "We propose selective-training Gaussian head avatars (STGA) to enhance the details of dynamic head Gaussian. The dynamic head Gaussian model is trained based on the FLAME parameterized model. Each Gaussian splat is embedded within the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before training, our selection strategy calculates the 3D Gaussian splat to be optimized in each frame. The parameters of these 3D Gaussian splats are optimized in the training of each frame, while those of the other splats are frozen. This means that the splats participating in the optimization process differ in each frame, to improve the realism of fine details. Compared with network-based methods, our method achieves better results with shorter training time. Compared with mesh-based methods, our method produces more realistic details within the same training time. Additionally, the ablation experiment confirms that our method effectively enhances the quality of details.",
    "arxiv_url": "http://arxiv.org/abs/2503.05196v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05196v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "head",
      "3d gaussian",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects",
    "authors": [
      "Justin Yu",
      "Kush Hari",
      "Karim El-Refai",
      "Arnav Dalal",
      "Justin Kerr",
      "Chung Min Kim",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "abstract": "Tracking and manipulating irregularly-shaped, previously unseen objects in dynamic environments is important for robotic applications in manufacturing, assembly, and logistics. Recently introduced Gaussian Splats efficiently model object geometry, but lack persistent state estimation for task-oriented manipulation. We present Persistent Object Gaussian Splat (POGS), a system that embeds semantics, self-supervised visual features, and object grouping features into a compact representation that can be continuously updated to estimate the pose of scanned objects. POGS updates object states without requiring expensive rescanning or prior CAD models of objects. After an initial multi-view scene capture and training phase, POGS uses a single stereo camera to integrate depth estimates along with self-supervised vision encoder features for object pose estimation. POGS supports grasping, reorientation, and natural language-driven manipulation by refining object pose estimates, facilitating sequential object reset operations with human-induced object perturbations and tool servoing, where robots recover tool pose despite tool perturbations of up to 30{\\deg}. POGS achieves up to 12 consecutive successful object resets and recovers from 80% of in-grasp tool perturbations.",
    "arxiv_url": "http://arxiv.org/abs/2503.05189v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05189v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "semantic",
      "geometry",
      "human",
      "ar",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions",
    "authors": [
      "Qingyuan Zhou",
      "Yuehu Gong",
      "Weidong Yang",
      "Jiaze Li",
      "Yeqi Luo",
      "Baixin Xu",
      "Shuhao Li",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Novel view synthesis (NVS) and surface reconstruction (SR) are essential tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks are often addressed independently, with GS-based rendering methods struggling under diverse light conditions and failing to produce accurate surfaces, while GS-based reconstruction methods frequently compromise rendering quality. This raises a central question: must rendering and reconstruction always involve a trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian splatting for Surface Reconstruction that enhances both rendering quality and 3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction, providing precise geometry information to the 3D-GS branch. Leveraging this geometry, the 3D-GS branch employs a geometry-guided illumination decomposition module that captures reflected and transmitted components, enabling realistic rendering under varied light conditions. Using the transmitted component as supervision, the 2D-GS branch also achieves high-fidelity surface reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS branches undergo alternating optimization, providing mutual supervision. Prior to this, each branch completes an independent warm-up phase, with an early stopping strategy implemented to reduce computational costs. We evaluate MGSR on a diverse set of synthetic and real-world datasets, at both object and scene levels, demonstrating strong performance in rendering and surface reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2503.05182v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05182v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting",
    "authors": [
      "Linqi Yang",
      "Xiongwei Zhao",
      "Qihao Sun",
      "Ke Wang",
      "Ao Chen",
      "Peng Kang"
    ],
    "abstract": "6-DoF pose estimation is a fundamental task in computer vision with wide-ranging applications in augmented reality and robotics. Existing single RGB-based methods often compromise accuracy due to their reliance on initial pose estimates and susceptibility to rotational ambiguity, while approaches requiring depth sensors or multi-view setups incur significant deployment costs. To address these limitations, we introduce SplatPose, a novel framework that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural architecture to achieve high-precision pose estimation using only a single RGB image. Central to our approach is the Dual-Attention Ray Scoring Network (DARS-Net), which innovatively decouples positional and angular alignment through geometry-domain attention mechanisms, explicitly modeling directional dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine optimization pipeline progressively refines pose estimates by aligning dense 2D features between query images and 3DGS-synthesized views, effectively correcting feature misalignment and depth errors from sparse ray sampling. Experiments on three benchmark datasets demonstrate that SplatPose achieves state-of-the-art 6-DoF pose estimation accuracy in single RGB settings, rivaling approaches that depend on depth or multi-view images.",
    "arxiv_url": "http://arxiv.org/abs/2503.05174v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05174v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting",
    "authors": [
      "Xiaotong Huang",
      "He Zhu",
      "Zihan Liu",
      "Weikai Lin",
      "Xiaohong Liu",
      "Zhezhi He",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a crucial rendering technique for many real-time applications. However, the limited hardware resources on today's mobile platforms hinder these applications, as they struggle to achieve real-time performance. In this paper, we propose SeeLe, a general framework designed to accelerate the 3DGS pipeline for resource-constrained mobile devices.   Specifically, we propose two GPU-oriented techniques: hybrid preprocessing and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU compute and memory pressure by reducing the number of irrelevant Gaussians during rendering. The key is to combine our view-dependent scene representation with online filtering. Meanwhile, contribution-aware rasterization improves the GPU utilization at the rasterization stage by prioritizing Gaussians with high contributions while reducing computations for those with low contributions. Both techniques can be seamlessly integrated into existing 3DGS pipelines with minimal fine-tuning. Collectively, our framework achieves 2.6$\\times$ speedup and 32.3\\% model reduction while achieving superior rendering quality compared to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.05168v2",
    "pdf_url": "http://arxiv.org/pdf/2503.05168v2",
    "published_date": "2025-03-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "acceleration",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D Gaussian Representation",
    "authors": [
      "Chao Zhang",
      "Yifeng Zhou",
      "Shuheng Wang",
      "Wenfa Li",
      "Degang Wang",
      "Yi Xu",
      "Shaohui Jiao"
    ],
    "abstract": "We have recently seen great progress in 3D scene reconstruction through explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality and fast rendering speed. However, reconstructing dynamic scenes such as complex human performances with long durations remains challenging. Prior efforts fall short of modeling a long-term sequence with drastic motions, frequent topology changes or interactions with props, and resort to segmenting the whole sequence into groups of frames that are processed independently, which undermines temporal stability and thereby leads to an unpleasant viewing experience and inefficient storage footprint. In view of this, we introduce EvolvingGS, a two-stage strategy that first deforms the Gaussian model to coarsely align with the target frame, and then refines it with minimal point addition/subtraction, particularly in fast-changing areas. Owing to the flexibility of the incrementally evolving representation, our method outperforms existing approaches in terms of both per-frame and temporal quality metrics while maintaining fast rendering through its purely explicit representation. Moreover, by exploiting temporal coherence between successive frames, we propose a simple yet effective compression algorithm that achieves over 50x compression rate. Extensive experiments on both public benchmarks and challenging custom datasets demonstrate that our method significantly advances the state-of-the-art in dynamic scene reconstruction, particularly for extended sequences with complex human performances.",
    "arxiv_url": "http://arxiv.org/abs/2503.05162v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05162v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "efficient",
      "high-fidelity",
      "motion",
      "fast",
      "high quality",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three Orthographic Views Using 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhou",
      "Zhe Li",
      "Bo Yu",
      "Lina Hu",
      "Liang Dong",
      "Zijian Yang",
      "Xiaoli Liu",
      "Ning Xu",
      "Ziwei Wang",
      "Yonghao Dang",
      "Jianqin Yin"
    ],
    "abstract": "The automatic reconstruction of 3D computer-aided design (CAD) models from CAD sketches has recently gained significant attention in the computer vision community. Most existing methods, however, rely on vector CAD sketches and 3D ground truth for supervision, which are often difficult to be obtained in industrial applications and are sensitive to noise inputs. We propose viewing CAD reconstruction as a specific instance of sparse-view 3D reconstruction to overcome these limitations. While this reformulation offers a promising perspective, existing 3D reconstruction methods typically require natural images and corresponding camera poses as inputs, which introduces two major significant challenges: (1) modality discrepancy between CAD sketches and natural images, and (2) difficulty of accurate camera pose estimation for CAD sketches. To solve these issues, we first transform the CAD sketches into representations resembling natural images and extract corresponding masks. Next, we manually calculate the camera poses for the orthographic views to ensure accurate alignment within the 3D coordinate system. Finally, we employ a customized sparse-view 3D reconstruction method to achieve high-quality reconstructions from aligned orthographic views. By leveraging raster CAD sketches for self-supervision, our approach eliminates the reliance on vector CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset demonstrate that our proposed method significantly outperforms previous approaches in CAD reconstruction performance and exhibits strong robustness to noisy inputs.",
    "arxiv_url": "http://arxiv.org/abs/2503.05161v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05161v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV",
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using Gaussian Splatting",
    "authors": [
      "Kohei Honda",
      "Takeshi Ishita",
      "Yasuhiro Yoshimura",
      "Ryo Yonetani"
    ],
    "abstract": "This paper presents a novel approach to image-goal navigation by integrating 3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal navigation by guiding a robot through a sequence of point-of-view images without requiring metrical localization or environment-specific training. However, constructing a dense and traversable sequence of target viewpoints from start to goal remains a central challenge, particularly when the available image database is sparse. To address these challenges, we propose a 3DGS-based viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints to seamlessly bridge gaps in sparse data while significantly reducing storage overhead. Experimental results in a photorealistic simulator demonstrate that our approach not only enhances navigation efficiency but also exhibits robustness under varying levels of image database sparsity.",
    "arxiv_url": "http://arxiv.org/abs/2503.05152v2",
    "pdf_url": "http://arxiv.org/pdf/2503.05152v2",
    "published_date": "2025-03-07",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "head",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs",
    "authors": [
      "Yingji Zhong",
      "Zhihao Li",
      "Dave Zhenyu Chen",
      "Lanqing Hong",
      "Dan Xu"
    ],
    "abstract": "Despite recent successes in novel view synthesis using 3D Gaussian Splatting (3DGS), modeling scenes with sparse inputs remains a challenge. In this work, we address two critical yet overlooked issues in real-world sparse-input modeling: extrapolation and occlusion. To tackle these issues, we propose to use a reconstruction by generation pipeline that leverages learned priors from video diffusion models to provide plausible interpretations for regions outside the field of view or occluded. However, the generated sequences exhibit inconsistencies that do not fully benefit subsequent 3DGS modeling. To address the challenge of inconsistencies, we introduce a novel scene-grounding guidance based on rendered sequences from an optimized 3DGS, which tames the diffusion model to generate consistent sequences. This guidance is training-free and does not require any fine-tuning of the diffusion model. To facilitate holistic scene modeling, we also propose a trajectory initialization method. It effectively identifies regions that are outside the field of view and occluded. We further design a scheme tailored for 3DGS optimization with generated sequences. Experiments demonstrate that our method significantly improves upon the baseline and achieves state-of-the-art performance on challenging benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2503.05082v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05082v1",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVideo: Efficient Video Representation and Compression by Gaussian Splatting",
    "authors": [
      "Inseo Lee",
      "Youngyoon Choi",
      "Joonseok Lee"
    ],
    "abstract": "Implicit Neural Representation for Videos (NeRV) has introduced a novel paradigm for video representation and compression, outperforming traditional codecs. As model size grows, however, slow encoding and decoding speed and high memory consumption hinder its application in practice. To address these limitations, we propose a new video representation and compression method based on 2D Gaussian Splatting to efficiently handle video data. Our proposed deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D Gaussians at each frame, significantly reducing memory cost. Equipped with a multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts changes in color, coordinates, and shape of initialized Gaussians, given the time step. By leveraging temporal gradients, our model effectively captures temporal redundancy at negligible cost, significantly enhancing video representation efficiency. Our method reduces GPU memory usage by up to 78.4%, and significantly expedites video processing, achieving 5.5x faster training and 12.5x faster decoding compared to the state-of-the-art NeRV methods.",
    "arxiv_url": "http://arxiv.org/abs/2503.04333v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04333v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting",
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Yuanshuo Cheng",
      "Wangmeng Zuo"
    ],
    "abstract": "In this paper, we aim ambitiously for a realistic yet challenging problem, namely, how to reconstruct high-quality 3D scenes from sparse low-resolution views that simultaneously suffer from deficient perspectives and clarity. Whereas existing methods only deal with either sparse views or low-resolution observations, they fail to handle such hybrid and complicated scenarios. To this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting framework, dubbed S2Gaussian, that can reconstruct structure-accurate and detail-faithful 3D scenes with only sparse and low-resolution views. The S2Gaussian operates in a two-stage fashion. In the first stage, we initially optimize a low-resolution Gaussian representation with depth regularization and densify it to initialize the high-resolution Gaussians through a tailored Gaussian Shuffle Split operation. In the second stage, we refine the high-resolution Gaussians with the super-resolved images generated from both original sparse views and pseudo-views rendered by the low-resolution Gaussians. In which a customized blur-free inconsistency modeling scheme and a 3D robust optimization strategy are elaborately designed to mitigate multi-view inconsistency and eliminate erroneous updates caused by imperfect supervision. Extensive experiments demonstrate superior results and in particular establishing new state-of-the-art performances with more consistent geometry and finer details.",
    "arxiv_url": "http://arxiv.org/abs/2503.04314v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04314v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Manboformer: Learning Gaussian Representations via Spatial-temporal Attention Mechanism",
    "authors": [
      "Ziyue Zhao",
      "Qining Qi",
      "Jianfa Ma"
    ],
    "abstract": "Compared with voxel-based grid prediction, in the field of 3D semantic occupation prediction for autonomous driving, GaussianFormer proposed using 3D Gaussian to describe scenes with sparse 3D semantic Gaussian based on objects is another scheme with lower memory requirements. Each 3D Gaussian function represents a flexible region of interest and its semantic features, which are iteratively refined by the attention mechanism. In the experiment, it is found that the Gaussian function required by this method is larger than the query resolution of the original dense grid network, resulting in impaired performance. Therefore, we consider optimizing GaussianFormer by using unused temporal information. We learn the Spatial-Temporal Self-attention Mechanism from the previous grid-given occupation network and improve it to GaussianFormer. The experiment was conducted with the NuScenes dataset, and the experiment is currently underway.",
    "arxiv_url": "http://arxiv.org/abs/2503.04863v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04863v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting",
    "authors": [
      "Shuojue Yang",
      "Zijian Wu",
      "Mingxuan Hong",
      "Qian Li",
      "Daiyun Shen",
      "Septimiu E. Salcudean",
      "Yueming Jin"
    ],
    "abstract": "Real2Sim is becoming increasingly important with the rapid development of surgical artificial intelligence (AI) and autonomy. In this work, we propose a novel Real2Sim methodology, Instrument-Splatting, that leverages 3D Gaussian Splatting to provide fully controllable 3D reconstruction of surgical instruments from monocular surgical videos. To maintain both high visual fidelity and manipulability, we introduce a geometry pre-training to bind Gaussian point clouds on part mesh with accurate geometric priors and define a forward kinematics to control the Gaussians as flexible as real instruments. Afterward, to handle unposed videos, we design a novel instrument pose tracking method leveraging semantics-embedded Gaussians to robustly refine per-frame instrument poses and joint states in a render-and-compare manner, which allows our instrument Gaussian to accurately learn textures and reach photorealistic rendering. We validated our method on 2 publicly released surgical videos and 4 videos collected on ex vivo tissues and green screens. Quantitative and qualitative evaluations demonstrate the effectiveness and superiority of the proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2503.04082v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04082v2",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "semantic",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene Rendering",
    "authors": [
      "Idris O. Sunmola",
      "Zhenjun Zhao",
      "Samuel Schmidgall",
      "Yumeng Wang",
      "Paul Maria Scheikl",
      "Axel Krieger"
    ],
    "abstract": "Accurate geometric reconstruction of deformable tissues in monocular endoscopic video remains a fundamental challenge in robot-assisted minimally invasive surgery. Although recent volumetric and point primitive methods based on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently rendered surgical scenes, they still struggle with handling artifact-free tool occlusions and preserving fine anatomical details. These limitations stem from unrestricted Gaussian scaling and insufficient surface alignment constraints during reconstruction. To address these issues, we introduce Surgical Gaussian Surfels (SGS), which transforms anisotropic point primitives into surface-aligned elliptical splats by constraining the scale component of the Gaussian covariance matrix along the view-aligned axis. We predict accurate surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled with locality constraints to handle complex tissue deformations. We use homodirectional view-space positional gradients to capture fine image details by splitting Gaussian Surfels in over-reconstructed regions. In addition, we define surface normals as the direction of the steepest density change within each Gaussian surfel primitive, enabling accurate normal estimation without requiring monocular normal priors. We evaluate our method on two in-vivo surgical datasets, where it outperforms current state-of-the-art methods in surface geometry, normal map quality, and rendering efficiency, while remaining competitive in real-time rendering performance. We make our code available at https://github.com/aloma85/SurgicalGaussianSurfels",
    "arxiv_url": "http://arxiv.org/abs/2503.04079v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04079v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/aloma85/SurgicalGaussianSurfels",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details",
    "authors": [
      "Yifei Gao",
      "Jun Huang",
      "Lei Wang",
      "Ruiting Dai",
      "Jun Cheng"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3D-GS) has significantly advanced 3D reconstruction by providing high fidelity and fast training speeds across various scenarios. While recent efforts have mainly focused on improving model structures to compress data volume or reduce artifacts during zoom-in and zoom-out operations, they often overlook an underlying issue: training sampling deficiency. In zoomed-in views, Gaussian primitives can appear unregulated and distorted due to their dilation limitations and the insufficient availability of scale-specific training samples. Consequently, incorporating pseudo-details that ensure the completeness and alignment of the scene becomes essential. In this paper, we introduce a new training method that integrates diffusion models and multi-scale training using pseudo-ground-truth data. This approach not only notably mitigates the dilation and zoomed-in artifacts but also enriches reconstructed scenes with precise details out of existing scenarios. Our method achieves state-of-the-art performance across various benchmarks and extends the capabilities of 3D reconstruction beyond training datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.04037v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04037v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.GR",
      "cs.CV",
      "I.3.5"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world Scene Understanding",
    "authors": [
      "Xihan Wang",
      "Dianyi Yang",
      "Yu Gao",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting(3DGS) have significantly improved semantic scene understanding, enabling natural language queries to localize objects within a scene. However, existing methods primarily focus on embedding compressed CLIP features to 3D Gaussians, suffering from low object segmentation accuracy and lack spatial reasoning capabilities. To address these limitations, we propose GaussianGraph, a novel framework that enhances 3DGS-based scene understanding by integrating adaptive semantic clustering and scene graph generation. We introduce a \"Control-Follow\" clustering strategy, which dynamically adapts to scene scale and feature distribution, avoiding feature compression and significantly improving segmentation accuracy. Additionally, we enrich scene representation by integrating object attributes and spatial relations extracted from 2D foundation models. To address inaccuracies in spatial relationships, we propose 3D correction modules that filter implausible relations through spatial consistency verification, ensuring reliable scene graph construction. Extensive experiments on three datasets demonstrate that GaussianGraph outperforms state-of-the-art methods in both semantic segmentation and object grounding tasks, providing a robust solution for complex scene understanding and interaction.",
    "arxiv_url": "http://arxiv.org/abs/2503.04034v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04034v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics",
    "authors": [
      "Qianzhong Chen",
      "Jiankai Sun",
      "Naixiang Gao",
      "JunEn Low",
      "Timothy Chen",
      "Mac Schwager"
    ],
    "abstract": "Autonomous visual navigation is an essential element in robot autonomy. Reinforcement learning (RL) offers a promising policy training paradigm. However existing RL methods suffer from high sample complexity, poor sim-to-real transfer, and limited runtime adaptability to navigation scenarios not seen during training. These problems are particularly challenging for drones, with complex nonlinear and unstable dynamics, and strong dynamic coupling between control and perception. In this paper, we propose a novel framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep reinforcement learning (DDRL) to train vision-based drone navigation policies. By leveraging high-fidelity 3D scene representations and differentiable simulation, our method improves sample efficiency and sim-to-real transfer. Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt to environmental variations at runtime. Moreover, by curriculum training in a mixture of different surrounding environments, we achieve in-task generalization, the ability to solve new instances of a task not seen during training. Drone hardware experiments demonstrate our method's high training efficiency compared to state-of-the-art RL methods, zero shot sim-to-real transfer for real robot deployment without fine tuning, and ability to adapt to new instances within the same task class (e.g. to fly through a gate at different locations with different distractors in the environment).",
    "arxiv_url": "http://arxiv.org/abs/2503.03984v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03984v1",
    "published_date": "2025-03-06",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation",
    "authors": [
      "Qian Feng",
      "David S. Martinez Lema",
      "Jianxiang Feng",
      "Zhaopeng Chen",
      "Alois Knoll"
    ],
    "abstract": "Learning dexterous manipulation from few-shot demonstrations is a significant yet challenging problem for advanced, human-like robotic systems. Dense distilled feature fields have addressed this challenge by distilling rich semantic features from 2D visual foundation models into the 3D domain. However, their reliance on neural rendering models such as Neural Radiance Fields (NeRF) or Gaussian Splatting results in high computational costs. In contrast, previous approaches based on sparse feature fields either suffer from inefficiencies due to multi-view dependencies and extensive training or lack sufficient grasp dexterity. To overcome these limitations, we propose Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently distills view-consistent 2D features onto 3D points using our novel language-enhanced feature fusion strategy, thereby enabling single-view few-shot generalization. Based on LensDFF, we further introduce a few-shot dexterous manipulation framework that integrates grasp primitives into the demonstrations to generate stable and highly dexterous grasps. Moreover, we present a real2sim grasp evaluation pipeline for efficient grasp assessment and hyperparameter tuning. Through extensive simulation experiments based on the real2sim pipeline and real-world experiments, our approach achieves competitive grasping performance, outperforming state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2503.03890v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03890v1",
    "published_date": "2025-03-05",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "few-shot",
      "semantic",
      "human",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor Environments",
    "authors": [
      "Jie Deng",
      "Fengtian Lang",
      "Zikang Yuan",
      "Xin Yang"
    ],
    "abstract": "Accurate localization is essential for robotics and augmented reality applications such as autonomous navigation. Vision-based methods combining prior maps aim to integrate LiDAR-level accuracy with camera cost efficiency for robust pose estimation. Existing approaches, however, often depend on unreliable interpolation procedures when associating discrete point cloud maps with dense image pixels, which inevitably introduces depth errors and degrades pose estimation accuracy. We propose a monocular visual odometry framework utilizing a continuous 3D Gaussian map, which directly assigns geometrically consistent depth values to all extracted high-gradient points without interpolation. Evaluations on two public datasets demonstrate superior tracking accuracy compared to existing methods. We have released the source code of this work for the development of the community.",
    "arxiv_url": "http://arxiv.org/abs/2503.03373v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03373v1",
    "published_date": "2025-03-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "localization",
      "tracking",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics",
    "authors": [
      "Kun Yang",
      "Yuxiang Liu",
      "Zeyu Cui",
      "Yu Liu",
      "Maojun Zhang",
      "Shen Yan",
      "Qing Wang"
    ],
    "abstract": "Thermal infrared imaging offers the advantage of all-weather capability, enabling non-intrusive measurement of an object's surface temperature. Consequently, thermal infrared images are employed to reconstruct 3D models that accurately reflect the temperature distribution of a scene, aiding in applications such as building monitoring and energy management. However, existing approaches predominantly focus on static 3D reconstruction for a single time period, overlooking the impact of environmental factors on thermal radiation and failing to predict or analyze temperature variations over time. To address these challenges, we propose the NTR-Gaussian method, which treats temperature as a form of thermal radiation, incorporating elements like convective heat transfer and radiative heat dissipation. Our approach utilizes neural networks to predict thermodynamic parameters such as emissivity, convective heat transfer coefficient, and heat capacity. By integrating these predictions, we can accurately forecast thermal temperatures at various times throughout a nighttime scene. Furthermore, we introduce a dynamic dataset specifically for nighttime thermal imagery. Extensive experiments and evaluations demonstrate that NTR-Gaussian significantly outperforms comparison methods in thermal reconstruction, achieving a predicted temperature error within 1 degree Celsius.",
    "arxiv_url": "http://arxiv.org/abs/2503.03115v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03115v1",
    "published_date": "2025-03-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "4d",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting",
    "authors": [
      "Qipeng Yan",
      "Mingyang Sun",
      "Lihua Zhang"
    ],
    "abstract": "Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics.",
    "arxiv_url": "http://arxiv.org/abs/2503.02452v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02452v1",
    "published_date": "2025-03-04",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "high-fidelity",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting",
    "authors": [
      "Haoyuan Li",
      "Ziqin Ye",
      "Yue Hao",
      "Weiyang Lin",
      "Chao Ye"
    ],
    "abstract": "Accurate object perception is essential for robotic applications such as object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM system that seamlessly integrates object pose estimation and reconstruction. We employ 3D Gaussian Splatting for high-fidelity object reconstruction and leverage quadrics for precise object pose estimation. Both of them management is handled on the CPU, while optimization is performed on the GPU, significantly improving system efficiency. By associating objects with unique IDs, our system enables rapid object extraction from the scene. Extensive experimental results on object reconstruction and pose estimation demonstrate that DQO-MAP achieves outstanding performance in terms of precision, reconstruction quality, and computational efficiency. The code and dataset are available at: https://github.com/LiHaoy-ux/DQO-MAP.",
    "arxiv_url": "http://arxiv.org/abs/2503.02223v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02223v1",
    "published_date": "2025-03-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/LiHaoy-ux/DQO-MAP",
    "keywords": [
      "high-fidelity",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization",
    "authors": [
      "Jamie Wynn",
      "Zawar Qureshi",
      "Jakub Powierza",
      "Jamie Watson",
      "Mohamed Sayed"
    ],
    "abstract": "Exploring real-world spaces using novel-view synthesis is fun, and reimagining those worlds in a different style adds another layer of excitement. Stylized worlds can also be used for downstream tasks where there is limited training data and a need to expand a model's training distribution. Most current novel-view synthesis stylization techniques lack the ability to convincingly change geometry. This is because any geometry change requires increased style strength which is often capped for stylization stability and consistency. In this work, we propose a new autoregressive 3D Gaussian Splatting stylization method. As part of this method, we contribute a new RGBD diffusion model that allows for strength control over appearance and shape stylization. To ensure consistency across stylized frames, we use a combination of novel depth-guided cross attention, feature injection, and a Warp ControlNet conditioned on composite frames for guiding the stylization of new frames. We validate our method via extensive qualitative results, quantitative experiments, and a user study. Code online.",
    "arxiv_url": "http://arxiv.org/abs/2503.02009v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02009v2",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "authors": [
      "Jay Zhangjie Wu",
      "Yuxuan Zhang",
      "Haithem Turki",
      "Xuanchi Ren",
      "Jun Gao",
      "Mike Zheng Shou",
      "Sanja Fidler",
      "Zan Gojcic",
      "Huan Ling"
    ],
    "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\\times$ improvement in FID score over baselines while maintaining 3D consistency.",
    "arxiv_url": "http://arxiv.org/abs/2503.01774v1",
    "pdf_url": "http://arxiv.org/pdf/2503.01774v1",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding",
    "authors": [
      "Dianyi Yang",
      "Yu Gao",
      "Xihan Wang",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: https://young-bit.github.io/opengs-github.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.01646v1",
    "pdf_url": "http://arxiv.org/pdf/2503.01646v1",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "fast",
      "semantic",
      "mapping",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "slam",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior",
    "authors": [
      "Chen Guo",
      "Junxuan Li",
      "Yash Kant",
      "Yaser Sheikh",
      "Shunsuke Saito",
      "Chen Cao"
    ],
    "abstract": "We present Vid2Avatar-Pro, a method to create photorealistic and animatable 3D human avatars from monocular in-the-wild videos. Building a high-quality avatar that supports animation with diverse poses from a monocular video is challenging because the observation of pose diversity and view points is inherently limited. The lack of pose variations typically leads to poor generalization to novel poses, and avatars can easily overfit to limited input view points, producing artifacts and distortions from other views. In this work, we address these limitations by leveraging a universal prior model (UPM) learned from a large corpus of multi-view clothed human performance capture data. We build our representation on top of expressive 3D Gaussians with canonical front and back maps shared across identities. Once the UPM is learned to accurately reproduce the large-scale multi-view human images, we fine-tune the model with an in-the-wild video via inverse rendering to obtain a personalized photorealistic human avatar that can be faithfully animated to novel human motions and rendered from novel views. The experiments show that our approach based on the learned universal prior sets a new state-of-the-art in monocular avatar reconstruction by substantially outperforming existing approaches relying only on heuristic regularization or a shape prior of minimally clothed bodies (e.g., SMPL) on publicly available datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.01610v1",
    "pdf_url": "http://arxiv.org/pdf/2503.01610v1",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiteGS: A High-Performance Modular Framework for Gaussian Splatting Training",
    "authors": [
      "Kaimin Liao"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful technique for reconstruction of 3D scenes in computer graphics and vision. However, conventional implementations often suffer from inefficiencies, limited flexibility, and high computational overhead, which constrain their adaptability to diverse applications. In this paper, we present LiteGS,a high-performance and modular framework that enhances both the efficiency and usability of Gaussian splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation while reducing GPU memory usage by approximately 30%. Its modular design decomposes the splatting process into multiple highly optimized operators, and it provides dual API support via a script-based interface and a CUDA-based interface. The script-based interface, in combination with autograd, enables rapid prototyping and straightforward customization of new ideas, while the CUDA-based interface delivers optimal training speeds for performance-critical applications. LiteGS retains the core algorithm of 3DGS, ensuring compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset demonstrate that LiteGS accelerates training without compromising accuracy, making it an ideal solution for both rapid prototyping and production environments.",
    "arxiv_url": "http://arxiv.org/abs/2503.01199v1",
    "pdf_url": "http://arxiv.org/pdf/2503.01199v1",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion",
    "authors": [
      "Yansong Xu",
      "Junlin Li",
      "Wei Zhang",
      "Siyu Chen",
      "Shengyong Zhang",
      "Yuquan Leng",
      "Weijia Zhou"
    ],
    "abstract": "3D gaussian splatting has advanced simultaneous localization and mapping (SLAM) technology by enabling real-time positioning and the construction of high-fidelity maps. However, the uncertainty in gaussian position and initialization parameters introduces challenges, often requiring extensive iterative convergence and resulting in redundant or insufficient gaussian representations. To address this, we introduce a novel adaptive densification method based on Fourier frequency domain analysis to establish gaussian priors for rapid convergence. Additionally, we propose constructing independent and unified sparse and dense maps, where a sparse map supports efficient tracking via Generalized Iterative Closest Point (GICP) and a dense map creates high-fidelity visual representations. This is the first SLAM system leveraging frequency domain analysis to achieve high-quality gaussian mapping in real-time. Experimental results demonstrate an average frame rate of 36 FPS on Replica and TUM RGB-D datasets, achieving competitive accuracy in both localization and mapping.",
    "arxiv_url": "http://arxiv.org/abs/2503.01109v1",
    "pdf_url": "http://arxiv.org/pdf/2503.01109v1",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
    "authors": [
      "You Shen",
      "Zhipeng Zhang",
      "Xinyang Li",
      "Yansong Qu",
      "Yu Lin",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "abstract": "Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size.",
    "arxiv_url": "http://arxiv.org/abs/2503.00881v1",
    "pdf_url": "http://arxiv.org/pdf/2503.00881v1",
    "published_date": "2025-03-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with Generative Gaussian Splatting",
    "authors": [
      "Zhiwei Zhao",
      "Alan Zhao",
      "Minchen Li",
      "Yixin Hu"
    ],
    "abstract": "The generation of 3D content from single-view images has been extensively studied, but 3D dynamic scene generation with physical consistency from videos remains in its early stages. We propose a novel framework leveraging generative 3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from single-view videos. The fluid geometry represented by 3DGS is initially generated from single-frame images, then denoised, densified, and aligned across frames. We estimate the fluid surface velocity using optical flow and compute the mainstream of the fluid to refine it. The 3D volumetric velocity field is then derived from the enclosed surface. The velocity field is then converted into a divergence-free, grid-based representation, enabling the optimization of simulation parameters through its differentiability across frames. This process results in simulation-ready fluid assets with physical dynamics closely matching those observed in the source video. Our approach is applicable to various fluid types, including gas, liquid, and viscous fluids, and allows users to edit the output geometry or extend movement durations seamlessly. Our automatic method for creating 3D dynamic fluid assets from single-view videos, easily obtainable from the internet, shows great potential for generating large-scale 3D fluid assets at a low cost.",
    "arxiv_url": "http://arxiv.org/abs/2503.00868v1",
    "pdf_url": "http://arxiv.org/pdf/2503.00868v1",
    "published_date": "2025-03-02",
    "categories": [
      "cs.GR",
      "I.2.0; I.3.7"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery",
    "authors": [
      "BoCheng Li",
      "WenJuan Zhang",
      "Bing Zhang",
      "YiLing Yao",
      "YaNing Wang"
    ],
    "abstract": "3D Gaussian Splatting (3D GS) achieves impressive results in novel view synthesis for small, single-object scenes through Gaussian ellipsoid initialization and adaptive density control. However, when applied to large-scale remote sensing scenes, 3D GS faces challenges: the point clouds generated by Structure-from-Motion (SfM) are often sparse, and the inherent smoothing behavior of 3D GS leads to over-reconstruction in high-frequency regions, where have detailed textures and color variations. This results in the generation of large, opaque Gaussian ellipsoids that cause gradient artifacts. Moreover, the simultaneous optimization of both geometry and texture may lead to densification of Gaussian ellipsoids at incorrect geometric locations, resulting in artifacts in other views. To address these issues, we propose PSRGS, a progressive optimization scheme based on spectral residual maps. Specifically, we create a spectral residual significance map to separate low-frequency and high-frequency regions. In the low-frequency region, we apply depth-aware and depth-smooth losses to initialize the scene geometry with low threshold. For the high-frequency region, we use gradient features with higher threshold to split and clone ellipsoids, refining the scene. The sampling rate is determined by feature responses and gradient loss. Finally, we introduce a pre-trained network that jointly computes perceptual loss from multiple views, ensuring accurate restoration of high-frequency details in both Gaussian ellipsoids geometry and color. We conduct experiments on multiple datasets to assess the effectiveness of our method, which demonstrates competitive rendering quality, especially in recovering texture details in high-frequency regions.",
    "arxiv_url": "http://arxiv.org/abs/2503.00848v1",
    "pdf_url": "http://arxiv.org/pdf/2503.00848v1",
    "published_date": "2025-03-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting",
    "authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Jiaqi Li",
      "Zhiguo Cao",
      "Wei Li",
      "Chen Change Loy"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. To ensure accurate scene geometry, we incorporate depth priors adjusted per scene, and we apply defocus-to-focus adaptation to minimize the gap in the circle of confusion. We also introduce a synthetic dataset to assess refocusing capabilities and the model's ability to learn precise lens parameters. Our framework is customizable and supports various interactive applications. Extensive experiments confirm the effectiveness of our method. Our project is available at https://dof-gaussian.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2503.00746v3",
    "pdf_url": "http://arxiv.org/pdf/2503.00746v3",
    "published_date": "2025-03-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Monocular 3D Scene Completion with Diffusion Model",
    "authors": [
      "Changlin Song",
      "Jiaqi Wang",
      "Liyun Zhu",
      "He Weng"
    ],
    "abstract": "3D scene reconstruction is essential for applications in virtual reality, robotics, and autonomous driving, enabling machines to understand and interact with complex environments. Traditional 3D Gaussian Splatting techniques rely on images captured from multiple viewpoints to achieve optimal performance, but this dependence limits their use in scenarios where only a single image is available. In this work, we introduce FlashDreamer, a novel approach for reconstructing a complete 3D scene from a single image, significantly reducing the need for multi-view inputs. Our approach leverages a pre-trained vision-language model to generate descriptive prompts for the scene, guiding a diffusion model to produce images from various perspectives, which are then fused to form a cohesive 3D reconstruction. Extensive experiments show that our method effectively and robustly expands single-image inputs into a comprehensive 3D scene, extending monocular 3D reconstruction capabilities without further training. Our code is available https://github.com/CharlieSong1999/FlashDreamer/tree/main.",
    "arxiv_url": "http://arxiv.org/abs/2503.00726v1",
    "pdf_url": "http://arxiv.org/pdf/2503.00726v1",
    "published_date": "2025-03-02",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "https://github.com/CharlieSong1999/FlashDreamer",
    "keywords": [
      "robotics",
      "autonomous driving",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation Model",
    "authors": [
      "Runyi Li",
      "Xuanyu Zhang",
      "Chuhan Tong",
      "Zhipei Xu",
      "Jian Zhang"
    ],
    "abstract": "With the advancement of AIGC technologies, the modalities generated by models have expanded from images and videos to 3D objects, leading to an increasing number of works focused on 3D Gaussian Splatting (3DGS) generative models. Existing research on copyright protection for generative models has primarily concentrated on watermarking in image and text modalities, with little exploration into the copyright protection of 3D object generative models. In this paper, we propose the first bit watermarking framework for 3DGS generative models, named GaussianSeal, to enable the decoding of bits as copyright identifiers from the rendered outputs of generated 3DGS. By incorporating adaptive bit modulation modules into the generative model and embedding them into the network blocks in an adaptive way, we achieve high-precision bit decoding with minimal training overhead while maintaining the fidelity of the model's outputs. Experiments demonstrate that our method outperforms post-processing watermarking approaches for 3DGS objects, achieving superior performance of watermark decoding accuracy and preserving the quality of the generated results.",
    "arxiv_url": "http://arxiv.org/abs/2503.00531v1",
    "pdf_url": "http://arxiv.org/pdf/2503.00531v1",
    "published_date": "2025-03-01",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic Pick-and-Place Setups",
    "authors": [
      "Nicholas Pfaff",
      "Evelyn Fu",
      "Jeremy Binagia",
      "Phillip Isola",
      "Russ Tedrake"
    ],
    "abstract": "Simulating object dynamics from real-world perception shows great promise for digital twins and robotic manipulation but often demands labor-intensive measurements and expertise. We present a fully automated Real2Sim pipeline that generates simulation-ready assets for real-world objects through robotic interaction. Using only a robot's joint torque sensors and an external camera, the pipeline identifies visual geometry, collision geometry, and physical properties such as inertial parameters. Our approach introduces a general method for extracting high-quality, object-centric meshes from photometric reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing alpha-transparent training while explicitly distinguishing foreground occlusions from background subtraction. We validate the full pipeline through extensive experiments, demonstrating its effectiveness across diverse objects. By eliminating the need for manual intervention or environment modifications, our pipeline can be integrated directly into existing pick-and-place setups, enabling scalable and efficient dataset creation. Project page (with code and data): https://scalable-real2sim.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2503.00370v2",
    "pdf_url": "http://arxiv.org/pdf/2503.00370v2",
    "published_date": "2025-03-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression",
    "authors": [
      "Yu-Ting Zhan",
      "Cheng-Yuan Ho",
      "Hebi Yang",
      "Yi-Hsin Chen",
      "Jui Chiu Chiang",
      "Yu-Lun Liu",
      "Wen-Hsiao Peng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D representation. Much research has been focused on reducing its storage requirements and memory footprint. However, the needs to compress and transmit the 3DGS representation to the remote side are overlooked. This new application calls for rate-distortion-optimized 3DGS compression. How to quantize and entropy encode sparse Gaussian primitives in the 3D space remains largely unexplored. Few early attempts resort to the hyperprior framework from learned image compression. But, they fail to utilize fully the inter and intra correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work, termed CAT-3DGS, introduces a context-adaptive triplane approach to their rate-distortion-optimized coding. It features multi-scale triplanes, oriented according to the principal axes of Gaussian primitives in the 3D space, to capture their inter correlation (i.e. spatial correlation) for spatial autoregressive coding in the projected 2D planes. With these triplanes serving as the hyperprior, we further perform channel-wise autoregressive coding to leverage the intra correlation within each individual Gaussian primitive. Our CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively skips from coding those Gaussian primitives that potentially have little impact on the rendering quality. When trained end-to-end to strike a good rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art compression performance on the commonly used real-world datasets.",
    "arxiv_url": "http://arxiv.org/abs/2503.00357v2",
    "pdf_url": "http://arxiv.org/pdf/2503.00357v2",
    "published_date": "2025-03-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes",
    "authors": [
      "Yangge Li",
      "Chenxi Ji",
      "Xiangru Zhong",
      "Huan Zhang",
      "Sayan Mitra"
    ],
    "abstract": "We introduce abstract rendering, a method for computing a set of images by rendering a scene from a continuously varying range of camera positions. The resulting abstract image-which encodes an infinite collection of possible renderings-is represented using constraints on the image matrix, enabling rigorous uncertainty propagation through the rendering process. This capability is particularly valuable for the formal verification of vision-based autonomous systems and other safety-critical applications. Our approach operates on Gaussian splat scenes, an emerging representation in computer vision and robotics. We leverage efficient piecewise linear bound propagation to abstract fundamental rendering operations, while addressing key challenges that arise in matrix inversion and depth sorting-two operations not directly amenable to standard approximations. To handle these, we develop novel linear relational abstractions that maintain precision while ensuring computational efficiency. These abstractions not only power our abstract rendering algorithm but also provide broadly applicable tools for other rendering problems. Our implementation, AbstractSplat, is optimized for scalability, handling up to 750k Gaussians while allowing users to balance memory and runtime through tile and batch-based computation. Compared to the only existing abstract image method for mesh-based scenes, AbstractSplat achieves 2-14x speedups while preserving precision. Our results demonstrate that continuous camera motion, rotations, and scene variations can be rigorously analyzed at scale, making abstract rendering a powerful tool for uncertainty-aware vision applications.",
    "arxiv_url": "http://arxiv.org/abs/2503.00308v2",
    "pdf_url": "http://arxiv.org/pdf/2503.00308v2",
    "published_date": "2025-03-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing A 3D World in A Grain of Sand",
    "authors": [
      "Yufan Zhang",
      "Yu Ji",
      "Yu Guo",
      "Jinwei Ye"
    ],
    "abstract": "We present a snapshot imaging technique for recovering 3D surrounding views of miniature scenes. Due to their intricacy, miniature scenes with objects sized in millimeters are difficult to reconstruct, yet miniatures are common in life and their 3D digitalization is desirable. We design a catadioptric imaging system with a single camera and eight pairs of planar mirrors for snapshot 3D reconstruction from a dollhouse perspective. We place paired mirrors on nested pyramid surfaces for capturing surrounding multi-view images in a single shot. Our mirror design is customizable based on the size of the scene for optimized view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene reconstruction and novel view synthesis. We overcome the challenge posed by our sparse view input by integrating visual hull-derived depth constraint. Our method demonstrates state-of-the-art performance on a variety of synthetic and real miniature scenes.",
    "arxiv_url": "http://arxiv.org/abs/2503.00260v2",
    "pdf_url": "http://arxiv.org/pdf/2503.00260v2",
    "published_date": "2025-03-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering",
    "authors": [
      "Jingqiu Zhou",
      "Lue Fan",
      "Linjiang Huang",
      "Xiaoyu Shi",
      "Si Liu",
      "Zhaoxiang Zhang",
      "Hongsheng Li"
    ],
    "abstract": "Driving scene reconstruction and rendering have advanced significantly using the 3D Gaussian Splatting. However, most prior research has focused on the rendering quality along a pre-recorded vehicle path and struggles to generalize to out-of-path viewpoints, which is caused by the lack of high-quality supervision in those out-of-path views. To address this issue, we introduce an Inverse View Warping technique to create compact and high-quality images as supervision for the reconstruction of the out-of-path views, enabling high-quality rendering results for those views. For accurate and robust inverse view warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense depth maps during the optimization process, overcoming the sparsity and incompleteness of LiDAR depth data. Our method achieves superior in-path and out-of-path reconstruction and rendering performance on the widely used Waymo Open dataset. In addition, a simulator-based benchmark is proposed to obtain the out-of-path ground truth and quantitatively evaluate the performance of out-of-path rendering, where our method outperforms previous methods by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2502.21093v2",
    "pdf_url": "http://arxiv.org/pdf/2502.21093v2",
    "published_date": "2025-02-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering",
    "authors": [
      "John J. Han",
      "Jie Ying Wu"
    ],
    "abstract": "The lack of labeled datasets in 3D vision for surgical scenes inhibits the development of robust 3D reconstruction algorithms in the medical domain. Despite the popularity of Neural Radiance Fields and 3D Gaussian Splatting in the general computer vision community, these systems have yet to find consistent success in surgical scenes due to challenges such as non-stationary lighting and non-Lambertian surfaces. As a result, the need for labeled surgical datasets continues to grow. In this work, we introduce a differentiable rendering framework for material and lighting estimation from endoscopic images and known geometry. Compared to previous approaches that model lighting and material jointly as radiance, we explicitly disentangle these scene properties for robust and photorealistic novel view synthesis. To disambiguate the training process, we formulate domain-specific properties inherent in surgical scenes. Specifically, we model the scene lighting as a simple spotlight and material properties as a bidirectional reflectance distribution function, parameterized by a neural network. By grounding color predictions in the rendering equation, we can generate photorealistic images at arbitrary camera poses. We evaluate our method with various sequences from the Colonoscopy 3D Video Dataset and show that our method produces competitive novel view synthesis results compared with other approaches. Furthermore, we demonstrate that synthetic data can be used to develop 3D vision algorithms by finetuning a depth estimation model with our rendered outputs. Overall, we see that the depth estimation performance is on par with fine-tuning with the original real images.",
    "arxiv_url": "http://arxiv.org/abs/2502.20669v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20669v1",
    "published_date": "2025-02-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "medical",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Thousands to Billions: 3D Visual Language Grounding via Render-Supervised Distillation from 2D VLMs",
    "authors": [
      "Ang Cao",
      "Sergio Arnaud",
      "Oleksandr Maksymets",
      "Jianing Yang",
      "Ayush Jain",
      "Sriram Yenamandra",
      "Ada Martin",
      "Vincent-Pierre Berges",
      "Paul McVay",
      "Ruslan Partsey",
      "Aravind Rajeswaran",
      "Franziska Meier",
      "Justin Johnson",
      "Jeong Joon Park",
      "Alexander Sax"
    ],
    "abstract": "3D vision-language grounding faces a fundamental data bottleneck: while 2D models train on billions of images, 3D models have access to only thousands of labeled scenes--a six-order-of-magnitude gap that severely limits performance. We introduce $\\textbf{LIFT-GS}$, a practical distillation technique that overcomes this limitation by using differentiable rendering to bridge 3D and 2D supervision. LIFT-GS predicts 3D Gaussian representations from point clouds and uses them to render predicted language-conditioned 3D masks into 2D views, enabling supervision from 2D foundation models (SAM, CLIP, LLaMA) without requiring any 3D annotations. This render-supervised formulation enables end-to-end training of complete encoder-decoder architectures and is inherently model-agnostic. LIFT-GS achieves state-of-the-art results with $25.7\\%$ mAP on open-vocabulary instance segmentation (vs. $20.2\\%$ prior SOTA) and consistent $10-30\\%$ improvements on referential grounding tasks. Remarkably, pretraining effectively multiplies fine-tuning datasets by 2X, demonstrating strong scaling properties that suggest 3D VLG currently operates in a severely data-scarce regime. Project page: https://liftgs.github.io",
    "arxiv_url": "http://arxiv.org/abs/2502.20389v2",
    "pdf_url": "http://arxiv.org/pdf/2502.20389v2",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "segmentation",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InsTaG: Learning Personalized 3D Talking Head from Few-Second Video",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Jun Zhou",
      "Lin Gu"
    ],
    "abstract": "Despite exhibiting impressive performance in synthesizing lifelike personalized 3D talking heads, prevailing methods based on radiance fields suffer from high demands for training data and time for each new identity. This paper introduces InsTaG, a 3D talking head synthesis framework that allows a fast learning of realistic personalized 3D talking head from few training data. Built upon a lightweight 3DGS person-specific synthesizer with universal motion priors, InsTaG achieves high-quality and fast adaptation while preserving high-level personalization and efficiency. As preparation, we first propose an Identity-Free Pre-training strategy that enables the pre-training of the person-specific model and encourages the collection of universal motion priors from long-video data corpus. To fully exploit the universal motion priors to learn an unseen new identity, we then present a Motion-Aligned Adaptation strategy to adaptively align the target head to the pre-trained field, and constrain a robust dynamic head structure under few training data. Experiments demonstrate our outstanding performance and efficiency under various data scenarios to render high-quality personalized talking heads.",
    "arxiv_url": "http://arxiv.org/abs/2502.20387v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20387v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "fast",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "We address the challenge of task-oriented navigation in unstructured and unknown environments, where robots must incrementally build and reason on rich, metric-semantic maps in real time. Since tasks may require clarification or re-specification, it is necessary for the information in the map to be rich enough to enable generalization across a wide range of tasks. To effectively execute tasks specified in natural language, we propose a hierarchical representation built on language-embedded Gaussian splatting that enables both sparse semantic planning that lends itself to online operation and dense geometric representation for collision-free navigation. We validate the effectiveness of our method through real-world robot experiments conducted in both cluttered indoor and kilometer-scale outdoor environments, with a competitive ratio of about 60% against privileged baselines. Experiment videos and more details can be found on our project page: https://atlasnav.github.io",
    "arxiv_url": "http://arxiv.org/abs/2502.20386v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20386v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling",
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.20378v1",
    "pdf_url": "http://arxiv.org/pdf/2502.20378v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TrackGS: Optimizing COLMAP-Free 3D Gaussian Splatting with Global Track Constraints",
    "authors": [
      "Dongbo Shi",
      "Shen Cao",
      "Lubin Fan",
      "Bojian Wu",
      "Jinhui Guo",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has advanced ability on novel view synthesis, it still depends on accurate pre-computaed camera parameters, which are hard to obtain and prone to noise. Previous COLMAP-Free methods optimize camera poses using local constraints, but they often struggle in complex scenarios. To address this, we introduce TrackGS, which incorporates feature tracks to globally constrain multi-view geometry. We select the Gaussians associated with each track, which will be trained and rescaled to an infinitesimally small size to guarantee the spatial accuracy. We also propose minimizing both reprojection and backprojection errors for better geometric consistency. Moreover, by deriving the gradient of intrinsics, we unify camera parameter estimation with 3DGS training into a joint optimization framework, achieving SOTA performance on challenging datasets with severe camera movements.",
    "arxiv_url": "http://arxiv.org/abs/2502.19800v2",
    "pdf_url": "http://arxiv.org/pdf/2502.19800v2",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Open-Vocabulary Semantic Part Segmentation of 3D Human",
    "authors": [
      "Keito Suzuki",
      "Bang Du",
      "Girish Krishnan",
      "Kunyao Chen",
      "Runfa Blark Li",
      "Truong Nguyen"
    ],
    "abstract": "3D part segmentation is still an open problem in the field of 3D vision and AR/VR. Due to limited 3D labeled data, traditional supervised segmentation methods fall short in generalizing to unseen shapes and categories. Recently, the advancement in vision-language models' zero-shot abilities has brought a surge in open-world 3D segmentation methods. While these methods show promising results for 3D scenes or objects, they do not generalize well to 3D humans. In this paper, we present the first open-vocabulary segmentation method capable of handling 3D human. Our framework can segment the human category into desired fine-grained parts based on the textual prompt. We design a simple segmentation pipeline, leveraging SAM to generate multi-view proposals in 2D and proposing a novel HumanCLIP model to create unified embeddings for visual and textual inputs. Compared with existing pre-trained CLIP models, the HumanCLIP model yields more accurate embeddings for human-centric contents. We also design a simple-yet-effective MaskFusion module, which classifies and fuses multi-view features into 3D semantic masks without complex voting and grouping mechanisms. The design of decoupling mask proposals and text input also significantly boosts the efficiency of per-prompt inference. Experimental results on various 3D human datasets show that our method outperforms current state-of-the-art open-vocabulary 3D segmentation methods by a large margin. In addition, we show that our method can be directly applied to various 3D representations including meshes, point clouds, and 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2502.19782v1",
    "pdf_url": "http://arxiv.org/pdf/2502.19782v1",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "semantic",
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Does 3D Gaussian Splatting Need Accurate Volumetric Rendering?",
    "authors": [
      "Adam Celarek",
      "George Kopanas",
      "George Drettakis",
      "Michael Wimmer",
      "Bernhard Kerbl"
    ],
    "abstract": "Since its introduction, 3D Gaussian Splatting (3DGS) has become an important reference method for learning 3D representations of a captured scene, allowing real-time novel-view synthesis with high visual quality and fast training times. Neural Radiance Fields (NeRFs), which preceded 3DGS, are based on a principled ray-marching approach for volumetric rendering. In contrast, while sharing a similar image formation model with NeRF, 3DGS uses a hybrid rendering solution that builds on the strengths of volume rendering and primitive rasterization. A crucial benefit of 3DGS is its performance, achieved through a set of approximations, in many cases with respect to volumetric rendering theory. A naturally arising question is whether replacing these approximations with more principled volumetric rendering solutions can improve the quality of 3DGS. In this paper, we present an in-depth analysis of the various approximations and assumptions used by the original 3DGS solution. We demonstrate that, while more accurate volumetric rendering can help for low numbers of primitives, the power of efficient optimization and the large number of Gaussians allows 3DGS to outperform volumetric rendering despite its approximations.",
    "arxiv_url": "http://arxiv.org/abs/2502.19318v1",
    "pdf_url": "http://arxiv.org/pdf/2502.19318v1",
    "published_date": "2025-02-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS: Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting",
    "authors": [
      "Yu Liu",
      "Baoxiong Jia",
      "Ruijie Lu",
      "Junfeng Ni",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ],
    "abstract": "Building articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement. Our work is made publicly available at: https://articulate-gs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2502.19459v2",
    "pdf_url": "http://arxiv.org/pdf/2502.19459v2",
    "published_date": "2025-02-26",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compression in 3D Gaussian Splatting: A Survey of Methods, Trends, and Future Directions",
    "authors": [
      "Muhammad Salman Ali",
      "Chaoning Zhang",
      "Marco Cagnazzo",
      "Giuseppe Valenzise",
      "Enzo Tartaglione",
      "Sung-Ho Bae"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a pioneering approach in explicit scene rendering and computer graphics. Unlike traditional neural radiance field (NeRF) methods, which typically rely on implicit, coordinate-based models to map spatial coordinates to pixel values, 3DGS utilizes millions of learnable 3D Gaussians. Its differentiable rendering technique and inherent capability for explicit scene representation and manipulation positions 3DGS as a potential game-changer for the next generation of 3D reconstruction and representation technologies. This enables 3DGS to deliver real-time rendering speeds while offering unparalleled editability levels. However, despite its advantages, 3DGS suffers from substantial memory and storage requirements, posing challenges for deployment on resource-constrained devices. In this survey, we provide a comprehensive overview focusing on the scalability and compression of 3DGS. We begin with a detailed background overview of 3DGS, followed by a structured taxonomy of existing compression methods. Additionally, we analyze and compare current methods from the topological perspective, evaluating their strengths and limitations in terms of fidelity, compression ratios, and computational efficiency. Furthermore, we explore how advancements in efficient NeRF representations can inspire future developments in 3DGS optimization. Finally, we conclude with current research challenges and highlight key directions for future exploration.",
    "arxiv_url": "http://arxiv.org/abs/2502.19457v1",
    "pdf_url": "http://arxiv.org/pdf/2502.19457v1",
    "published_date": "2025-02-26",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation",
    "authors": [
      "Yunpeng Gao",
      "Chenhui Li",
      "Zhongrui You",
      "Junli Liu",
      "Zhen Li",
      "Pengan Chen",
      "Qizhi Chen",
      "Zhonghan Tang",
      "Liansheng Wang",
      "Penghui Yang",
      "Yiwen Tang",
      "Yuhang Tang",
      "Shuai Liang",
      "Songyi Zhu",
      "Ziqin Xiong",
      "Yifei Su",
      "Xinyi Ye",
      "Jianan Li",
      "Yan Ding",
      "Dong Wang",
      "Zhigang Wang",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced.",
    "arxiv_url": "http://arxiv.org/abs/2502.18041v5",
    "pdf_url": "http://arxiv.org/pdf/2502.18041v5",
    "published_date": "2025-02-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting",
    "authors": [
      "Haoyuan Li",
      "Yanpeng Zhou",
      "Tao Tang",
      "Jifei Song",
      "Yihan Zeng",
      "Michael Kampffmeyer",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "abstract": "Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap between the discrete points and the dense 2D pixels of images. To tackle this issue, we propose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modal pre-training to enhance the 3D representation. We first rely on the 3DGS representation to model the 3D world as a collection of 3D Gaussians with color and opacity, incorporating all the information of the 3D scene while establishing a strong connection with 2D images. Then, to achieve Language-Image-3D pertaining, UniGS starts with a pre-trained vision-language model to establish a shared visual and textual space through extensive real-world image-text pairs. Subsequently, UniGS employs a 3D encoder to align the optimized 3DGS with the Language-Image representations to learn unified multi-modal representations. To facilitate the extraction of global explicit 3D features by the 3D encoder and achieve better cross-modal alignment, we additionally introduce a novel Gaussian-Aware Guidance module that guides the learning of fine-grained representations of the 3D domain. Through extensive experiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets with zero-shot classification, text-driven retrieval and open-world understanding tasks, we demonstrate the effectiveness of UniGS in learning a more general and stronger aligned multi-modal representation. Specifically, UniGS achieves leading results across different 3D tasks with remarkable improvements over previous SOTA, Uni3D, including on zero-shot classification (+9.36%), text-driven retrieval (+4.3%) and open-world understanding (+7.92%).",
    "arxiv_url": "http://arxiv.org/abs/2502.17860v2",
    "pdf_url": "http://arxiv.org/pdf/2502.17860v2",
    "published_date": "2025-02-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting",
    "authors": [
      "Chong Cheng",
      "Gaochao Song",
      "Yiyang Yao",
      "Qinzheng Zhou",
      "Gangjian Zhang",
      "Hao Wang"
    ],
    "abstract": "This paper investigates an open research challenge of reconstructing high-quality, large 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. To perform effective and efficient 3D scene reconstruction, we propose a novel graph-guided 3D scene reconstruction framework, GraphGS. Specifically, given a set of images captured by RGB cameras on a scene, we first design a spatial prior-based scene structure estimation method. This is then used to create a camera graph that includes information about the camera topology. Further, we propose to apply the graph-guided multi-view consistency constraint and adaptive sampling strategy to the 3D Gaussian Splatting optimization process. This greatly alleviates the issue of Gaussian points overfitting to specific sparse viewpoints and expedites the 3D reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D reconstruction from images, which presents state-of-the-art performance through quantitative and qualitative evaluation across multiple datasets. Project Page: https://3dagentworld.github.io/graphgs.",
    "arxiv_url": "http://arxiv.org/abs/2502.17377v1",
    "pdf_url": "http://arxiv.org/pdf/2502.17377v1",
    "published_date": "2025-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow",
    "authors": [
      "Simon Boeder",
      "Fabian Gigengack",
      "Benjamin Risse"
    ],
    "abstract": "Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.",
    "arxiv_url": "http://arxiv.org/abs/2502.17288v2",
    "pdf_url": "http://arxiv.org/pdf/2502.17288v2",
    "published_date": "2025-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "fast",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Laplace-Beltrami Operator for Gaussian Splatting",
    "authors": [
      "Hongyu Zhou",
      "Zorah L√§hner"
    ],
    "abstract": "With the rising popularity of 3D Gaussian splatting and the expanse of applications from rendering to 3D reconstruction, there comes also a need for geometry processing applications directly on this new representation. While considering the centers of Gaussians as a point cloud or meshing them is an option that allows to apply existing algorithms, this might ignore information present in the data or be unnecessarily expensive. Additionally, Gaussian splatting tends to contain a large number of outliers which do not affect the rendering quality but need to be handled correctly in order not to produce noisy results in geometry processing applications. In this work, we propose a formulation to compute the Laplace-Beltrami operator, a widely used tool in geometry processing, directly on Gaussian splatting using the Mahalanobis distance. While conceptually similar to a point cloud Laplacian, our experiments show superior accuracy on the point clouds encoded in the Gaussian splatting centers and, additionally, the operator can be used to evaluate the quality of the output during optimization.",
    "arxiv_url": "http://arxiv.org/abs/2502.17531v1",
    "pdf_url": "http://arxiv.org/pdf/2502.17531v1",
    "published_date": "2025-02-24",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering",
    "authors": [
      "Junseo Lee",
      "Jaisung Kim",
      "Junyong Park",
      "Jaewoong Sim"
    ],
    "abstract": "Graphics rendering that builds on machine learning and radiance fields is gaining significant attention due to its outstanding quality and speed in generating photorealistic images from novel viewpoints. However, prior work has primarily focused on evaluating its performance through software-based rendering on programmable shader cores, leaving its performance when exploiting fixed-function graphics units largely unexplored.   In this paper, we investigate the performance implications of performing radiance field rendering on the hardware graphics pipeline. In doing so, we implement the state-of-the-art radiance field method, 3D Gaussian splatting, using graphics APIs and evaluate it across synthetic and real-world scenes on today's graphics hardware. Based on our analysis, we present VR-Pipe, which seamlessly integrates two innovations into graphics hardware to streamline the hardware pipeline for volume rendering, such as radiance field methods. First, we introduce native hardware support for early termination by repurposing existing special-purpose hardware in modern GPUs. Second, we propose multi-granular tile binning with quad merging, which opportunistically blends fragments in shader cores before passing them to fixed-function blending units. Our evaluation shows that VR-Pipe greatly improves rendering performance, achieving up to a 2.78x speedup over the conventional graphics pipeline with negligible hardware overhead.",
    "arxiv_url": "http://arxiv.org/abs/2502.17078v1",
    "pdf_url": "http://arxiv.org/pdf/2502.17078v1",
    "published_date": "2025-02-24",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Difference: Find Any Change Instance in 3D Scenes",
    "authors": [
      "Binbin Jiang",
      "Rui Huang",
      "Qingyi Zhao",
      "Yuxiang Zhang"
    ],
    "abstract": "Instance-level change detection in 3D scenes presents significant challenges, particularly in uncontrolled environments lacking labeled image pairs, consistent camera poses, or uniform lighting conditions. This paper addresses these challenges by introducing a novel approach for detecting changes in real-world scenarios. Our method leverages 4D Gaussians to embed multiple images into Gaussian distributions, enabling the rendering of two coherent image sequences. We segment each image and assign unique identifiers to instances, facilitating efficient change detection through ID comparison. Additionally, we utilize change maps and classification encodings to categorize 4D Gaussians as changed or unchanged, allowing for the rendering of comprehensive change maps from any viewpoint. Extensive experiments across various instance-level change detection datasets demonstrate that our method significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D, especially in scenarios with substantial lighting variations. Our approach offers improved detection accuracy, robustness to lighting changes, and efficient processing times, advancing the field of 3D change detection.",
    "arxiv_url": "http://arxiv.org/abs/2502.16941v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16941v1",
    "published_date": "2025-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "4d",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis",
    "authors": [
      "Anand Kumar",
      "Kavinder Roghit Kanthen",
      "Josna John"
    ],
    "abstract": "We can achieve fast and consistent early skin cancer detection with recent developments in computer vision and deep learning techniques. However, the existing skin lesion segmentation and classification prediction models run independently, thus missing potential efficiencies from their integrated execution. To unify skin lesion analysis, our paper presents the Gaussian Splatting - Transformer UNet (GS-TransUNet), a novel approach that synergistically combines 2D Gaussian splatting with the Transformer UNet architecture for automated skin cancer diagnosis. Our unified deep learning model efficiently delivers dual-function skin lesion classification and segmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets, our network demonstrates superior performance compared to existing state-of-the-art models across multiple metrics through 5-fold cross-validation. Our findings illustrate significant advancements in the precision of segmentation and classification. This integration sets new benchmarks in the field and highlights the potential for further research into multi-task medical image analysis methodologies, promising enhancements in automated diagnostic systems.",
    "arxiv_url": "http://arxiv.org/abs/2502.16748v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16748v1",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "medical",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
    "authors": [
      "Kim Jun-Seong",
      "GeonU Kim",
      "Kim Yu-Ji",
      "Yu-Chiang Frank Wang",
      "Jaesung Choe",
      "Tae-Hyun Oh"
    ],
    "abstract": "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large-scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. For video results, please visit : https://drsplat.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2502.16652v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16652v1",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "compact",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient 4D Gaussian Stream with Low Rank Adaptation",
    "authors": [
      "Zhenhuan Liu",
      "Shuai Liu",
      "Yidong Lu",
      "Yirui Chen",
      "Jie Yang",
      "Wei Liu"
    ],
    "abstract": "Recent methods have made significant progress in synthesizing novel views with long video sequences. This paper proposes a highly scalable method for dynamic novel view synthesis with continual learning. We leverage the 3D Gaussians to represent the scene and a low-rank adaptation-based deformation model to capture the dynamic scene changes. Our method continuously reconstructs the dynamics with chunks of video frames, reduces the streaming bandwidth by $90\\%$ while maintaining high rendering quality comparable to the off-line SOTA methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.16575v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16575v1",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control",
    "authors": [
      "Jinbo Yan",
      "Alan Zhao",
      "Yixin Hu"
    ],
    "abstract": "Single-image 3D generation has emerged as a prominent research topic, playing a vital role in virtual reality, 3D modeling, and digital content creation. However, existing methods face challenges such as a lack of multi-view geometric consistency and limited controllability during the generation process, which significantly restrict their usability. % To tackle these challenges, we introduce Dragen3D, a novel approach that achieves geometrically consistent and controllable 3D generation leveraging 3D Gaussian Splatting (3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), which encodes a point cloud and a single image into anchor latents and decode these latents into 3DGS, enabling efficient latent-space generation. To enable multi-view geometry consistent and controllable generation, we propose a Seed-Point-Driven strategy: first generate sparse seed points as a coarse geometry representation, then map them to anchor latents via the Seed-Anchor Mapping Module. Geometric consistency is ensured by the easily learned sparse seed points, and users can intuitively drag the seed points to deform the final 3DGS geometry, with changes propagated through the anchor latents. To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.16475v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16475v1",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field",
    "authors": [
      "Wenhao Hu",
      "Wenhao Chai",
      "Shengyu Hao",
      "Xiaotong Cui",
      "Xuexiang Wen",
      "Jenq-Neng Hwang",
      "Gaoang Wang"
    ],
    "abstract": "Achieving a consistent and compact 3D segmentation field is crucial for maintaining semantic coherence across views and accurately representing scene structures. Previous 3D scene segmentation methods rely on video segmentation models to address inconsistencies across views, but the absence of spatial information often leads to object misassociation when object temporarily disappear and reappear. Furthermore, in the process of 3D scene reconstruction, segmentation and optimization are often treated as separate tasks. As a result, optimization typically lacks awareness of semantic category information, which can result in floaters with ambiguous segmentation. To address these challenges, we introduce CCGS, a method designed to achieve both view consistent 2D segmentation and a compact 3D Gaussian segmentation field. CCGS incorporates pointmap association and a piecewise-plane constraint. First, we establish pixel correspondence between adjacent images by minimizing the Euclidean distance between their pointmaps. We then redefine object mask overlap accordingly. The Hungarian algorithm is employed to optimize mask association by minimizing the total matching cost, while allowing for partial matches. To further enhance compactness, the piecewise-plane constraint restricts point displacement within local planes during optimization, thereby preserving structural integrity. Experimental results on ScanNet and Replica datasets demonstrate that CCGS outperforms existing methods in both 2D panoptic segmentation and 3D Gaussian segmentation.",
    "arxiv_url": "http://arxiv.org/abs/2502.16303v1",
    "pdf_url": "http://arxiv.org/pdf/2502.16303v1",
    "published_date": "2025-02-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "compact",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis",
    "authors": [
      "Ziqian Ni",
      "Sicong Du",
      "Zhenghua Hou",
      "Chenming Wu",
      "Sheng Yang"
    ],
    "abstract": "To evaluate end-to-end autonomous driving systems, a simulation environment based on Novel View Synthesis (NVS) techniques is essential, which synthesizes photo-realistic images and point clouds from previously recorded sequences under new vehicle poses, particularly in cross-lane scenarios. Therefore, the development of a multi-lane dataset and benchmark is necessary. While recent synthetic scene-based NVS datasets have been prepared for cross-lane benchmarking, they still lack the realism of captured images and point clouds. To further assess the performance of existing methods based on NeRF and 3DGS, we present the first multi-lane dataset registering parallel scans specifically for novel driving view synthesis dataset derived from real-world scans, comprising 25 groups of associated sequences, including 16,000 front-view images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are labeled to differentiate moving objects from static elements. Using this dataset, we evaluate the performance of existing approaches in various testing scenarios at different lanes and distances. Additionally, our method provides the solution for solving and assessing the quality of multi-sensor poses for multi-modal data alignment for curating such a dataset in real-world. We plan to continually add new sequences to test the generalization of existing methods across different scenarios. The dataset is released publicly at the project page: https://nizqleo.github.io/paralane-dataset/.",
    "arxiv_url": "http://arxiv.org/abs/2502.15635v2",
    "pdf_url": "http://arxiv.org/pdf/2502.15635v2",
    "published_date": "2025-02-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "nerf",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes",
    "authors": [
      "Sicheng Yu",
      "Chong Cheng",
      "Yifan Zhou",
      "Xiaojun Yang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can produce high-fidelity novel views. However, previous GS-based methods primarily target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation models, hence underperforming in outdoor scenarios. To address this issue, we propose a RGB-only gaussian splatting SLAM method for unbounded outdoor scenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network to generate consistent pointmaps between frames for pose estimation. Compared to commonly used depth maps, pointmaps include spatial relationships and scene geometry across multiple views, enabling robust camera pose estimation. Then, we propose integrating the estimated camera poses with 3DGS rendering as an end-to-end differentiable pipeline. Our method achieves simultaneous optimization of camera poses and 3DGS scene parameters, significantly enhancing system tracking accuracy. Specifically, we also design an adaptive scale mapper for the pointmap regression network, which provides more accurate pointmap mapping to the 3DGS map representation. Our experiments on the Waymo dataset demonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS methods, and achieves state-of-the-art results in novel view synthesis. Project Page: https://3dagentworld.github.io/opengs-slam/",
    "arxiv_url": "http://arxiv.org/abs/2502.15633v1",
    "pdf_url": "http://arxiv.org/pdf/2502.15633v1",
    "published_date": "2025-02-21",
    "categories": [
      "cs.CV",
      "I.2.10; I.4.5"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "outdoor",
      "mapping",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation",
    "authors": [
      "Luzhou Ge",
      "Xiangyu Zhu",
      "Zhuo Yang",
      "Xuesong Li"
    ],
    "abstract": "In real-world scenarios, environment changes caused by human or agent activities make it extremely challenging for robots to perform various long-term tasks. Recent works typically struggle to effectively understand and adapt to dynamic environments due to the inability to update their environment representations in memory according to environment changes and lack of fine-grained reconstruction of the environments. To address these challenges, we propose DynamicGSG, a dynamic, high-fidelity, open-vocabulary scene graph construction system leveraging Gaussian splatting. DynamicGSG builds hierarchical scene graphs using advanced vision language models to represent the spatial and semantic relationships between objects in the environments, utilizes a joint feature loss we designed to supervise Gaussian instance grouping while optimizing the Gaussian maps, and locally updates the Gaussian scene graphs according to real environment changes for long-term environment adaptation. Experiments and ablation studies demonstrate the performance and efficacy of our proposed method in terms of semantic segmentation, language-guided object retrieval, and reconstruction quality. Furthermore, we validate the dynamic updating capabilities of our system in real laboratory environments. The source code and supplementary experimental materials will be released at:~\\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/Dynamic-GSG}.",
    "arxiv_url": "http://arxiv.org/abs/2502.15309v2",
    "pdf_url": "http://arxiv.org/pdf/2502.15309v2",
    "published_date": "2025-02-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "https://github.com/GeLuzhou/Dynamic-GSG",
    "keywords": [
      "high-fidelity",
      "semantic",
      "segmentation",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting",
    "authors": [
      "Qilin Zhang",
      "Olaf Wysocki",
      "Steffen Urban",
      "Boris Jutzi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown significant advantages in novel view synthesis (NVS), particularly in achieving high rendering speeds and high-quality results. However, its geometric accuracy in 3D reconstruction remains limited due to the lack of explicit geometric constraints during optimization. This paper introduces CDGS, a confidence-aware depth regularization approach developed to enhance 3DGS. We leverage multi-cue confidence maps of monocular depth estimation and sparse Structure-from-Motion depth to adaptively adjust depth supervision during the optimization process. Our method demonstrates improved geometric detail preservation in early training stages and achieves competitive performance in both NVS quality and geometric accuracy. Experiments on the publicly available Tanks and Temples benchmark dataset show that our method achieves more stable convergence behavior and more accurate geometric reconstruction results, with improvements of up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in M3C2 distance metrics. Notably, our method reaches comparable F-scores to the original 3DGS with only 50% of the training iterations. We expect this work will facilitate the development of efficient and accurate 3D reconstruction systems for real-world applications such as digital twin creation, heritage preservation, or forestry applications.",
    "arxiv_url": "http://arxiv.org/abs/2502.14684v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14684v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian Splatting Models",
    "authors": [
      "Miao Tao",
      "Yuanzhen Zhou",
      "Haoran Xu",
      "Zeyu He",
      "Zhenyu Yang",
      "Yuchang Zhang",
      "Zhongling Su",
      "Linning Xu",
      "Zhenxiang Ma",
      "Rong Fu",
      "Hengjie Li",
      "Xingcheng Zhang",
      "Jidong Zhai"
    ],
    "abstract": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant challenges in achieving real-time, high-fidelity performance on consumer-grade devices. Fully realizing the potential of 3DGS in applications such as virtual reality (VR) requires addressing critical system-level challenges to support real-time, immersive experiences. We propose GS-Cache, an end-to-end framework that seamlessly integrates 3DGS's advanced representation with a highly optimized rendering system. GS-Cache introduces a cache-centric pipeline to eliminate redundant computations, an efficiency-aware scheduler for elastic multi-GPU rendering, and optimized CUDA kernels to overcome computational bottlenecks. This synergy between 3DGS and system design enables GS-Cache to achieve up to 5.35x performance improvement, 35% latency reduction, and 42% lower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with high visual quality. By bridging the gap between 3DGS's representation power and the demands of VR systems, GS-Cache establishes a scalable and efficient framework for real-time neural rendering in immersive environments.",
    "arxiv_url": "http://arxiv.org/abs/2502.14938v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14938v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "vr",
      "face",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploiting Deblurring Networks for Radiance Fields",
    "authors": [
      "Haeyun Choi",
      "Heemin Yang",
      "Janghyeok Han",
      "Sunghyun Cho"
    ],
    "abstract": "In this paper, we propose DeepDeblurRF, a novel radiance field deblurring approach that can synthesize high-quality novel views from blurred training views with significantly reduced training time. DeepDeblurRF leverages deep neural network (DNN)-based deblurring modules to enjoy their deblurring performance and computational efficiency. To effectively combine DNN-based deblurring and radiance field construction, we propose a novel radiance field (RF)-guided deblurring and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner. Moreover, DeepDeblurRF is compatible with various scene representations, such as voxel grids and 3D Gaussians, expanding its applicability. We also present BlurRF-Synth, the first large-scale synthetic dataset for training radiance field deblurring frameworks. We conduct extensive experiments on both camera motion blur and defocus blur, demonstrating that DeepDeblurRF achieves state-of-the-art novel-view synthesis quality with significantly reduced training time.",
    "arxiv_url": "http://arxiv.org/abs/2502.14454v2",
    "pdf_url": "http://arxiv.org/pdf/2502.14454v2",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting",
    "authors": [
      "Boying Li",
      "Vuong Chi Hao",
      "Peter J. Stuckey",
      "Ian Reid",
      "Hamid Rezatofighi"
    ],
    "abstract": "We propose Hier-SLAM++, a comprehensive Neuro-Symbolic semantic 3D Gaussian Splatting SLAM method with both RGB-D and monocular input featuring an advanced hierarchical categorical representation, which enables accurate pose estimation as well as global 3D semantic mapping. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making scene understanding particularly challenging and costly. To address this problem, we introduce a novel and general hierarchical representation that encodes both semantic and geometric information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs) as well as the 3D generative model. By utilizing the proposed hierarchical tree structure, semantic information is symbolically represented and learned in an end-to-end manner. We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization. Additionally, we propose an improved SLAM system to support both RGB-D and monocular inputs using a feed-forward model. To the best of our knowledge, this is the first semantic monocular Gaussian Splatting SLAM system, significantly reducing sensor requirements for 3D semantic understanding and broadening the applicability of semantic Gaussian SLAM system. We conduct experiments on both synthetic and real-world datasets, demonstrating superior or on-par performance with state-of-the-art NeRF-based and Gaussian-based SLAM systems, while significantly reducing storage and training time requirements.",
    "arxiv_url": "http://arxiv.org/abs/2502.14931v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14931v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "mapping",
      "understanding",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving",
    "authors": [
      "Yedong Shen",
      "Xinran Zhang",
      "Yifan Duan",
      "Shiqi Zhang",
      "Heng Li",
      "Yilong Wu",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "abstract": "Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead.",
    "arxiv_url": "http://arxiv.org/abs/2502.14235v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14235v1",
    "published_date": "2025-02-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "autonomous driving",
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian",
    "authors": [
      "Bang Du",
      "Runfa Blark Li",
      "Chen Du",
      "Truong Nguyen"
    ],
    "abstract": "The reconstruction of 3D objects from calibrated photographs represents a fundamental yet intricate challenge in the domains of computer graphics and vision. Although neural reconstruction approaches based on Neural Radiance Fields (NeRF) have shown remarkable capabilities, their processing costs remain substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely improves the training efficiency and facilitates to generate realistic rendering in real-time. However, due to the limited ability of Spherical Harmonics (SH) to represent high-frequency information, 3D-GS falls short in reconstructing glossy objects. Researchers have turned to enhance the specular expressiveness of 3D-GS through inverse rendering. Yet these methods often struggle to maintain the training and rendering efficiency, undermining the benefits of Gaussian Splatting techniques. In this paper, we introduce GlossGau, an efficient inverse rendering framework that reconstructs scenes with glossy surfaces while maintaining training and rendering speeds comparable to vanilla 3D-GS. Specifically, we explicitly model the surface normals, Bidirectional Reflectance Distribution Function (BRDF) parameters, as well as incident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the per-Gaussian Normal Distribution Function under the microfacet model. We utilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply regularization to significantly alleviate the normal estimation challenge encountered in related works. Experiments demonstrate that GlossGau achieves competitive or superior reconstruction on datasets with glossy surfaces. Compared with previous GS-based works that address the specular surface, our optimization time is considerably less.",
    "arxiv_url": "http://arxiv.org/abs/2502.14129v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14129v1",
    "published_date": "2025-02-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting aided Localization for Large and Complex Indoor-Environments",
    "authors": [
      "Vincent Ress",
      "Jonas Meyer",
      "Wei Zhang",
      "David Skuddis",
      "Uwe Soergel",
      "Norbert Haala"
    ],
    "abstract": "The field of visual localization has been researched for several decades and has meanwhile found many practical applications. Despite the strong progress in this field, there are still challenging situations in which established methods fail. We present an approach to significantly improve the accuracy and reliability of established visual localization methods by adding rendered images. In detail, we first use a modern visual SLAM approach that provides a 3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate that enriching reference data with images rendered from 3DGS at randomly sampled poses significantly improves the performance of both geometry-based visual localization and Scene Coordinate Regression (SCR) methods. Through comprehensive evaluation in a large industrial environment, we analyze the performance impact of incorporating these additional rendered views.",
    "arxiv_url": "http://arxiv.org/abs/2502.13803v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13803v1",
    "published_date": "2025-02-19",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction",
    "authors": [
      "Gan Chen",
      "Ying He",
      "Mulin Yu",
      "F. Richard Yu",
      "Gang Xu",
      "Fei Ma",
      "Ming Li",
      "Guang Zhou"
    ],
    "abstract": "Recent advancements in implicit 3D reconstruction methods, e.g., neural rendering fields and Gaussian splatting, have primarily focused on novel view synthesis of static or dynamic objects with continuous motion states. However, these approaches struggle to efficiently model a human-interactive object with n movable parts, requiring 2^n separate models to represent all discrete states. To overcome this limitation, we propose Inter3D, a new benchmark and approach for novel state synthesis of human-interactive objects. We introduce a self-collected dataset featuring commonly encountered interactive objects and a new evaluation pipeline, where only individual part states are observed during training, while part combination states remain unseen. We also propose a strong baseline approach that leverages Space Discrepancy Tensors to efficiently modelling all states of an object. To alleviate the impractical constraints on camera trajectories across training states, we propose a Mutual State Regularization mechanism to enhance the spatial density consistency of movable parts. In addition, we explore two occupancy grid sampling strategies to facilitate training efficiency. We conduct extensive experiments on the proposed benchmark, showcasing the challenges of the task and the superiority of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2502.14004v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14004v1",
    "published_date": "2025-02-19",
    "categories": [
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "human",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "authors": [
      "Hao Gao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Bencheng Liao",
      "Yiang Shi",
      "Xiaoyang Guo",
      "Yuechuan Pu",
      "Haoran Yin",
      "Xiangyu Li",
      "Xinbang Zhang",
      "Ying Zhang",
      "Wenyu Liu",
      "Qian Zhang",
      "Xinggang Wang"
    ],
    "abstract": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD.",
    "arxiv_url": "http://arxiv.org/abs/2502.13144v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13144v1",
    "published_date": "2025-02-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "human",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis",
    "authors": [
      "Pedro Martin",
      "Ant√≥nio Rodrigues",
      "Jo√£o Ascenso",
      "Maria Paula Queluz"
    ],
    "abstract": "Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.",
    "arxiv_url": "http://arxiv.org/abs/2502.13196v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13196v1",
    "published_date": "2025-02-18",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection",
    "authors": [
      "Jingtong Yue",
      "Zhiwei Lin",
      "Xin Lin",
      "Xiaoyu Zhou",
      "Xiangtai Li",
      "Lu Qi",
      "Yongtao Wang",
      "Ming-Hsuan Yang"
    ],
    "abstract": "While recent low-cost radar-camera approaches have shown promising results in multi-modal 3D object detection, both sensors face challenges from environmental and intrinsic disturbances. Poor lighting or adverse weather conditions degrade camera performance, while radar suffers from noise and positional ambiguity. Achieving robust radar-camera 3D object detection requires consistent performance across varying conditions, a topic that has not yet been fully explored. In this work, we first conduct a systematic analysis of robustness in radar-camera detection on five kinds of noises and propose RobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D Gaussian Expansion (3DGE) module to mitigate inaccuracies in radar points, including position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS and velocity priors to generate a deformable kernel map and variance for kernel size adjustment and value distribution. Additionally, we introduce a weather-adaptive fusion module, which adaptively fuses radar and camera features based on camera signal confidence. Extensive experiments on the popular benchmark, nuScenes, show that our model achieves competitive results in regular and noisy conditions.",
    "arxiv_url": "http://arxiv.org/abs/2502.13071v1",
    "pdf_url": "http://arxiv.org/pdf/2502.13071v1",
    "published_date": "2025-02-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadSplatter: Extending 3D Gaussian Splatting to Radio Frequencies for Wireless Radiomap Extrapolation",
    "authors": [
      "Yiheng Wang",
      "Ye Xue",
      "Shutao Zhang",
      "Tsung-Hui Chang"
    ],
    "abstract": "A radiomap represents the spatial distribution of wireless signal strength, critical for applications like network optimization and autonomous driving. However, constructing radiomap relies on measuring radio signal power across the entire system, which is costly in outdoor environments due to large network scales. We present RadSplatter, a framework that extends 3D Gaussian Splatting (3DGS) to radio frequencies for efficient and accurate radiomap extrapolation from sparse measurements. RadSplatter models environmental scatterers and radio paths using 3D Gaussians, capturing key factors of radio wave propagation. It employs a relaxed-mean (RM) scheme to reparameterize the positions of 3D Gaussians from noisy and dense 3D point clouds. A camera-free 3DGS-based projection is proposed to map 3D Gaussians onto 2D radio beam patterns. Furthermore, a regularized loss function and recursive fine-tuning using highly structured sparse measurements in real-world settings are applied to ensure robust generalization. Experiments on synthetic and real-world data show state-of-the-art extrapolation accuracy and execution speed.",
    "arxiv_url": "http://arxiv.org/abs/2502.12686v1",
    "pdf_url": "http://arxiv.org/pdf/2502.12686v1",
    "published_date": "2025-02-18",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "outdoor",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation",
    "authors": [
      "Ziye Wang",
      "Yiran Qin",
      "Lin Zeng",
      "Ruimao Zhang"
    ],
    "abstract": "Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over $16\\times$ higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions.",
    "arxiv_url": "http://arxiv.org/abs/2502.14895v1",
    "pdf_url": "http://arxiv.org/pdf/2502.14895v1",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PUGS: Zero-shot Physical Understanding with Gaussian Splatting",
    "authors": [
      "Yinghao Shuai",
      "Ran Yu",
      "Yuantao Chen",
      "Zijian Jiang",
      "Xiaowei Song",
      "Nan Wang",
      "Jv Zheng",
      "Jianzhu Ma",
      "Meng Yang",
      "Zhicheng Wang",
      "Wenbo Ding",
      "Hao Zhao"
    ],
    "abstract": "Current robotic systems can understand the categories and poses of objects well. But understanding physical properties like mass, friction, and hardness, in the wild, remains challenging. We propose a new method that reconstructs 3D objects using the Gaussian splatting representation and predicts various physical properties in a zero-shot manner. We propose two techniques during the reconstruction phase: a geometry-aware regularization loss function to improve the shape quality and a region-aware feature contrastive loss function to promote region affinity. Two other new techniques are designed during inference: a feature-based property propagation module and a volume integration module tailored for the Gaussian representation. Our framework is named as zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS achieves new state-of-the-art results on the standard benchmark of ABO-500 mass prediction. We provide extensive quantitative ablations and qualitative visualization to demonstrate the mechanism of our designs. We show the proposed methodology can help address challenging real-world grasping tasks. Our codes, data, and models are available at https://github.com/EverNorif/PUGS",
    "arxiv_url": "http://arxiv.org/abs/2502.12231v2",
    "pdf_url": "http://arxiv.org/pdf/2502.12231v2",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/EverNorif/PUGS",
    "keywords": [
      "understanding",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Inpainting with Depth-Guided Cross-View Consistency",
    "authors": [
      "Sheng-Yu Huang",
      "Zi-Ting Chou",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively.",
    "arxiv_url": "http://arxiv.org/abs/2502.11801v2",
    "pdf_url": "http://arxiv.org/pdf/2502.11801v2",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring the Versal AI Engine for 3D Gaussian Splatting",
    "authors": [
      "Kotaro Shimamura",
      "Ayumi Ohno",
      "Shinya Takamaeda-Yamazaki"
    ],
    "abstract": "Dataflow-oriented spatial architectures are the emerging paradigm for higher computation performance and efficiency.   AMD Versal AI Engine is a commercial spatial architecture consisting of tiles of VLIW processors supporting SIMD operations arranged in a two-dimensional mesh.   The architecture requires the explicit design of task assignments and dataflow configurations for each tile to maximize performance, demanding advanced techniques and meticulous design.   However, a few works revealed the performance characteristics of the Versal AI Engine through practical workloads.   In this work, we provide the comprehensive performance evaluation of the Versal AI Engine using Gaussian feature computation in 3D Gaussian splatting as a practical workload, and we then propose a novel dedicated algorithm to fully exploit the hardware architecture.   The computations of 3D Gaussian splatting include matrix multiplications and color computations utilizing high-dimensional spherical harmonic coefficients.   These tasks are processed efficiently by leveraging the SIMD capabilities and their instruction-level parallelism.   Additionally, pipelined processing is achieved by assigning different tasks to individual cores, thereby fully exploiting the spatial parallelism of AI Engines.   The proposed method demonstrated a 226-fold throughput increase in simulation-based evaluation, outperforming a naive approach.   These findings provide valuable insights for application development that effectively harnesses the spatial and architectural advantages of AI Engines.",
    "arxiv_url": "http://arxiv.org/abs/2502.11782v1",
    "pdf_url": "http://arxiv.org/pdf/2502.11782v1",
    "published_date": "2025-02-17",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text",
    "authors": [
      "Gyumin Shim",
      "Sangmin Lee",
      "Jaegul Choo"
    ],
    "abstract": "In this paper, we introduce GaussianMotion, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting. Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control. In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses. By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner. Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results. Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.",
    "arxiv_url": "http://arxiv.org/abs/2502.11642v1",
    "pdf_url": "http://arxiv.org/pdf/2502.11642v1",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "motion",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMG: Opacity Matters in Material Modeling with Gaussian Splatting",
    "authors": [
      "Silong Yong",
      "Venkata Nagarjun Pudureddiyur Manivannan",
      "Bernhard Kerbl",
      "Zifu Wan",
      "Simon Stepputtis",
      "Katia Sycara",
      "Yaqi Xie"
    ],
    "abstract": "Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting has boosted it to the next level by showing real-time rendering potentials. An intuitive finding is that the models used for inverse rendering do not take into account the dependency of opacity w.r.t. material properties, namely cross section, as suggested by optics. Therefore, we develop a novel approach that adds this dependency to the modeling itself. Inspired by radiative transfer, we augment the opacity term by introducing a neural network that takes as input material properties to provide modeling of cross section and a physically correct activation function. The gradients for material properties are therefore not only from color but also from opacity, facilitating a constraint for their optimization. Therefore, the proposed method incorporates more accurate physical properties compared to previous works. We implement our method into 3 different baselines that use Gaussian Splatting for inverse rendering and achieve significant improvements universally in terms of novel view synthesis and material modeling.",
    "arxiv_url": "http://arxiv.org/abs/2502.10988v2",
    "pdf_url": "http://arxiv.org/pdf/2502.10988v2",
    "published_date": "2025-02-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-GVINS: A Tightly-integrated GNSS-Visual-Inertial Navigation System Augmented by 3D Gaussian Splatting",
    "authors": [
      "Zelin Zhou",
      "Saurav Uprety",
      "Shichuang Nie",
      "Hongzhou Yang"
    ],
    "abstract": "Recently, the emergence of 3D Gaussian Splatting (3DGS) has drawn significant attention in the area of 3D map reconstruction and visual SLAM. While extensive research has explored 3DGS for indoor trajectory tracking using visual sensor alone or in combination with Light Detection and Ranging (LiDAR) and Inertial Measurement Unit (IMU), its integration with GNSS for large-scale outdoor navigation remains underexplored. To address these concerns, we proposed GS-GVINS: a tightly-integrated GNSS-Visual-Inertial Navigation System augmented by 3DGS. This system leverages 3D Gaussian as a continuous differentiable scene representation in largescale outdoor environments, enhancing navigation performance through the constructed 3D Gaussian map. Notably, GS-GVINS is the first GNSS-Visual-Inertial navigation application that directly utilizes the analytical jacobians of SE3 camera pose with respect to 3D Gaussians. To maintain the quality of 3DGS rendering in extreme dynamic states, we introduce a motionaware 3D Gaussian pruning mechanism, updating the map based on relative pose translation and the accumulated opacity along the camera ray. For validation, we test our system under different driving environments: open-sky, sub-urban, and urban. Both self-collected and public datasets are used for evaluation. The results demonstrate the effectiveness of GS-GVINS in enhancing navigation accuracy across diverse driving environments.",
    "arxiv_url": "http://arxiv.org/abs/2502.10975v1",
    "pdf_url": "http://arxiv.org/pdf/2502.10975v1",
    "published_date": "2025-02-16",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "outdoor",
      "motion",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting",
    "authors": [
      "Sohaib Zahid",
      "Viktor Rudnev",
      "Eddy Ilg",
      "Vladislav Golyanik"
    ],
    "abstract": "Novel view synthesis techniques predominantly utilize RGB cameras, inheriting their limitations such as the need for sufficient lighting, susceptibility to motion blur, and restricted dynamic range. In contrast, event cameras are significantly more resilient to these limitations but have been less explored in this domain, particularly in large-scale settings. Current methodologies primarily focus on front-facing or object-oriented (360-degree view) scenarios. For the first time, we introduce 3D Gaussians for event-based novel view synthesis. Our method reconstructs large and unbounded scenes with high visual quality. We contribute the first real and synthetic event datasets tailored for this setting. Our method demonstrates superior novel view synthesis and consistently outperforms the baseline EventNeRF by a margin of 11-25% in PSNR (dB) while being orders of magnitude faster in reconstruction and rendering.",
    "arxiv_url": "http://arxiv.org/abs/2502.10827v1",
    "pdf_url": "http://arxiv.org/pdf/2502.10827v1",
    "published_date": "2025-02-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "fast",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction",
    "authors": [
      "Youming Deng",
      "Wenqi Xian",
      "Guandao Yang",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Steve Marschner",
      "Paul Debevec"
    ],
    "abstract": "In this paper, we present a self-calibrating framework that jointly optimizes camera parameters, lens distortion and 3D Gaussian representations, enabling accurate and efficient scene reconstruction. In particular, our technique enables high-quality scene reconstruction from Large field-of-view (FOV) imagery taken with wide-angle lenses, allowing the scene to be modeled from a smaller number of images. Our approach introduces a novel method for modeling complex lens distortions using a hybrid network that combines invertible residual networks with explicit grids. This design effectively regularizes the optimization process, achieving greater accuracy than conventional camera models. Additionally, we propose a cubemap-based resampling strategy to support large FOV images without sacrificing resolution or introducing distortion artifacts. Our method is compatible with the fast rasterization of Gaussian Splatting, adaptable to a wide variety of camera lens distortion, and demonstrates state-of-the-art performance on both synthetic and real-world datasets.",
    "arxiv_url": "http://arxiv.org/abs/2502.09563v2",
    "pdf_url": "http://arxiv.org/pdf/2502.09563v2",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks",
    "authors": [
      "Zihang Cheng",
      "Huiping Zhuang",
      "Chun Li",
      "Xin Meng",
      "Ming Li",
      "Fei Richard Yu",
      "Liqiang Nie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protection. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages while keeping the original 3DGS scene almost unchanged. Generally, we have a X-SG$^2$S injector for adding multi-modal messages simultaneously and an extractor for extract them. Specifically, we first split the watermarks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermarking. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the location recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG$^2$S can effectively conceal multi modal messages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Meanwhile, with simple and efficient model structure and high practicality, X-SG$^2$S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to 3D watermarking model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later researches.",
    "arxiv_url": "http://arxiv.org/abs/2502.10475v2",
    "pdf_url": "http://arxiv.org/pdf/2502.10475v2",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior",
    "authors": [
      "Mingrui Li",
      "Shuhong Liu",
      "Tianchen Deng",
      "Hongyu Wang"
    ],
    "abstract": "Gaussian SLAM systems excel in real-time rendering and fine-grained reconstruction compared to NeRF-based systems. However, their reliance on extensive keyframes is impractical for deployment in real-world robotic systems, which typically operate under sparse-view conditions that can result in substantial holes in the map. To address these challenges, we introduce DenseSplat, the first SLAM system that effectively combines the advantages of NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for initializing primitives that densely populate maps and seamlessly fill gaps. It also implements geometry-aware primitive sampling and pruning strategies to manage granularity and enhance rendering efficiency. Moreover, DenseSplat integrates loop closure and bundle adjustment, significantly enhancing frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale datasets demonstrate that DenseSplat achieves superior performance in tracking and mapping compared to current state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.09111v1",
    "pdf_url": "http://arxiv.org/pdf/2502.09111v1",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "tracking",
      "mapping",
      "geometry",
      "real-time rendering",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization",
    "authors": [
      "Qiwei Wang",
      "Shaoxun Wu",
      "Yujiao Shi"
    ],
    "abstract": "This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.",
    "arxiv_url": "http://arxiv.org/abs/2502.09080v2",
    "pdf_url": "http://arxiv.org/pdf/2502.09080v2",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting",
    "authors": [
      "Lingting Zhu",
      "Guying Lin",
      "Jinnan Chen",
      "Xinjie Zhang",
      "Zhenchao Jin",
      "Zhao Wang",
      "Lequan Yu"
    ],
    "abstract": "While Implicit Neural Representations (INRs) have demonstrated significant success in image representation, they are often hindered by large training memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged as a promising solution in 3D reconstruction due to its high-quality novel view synthesis and rapid rendering capabilities, positioning it as a valuable tool for a broad spectrum of applications. In particular, a GS-based representation, 2DGS, has shown potential for image fitting. In our work, we present \\textbf{L}arge \\textbf{I}mages are \\textbf{G}aussians (\\textbf{LIG}), which delves deeper into the application of 2DGS for image representations, addressing the challenge of fitting large images with 2DGS in the situation of numerous Gaussian points, through two distinct modifications: 1) we adopt a variant of representation and optimization strategy, facilitating the fitting of a large number of Gaussian points; 2) we propose a Level-of-Gaussian approach for reconstructing both coarse low-frequency initialization and fine high-frequency details. Consequently, we successfully represent large images as Gaussian points and achieve high-quality large image representation, demonstrating its efficacy across various types of large images. Code is available at {\\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.",
    "arxiv_url": "http://arxiv.org/abs/2502.09039v1",
    "pdf_url": "http://arxiv.org/pdf/2502.09039v1",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/HKU-MedAI/LIG",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video",
    "authors": [
      "Yu Hong",
      "Yize Wu",
      "Zhehao Shen",
      "Chengcheng Guo",
      "Yuheng Jiang",
      "Yingliang Zhang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "Volumetric video enables immersive experiences by capturing dynamic 3D scenes, enabling diverse applications for virtual reality, education, and telepresence. However, traditional methods struggle with fixed lighting conditions, while neural approaches face trade-offs in efficiency, quality, or adaptability for relightable scenarios. To address these limitations, we present BEAM, a novel pipeline that bridges 4D Gaussian representations with physically-based rendering (PBR) to produce high-quality, relightable volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry and PBR properties via a series of available Gaussian-based techniques. It first combines Gaussian-based performance tracking with geometry-aware rasterization in a coarse-to-fine optimization framework to recover spatially and temporally consistent geometries. We further enhance Gaussian attributes by incorporating PBR properties step by step. We generate roughness via a multi-view-conditioned diffusion model, and then derive AO and base color using a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for efficient visibility computation. Once recovered, these dynamic, relightable assets integrate seamlessly into traditional CG pipelines, supporting real-time rendering with deferred shading and offline rendering with ray tracing. By offering realistic, lifelike visualizations under diverse lighting conditions, BEAM opens new possibilities for interactive entertainment, storytelling, and creative visualization.",
    "arxiv_url": "http://arxiv.org/abs/2502.08297v1",
    "pdf_url": "http://arxiv.org/pdf/2502.08297v1",
    "published_date": "2025-02-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "ray tracing",
      "tracking",
      "lighting",
      "face",
      "geometry",
      "4d",
      "real-time rendering",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interactive Holographic Visualization for 3D Facial Avatar",
    "authors": [
      "Tri Tung Nguyen Nguyen",
      "Fujii Yasuyuki",
      "Dinh Tuan Tran",
      "Joo-Ho Lee"
    ],
    "abstract": "Traditional methods for visualizing dynamic human expressions, particularly in medical training, often rely on flat-screen displays or static mannequins, which have proven inefficient for realistic simulation. In response, we propose a platform that leverages a 3D interactive facial avatar capable of displaying non-verbal feedback, including pain signals. This avatar is projected onto a stereoscopic, view-dependent 3D display, offering a more immersive and realistic simulated patient experience for pain assessment practice. However, there is no existing solution that dynamically predicts and projects interactive 3D facial avatars in real-time. To overcome this, we emphasize the need for a 3D display projection system that can project the facial avatar holographically, allowing users to interact with the avatar from any viewpoint. By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent calibration, we significantly improve the training environment for accurate pain recognition and assessment.",
    "arxiv_url": "http://arxiv.org/abs/2502.08085v1",
    "pdf_url": "http://arxiv.org/pdf/2502.08085v1",
    "published_date": "2025-02-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "efficient",
      "medical",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization",
    "authors": [
      "Rafa≈Ç Tobiasz",
      "Grzegorz Wilczy≈Ñski",
      "Marcin Mazur",
      "S≈Çawomir Tadeja",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer graphics. GS-based algorithms almost always bypass classical methods such as ray tracing, which offers numerous inherent advantages for rendering. For example, ray tracing is able to handle incoherent rays for advanced lighting effects, including shadows and reflections. To address this limitation, we introduce MeshSplats, a method which converts GS to a mesh-like format. Following the completion of training, MeshSplats transforms Gaussian elements into mesh faces, enabling rendering using ray tracing methods with all their associated benefits. Our model can be utilized immediately following transformation, yielding a mesh of slightly reduced quality without additional training. Furthermore, we can enhance the reconstruction quality through the application of a dedicated optimization algorithm that operates on mesh faces rather than Gaussian components. The efficacy of our method is substantiated by experimental results, underscoring its extensive applications in computer graphics and image processing.",
    "arxiv_url": "http://arxiv.org/abs/2502.07754v1",
    "pdf_url": "http://arxiv.org/pdf/2502.07754v1",
    "published_date": "2025-02-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "reflection",
      "lighting",
      "shadow",
      "face",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors",
    "authors": [
      "Lin-Zhuo Chen",
      "Kangjie Liu",
      "Youtian Lin",
      "Siyu Zhu",
      "Zhihao Li",
      "Xun Cao",
      "Yao Yao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with fast training and rendering speed. However, its optimization process lacks explicit geometric constraints, leading to suboptimal geometric reconstruction in regions with sparse or no observational input views. In this work, we try to mitigate the issue by incorporating a pre-trained matching prior to the 3DGS optimization process. We introduce Flow Distillation Sampling (FDS), a technique that leverages pre-trained geometric knowledge to bolster the accuracy of the Gaussian radiance field. Our method employs a strategic sampling technique to target unobserved views adjacent to the input views, utilizing the optical flow calculated from the matching model (Prior Flow) to guide the flow analytically calculated from the 3DGS geometry (Radiance Flow). Comprehensive experiments in depth rendering, mesh reconstruction, and novel view synthesis showcase the significant advantages of FDS over state-of-the-art methods. Additionally, our interpretive experiments and analysis aim to shed light on the effects of FDS on geometric accuracy and rendering quality, potentially providing readers with insights into its performance. Project page: https://nju-3dv.github.io/projects/fds",
    "arxiv_url": "http://arxiv.org/abs/2502.07615v1",
    "pdf_url": "http://arxiv.org/pdf/2502.07615v1",
    "published_date": "2025-02-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The establishment of static digital humans and the integration with spinal models",
    "authors": [
      "Fujiao Ju",
      "Yuxuan Wang",
      "Shuo Wang",
      "Chengyin Wang",
      "Yinbo Chen",
      "Jianfeng Li",
      "Mingjie Dong",
      "Bin Fang",
      "Qianyu Zhuang"
    ],
    "abstract": "Adolescent idiopathic scoliosis (AIS), a prevalent spinal deformity, significantly affects individuals' health and quality of life. Conventional imaging techniques, such as X - rays, computed tomography (CT), and magnetic resonance imaging (MRI), offer static views of the spine. However, they are restricted in capturing the dynamic changes of the spine and its interactions with overall body motion. Therefore, developing new techniques to address these limitations has become extremely important. Dynamic digital human modeling represents a major breakthrough in digital medicine. It enables a three - dimensional (3D) view of the spine as it changes during daily activities, assisting clinicians in detecting deformities that might be missed in static imaging. Although dynamic modeling holds great potential, constructing an accurate static digital human model is a crucial initial step for high - precision simulations. In this study, our focus is on constructing an accurate static digital human model integrating the spine, which is vital for subsequent dynamic digital human research on AIS. First, we generate human point - cloud data by combining the 3D Gaussian method with the Skinned Multi - Person Linear (SMPL) model from the patient's multi - view images. Then, we fit a standard skeletal model to the generated human model. Next, we align the real spine model reconstructed from CT images with the standard skeletal model. We validated the resulting personalized spine model using X - ray data from six AIS patients, with Cobb angles (used to measure the severity of scoliosis) as evaluation metrics. The results indicate that the model's error was within 1 degree of the actual measurements. This study presents an important method for constructing digital humans.",
    "arxiv_url": "http://arxiv.org/abs/2502.07844v1",
    "pdf_url": "http://arxiv.org/pdf/2502.07844v1",
    "published_date": "2025-02-11",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TranSplat: Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation",
    "authors": [
      "Jeongyun Kim",
      "Jeongho Noh",
      "Dong-Guw Lee",
      "Ayoung Kim"
    ],
    "abstract": "Transparent object manipulation remains a significant challenge in robotics due to the difficulty of acquiring accurate and dense depth measurements. Conventional depth sensors often fail with transparent objects, resulting in incomplete or erroneous depth data. Existing depth completion methods struggle with interframe consistency and incorrectly model transparent objects as Lambertian surfaces, leading to poor depth reconstruction. To address these challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian Splatting method tailored for transparent objects. TranSplat uses a latent diffusion model to generate surface embeddings that provide consistent and continuous representations, making it robust to changes in viewpoint and lighting. By integrating these surface embeddings with input RGB images, TranSplat effectively captures the complexities of transparent surfaces, enhancing the splatting of 3D Gaussians and improving depth completion. Evaluations on synthetic and real-world transparent object benchmarks, as well as robot grasping tasks, show that TranSplat achieves accurate and dense depth completion, demonstrating its effectiveness in practical applications. We open-source synthetic dataset and model: https://github. com/jeongyun0609/TranSplat",
    "arxiv_url": "http://arxiv.org/abs/2502.07840v1",
    "pdf_url": "http://arxiv.org/pdf/2502.07840v1",
    "published_date": "2025-02-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC",
    "authors": [
      "Siwei Meng",
      "Yawei Luo",
      "Ping Liu"
    ],
    "abstract": "Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation.",
    "arxiv_url": "http://arxiv.org/abs/2502.07007v1",
    "pdf_url": "http://arxiv.org/pdf/2502.07007v1",
    "published_date": "2025-02-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "lighting",
      "motion",
      "deformation",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps",
    "authors": [
      "Ola Shorinwa",
      "Jiankai Sun",
      "Mac Schwager",
      "Anirudha Majumdar"
    ],
    "abstract": "We present SIREN for registration of multi-robot Gaussian Splatting (GSplat) maps, with zero access to camera poses, images, and inter-map transforms for initialization or fusion of local submaps. To realize these capabilities, SIREN harnesses the versatility and robustness of semantics in three critical ways to derive a rigorous registration pipeline for multi-robot GSplat maps. First, SIREN utilizes semantics to identify feature-rich regions of the local maps where the registration problem is better posed, eliminating the need for any initialization which is generally required in prior work. Second, SIREN identifies candidate correspondences between Gaussians in the local maps using robust semantic features, constituting the foundation for robust geometric optimization, coarsely aligning 3D Gaussian primitives extracted from the local maps. Third, this key step enables subsequent photometric refinement of the transformation between the submaps, where SIREN leverages novel-view synthesis in GSplat maps along with a semantics-based image filter to compute a high-accuracy non-rigid transformation for the generation of a high-fidelity fused map. We demonstrate the superior performance of SIREN compared to competing baselines across a range of real-world datasets, and in particular, across the most widely-used robot hardware platforms, including a manipulator, drone, and quadruped. In our experiments, SIREN achieves about 90x smaller rotation errors, 300x smaller translation errors, and 44x smaller scale errors in the most challenging scenes, where competing methods struggle. We will release the code and provide a link to the project page after the review process.",
    "arxiv_url": "http://arxiv.org/abs/2502.06519v1",
    "pdf_url": "http://arxiv.org/pdf/2502.06519v1",
    "published_date": "2025-02-10",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Three-Dimensional MRI Reconstruction with Gaussian Representations: Tackling the Undersampling Problem",
    "authors": [
      "Tengya Peng",
      "Ruyi Zha",
      "Zhen Li",
      "Xiaofeng Liu",
      "Qing Zou"
    ],
    "abstract": "Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in the field of computer vision, but remains unexplored in the field of magnetic resonance imaging (MRI). This study explores its potential for the reconstruction of isotropic resolution 3D MRI from undersampled k-space data. We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs 3D Gaussian distributions as an explicit representation for MR volumes. Experimental evaluations indicate that this method can effectively reconstruct voxelized MR images, achieving a quality on par with that of well-established 3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR scheme operates under a self-supervised framework, obviating the need for extensive training datasets or prior model training. This approach introduces significant innovations to the domain, notably the adaptation of 3DGS to MRI reconstruction and the novel application of the existing 3DGS methodology to decompose MR signals, which are presented in a complex-valued format.",
    "arxiv_url": "http://arxiv.org/abs/2502.06510v1",
    "pdf_url": "http://arxiv.org/pdf/2502.06510v1",
    "published_date": "2025-02-10",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform",
    "authors": [
      "Kyle Gao",
      "Dening Lu",
      "Liangzhi Li",
      "Nan Chen",
      "Hongjie He",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building's 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building's address, postal code, or geographic coordinates.",
    "arxiv_url": "http://arxiv.org/abs/2502.05769v3",
    "pdf_url": "http://arxiv.org/pdf/2502.05769v3",
    "published_date": "2025-02-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map",
    "authors": [
      "Yue Pan",
      "Xingguang Zhong",
      "Liren Jin",
      "Louis Wiesmann",
      "Marija Popoviƒá",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Robots require high-fidelity reconstructions of their environment for effective operation. Such scene representations should be both, geometrically accurate and photorealistic to support downstream tasks. While this can be achieved by building distance fields from range sensors and radiance fields from cameras, the scalable incremental mapping of both fields consistently and at the same time with high quality remains challenging. In this paper, we propose a novel map representation that unifies a continuous signed distance field and a Gaussian splatting radiance field within an elastic and compact point-based implicit neural map. By enforcing geometric consistency between these fields, we achieve mutual improvements by exploiting both modalities. We devise a LiDAR-visual SLAM system called PINGS using the proposed map representation and evaluate it on several challenging large-scale datasets. Experimental results demonstrate that PINGS can incrementally build globally consistent distance and radiance fields encoded with a compact set of neural points. Compared to the state-of-the-art methods, PINGS achieves superior photometric and geometric rendering at novel views by leveraging the constraints from the distance field. Furthermore, by utilizing dense photometric cues and multi-view consistency from the radiance field, PINGS produces more accurate distance fields, leading to improved odometry estimation and mesh reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2502.05752v1",
    "pdf_url": "http://arxiv.org/pdf/2502.05752v1",
    "published_date": "2025-02-09",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "mapping",
      "high quality",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment",
    "authors": [
      "Maneesha Wickramasuriya",
      "Beomyeol Yu",
      "Taeyoung Lee",
      "Murray Snyder"
    ],
    "abstract": "This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.",
    "arxiv_url": "http://arxiv.org/abs/2502.05409v1",
    "pdf_url": "http://arxiv.org/pdf/2502.05409v1",
    "published_date": "2025-02-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360¬∞ Unbounded Scene Inpainting",
    "authors": [
      "Chung-Ho Wu",
      "Yang-Jung Chen",
      "Ying-Huan Chen",
      "Jie-Ying Lee",
      "Bo-Hsu Ke",
      "Chun-Wei Tuan Mu",
      "Yi-Chuan Huang",
      "Chin-Yang Lin",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ],
    "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes.",
    "arxiv_url": "http://arxiv.org/abs/2502.05176v3",
    "pdf_url": "http://arxiv.org/pdf/2502.05176v3",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering",
    "authors": [
      "Lo√Øck Chambon",
      "Eloi Zablocki",
      "Alexandre Boulch",
      "Micka√´l Chen",
      "Matthieu Cord"
    ],
    "abstract": "Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving. Recent advances in 3D occupancy prediction have improved scene representation but often suffer from spatial inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. In this paper, we propose GaussRender, a module that improves 3D occupancy learning by enforcing projective consistency. Our key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views, where we apply supervision. Our method penalizes 3D configurations that produce inconsistent 2D projections, thereby enforcing a more coherent 3D structure. To achieve this efficiently, we leverage differentiable rendering with Gaussian splatting. GaussRender seamlessly integrates with existing architectures while maintaining efficiency and requiring no inference-time modifications. Extensive evaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate that GaussRender significantly improves geometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), achieving state-of-the-art results, particularly on surface-sensitive metrics. The code is open-sourced at https://github.com/valeoai/GaussRender.",
    "arxiv_url": "http://arxiv.org/abs/2502.05040v2",
    "pdf_url": "http://arxiv.org/pdf/2502.05040v2",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/valeoai/GaussRender",
    "keywords": [
      "localization",
      "efficient",
      "autonomous driving",
      "face",
      "semantic",
      "understanding",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting",
    "authors": [
      "Xiaoyu Zhou",
      "Jingqi Wang",
      "Yongtao Wang",
      "Yufei Wei",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, an vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios. All the source codes and trained models will be released.",
    "arxiv_url": "http://arxiv.org/abs/2502.04981v2",
    "pdf_url": "http://arxiv.org/pdf/2502.04981v2",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression",
    "authors": [
      "Feifei Li",
      "Qi Song",
      "Chi Zhang",
      "Hui Shuai",
      "Rui Huang"
    ],
    "abstract": "The task of estimating camera poses can be enhanced through novel view synthesis techniques such as NeRF and Gaussian Splatting to increase the diversity and extension of training data. However, these techniques often produce rendered images with issues like blurring and ghosting, which compromise their reliability. These issues become particularly pronounced for Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the pixel level. To mitigate the problems associated with unreliable rendered images, we introduce a novel filtering approach, which selectively extracts well-rendered pixels while discarding the inferior ones. This filter simultaneously measures the SCR model's real-time reprojection loss and gradient during training. Building on this filtering technique, we also develop a new strategy to improve scene coordinate regression using sparse inputs, drawing on successful applications of sparse input techniques in novel view synthesis. Our experimental results validate the effectiveness of our method, demonstrating state-of-the-art performance on indoor and outdoor datasets.",
    "arxiv_url": "http://arxiv.org/abs/2502.04843v2",
    "pdf_url": "http://arxiv.org/pdf/2502.04843v2",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "outdoor",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting",
    "authors": [
      "Huajian Huang",
      "Yingshu Chen",
      "Longwei Li",
      "Hui Cheng",
      "Tristan Braud",
      "Yajie Zhao",
      "Sai-Kit Yeung"
    ],
    "abstract": "360-degree cameras streamline data collection for radiance field 3D reconstruction by capturing comprehensive scene data. However, traditional radiance field methods do not address the specific challenges inherent to 360-degree images. We present SC-OmniGS, a novel self-calibrating omnidirectional Gaussian splatting system for fast and accurate omnidirectional radiance field reconstruction using 360-degree images. Rather than converting 360-degree images to cube maps and performing perspective image calibration, we treat 360-degree images as a whole sphere and derive a mathematical framework that enables direct omnidirectional camera pose calibration accompanied by 3D Gaussians optimization. Furthermore, we introduce a differentiable omnidirectional camera model in order to rectify the distortion of real-world data for performance enhancement. Overall, the omnidirectional camera intrinsic model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing weighted spherical photometric loss. Extensive experiments have demonstrated that our proposed SC-OmniGS is able to recover a high-quality radiance field from noisy camera poses or even no pose prior in challenging scenarios characterized by wide baselines and non-object-centric configurations. The noticeable performance gain in the real-world dataset captured by consumer-grade omnidirectional cameras verifies the effectiveness of our general omnidirectional camera model in reducing the distortion of 360-degree images.",
    "arxiv_url": "http://arxiv.org/abs/2502.04734v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04734v1",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting",
    "authors": [
      "Zihao Zou",
      "Ziyuan Qu",
      "Xi Peng",
      "Vivek Boominathan",
      "Adithya Pediredla",
      "Praneeth Chakravarthula"
    ],
    "abstract": "Capturing and reconstructing high-speed dynamic 3D scenes has numerous applications in computer graphics, vision, and interdisciplinary fields such as robotics, aerodynamics, and evolutionary biology. However, achieving this using a single imaging modality remains challenging. For instance, traditional RGB cameras suffer from low frame rates, limited exposure times, and narrow baselines. To address this, we propose a novel sensor fusion approach using Gaussian splatting, which combines RGB, depth, and event cameras to capture and reconstruct deforming scenes at high speeds. The key insight of our method lies in leveraging the complementary strengths of these imaging modalities: RGB cameras capture detailed color information, event cameras record rapid scene changes with microsecond resolution, and depth cameras provide 3D scene geometry. To unify the underlying scene representation across these modalities, we represent the scene using deformable 3D Gaussians. To handle rapid scene movements, we jointly optimize the 3D Gaussian parameters and their temporal deformation fields by integrating data from all three sensor modalities. This fusion enables efficient, high-quality imaging of fast and complex scenes, even under challenging conditions such as low light, narrow baselines, or rapid motion. Experiments on synthetic and real datasets captured with our prototype sensor fusion setup demonstrate that our method significantly outperforms state-of-the-art techniques, achieving noticeable improvements in both rendering fidelity and structural accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2502.04630v1",
    "pdf_url": "http://arxiv.org/pdf/2502.04630v1",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "motion",
      "fast",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing World Dynamics in a Nutshell",
    "authors": [
      "Qiuhong Shen",
      "Xuanyu Yi",
      "Mingbao Lin",
      "Hanwang Zhang",
      "Shuicheng Yan",
      "Xinchao Wang"
    ],
    "abstract": "We consider the problem of efficiently representing casually captured monocular videos in a spatially- and temporally-coherent manner. While existing approaches predominantly rely on 2D/2.5D techniques treating videos as collections of spatiotemporal pixels, they struggle with complex motions, occlusions, and geometric consistency due to absence of temporal coherence and explicit 3D structure. Drawing inspiration from monocular video as a projection of the dynamic 3D world, we explore representing videos in their intrinsic 3D form through continuous flows of Gaussian primitives in space-time. In this paper, we propose NutWorld, a novel framework that efficiently transforms monocular videos into dynamic 3D Gaussian representations in a single forward pass. At its core, NutWorld introduces a structured spatial-temporal aligned Gaussian (STAG) representation, enabling optimization-free scene modeling with effective depth and flow regularization. Through comprehensive experiments, we demonstrate that NutWorld achieves high-fidelity video reconstruction quality while enabling various downstream applications in real-time. Demos and code will be available at https://github.com/Nut-World/NutWorld.",
    "arxiv_url": "http://arxiv.org/abs/2502.03465v2",
    "pdf_url": "http://arxiv.org/pdf/2502.03465v2",
    "published_date": "2025-02-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "https://github.com/Nut-World/NutWorld",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM",
    "authors": [
      "Mingrui Li",
      "Weijian Chen",
      "Na Cheng",
      "Jingyuan Xu",
      "Dong Li",
      "Hongyu Wang"
    ],
    "abstract": "The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread attention due to its excellent performance in real-time high-fidelity rendering. However, in real-world environments with dynamic objects, existing 3DGS-based SLAM systems often face mapping errors and tracking drift issues. To address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM system tailored for dynamic scenes. In terms of tracking, unlike traditional methods, we directly perform dynamic segmentation on Gaussians and map them back to the front-end to obtain dynamic point labels through a Gaussian pyramid network, achieving precise dynamic removal and robust tracking. For mapping, we impose rendering penalties on dynamically labeled Gaussians, which are updated through the network, to avoid irreversible erroneous removal caused by simple pruning. Our results on real-world datasets demonstrate that our method is competitive in tracking compared to baseline methods, generating fewer artifacts and higher-quality reconstructions in rendering.",
    "arxiv_url": "http://arxiv.org/abs/2502.03228v2",
    "pdf_url": "http://arxiv.org/pdf/2502.03228v2",
    "published_date": "2025-02-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "face",
      "mapping",
      "segmentation",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
    "authors": [
      "Zhihao Guo",
      "Jingxuan Su",
      "Shenglin Wang",
      "Jinlong Fan",
      "Jing Zhang",
      "Wei Zhou",
      "Hadi Amirpour",
      "Yunlong Zhao",
      "Liangxiu Han",
      "Peng Wang"
    ],
    "abstract": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.",
    "arxiv_url": "http://arxiv.org/abs/2502.02283v5",
    "pdf_url": "http://arxiv.org/pdf/2502.02283v5",
    "published_date": "2025-02-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation",
    "authors": [
      "Joohyun Kwon",
      "Hanbyel Cho",
      "Junmo Kim"
    ],
    "abstract": "Recent 4D dynamic scene editing methods require editing thousands of 2D images used for dynamic scene synthesis and updating the entire scene with additional training loops, resulting in several hours of processing to edit a single dynamic scene. Therefore, these methods are not scalable with respect to the temporal dimension of the dynamic scene (i.e., the number of timesteps). In this work, we propose Instruct-4DGS, an efficient dynamic scene editing method that is more scalable in terms of temporal dimension. To achieve computational efficiency, we leverage a 4D Gaussian representation that models a 4D dynamic scene by combining static 3D Gaussians with a Hexplane-based deformation field, which captures dynamic information. We then perform editing solely on the static 3D Gaussians, which is the minimal but sufficient component required for visual editing. To resolve the misalignment between the edited 3D Gaussians and the deformation field, which may arise from the editing process, we introduce a refinement stage using a score distillation mechanism. Extensive editing results demonstrate that Instruct-4DGS is efficient, reducing editing time by more than half compared to existing methods while achieving high-quality edits that better follow user instructions.",
    "arxiv_url": "http://arxiv.org/abs/2502.02091v2",
    "pdf_url": "http://arxiv.org/pdf/2502.02091v2",
    "published_date": "2025-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation",
    "authors": [
      "Yang Zhou",
      "Zongjin He",
      "Qixuan Li",
      "Chao Wang"
    ],
    "abstract": "Recently, the field of text-guided 3D scene generation has garnered significant attention. High-quality generation that aligns with physical realism and high controllability is crucial for practical 3D scene applications. However, existing methods face fundamental limitations: (i) difficulty capturing complex relationships between multiple objects described in the text, (ii) inability to generate physically plausible scene layouts, and (iii) lack of controllability and extensibility in compositional scenes. In this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian Splatting (3DGS) to facilitate high-quality, physically consistent compositional scene generation guided by text. Specifically, given a text prompt, we convert it into a directed scene graph and adaptively adjust the density and layout of the initial compositional 3D Gaussians. Subsequently, dynamic camera adjustments are made based on the training focal point to ensure entity-level generation quality. Finally, by extracting directed dependencies from the scene graph, we tailor physical and layout energy to ensure both realism and flexibility. Comprehensive experiments demonstrate that LayoutDreamer outperforms other compositional scene generation quality and semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA) performance in the multiple objects generation metric of T3Bench.",
    "arxiv_url": "http://arxiv.org/abs/2502.01949v2",
    "pdf_url": "http://arxiv.org/pdf/2502.01949v2",
    "published_date": "2025-02-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping",
    "authors": [
      "Aashish Rai",
      "Dilin Wang",
      "Mihir Jain",
      "Nikolaos Sarafianos",
      "Kefan Chen",
      "Srinath Sridhar",
      "Aayush Prakash"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D objects and scenes. However, generating 3DGS remains challenging due to their discrete, unstructured, and permutation-invariant nature. In this work, we present a simple yet effective method to overcome these challenges. We utilize spherical mapping to transform 3DGS into a structured 2D representation, termed UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a concatenation of Gaussian attributes such as position, scale, color, opacity, and rotation. We further find that these heterogeneous features can be compressed into a lower-dimensional (e.g., 3-channel) shared feature space using a carefully designed multi-branch network. The compressed UVGS can be treated as typical RGB images. Remarkably, we discover that typical VAEs trained with latent diffusion models can directly generalize to this new representation without additional training. Our novel representation makes it effortless to leverage foundational 2D models, such as diffusion models, to directly model 3DGS. Additionally, one can simply increase the 2D UV resolution to accommodate more Gaussians, making UVGS a scalable solution compared to typical 3D backbones. This approach immediately unlocks various novel generation applications of 3DGS by inherently utilizing the already developed superior 2D generation capabilities. In our experiments, we demonstrate various unconditional, conditional generation, and inpainting applications of 3DGS based on diffusion models, which were previously non-trivial.",
    "arxiv_url": "http://arxiv.org/abs/2502.01846v3",
    "pdf_url": "http://arxiv.org/pdf/2502.01846v3",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation Modeling",
    "authors": [
      "Kang Yang",
      "Gaofeng Dong",
      "Sijie Ji",
      "Wan Du",
      "Mani Srivastava"
    ],
    "abstract": "Effective network planning and sensing in wireless networks require resource-intensive site surveys for data collection. An alternative is Radio-Frequency (RF) signal spatial propagation modeling, which computes received signals given transceiver positions in a scene (e.g.s a conference room). We identify a fundamental trade-off between scalability and fidelity in the state-of-the-art method. To address this issue, we explore leveraging 3D Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D scenes in real-time from arbitrary camera poses. By integrating domain-specific insights, we design three components for adapting 3DGS to the RF domain, including Gaussian-based RF scene representation, gradient-guided RF attribute learning, and RF-customized CUDA for ray tracing. Building on them, we develop RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation Modeling. We evaluate RFSPM in four field studies and two applications across RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and scenes. The results show that RFSPM matches the fidelity of the state-of-the-art method while reducing data requirements, training GPU-hours, and inference latency by up to 9.8\\,$\\times$, 18.6\\,$\\times$, and 84.4\\,$\\times$, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2502.01826v1",
    "pdf_url": "http://arxiv.org/pdf/2502.01826v1",
    "published_date": "2025-02-03",
    "categories": [
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "survey",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion",
    "authors": [
      "Shaoting Zhu",
      "Linzhan Mou",
      "Derun Li",
      "Baijun Ye",
      "Runhan Huang",
      "Hang Zhao"
    ],
    "abstract": "Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive \"digital twin\" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.",
    "arxiv_url": "http://arxiv.org/abs/2502.01536v3",
    "pdf_url": "http://arxiv.org/pdf/2502.01536v3",
    "published_date": "2025-02-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "tracking",
      "motion",
      "lighting",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Transformers trained on proteins can learn to attend to Euclidean distance",
    "authors": [
      "Isaac Ellmen",
      "Constantin Schneider",
      "Matthew I. J. Raybould",
      "Charlotte M. Deane"
    ],
    "abstract": "While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models.",
    "arxiv_url": "http://arxiv.org/abs/2502.01533v1",
    "pdf_url": "http://arxiv.org/pdf/2502.01533v1",
    "published_date": "2025-02-03",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiant Foam: Real-Time Differentiable Ray Tracing",
    "authors": [
      "Shrisudhan Govindarajan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU.",
    "arxiv_url": "http://arxiv.org/abs/2502.01157v1",
    "pdf_url": "http://arxiv.org/pdf/2502.01157v1",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "reflection",
      "acceleration",
      "ar",
      "light transport",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation",
    "authors": [
      "Qixuan Li",
      "Chao Wang",
      "Zongjin He",
      "Yan Peng"
    ],
    "abstract": "Text-to-3D asset generation has achieved significant optimization under the supervision of 2D diffusion priors. However, when dealing with compositional scenes, existing methods encounter several challenges: 1). failure to ensure that composite scene layouts comply with physical laws; 2). difficulty in accurately capturing the assets and relationships described in complex scene descriptions; 3). limited autonomous asset generation capabilities among layout approaches leveraging large language models (LLMs). To avoid these compromises, we propose a novel framework for compositional scene generation, PhiP-G, which seamlessly integrates generation techniques with layout guidance based on a world model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene description to generate a scene graph, and integrating a multimodal 2D generation agent and a 3D Gaussian generation method for targeted assets creation. For the stage of layout, PhiP-G employs a physical pool with adhesion capabilities and a visual supervision agent, forming a world model for layout prediction and planning. Extensive experiments demonstrate that PhiP-G significantly enhances the generation quality and physical rationality of the compositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA) performance in CLIP scores, achieves parity with the leading methods in generation quality as measured by the T$^3$Bench, and improves efficiency by 24x.",
    "arxiv_url": "http://arxiv.org/abs/2502.00708v1",
    "pdf_url": "http://arxiv.org/pdf/2502.00708v1",
    "published_date": "2025-02-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head Synthesis",
    "authors": [
      "Junuk Cha",
      "Seongro Yoon",
      "Valeriya Strizhkova",
      "Francois Bremond",
      "Seungryul Baek"
    ],
    "abstract": "3D Gaussian splatting-based talking head synthesis has recently gained attention for its ability to render high-fidelity images with real-time inference speed. However, since it is typically trained on only a short video that lacks the diversity in facial emotions, the resultant talking heads struggle to represent a wide range of emotions. To address this issue, we propose a lip-aligned emotional face generator and leverage it to train our EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned on continuous emotion values (i.e., valence and arousal); while retaining synchronization of lip movements with input audio. Additionally, to achieve the accurate lip synchronization for in-the-wild audio, we introduce a self-supervised learning method that leverages a text-to-speech network and a visual-audio synchronization network. We experiment our EmoTalkingGaussian on publicly available videos and have obtained better results than state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS), emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy), and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.",
    "arxiv_url": "http://arxiv.org/abs/2502.00654v1",
    "pdf_url": "http://arxiv.org/pdf/2502.00654v1",
    "published_date": "2025-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation",
    "authors": [
      "Rohan Chacko",
      "Nicolai Haeni",
      "Eldar Khaliullin",
      "Lin Sun",
      "Douglas Lee"
    ],
    "abstract": "We introduce Lifting By Gaussians (LBG), a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently, 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM, etc.), together with features from CLIP and DINOv2, directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches, LBG requires no per-scene training, allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular, enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore, our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods.",
    "arxiv_url": "http://arxiv.org/abs/2502.00173v1",
    "pdf_url": "http://arxiv.org/pdf/2502.00173v1",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping",
    "authors": [
      "Yiming Huang",
      "Beilei Cui",
      "Long Bai",
      "Zhen Chen",
      "Jinlin Wu",
      "Zhen Li",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is essential for precise surgical interventions and robotic tasks in minimally invasive procedures. While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM with high-quality novel view synthesis and fast rendering, these systems struggle with accurate depth and surface reconstruction due to multi-view inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between the reconstructed frames. In this work, we present Endo-2DTAM, a real-time endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which consists of tracking, mapping, and bundle adjustment modules for geometrically accurate reconstruction. Our robust tracking module combines point-to-point and point-to-plane distance metrics, while the mapping module utilizes normal consistency and depth distortion to enhance surface reconstruction quality. We also introduce a pose-consistent strategy for efficient and geometrically coherent keyframe sampling. Extensive experiments on public endoscopic datasets demonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth reconstruction of surgical scenes while maintaining computationally efficient tracking, high-quality visual appearance, and real-time rendering. Our code will be released at github.com/lastbasket/Endo-2DTAM.",
    "arxiv_url": "http://arxiv.org/abs/2501.19319v1",
    "pdf_url": "http://arxiv.org/pdf/2501.19319v1",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/lastbasket/Endo-2DTAM",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "fast",
      "face",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaySplats: Ray Tracing based Gaussian Splatting",
    "authors": [
      "Krzysztof Byrski",
      "Marcin Mazur",
      "Jacek Tabor",
      "Tadeusz Dziarmaga",
      "Marcin KƒÖdzio≈Çka",
      "Dawid Baran",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a process that enables the direct creation of 3D objects from 2D images. This representation offers numerous advantages, including rapid training and rendering. However, a significant limitation of 3DGS is the challenge of incorporating light and shadow reflections, primarily due to the utilization of rasterization rather than ray tracing for rendering. This paper introduces RaySplats, a model that employs ray-tracing based Gaussian Splatting. Rather than utilizing the projection of Gaussians, our method employs a ray-tracing mechanism, operating directly on Gaussian primitives represented by confidence ellipses with RGB colors. In practice, we compute the intersection between ellipses and rays to construct ray-tracing algorithms, facilitating the incorporation of meshes with Gaussian Splatting models and the addition of lights, shadows, and other related effects.",
    "arxiv_url": "http://arxiv.org/abs/2501.19196v1",
    "pdf_url": "http://arxiv.org/pdf/2501.19196v1",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "reflection",
      "shadow",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting",
    "authors": [
      "Zhoutao Sun",
      "Xukun Shen",
      "Yong Hu",
      "Yuyou Zhong",
      "Xueyang Zhou"
    ],
    "abstract": "Since hands are the primary interface in daily interactions, modeling high-quality digital human hands and rendering realistic images is a critical research problem. Furthermore, considering the requirements of interactive and rendering applications, it is essential to achieve real-time rendering and driveability of the digital model without compromising rendering quality. Thus, we propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian Splatting (3DGS)-based hand representation that renders high-fidelity hand images in real-time for various poses and characters. Distinct from existing articulated neural rendering techniques, we introduce a differentiable process for spatial transformations based on 3D key points. This process supports deformations from the canonical template to a mesh with arbitrary bone lengths and poses. Additionally, we propose a real-time shadow simulation method based on per-pixel depth to simulate self-occlusion shadows caused by finger movements. Finally, we embed the hand prior and propose an animatable 3DGS representation of the hand driven solely by 3D key points. We validate the effectiveness of each component of our approach through comprehensive ablation studies. Experimental results on public datasets demonstrate that JGHand achieves real-time rendering speeds with enhanced quality, surpassing state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2501.19088v1",
    "pdf_url": "http://arxiv.org/pdf/2501.19088v1",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "shadow",
      "face",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation",
    "authors": [
      "Yuchen Lin",
      "Chenguo Lin",
      "Jianjin Xu",
      "Yadong Mu"
    ],
    "abstract": "Recently, significant advancements have been made in the reconstruction and generation of 3D assets, including static cases and those with physical interactions. To recover the physical properties of 3D assets, existing methods typically assume that all materials belong to a specific predefined category (e.g., elasticity). However, such assumptions ignore the complex composition of multiple heterogeneous objects in real scenarios and tend to render less physically plausible animation given a wider range of objects. We propose OmniPhysGS for synthesizing a physics-based 3D dynamic scene composed of more general objects. A key design of OmniPhysGS is treating each 3D asset as a collection of constitutive 3D Gaussians. For each Gaussian, its physical material is represented by an ensemble of 12 physical domain-expert sub-models (rubber, metal, honey, water, etc.), which greatly enhances the flexibility of the proposed model. In the implementation, we define a scene by user-specified prompts and supervise the estimation of material weighting factors via a pretrained video diffusion model. Comprehensive experiments demonstrate that OmniPhysGS achieves more general and realistic physical dynamics across a broader spectrum of materials, including elastic, viscoelastic, plastic, and fluid substances, as well as interactions between different materials. Our method surpasses existing methods by approximately 3% to 16% in metrics of visual quality and text alignment.",
    "arxiv_url": "http://arxiv.org/abs/2501.18982v1",
    "pdf_url": "http://arxiv.org/pdf/2501.18982v1",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "3d gaussian",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
    "authors": [
      "Vitor Guizilini",
      "Muhammad Zubair Irshad",
      "Dian Chen",
      "Greg Shakhnarovich",
      "Rares Ambrus"
    ],
    "abstract": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.",
    "arxiv_url": "http://arxiv.org/abs/2501.18804v1",
    "pdf_url": "http://arxiv.org/pdf/2501.18804v1",
    "published_date": "2025-01-30",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation for 3D Gaussian Splatting",
    "authors": [
      "Yansong Qu",
      "Dian Chen",
      "Xinyang Li",
      "Xiaofan Li",
      "Shengchuan Zhang",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "abstract": "Recent advancements in 3D scene editing have been propelled by the rapid development of generative models. Existing methods typically utilize generative models to perform text-guided editing on 3D representations, such as 3D Gaussian Splatting (3DGS). However, these methods are often limited to texture modifications and fail when addressing geometric changes, such as editing a character's head to turn around. Moreover, such methods lack accurate control over the spatial position of editing results, as language struggles to precisely describe the extent of edits. To overcome these limitations, we introduce DYG, an effective 3D drag-based editing method for 3D Gaussian Splatting. It enables users to conveniently specify the desired editing region and the desired dragging direction through the input of 3D masks and pairs of control points, thereby enabling precise control over the extent of editing. DYG integrates the strengths of the implicit triplane representation to establish the geometric scaffold of the editing results, effectively overcoming suboptimal editing outcomes caused by the sparsity of 3DGS in the desired editing regions. Additionally, we incorporate a drag-based Latent Diffusion Model into our method through the proposed Drag-SDS loss function, enabling flexible, multi-view consistent, and fine-grained editing. Extensive experiments demonstrate that DYG conducts effective drag-based editing guided by control point prompts, surpassing other baselines in terms of editing effect and quality, both qualitatively and quantitatively. Visit our project page at https://quyans.github.io/Drag-Your-Gaussian.",
    "arxiv_url": "http://arxiv.org/abs/2501.18672v6",
    "pdf_url": "http://arxiv.org/pdf/2501.18672v6",
    "published_date": "2025-01-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StructuredField: Unifying Structured Geometry and Radiance Field",
    "authors": [
      "Kaiwen Song",
      "Jinkai Cui",
      "Zherui Qiu",
      "Juyong Zhang"
    ],
    "abstract": "Recent point-based differentiable rendering techniques have achieved significant success in high-fidelity reconstruction and fast rendering. However, due to the unstructured nature of point-based representations, they are difficult to apply to modern graphics pipelines designed for structured meshes, as well as to a variety of simulation and editing algorithms that work well with structured mesh representations. To this end, we propose StructuredField, a novel representation that achieves both a structured geometric representation of the reconstructed object and a high-fidelity rendering reconstruction of the object. We employ structured tetrahedral meshes to represent the reconstructed object. We reparameterize the geometric parameters of the tetrahedral mesh into the geometric shape parameters of a 3D Gaussians, thereby achieving differentiable high-fidelity rendering of the tetrahedral mesh. We propose a novel inversion-free homeomorphism to constrain the optimization of the tetrahedral mesh, which strictly guarantees that the tetrahedral mesh is remains both inversion-free and self-intersection-free during the optimization process and the final result. Based on our proposed StructuredField, we achieve high-quality structured meshes and high-fidelity reconstruction. We also demonstrate the applicability of our representation to various applications such as physical simulation and deformation.",
    "arxiv_url": "http://arxiv.org/abs/2501.18152v1",
    "pdf_url": "http://arxiv.org/pdf/2501.18152v1",
    "published_date": "2025-01-30",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting",
    "authors": [
      "Mateusz Nowak",
      "Wojciech Jarosz",
      "Peter Chin"
    ],
    "abstract": "Reconstructing a 3D scene from images is challenging due to the different ways light interacts with surfaces depending on the viewer's position and the surface's material. In classical computer graphics, materials can be classified as diffuse or specular, interacting with light differently. The standard 3D Gaussian Splatting model struggles to represent view-dependent content, since it cannot differentiate an object within the scene from the light interacting with its specular surfaces, which produce highlights or reflections. In this paper, we propose to extend the 3D Gaussian Splatting model by introducing an additional symmetric matrix to enhance the opacity representation of each 3D Gaussian. This improvement allows certain Gaussians to be suppressed based on the viewer's perspective, resulting in a more accurate representation of view-dependent reflections and specular highlights without compromising the scene's integrity. By allowing the opacity to be view dependent, our enhanced model achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep Blending, and Nerf-Synthetic datasets without a significant loss in rendering speed, achieving >60FPS, and only incurring a minimal increase in memory used.",
    "arxiv_url": "http://arxiv.org/abs/2501.17978v2",
    "pdf_url": "http://arxiv.org/pdf/2501.17978v2",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering",
    "authors": [
      "Xiaohan Sun",
      "Yinghan Xu",
      "John Dingliana",
      "Carol O'Sullivan"
    ],
    "abstract": "We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.",
    "arxiv_url": "http://arxiv.org/abs/2501.17792v2",
    "pdf_url": "http://arxiv.org/pdf/2501.17792v2",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for Geometrically Accurate and Artifact-Reduced Reconstruction",
    "authors": [
      "Miriam J√§ger",
      "Markus Hillemann",
      "Boris Jutzi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene reconstruction using 3D Gaussians. However, neither the centers nor surfaces of the Gaussians are accurately aligned to the object surface, complicating their direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically produces floater artifacts, increasing the number of Gaussians and storage requirements. To address these issues, we present FeatureGS, which incorporates an additional geometric loss term based on an eigenvalue-derived 3D shape feature into the optimization process of 3DGS. The goal is to improve geometric accuracy and enhance properties of planar surfaces with reduced structural entropy in local 3D neighborhoods.We present four alternative formulations for the geometric loss term based on 'planarity' of Gaussians, as well as 'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We provide quantitative and qualitative evaluations on 15 scenes of the DTU benchmark dataset focusing on following key aspects: Geometric accuracy and artifact-reduction, measured by the Chamfer distance, and memory efficiency, evaluated by the total number of Gaussians. Additionally, rendering quality is monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses floater artifacts, while maintaining comparable photometric rendering quality. The geometric loss with 'planarity' from Gaussians provides the highest geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces floater artifacts and number of Gaussians the most. This makes FeatureGS a strong method for geometrically accurate, artifact-reduced and memory-efficient 3D scene reconstruction, enabling the direct use of Gaussian centers for geometric representation.",
    "arxiv_url": "http://arxiv.org/abs/2501.17655v1",
    "pdf_url": "http://arxiv.org/pdf/2501.17655v1",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HOMER: Homography-Based Efficient Multi-view 3D Object Removal",
    "authors": [
      "Jingcheng Ni",
      "Weiguang Zhao",
      "Daniel Wang",
      "Ziyao Zeng",
      "Chenyu You",
      "Alex Wong",
      "Kaizhu Huang"
    ],
    "abstract": "3D object removal is an important sub-task in 3D scene editing, with broad applications in scene understanding, augmented reality, and robotics. However, existing methods struggle to achieve a desirable balance among consistency, usability, and computational efficiency in multi-view settings. These limitations are primarily due to unintuitive user interaction in the source view, inefficient multi-view object mask generation, computationally expensive inpainting procedures, and a lack of applicability across different radiance field representations. To address these challenges, we propose a novel pipeline that improves the quality and efficiency of multi-view object mask generation and inpainting. Our method introduces an intuitive region-based interaction mechanism in the source view and eliminates the need for camera poses or extra model training. Our lightweight HoMM module is employed to achieve high-quality multi-view mask propagation with enhanced efficiency. In the inpainting stage, we further reduce computational costs by performing inpainting only on selected key views and propagating the results to other views via homography-based mapping. Our pipeline is compatible with a variety of radiance field frameworks, including NeRF and 3D Gaussian Splatting, demonstrating improved generalizability and practicality in real-world scenarios. Additionally, we present a new 3D multi-object removal dataset with greater object diversity and viewpoint variation than existing datasets. Experiments on public benchmarks and our proposed dataset show that our method achieves state-of-the-art performance while reducing runtime to one-fifth of that required by leading baselines.",
    "arxiv_url": "http://arxiv.org/abs/2501.17636v3",
    "pdf_url": "http://arxiv.org/pdf/2501.17636v3",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "mapping",
      "understanding",
      "lightweight",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Reconstruction of Shoes for Augmented Reality",
    "authors": [
      "Pratik Shrestha",
      "Sujan Kapali",
      "Swikar Gautam",
      "Vishal Pokharel",
      "Santosh Giri"
    ],
    "abstract": "This paper introduces a mobile-based solution that enhances online shoe shopping through 3D modeling and Augmented Reality (AR), leveraging the efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D images, the framework generates realistic 3D shoe models from 2D images, achieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables immersive AR interactions via smartphones. A custom shoe segmentation dataset of 3120 images was created, with the best-performing segmentation model achieving an Intersection over Union (IoU) score of 0.95. This paper demonstrates the potential of 3D modeling and AR to revolutionize online shopping by offering realistic virtual interactions, with applicability across broader fashion categories.",
    "arxiv_url": "http://arxiv.org/abs/2501.18643v2",
    "pdf_url": "http://arxiv.org/pdf/2501.18643v2",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds",
    "authors": [
      "Xiaohan Sun",
      "Yinghan Xu",
      "John Dingliana",
      "Carol O'Sullivan"
    ],
    "abstract": "Efficient and realistic crowd rendering is an important element of many real-time graphics applications such as Virtual Reality (VR) and games. To this end, Levels of Detail (LOD) avatar representations such as polygonal meshes, image-based impostors, and point clouds have been proposed and evaluated. More recently, 3D Gaussian Splatting has been explored as a potential method for real-time crowd rendering. In this paper, we present a two-alternative forced choice (2AFC) experiment that aims to determine the perceived quality of 3D Gaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians), and the avatar height in Pixels (corresponding to the viewing distance). Participants viewed pairs of animated 3D Gaussian avatars and were tasked with choosing the most detailed one. Our findings can inform the optimization of LOD strategies in Gaussian-based crowd rendering, thereby helping to achieve efficient rendering while maintaining visual quality in real-time applications.",
    "arxiv_url": "http://arxiv.org/abs/2501.17085v2",
    "pdf_url": "http://arxiv.org/pdf/2501.17085v2",
    "published_date": "2025-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "efficient rendering",
      "motion",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "authors": [
      "Chenguo Lin",
      "Panwang Pan",
      "Bangbang Yang",
      "Zeming Li",
      "Yadong Mu"
    ],
    "abstract": "Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffSplat in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism.",
    "arxiv_url": "http://arxiv.org/abs/2501.16764v1",
    "pdf_url": "http://arxiv.org/pdf/2501.16764v1",
    "published_date": "2025-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lightweight",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable Beta Splatting",
    "authors": [
      "Rong Liu",
      "Dylan Sun",
      "Meida Chen",
      "Yue Wang",
      "Andrew Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by enabling real-time rendering. However, its reliance on Gaussian kernels for geometry and low-order Spherical Harmonics (SH) for color encoding limits its ability to capture complex geometries and diverse colors. We introduce Deformable Beta Splatting (DBS), a deformable and compact approach that enhances both geometry and color representation. DBS replaces Gaussian kernels with deformable Beta Kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. In addition, we extended the Beta Kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to SH-based methods. Furthermore, Unlike prior densification techniques that depend on Gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of the splatting kernel type. Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of DBS for real-time radiance field rendering. Interactive demonstrations and source code are available on our project website: https://rongliu-leo.github.io/beta-splatting/.",
    "arxiv_url": "http://arxiv.org/abs/2501.18630v2",
    "pdf_url": "http://arxiv.org/pdf/2501.18630v2",
    "published_date": "2025-01-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering",
    "authors": [
      "Nicolas von L√ºtzow",
      "Matthias Nie√üner"
    ],
    "abstract": "Volumetric rendering has become central to modern novel view synthesis methods, which use differentiable rendering to optimize 3D scene representations directly from observed views. While many recent works build on NeRF or 3D Gaussians, we explore an alternative volumetric scene representation. More specifically, we introduce two new scene representations based on linear primitives - octahedra and tetrahedra - both of which define homogeneous volumes bounded by triangular faces. To optimize these primitives, we present a differentiable rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based optimization while maintaining real-time rendering capabilities. Through experiments on real-world datasets, we demonstrate comparable performance to state-of-the-art volumetric methods while requiring fewer primitives to achieve similar reconstruction fidelity. Our findings deepen the understanding of 3D representations by providing insights into the fidelity and performance characteristics of transparent polyhedra and suggest that adopting novel primitives can expand the available design space.",
    "arxiv_url": "http://arxiv.org/abs/2501.16312v3",
    "pdf_url": "http://arxiv.org/pdf/2501.16312v3",
    "published_date": "2025-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting",
    "authors": [
      "Jiajun Dong",
      "Chengkun Wang",
      "Wenzhao Zheng",
      "Lei Chen",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "Effective image tokenization is crucial for both multi-modal understanding and generation tasks due to the necessity of the alignment with discrete text data. To this end, existing approaches utilize vector quantization (VQ) to project pixels onto a discrete codebook and reconstruct images from the discrete representation. However, compared with the continuous latent space, the limited discrete codebook space significantly restrict the representational ability of these image tokenizers. In this paper, we propose GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting as a solution. We first represent the encoded samples as multiple flexible featured 2D Gaussians characterized by positions, rotation angles, scaling factors, and feature coefficients. We adopt the standard quantization for the Gaussian features and then concatenate the quantization results with the other intrinsic Gaussian parameters before the corresponding splatting operation and the subsequent decoding module. In general, GaussianToken integrates the local influence of 2D Gaussian distribution into the discrete space and thus enhances the representation capability of the image tokenizer. Competitive reconstruction performances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the effectiveness of our framework. Our code is available at: https://github.com/ChrisDong-THU/GaussianToken.",
    "arxiv_url": "http://arxiv.org/abs/2501.15619v1",
    "pdf_url": "http://arxiv.org/pdf/2501.15619v1",
    "published_date": "2025-01-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/ChrisDong-THU/GaussianToken",
    "keywords": [
      "understanding",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Better Robustness: Pose-Free 3D Gaussian Splatting for Arbitrarily Long Videos",
    "authors": [
      "Zhen-Hui Dong",
      "Sheng Ye",
      "Yu-Hui Wen",
      "Nannan Li",
      "Yong-Jin Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to its efficiency and high-fidelity rendering. 3DGS training requires a known camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax this restriction but still face difficulties when handling long sequences with complex camera trajectories. In this paper, we propose Rob-GS, a robust framework to progressively estimate camera poses and optimize 3DGS for arbitrarily long video inputs. In particular, by leveraging the inherent continuity of videos, we design an adjacent pose tracking method to ensure stable pose estimation between consecutive frames. To handle arbitrarily long inputs, we propose a Gaussian visibility retention check strategy to adaptively split the video sequence into several segments and optimize them separately. Extensive experiments on Tanks and Temples, ScanNet, and a self-captured dataset show that Rob-GS outperforms the state-of-the-arts.",
    "arxiv_url": "http://arxiv.org/abs/2501.15096v2",
    "pdf_url": "http://arxiv.org/pdf/2501.15096v2",
    "published_date": "2025-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion",
    "authors": [
      "Yingzhi Tang",
      "Qijian Zhang",
      "Junhui Hou"
    ],
    "abstract": "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS) learning pipeline to achieve novel view synthesis (NVS) of human characters from single-view input images. Existing approaches typically require monocular videos or calibrated multi-view images as inputs, whose applicability could be weakened in real-world scenarios with arbitrary and/or unknown camera poses. In this paper, we aim to generate the set of 3DGS attributes via a diffusion-based framework conditioned on human priors extracted from a single image. Specifically, we begin with carefully integrated human-centric feature extraction procedures to deduce informative conditioning signals. Based on our empirical observations that jointly learning the whole 3DGS attributes is challenging to optimize, we design a multi-stage generation strategy to obtain different types of 3DGS attributes. To facilitate the training process, we investigate constructing proxy ground-truth 3D Gaussian attributes as high-quality attribute-level supervision signals. Through extensive experiments, our HuGDiffusion shows significant performance improvements over the state-of-the-art methods. Our code will be made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2501.15008v1",
    "pdf_url": "http://arxiv.org/pdf/2501.15008v1",
    "published_date": "2025-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting",
    "authors": [
      "Anil Armagan",
      "Albert Sa√†-Garriga",
      "Bruno Manganelli",
      "Mateusz Nowak",
      "Mehmet Kerim Yucel"
    ],
    "abstract": "Gaussian splatting (GS) for 3D reconstruction has become quite popular due to their fast training, inference speeds and high quality reconstruction. However, GS-based reconstructions generally consist of millions of Gaussians, which makes them hard to use on computationally constrained devices such as smartphones. In this paper, we first propose a principled analysis of advances in efficient GS methods. Then, we propose Trick-GS, which is a careful combination of several strategies including (1) progressive training with resolution, noise and Gaussian scales, (2) learning to prune and mask primitives and SH bands by their significance, and (3) accelerated GS training framework. Trick-GS takes a large step towards resource-constrained GS, where faster run-time, smaller and faster-convergence of models is of paramount concern. Our results on three datasets show that Trick-GS achieves up to 2x faster training, 40x smaller disk size and 2x faster rendering speed compared to vanilla GS, while having comparable accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2501.14534v1",
    "pdf_url": "http://arxiv.org/pdf/2501.14534v1",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video",
    "authors": [
      "Xiaohao Xu",
      "Tianyi Zhang",
      "Shibo Zhao",
      "Xiang Li",
      "Sibo Wang",
      "Yongqi Chen",
      "Ye Li",
      "Bhiksha Raj",
      "Matthew Johnson-Roberson",
      "Sebastian Scherer",
      "Xiaonan Huang"
    ],
    "abstract": "We aim to redefine robust ego-motion estimation and photorealistic 3D reconstruction by addressing a critical limitation: the reliance on noise-free data in existing models. While such sanitized conditions simplify evaluation, they fail to capture the unpredictable, noisy complexities of real-world environments. Dynamic motion, sensor imperfections, and synchronization perturbations lead to sharp performance declines when these models are deployed in practice, revealing an urgent need for frameworks that embrace and excel under real-world noise. To bridge this gap, we tackle three core challenges: scalable data generation, comprehensive benchmarking, and model robustness enhancement. First, we introduce a scalable noisy data synthesis pipeline that generates diverse datasets simulating complex motion, sensor imperfections, and synchronization errors. Second, we leverage this pipeline to create Robust-Ego3D, a benchmark rigorously designed to expose noise-induced performance degradation, highlighting the limitations of current learning-based methods in ego-motion accuracy and 3D reconstruction quality. Third, we propose Correspondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation method that progressively refines an internal clean 3D representation by aligning noisy observations with rendered RGB-D frames from clean 3D map, enhancing geometric alignment and appearance restoration through visual correspondence. Extensive experiments on synthetic and real-world data demonstrate that CorrGS consistently outperforms prior state-of-the-art methods, particularly in scenarios involving rapid motion and dynamic illumination.",
    "arxiv_url": "http://arxiv.org/abs/2501.14319v1",
    "pdf_url": "http://arxiv.org/pdf/2501.14319v1",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "motion",
      "3d reconstruction",
      "ar",
      "illumination",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
    "authors": [
      "JongMin Lee",
      "Sungjoo Yoo"
    ],
    "abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods. Project page: https://icetea-cv.github.io/densesfm/.",
    "arxiv_url": "http://arxiv.org/abs/2501.14277v2",
    "pdf_url": "http://arxiv.org/pdf/2501.14277v2",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations: Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; and Wavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2501.14231v1",
    "pdf_url": "http://arxiv.org/pdf/2501.14231v1",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting",
    "authors": [
      "Javier Yu",
      "Timothy Chen",
      "Mac Schwager"
    ],
    "abstract": "3D Gaussian Splatting offers expressive scene reconstruction, modeling a broad range of visual, geometric, and semantic information. However, efficient real-time map reconstruction with data streamed from multiple robots and devices remains a challenge. To that end, we propose HAMMER, a server-based collaborative Gaussian Splatting method that leverages widely available ROS communication infrastructure to generate 3D, metric-semantic maps from asynchronous robot data-streams with no prior knowledge of initial robot positions and varying on-device pose estimators. HAMMER consists of (i) a frame alignment module that transforms local SLAM poses and image data into a global frame and requires no prior relative pose knowledge, and (ii) an online module for training semantic 3DGS maps from streaming data. HAMMER handles mixed perception modes, adjusts automatically for variations in image pre-processing among different devices, and distills CLIP semantic codes into the 3D scene for open-vocabulary language queries. In our real-world experiments, HAMMER creates higher-fidelity maps (2x) compared to competing baselines and is useful for downstream tasks, such as semantic goal-conditioned navigation (e.g., \"go to the couch\"). Accompanying content available at hammer-project.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2501.14147v2",
    "pdf_url": "http://arxiv.org/pdf/2501.14147v2",
    "published_date": "2025-01-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression",
    "authors": [
      "Francesco Di Sario",
      "Riccardo Renzulli",
      "Marco Grangetto",
      "Akihiro Sugimoto",
      "Enzo Tartaglione"
    ],
    "abstract": "3D Gaussian Splatting enhances real-time performance in novel view synthesis by representing scenes with mixtures of Gaussians and utilizing differentiable rasterization. However, it typically requires large storage capacity and high VRAM, demanding the design of effective pruning and compression techniques. Existing methods, while effective in some scenarios, struggle with scalability and fail to adapt models based on critical factors such as computing capabilities or bandwidth, requiring to re-train the model under different configurations. In this work, we propose a novel, model-agnostic technique that organizes Gaussians into several hierarchical layers, enabling progressive Level of Detail (LoD) strategy. This method, combined with recent approach of compression of 3DGS, allows a single model to instantly scale across several compression ratios, with minimal to none impact to quality compared to a single non-scalable model and without requiring re-training. We validate our approach on typical datasets and benchmarks, showcasing low distortion and substantial gains in terms of scalability and adaptability.",
    "arxiv_url": "http://arxiv.org/abs/2501.13558v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13558v2",
    "published_date": "2025-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MultiDreamer3D: Multi-concept 3D Customization with Concept-Aware Diffusion Guidance",
    "authors": [
      "Wooseok Song",
      "Seunggyu Chang",
      "Jaejun Yoo"
    ],
    "abstract": "While single-concept customization has been studied in 3D, multi-concept customization remains largely unexplored. To address this, we propose MultiDreamer3D that can generate coherent multi-concept 3D content in a divide-and-conquer manner. First, we generate 3D bounding boxes using an LLM-based layout controller. Next, a selective point cloud generator creates coarse point clouds for each concept. These point clouds are placed in the 3D bounding boxes and initialized into 3D Gaussian Splatting with concept labels, enabling precise identification of concept attributions in 2D projections. Finally, we refine 3D Gaussians via concept-aware interval score matching, guided by concept-aware diffusion. Our experimental results show that MultiDreamer3D not only ensures object presence and preserves the distinct identities of each concept but also successfully handles complex cases such as property change or interaction. To the best of our knowledge, we are the first to address the multi-concept customization in 3D.",
    "arxiv_url": "http://arxiv.org/abs/2501.13449v1",
    "pdf_url": "http://arxiv.org/pdf/2501.13449v1",
    "published_date": "2025-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeomGS: LiDAR-Guided Geometry-Aware Gaussian Splatting for Robot Localization",
    "authors": [
      "Jaewon Lee",
      "Mangyu Kong",
      "Minseong Park",
      "Euntai Kim"
    ],
    "abstract": "Mapping and localization are crucial problems in robotics and autonomous driving. Recent advances in 3D Gaussian Splatting (3DGS) have enabled precise 3D mapping and scene understanding by rendering photo-realistic images. However, existing 3DGS methods often struggle to accurately reconstruct a 3D map that reflects the actual scale and geometry of the real world, which degrades localization performance. To address these limitations, we propose a novel 3DGS method called Geometry-Aware Gaussian Splatting (GeomGS). This method fully integrates LiDAR data into 3D Gaussian primitives via a probabilistic approach, as opposed to approaches that only use LiDAR as initial points or introduce simple constraints for Gaussian points. To this end, we introduce a Geometric Confidence Score (GCS), which identifies the structural reliability of each Gaussian point. The GCS is optimized simultaneously with Gaussians under probabilistic distance constraints to construct a precise structure. Furthermore, we propose a novel localization method that fully utilizes both the geometric and photometric properties of GeomGS. Our GeomGS demonstrates state-of-the-art geometric and localization performance across several benchmarks, while also improving photometric performance.",
    "arxiv_url": "http://arxiv.org/abs/2501.13417v1",
    "pdf_url": "http://arxiv.org/pdf/2501.13417v1",
    "published_date": "2025-01-23",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "autonomous driving",
      "mapping",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM",
    "authors": [
      "Gyuhyeon Pak",
      "Euntai Kim"
    ],
    "abstract": "Recently, map representations based on radiance fields such as 3D Gaussian Splatting and NeRF, which excellent for realistic depiction, have attracted considerable attention, leading to attempts to combine them with SLAM. While these approaches can build highly realistic maps, large-scale SLAM still remains a challenge because they require a large number of Gaussian images for mapping and adjacent images as keyframes for tracking. We propose a novel 3D Gaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D and IMU sensors for large-scale indoor environments. To reduce the computational load of 3DGS-based tracking, we adopt an ICP-based tracking framework that combines IMU preintegration to provide a good initial guess for accurate pose estimation. Our proposed method is the first to propose that Gaussian Splatting-based SLAM can be effectively performed in large-scale environments by integrating IMU sensor measurements. This proposal not only enhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios but also achieves SLAM performance comparable to state-of-the-art methods in large-scale indoor environments.",
    "arxiv_url": "http://arxiv.org/abs/2501.13402v1",
    "pdf_url": "http://arxiv.org/pdf/2501.13402v1",
    "published_date": "2025-01-23",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos",
    "authors": [
      "Xianrui Luo",
      "Juewen Peng",
      "Zhongang Cai",
      "Lei Yang",
      "Fan Yang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ],
    "abstract": "We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions.",
    "arxiv_url": "http://arxiv.org/abs/2501.13335v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13335v2",
    "published_date": "2025-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "body",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting",
    "authors": [
      "Lei Lan",
      "Tianjia Shao",
      "Zixuan Lu",
      "Yu Zhang",
      "Chenfanfu Jiang",
      "Yin Yang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over $10\\times$ fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions.",
    "arxiv_url": "http://arxiv.org/abs/2501.13975v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13975v2",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "acceleration",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes",
    "authors": [
      "Yuang Shi",
      "Simone Gasparini",
      "G√©raldine Morin",
      "Chenggang Yang",
      "Wei Tsang Ooi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising representation for photorealistic rendering of 3D scenes. However, its high storage requirements pose significant challenges for practical applications. We observe that Gaussians exhibit distinct roles and characteristics that are analogous to traditional artistic techniques -- Like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features like edges and contours; While other Gaussians represent broader, smoother regions, that are analogous to broader brush strokes that add volume and depth to a painting. Based on this observation, we propose a novel hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians, which cover smooth regions. Sketch Gaussians are efficiently encoded using parametric models, leveraging their geometric coherence, while Patch Gaussians undergo optimized pruning, retraining, and vector quantization to maintain volumetric consistency and storage efficiency. Our comprehensive evaluation across diverse indoor and outdoor scenes demonstrates that this structure-aware approach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41% in LPIPS at equivalent model sizes, and correspondingly, for an indoor scene, our model maintains the visual quality with 2.3% of the original model size.",
    "arxiv_url": "http://arxiv.org/abs/2501.13045v1",
    "pdf_url": "http://arxiv.org/pdf/2501.13045v1",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting",
    "authors": [
      "Junzhe Jiang",
      "Chun Gu",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2501.13971v2",
    "pdf_url": "http://arxiv.org/pdf/2501.13971v2",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet Transform",
    "authors": [
      "Hung Nguyen",
      "Blark Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGP's fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF additionally includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRF's overfitting on high frequencies in earlier training stages. We also introduce a model-based approach, based on multi-head attention, that is compatible with INGP, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for fast-converging implicit representations like INGP or 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2501.12637v2",
    "pdf_url": "http://arxiv.org/pdf/2501.12637v2",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "few-shot",
      "fast",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions",
    "authors": [
      "Vishagar Arunan",
      "Saeedha Nazar",
      "Hashiru Pramuditha",
      "Vinasirajan Viruthshaan",
      "Sameera Ramasinghe",
      "Simon Lucey",
      "Ranga Rodrigo"
    ],
    "abstract": "Splatting-based 3D reconstruction methods have gained popularity with the advent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel views. These methods commonly resort to using exponential family functions, such as the Gaussian function, as reconstruction kernels due to their anisotropic nature, ease of projection, and differentiability in rasterization. However, the field remains restricted to variations within the exponential family, leaving generalized reconstruction kernels largely underexplored, partly due to the lack of easy integrability in 3D to 2D projections. In this light, we show that a class of decaying anisotropic radial basis functions (DARBFs), which are non-negative functions of the Mahalanobis distance, supports splatting by approximating the Gaussian function's closed-form integration advantage. With this fresh perspective, we demonstrate up to 34% faster convergence during training and a 45% reduction in memory consumption across various DARB reconstruction kernels, while maintaining comparable PSNR, SSIM, and LPIPS results. We will make the code available.",
    "arxiv_url": "http://arxiv.org/abs/2501.12369v2",
    "pdf_url": "http://arxiv.org/pdf/2501.12369v2",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting",
    "authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To achieve a compact size, we propose HAC++, which leverages the relationships between unorganized anchors and a structured hash grid, utilizing their mutual information for context modeling. Additionally, HAC++ captures intra-anchor contextual relationships to further enhance compression performance. To facilitate entropy coding, we utilize Gaussian distributions to precisely estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Moreover, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Overall, HAC++ achieves a remarkable size reduction of over 100X compared to vanilla 3DGS when averaged on all datasets, while simultaneously improving fidelity. It also delivers more than 20X size reduction compared to Scaffold-GS. Our code is available at https://github.com/YihangChen-ee/HAC-plus.",
    "arxiv_url": "http://arxiv.org/abs/2501.12255v4",
    "pdf_url": "http://arxiv.org/pdf/2501.12255v4",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YihangChen-ee/HAC-plus",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSVC: Efficient Video Representation and Compression Through 2D Gaussian Splatting",
    "authors": [
      "Longan Wang",
      "Yuang Shi",
      "Wei Tsang Ooi"
    ],
    "abstract": "3D Gaussian splats have emerged as a revolutionary, effective, learned representation for static 3D scenes. In this work, we explore using 2D Gaussian splats as a new primitive for representing videos. We propose GSVC, an approach to learning a set of 2D Gaussian splats that can effectively represent and compress video frames. GSVC incorporates the following techniques: (i) To exploit temporal redundancy among adjacent frames, which can speed up training and improve the compression efficiency, we predict the Gaussian splats of a frame based on its previous frame; (ii) To control the trade-offs between file size and quality, we remove Gaussian splats with low contribution to the video quality; (iii) To capture dynamics in videos, we randomly add Gaussian splats to fit content with large motion or newly-appeared objects; (iv) To handle significant changes in the scene, we detect key frames based on loss differences during the learning process. Experiment results show that GSVC achieves good rate-distortion trade-offs, comparable to state-of-the-art video codecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080 video.",
    "arxiv_url": "http://arxiv.org/abs/2501.12060v2",
    "pdf_url": "http://arxiv.org/pdf/2501.12060v2",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "See In Detail: Enhancing Sparse-view 3D Gaussian Splatting with Local Depth and Semantic Regularization",
    "authors": [
      "Zongqi He",
      "Zhe Xiao",
      "Kin-Chung Chan",
      "Yushen Zuo",
      "Jun Xiao",
      "Kin-Man Lam"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable performance in novel view synthesis. However, its rendering quality deteriorates with sparse inphut views, leading to distorted content and reduced details. This limitation hinders its practical application. To address this issue, we propose a sparse-view 3DGS method. Given the inherently ill-posed nature of sparse-view rendering, incorporating prior information is crucial. We propose a semantic regularization technique, using features extracted from the pretrained DINO-ViT model, to ensure multi-view semantic consistency. Additionally, we propose local depth regularization, which constrains depth values to improve generalization on unseen views. Our method outperforms state-of-the-art novel view synthesis approaches, achieving up to 0.4dB improvement in terms of PSNR on the LLFF dataset, with reduced distortion and enhanced visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2501.11508v1",
    "pdf_url": "http://arxiv.org/pdf/2501.11508v1",
    "published_date": "2025-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "semantic",
      "4d",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time Sparse-View 3D Rendering",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Yu Lin",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leverage monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information to regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world application.",
    "arxiv_url": "http://arxiv.org/abs/2501.11102v1",
    "pdf_url": "http://arxiv.org/pdf/2501.11102v1",
    "published_date": "2025-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Car-GS: Addressing Reflective and Transparent Surface Challenges in 3D Car Reconstruction",
    "authors": [
      "Congcong Li",
      "Jin Wang",
      "Xiaomeng Wang",
      "Xingchen Zhou",
      "Wei Wu",
      "Yuzhi Zhang",
      "Tongyi Cao"
    ],
    "abstract": "3D car modeling is crucial for applications in autonomous driving systems, virtual and augmented reality, and gaming. However, due to the distinctive properties of cars, such as highly reflective and transparent surface materials, existing methods often struggle to achieve accurate 3D car reconstruction.To address these limitations, we propose Car-GS, a novel approach designed to mitigate the effects of specular highlights and the coupling of RGB and geometry in 3D geometric and shading reconstruction (3DGS). Our method incorporates three key innovations: First, we introduce view-dependent Gaussian primitives to effectively model surface reflections. Second, we identify the limitations of using a shared opacity parameter for both image rendering and geometric attributes when modeling transparent objects. To overcome this, we assign a learnable geometry-specific opacity to each 2D Gaussian primitive, dedicated solely to rendering depth and normals. Third, we observe that reconstruction errors are most prominent when the camera view is nearly orthogonal to glass surfaces. To address this issue, we develop a quality-aware supervision module that adaptively leverages normal priors from a pre-trained large-scale normal model.Experimental results demonstrate that Car-GS achieves precise reconstruction of car surfaces and significantly outperforms prior methods. The project page is available at https://lcc815.github.io/Car-GS.",
    "arxiv_url": "http://arxiv.org/abs/2501.11020v1",
    "pdf_url": "http://arxiv.org/pdf/2501.11020v1",
    "published_date": "2025-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "reflection",
      "face",
      "geometry",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting",
    "authors": [
      "Jiaqi Lin",
      "Zhihao Li",
      "Binxiao Huang",
      "Xiao Tang",
      "Jianzhuang Liu",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Fenglong Song",
      "Wenming Yang"
    ],
    "abstract": "Gaussian Splatting has emerged as a prominent 3D representation in novel view synthesis, but it still suffers from appearance variations, which are caused by various factors, such as modern camera ISPs, different time of day, weather conditions, and local light changes. These variations can lead to floaters and color distortions in the rendered images/videos. Recent appearance modeling approaches in Gaussian Splatting are either tightly coupled with the rendering process, hindering real-time rendering, or they only account for mild global variations, performing poorly in scenes with local light changes. In this paper, we propose DAVIGS, a method that decouples appearance variations in a plug-and-play and efficient manner. By transforming the rendering results at the image level instead of the Gaussian level, our approach can model appearance variations with minimal optimization time and memory overhead. Furthermore, our method gathers appearance-related information in 3D space to transform the rendered images, thus building 3D consistency across views implicitly. We validate our method on several appearance-variant scenes, and demonstrate that it achieves state-of-the-art rendering quality with minimal training time and memory usage, without compromising rendering speeds. Additionally, it provides performance improvements for different Gaussian Splatting baselines in a plug-and-play manner.",
    "arxiv_url": "http://arxiv.org/abs/2501.10788v1",
    "pdf_url": "http://arxiv.org/pdf/2501.10788v1",
    "published_date": "2025-01-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSTAR: Gaussian Surface Tracking and Reconstruction",
    "authors": [
      "Chengwei Zheng",
      "Lixin Xue",
      "Juan Zarate",
      "Jie Song"
    ],
    "abstract": "3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GauSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GauSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GauSTAR maintains the mesh topology and tracks the meshes using Gaussians. For regions where topology changes, GauSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://eth-ait.github.io/GauSTAR/.",
    "arxiv_url": "http://arxiv.org/abs/2501.10283v3",
    "pdf_url": "http://arxiv.org/pdf/2501.10283v3",
    "published_date": "2025-01-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
    "authors": [
      "Xiangyue Liu",
      "Kunming Luo",
      "Heng Li",
      "Qi Zhang",
      "Yuan Liu",
      "Li Yi",
      "Ping Tan"
    ],
    "abstract": "We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: [Project Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).",
    "arxiv_url": "http://arxiv.org/abs/2501.09978v1",
    "pdf_url": "http://arxiv.org/pdf/2501.09978v1",
    "published_date": "2025-01-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "animation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study",
    "authors": [
      "Shi Qiu",
      "Binzhu Xie",
      "Qixuan Liu",
      "Pheng-Ann Heng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as an innovative and efficient 3D representation technique. While its potential for extended reality (XR) applications is frequently highlighted, its practical effectiveness remains underexplored. In this work, we examine three distinct 3DGS-based approaches for virtual environment (VE) creation, leveraging their unique strengths for efficient and visually compelling scene representation. By conducting a comparable study, we evaluate the feasibility of 3DGS in creating immersive VEs, identify its limitations in XR applications, and discuss future research and development opportunities.",
    "arxiv_url": "http://arxiv.org/abs/2501.09302v1",
    "pdf_url": "http://arxiv.org/pdf/2501.09302v1",
    "published_date": "2025-01-16",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityLoc: 6DoF Pose Distributional Localization for Text Descriptions in Large-Scale Scenes with Gaussian Representation",
    "authors": [
      "Qi Ma",
      "Runyi Yang",
      "Bin Ren",
      "Nicu Sebe",
      "Ender Konukoglu",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "Localizing textual descriptions within large-scale 3D scenes presents inherent ambiguities, such as identifying all traffic lights in a city. Addressing this, we introduce a method to generate distributions of camera poses conditioned on textual descriptions, facilitating robust reasoning for broadly defined concepts.   Our approach employs a diffusion-based architecture to refine noisy 6DoF camera poses towards plausible locations, with conditional signals derived from pre-trained text encoders. Integration with the pretrained Vision-Language Model, CLIP, establishes a strong linkage between text descriptions and pose distributions. Enhancement of localization accuracy is achieved by rendering candidate poses using 3D Gaussian splatting, which corrects misaligned samples through visual reasoning.   We validate our method's superiority by comparing it against standard distribution estimation methods across five large-scale datasets, demonstrating consistent outperformance. Code, datasets and more information will be publicly available at our project page.",
    "arxiv_url": "http://arxiv.org/abs/2501.08982v2",
    "pdf_url": "http://arxiv.org/pdf/2501.08982v2",
    "published_date": "2025-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation",
    "authors": [
      "Xiaolu Hou",
      "Mingcheng Li",
      "Dingkang Yang",
      "Jiawei Chen",
      "Ziyun Qian",
      "Xiao Zhao",
      "Yue Jiang",
      "Jinjie Wei",
      "Qingyao Xu",
      "Lihua Zhang"
    ],
    "abstract": "With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.",
    "arxiv_url": "http://arxiv.org/abs/2501.10462v1",
    "pdf_url": "http://arxiv.org/pdf/2501.10462v1",
    "published_date": "2025-01-15",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "compression",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping",
    "authors": [
      "Sheng Hong",
      "Chunran Zheng",
      "Yishu Shen",
      "Changze Li",
      "Fu Zhang",
      "Tong Qin",
      "Shaojie Shen"
    ],
    "abstract": "In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach. However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption. LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU. Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system. Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry. The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales. The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients. Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window. Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map. Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform. The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities. All implementation algorithms, hardware designs, and CAD models will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2501.08672v1",
    "pdf_url": "http://arxiv.org/pdf/2501.08672v1",
    "published_date": "2025-01-15",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "face",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering",
    "authors": [
      "Meenakshi Krishnan",
      "Liam Fowl",
      "Ramani Duraiswami"
    ],
    "abstract": "Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis. However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail. We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh. The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming. We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality.",
    "arxiv_url": "http://arxiv.org/abs/2501.08370v1",
    "pdf_url": "http://arxiv.org/pdf/2501.08370v1",
    "published_date": "2025-01-14",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "efficient",
      "vr",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes",
    "authors": [
      "Ke Wu",
      "Zicheng Zhang",
      "Muer Tie",
      "Ziqing Ai",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer, Score Manager, and Pose Refinement, which collectively improve mapping speed and localization accuracy. This enables the SLAM system to handle large-scale urban environments with up to 50 million Gaussian ellipsoids. To ensure global consistency in large-scale scenes, we design a Loop Closure module, which innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian Splatting for loop closure detection and correction of the Gaussian map. Additionally, we propose a Dynamic Eraser to address the inevitable presence of dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor and outdoor environments demonstrate that our approach achieves localization performance on par with Visual-Inertial Odometry while surpassing recent GS/NeRF SLAM methods. It also significantly outperforms all existing methods in terms of mapping and rendering quality. Furthermore, we developed a mobile app and verified that our framework can generate high-quality Gaussian maps in real time using only a smartphone camera and a low-frequency IMU sensor. To the best of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method capable of operating in outdoor environments and supporting kilometer-scale large scenes.",
    "arxiv_url": "http://arxiv.org/abs/2501.08286v1",
    "pdf_url": "http://arxiv.org/pdf/2501.08286v1",
    "published_date": "2025-01-14",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "outdoor",
      "mapping",
      "geometry",
      "ar",
      "slam",
      "large scene",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models",
    "authors": [
      "Marcel Rogge",
      "Didier Stricker"
    ],
    "abstract": "Current Gaussian Splatting approaches are effective for reconstructing entire scenes but lack the option to target specific objects, making them computationally expensive and unsuitable for object-specific applications. We propose a novel approach that leverages object masks to enable targeted reconstruction, resulting in object-centric models. Additionally, we introduce an occlusion-aware pruning strategy to minimize the number of Gaussians without compromising quality. Our method reconstructs compact object models, yielding object-centric Gaussian and mesh representations that are up to 96% smaller and up to 71% faster to train compared to the baseline while retaining competitive quality. These representations are immediately usable for downstream applications such as appearance editing and physics simulation without additional processing.",
    "arxiv_url": "http://arxiv.org/abs/2501.08174v2",
    "pdf_url": "http://arxiv.org/pdf/2501.08174v2",
    "published_date": "2025-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UnCommon Objects in 3D",
    "authors": [
      "Xingchen Liu",
      "Piyush Tayal",
      "Jianyuan Wang",
      "Jesus Zarzar",
      "Tom Monnier",
      "Konstantinos Tertikas",
      "Jiali Duan",
      "Antoine Toisoul",
      "Jason Y. Zhang",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Roman Shapovalov",
      "David Novotny"
    ],
    "abstract": "We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI. uCO3D is the largest publicly-available collection of high-resolution videos of objects with 3D annotations that ensures full-360$^{\\circ}$ coverage. uCO3D is significantly more diverse than MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of higher quality, due to extensive quality checks of both the collected videos and the 3D annotations. Similar to analogous datasets, uCO3D contains annotations for 3D camera poses, depth maps and sparse point clouds. In addition, each object is equipped with a caption and a 3D Gaussian Splat reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D and obtain superior results using the latter, showing that uCO3D is better for learning applications.",
    "arxiv_url": "http://arxiv.org/abs/2501.07574v1",
    "pdf_url": "http://arxiv.org/pdf/2501.07574v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh",
    "authors": [
      "Lewis A G Stuart",
      "Michael P Pound"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D reconstructions, but these scenes often require specialised renderers for effective visualisation. In contrast, point clouds are a widely used 3D representation and are compatible with most popular 3D processing software, yet converting 3DGS scenes into point clouds is a complex challenge. In this work we introduce 3DGS-to-PC, a flexible and highly customisable framework that is capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We sample points probabilistically from each Gaussian as a 3D density function. We additionally threshold new points using the Mahalanobis distance to the Gaussian centre, preventing extreme outliers. The result is a point cloud that closely represents the shape encoded into the 3D Gaussian scene. Individual Gaussians use spherical harmonics to adapt colours depending on view, and each point may contribute only subtle colour hints to the resulting rendered scene. To avoid spurious or incorrect colours that do not fit with the final point cloud, we recalculate Gaussian colours via a customised image rendering approach, assigning each Gaussian the colour of the pixel to which it contributes most across all views. 3DGS-to-PC also supports mesh generation through Poisson Surface Reconstruction, applied to points sampled from predicted surface Gaussians. This allows coloured meshes to be generated from 3DGS scenes without the need for re-training. This package is highly customisable and capability of simple integration into existing 3DGS pipelines. 3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud and surface-based formats.",
    "arxiv_url": "http://arxiv.org/abs/2501.07478v1",
    "pdf_url": "http://arxiv.org/pdf/2501.07478v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.GR",
      "cs.CV",
      "I.3.4; I.4.5; I.2.10"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes",
    "authors": [
      "Yuhang Zhang",
      "Joshua Maraval",
      "Zhengyu Zhang",
      "Nicolas Ramin",
      "Shishun Tian",
      "Lu Zhang"
    ],
    "abstract": "Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two groundbreaking technologies that have revolutionized the field of Novel View Synthesis (NVS), enabling immersive photorealistic rendering and user experiences by synthesizing multiple viewpoints from a set of images of sparse views. The potential applications of NVS, such as high-quality virtual and augmented reality, detailed 3D modeling, and realistic medical organ imaging, underscore the importance of quality assessment of NVS methods from the perspective of human perception. Although some previous studies have explored subjective quality assessments for NVS technology, they still face several challenges, especially in NVS methods selection, scenario coverage, and evaluation methodology. To address these challenges, we conducted two subjective experiments for the quality assessment of NVS technologies containing both GS-based and NeRF-based methods, focusing on dynamic and real-world scenes. This study covers 360{\\deg}, front-facing, and single-viewpoint videos while providing a richer and greater number of real scenes. Meanwhile, it's the first time to explore the impact of NVS methods in dynamic scenes with moving objects. The two types of subjective experiments help to fully comprehend the influences of different viewing paths from a human perception perspective and pave the way for future development of full-reference and no-reference quality metrics. In addition, we established a comprehensive benchmark of various state-of-the-art objective metrics on the proposed database, highlighting that existing methods still struggle to accurately capture subjective quality. The results give us some insights into the limitations of existing NVS methods and may promote the development of new NVS methods.",
    "arxiv_url": "http://arxiv.org/abs/2501.08072v1",
    "pdf_url": "http://arxiv.org/pdf/2501.08072v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "lighting",
      "face",
      "medical",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians",
    "authors": [
      "Sen Peng",
      "Weixing Xie",
      "Zilong Wang",
      "Xiaohu Guo",
      "Zhonggui Chen",
      "Baorong Yang",
      "Xiao Dong"
    ],
    "abstract": "We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2501.07104v1",
    "pdf_url": "http://arxiv.org/pdf/2501.07104v1",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "deformation",
      "geometry",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting",
    "authors": [
      "Yue Hu",
      "Rong Liu",
      "Meida Chen",
      "Peter Beerel",
      "Andrew Feng"
    ],
    "abstract": "Achieving high-fidelity 3D reconstruction from monocular video remains challenging due to the inherent limitations of traditional methods like Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene details. While differentiable rendering techniques such as Neural Radiance Fields (NeRF) address some of these challenges, their high computational costs make them unsuitable for real-time applications. Additionally, existing 3D Gaussian Splatting (3DGS) methods often focus on photometric consistency, neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and pose updates for scene refinement. We propose a framework integrating dense SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach introduces SLAM-Informed Adaptive Densification, which dynamically updates and densifies the Gaussian model by leveraging dense point clouds from SLAM. Additionally, we incorporate Geometry-Guided Optimization, which combines edge-aware geometric constraints and photometric consistency to jointly optimize the appearance and geometry of the 3DGS scene representation, enabling detailed and accurate SLAM mapping reconstruction. Experiments on the Replica and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving state-of-the-art results among monocular systems. Specifically, our method achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica, representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by 10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the potential of our framework in bridging the gap between photometric and geometric dense 3D scene representations, paving the way for practical and efficient monocular dense reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2501.07015v3",
    "pdf_url": "http://arxiv.org/pdf/2501.07015v3",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV",
      "I.4"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "mapping",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications",
    "authors": [
      "Xinyi Zheng",
      "Steve Zhang",
      "Weizhe Lin",
      "Aaron Zhang",
      "Walterio W. Mayol-Cuevas",
      "Junxiao Shen"
    ],
    "abstract": "In this paper, we present a large-scale fine-grained dataset using high-resolution images captured from locations worldwide. Compared to existing datasets, our dataset offers a significantly larger size and includes a higher level of detail, making it uniquely suited for fine-grained 3D applications. Notably, our dataset is built using drone-captured aerial imagery, which provides a more accurate perspective for capturing real-world site layouts and architectural structures. By reconstructing environments with these detailed images, our dataset supports applications such as the COLMAP format for Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible with widely-used techniques including SLAM, Multi-View Stereo, and Neural Radiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds. This makes it a benchmark for reconstruction and segmentation tasks. The dataset enables seamless integration with multi-modal data, supporting a range of 3D applications, from architectural reconstruction to virtual tourism. Its flexibility promotes innovation, facilitating breakthroughs in 3D modeling and analysis.",
    "arxiv_url": "http://arxiv.org/abs/2501.06927v2",
    "pdf_url": "http://arxiv.org/pdf/2501.06927v2",
    "published_date": "2025-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "gaussian splatting",
      "3d reconstruction",
      "slam",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Synthetic Prior for Few-Shot Drivable Head Avatar Inversion",
    "authors": [
      "Wojciech Zielonka",
      "Stephan J. Garbin",
      "Alexandros Lattas",
      "George Kopanas",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Justus Thies",
      "Timo Bolkart"
    ],
    "abstract": "We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participant's consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2501.06903v3",
    "pdf_url": "http://arxiv.org/pdf/2501.06903v3",
    "published_date": "2025-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "few-shot",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveGAMER: Active GAussian Mapping through Efficient Rendering",
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Kevin Chen",
      "Xiangyu Xu",
      "Qingan Yan",
      "Changjiang Cai",
      "Yi Xu"
    ],
    "abstract": "We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian Splatting (3DGS) to achieve high-quality, real-time scene mapping and exploration. Unlike traditional NeRF-based methods, which are computationally demanding and restrict active mapping performance, our approach leverages the efficient rendering capabilities of 3DGS, allowing effective and efficient exploration in complex environments. The core of our system is a rendering-based information gain module that dynamically identifies the most informative viewpoints for next-best-view planning, enhancing both geometric and photometric reconstruction accuracy. ActiveGAMER also integrates a carefully balanced framework, combining coarse-to-fine exploration, post-refinement, and a global-local keyframe selection strategy to maximize reconstruction completeness and fidelity. Our system autonomously explores and reconstructs environments with state-of-the-art geometric and photometric accuracy and completeness, significantly surpassing existing approaches in both aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D highlight ActiveGAMER's effectiveness in active mapping tasks.",
    "arxiv_url": "http://arxiv.org/abs/2501.06897v2",
    "pdf_url": "http://arxiv.org/pdf/2501.06897v2",
    "published_date": "2025-01-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "mapping",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution",
    "authors": [
      "Du Chen",
      "Liyi Chen",
      "Zhengqiang Zhang",
      "Lei Zhang"
    ],
    "abstract": "Implicit Neural Representation (INR) has been successfully employed for Arbitrary-scale Super-Resolution (ASR). However, INR-based models need to query the multi-layer perceptron module numerous times and render a pixel in each query, resulting in insufficient representation capability and computational efficiency. Recently, Gaussian Splatting (GS) has shown its advantages over INR in both visual quality and rendering speed in 3D tasks, which motivates us to explore whether GS can be employed for the ASR task. However, directly applying GS to ASR is exceptionally challenging because the original GS is an optimization-based method through overfitting each single scene, while in ASR we aim to learn a single model that can generalize to different images and scaling factors. We overcome these challenges by developing two novel techniques. Firstly, to generalize GS for ASR, we elaborately design an architecture to predict the corresponding image-conditioned Gaussians of the input low-resolution image in a feed-forward manner. Each Gaussian can fit the shape and direction of an area of complex textures, showing powerful representation capability. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based scale-aware rasterization to render super-resolved images by sampling discrete RGB values from the predicted continuous Gaussians. Via end-to-end training, our optimized network, namely GSASR, can perform ASR for any image and unseen scaling factors. Extensive experiments validate the effectiveness of our proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2501.06838v4",
    "pdf_url": "http://arxiv.org/pdf/2501.06838v4",
    "published_date": "2025-01-12",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with Cycle-Aggregative Gaussian Splatting",
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Dan Xu"
    ],
    "abstract": "This paper tackles the problem of generalizable 3D-aware generation from monocular datasets, e.g., ImageNet. The key challenge of this task is learning a robust 3D-aware representation without multi-view or dynamic data, while ensuring consistent texture and geometry across different viewpoints. Although some baseline methods are capable of 3D-aware generation, the quality of the generated images still lags behind state-of-the-art 2D generation approaches, which excel in producing high-quality, detailed images. To address this severe limitation, we propose a novel feed-forward pipeline based on pixel-aligned Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and reliable 3D renderings from monocular inputs. In addition, we introduce a self-supervised cycle-aggregative constraint to enforce cross-view consistency in the learned 3D representation. This training strategy naturally allows aggregation of multiple aligned Gaussian primitives and significantly alleviates the interpolation limitations inherent in single-view pixel-aligned Gaussian Splatting. Furthermore, we incorporate video model priors to perform geometry-aware refinement, enhancing the generation of fine details in wide-viewpoint scenarios and improving the model's capability to capture intricate 3D textures. Extensive experiments demonstrate that our approach not only achieves high-quality, multi-view consistent 3D-aware generation from monocular datasets, but also significantly improves training and inference efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2501.06714v3",
    "pdf_url": "http://arxiv.org/pdf/2501.06714v3",
    "published_date": "2025-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MapGS: Generalizable Pretraining and Data Augmentation for Online Mapping via Novel View Synthesis",
    "authors": [
      "Hengyuan Zhang",
      "David Paz",
      "Yuliang Guo",
      "Xinyu Huang",
      "Henrik I. Christensen",
      "Liu Ren"
    ],
    "abstract": "Online mapping reduces the reliance of autonomous vehicles on high-definition (HD) maps, significantly enhancing scalability. However, recent advancements often overlook cross-sensor configuration generalization, leading to performance degradation when models are deployed on vehicles with different camera intrinsics and extrinsics. With the rapid evolution of novel view synthesis methods, we investigate the extent to which these techniques can be leveraged to address the sensor configuration generalization challenge. We propose a novel framework leveraging Gaussian splatting to reconstruct scenes and render camera images in target sensor configurations. The target config sensor data, along with labels mapped to the target config, are used to train online mapping models. Our proposed framework on the nuScenes and Argoverse 2 datasets demonstrates a performance improvement of 18% through effective dataset augmentation, achieves faster convergence and efficient training, and exceeds state-of-the-art performance when using only 25% of the original training data. This enables data reuse and reduces the need for laborious data labeling. Project page at https://henryzhangzhy.github.io/mapgs.",
    "arxiv_url": "http://arxiv.org/abs/2501.06660v1",
    "pdf_url": "http://arxiv.org/pdf/2501.06660v1",
    "published_date": "2025-01-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "mapping",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References",
    "authors": [
      "Qiang Qu",
      "Yiran Shen",
      "Xiaoming Chen",
      "Yuk Ying Chung",
      "Weidong Cai",
      "Tongliang Liu"
    ],
    "abstract": "Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the \"same instance, similar representation\" assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).",
    "arxiv_url": "http://arxiv.org/abs/2501.06488v1",
    "pdf_url": "http://arxiv.org/pdf/2501.06488v1",
    "published_date": "2025-01-11",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Locality-aware Gaussian Compression for Fast and High-quality Rendering",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework that exploits the spatial coherence of 3D Gaussians for compact modeling of volumetric scenes. To this end, we first analyze the local coherence of 3D Gaussian attributes, and propose a novel locality-aware 3D Gaussian representation that effectively encodes locally-coherent Gaussian attributes using a neural field representation with a minimal storage requirement. On top of the novel representation, LocoGS is carefully designed with additional components such as dense initialization, an adaptive spherical harmonics bandwidth scheme and different encoding schemes for different Gaussian attributes to maximize compression performance. Experimental results demonstrate that our approach outperforms the rendering quality of existing compact Gaussian representations for representative real-world 3D datasets while achieving from 54.6$\\times$ to 96.6$\\times$ compressed storage size and from 2.1$\\times$ to 2.4$\\times$ rendering speed than 3DGS. Even our approach also demonstrates an averaged 2.4$\\times$ higher rendering speed than the state-of-the-art compression method with comparable compression performance.",
    "arxiv_url": "http://arxiv.org/abs/2501.05757v3",
    "pdf_url": "http://arxiv.org/pdf/2501.05757v3",
    "published_date": "2025-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation",
    "authors": [
      "Xuyi Meng",
      "Chen Wang",
      "Jiahui Lei",
      "Kostas Daniilidis",
      "Jiatao Gu",
      "Lingjie Liu"
    ],
    "abstract": "Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.",
    "arxiv_url": "http://arxiv.org/abs/2501.05427v1",
    "pdf_url": "http://arxiv.org/pdf/2501.05427v1",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance",
    "authors": [
      "Dimitrios Gerogiannis",
      "Foivos Paraperas Papantoniou",
      "Rolandos Alexandros Potamias",
      "Alexandros Lattas",
      "Stefanos Zafeiriou"
    ],
    "abstract": "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail. Please visit https://arc2avatar.github.io for more resources.",
    "arxiv_url": "http://arxiv.org/abs/2501.05379v2",
    "pdf_url": "http://arxiv.org/pdf/2501.05379v2",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping",
    "authors": [
      "Tianci Wen",
      "Zhiang Liu",
      "Biao Lu",
      "Yongchun Fang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.",
    "arxiv_url": "http://arxiv.org/abs/2501.05242v2",
    "pdf_url": "http://arxiv.org/pdf/2501.05242v2",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV",
      "68T40(Primary)68T45, 68U99 (Secondary)",
      "I.4.8; I.3.7"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting",
    "authors": [
      "Andrew Bond",
      "Jui-Hsien Wang",
      "Long Mai",
      "Erkut Erdem",
      "Aykut Erdem"
    ],
    "abstract": "Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2501.04782v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04782v1",
    "published_date": "2025-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "3d gaussian",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency",
    "authors": [
      "Han Huang",
      "Yulun Wu",
      "Chao Deng",
      "Ge Gao",
      "Ming Gu",
      "Yu-Shen Liu"
    ],
    "abstract": "Recently, Gaussian Splatting has sparked a new trend in the field of computer vision. Apart from novel view synthesis, it has also been extended to the area of multi-view reconstruction. The latest methods facilitate complete, detailed surface reconstruction while ensuring fast training speed. However, these methods still require dense input views, and their output quality significantly degrades with sparse views. We observed that the Gaussian primitives tend to overfit the few training views, leading to noisy floaters and incomplete reconstruction surfaces. In this paper, we present an innovative sparse-view reconstruction framework that leverages intra-view depth and multi-view feature consistency to achieve remarkably accurate surface reconstruction. Specifically, we utilize monocular depth ranking information to supervise the consistency of depth distribution within patches and employ a smoothness loss to enhance the continuity of the distribution. To achieve finer surface reconstruction, we optimize the absolute position of depth through multi-view projection features. Extensive experiments on DTU and BlendedMVS demonstrate that our method outperforms state-of-the-art methods with a speedup of 60x to 200x, achieving swift and fine-grained mesh reconstruction without the need for costly pre-training.",
    "arxiv_url": "http://arxiv.org/abs/2501.04628v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04628v1",
    "published_date": "2025-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "face",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spatiotemporal Gaussian Optimization for 4D Cone Beam CT Reconstruction from Sparse Projections",
    "authors": [
      "Yabo Fu",
      "Hao Zhang",
      "Weixing Cai",
      "Huiqiao Xie",
      "Licheng Kuo",
      "Laura Cervino",
      "Jean Moran",
      "Xiang Li",
      "Tianfang Li"
    ],
    "abstract": "In image-guided radiotherapy (IGRT), four-dimensional cone-beam computed tomography (4D-CBCT) is critical for assessing tumor motion during a patients breathing cycle prior to beam delivery. However, generating 4D-CBCT images with sufficient quality requires significantly more projection images than a standard 3D-CBCT scan, leading to extended scanning times and increased imaging dose to the patient. To address these limitations, there is a strong demand for methods capable of reconstructing high-quality 4D-CBCT images from a 1-minute 3D-CBCT acquisition. The challenge lies in the sparse sampling of projections, which introduces severe streaking artifacts and compromises image quality. This paper introduces a novel framework leveraging spatiotemporal Gaussian representation for 4D-CBCT reconstruction from sparse projections, achieving a balance between streak artifact reduction, dynamic motion preservation, and fine detail restoration. Each Gaussian is characterized by its 3D position, covariance, rotation, and density. Two-dimensional X-ray projection images can be rendered from the Gaussian point cloud representation via X-ray rasterization. The properties of each Gaussian were optimized by minimizing the discrepancy between the measured projections and the rendered X-ray projections. A Gaussian deformation network is jointly optimized to deform these Gaussian properties to obtain a 4D Gaussian representation for dynamic CBCT scene modeling. The final 4D-CBCT images are reconstructed by voxelizing the 4D Gaussians, achieving a high-quality representation that preserves both motion dynamics and spatial detail. The code and reconstruction results can be found at https://github.com/fuyabo/4DGS_for_4DCBCT/tree/main",
    "arxiv_url": "http://arxiv.org/abs/2501.04140v1",
    "pdf_url": "http://arxiv.org/pdf/2501.04140v1",
    "published_date": "2025-01-07",
    "categories": [
      "physics.med-ph",
      "eess.IV"
    ],
    "github_url": "https://github.com/fuyabo/4DGS_for_4DCBCT",
    "keywords": [
      "motion",
      "deformation",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting",
    "authors": [
      "Abhishek Saroha",
      "Florian Hofherr",
      "Mariia Gladkova",
      "Cecilia Curreli",
      "Or Litany",
      "Daniel Cremers"
    ],
    "abstract": "Stylizing a dynamic scene based on an exemplar image is critical for various real-world applications, including gaming, filmmaking, and augmented and virtual reality. However, achieving consistent stylization across both spatial and temporal dimensions remains a significant challenge. Most existing methods are designed for static scenes and often require an optimization process for each style image, limiting their adaptability. We introduce ZDySS, a zero-shot stylization framework for dynamic scenes, allowing our model to generalize to previously unseen style images at inference. Our approach employs Gaussian splatting for scene representation, linking each Gaussian to a learned feature vector that renders a feature map for any given view and timestamp. By applying style transfer on the learned feature vectors instead of the rendered feature map, we enhance spatio-temporal consistency across frames. Our method demonstrates superior performance and coherence over state-of-the-art baselines in tests on real-world dynamic scenes, making it a robust solution for practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2501.03875v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03875v1",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
    "authors": [
      "Sangwoon Kwak",
      "Joonsoo Kim",
      "Jun Young Jeong",
      "Won-Sik Cheong",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2501.03714v3",
    "pdf_url": "http://arxiv.org/pdf/2501.03714v3",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "neural rendering",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting",
    "authors": [
      "Jinze Yu",
      "Yiqun Wang",
      "Zhengda Lu",
      "Jianwei Guo",
      "Yong Li",
      "Hongxing Qin",
      "Xiaopeng Zhang"
    ],
    "abstract": "Current novel view synthesis tasks primarily rely on high-quality and clear images. However, in foggy scenes, scattering and attenuation can significantly degrade the reconstruction and rendering quality. Although NeRF-based dehazing reconstruction algorithms have been developed, their use of deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Moreover, NeRF's implicit representation struggles to recover fine details from hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly modeling point clouds into 3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation to explain the foggy image formation process through a physically accurate forward rendering process. We introduce DehazeGS, a method capable of decomposing and rendering a fog-free background from participating media using only muti-view foggy images as input. We model the transmission within each Gaussian distribution to simulate the formation of fog. During this process, we jointly learn the atmospheric light and scattering coefficient while optimizing the Gaussian representation of the hazy scene. In the inference stage, we eliminate the effects of scattering and attenuation on the Gaussians and directly project them onto a 2D plane to obtain a clear view. Experiments on both synthetic and real-world foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance in terms of both rendering quality and computational efficiency. visualizations are available at https://dehazegs.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2501.03659v4",
    "pdf_url": "http://arxiv.org/pdf/2501.03659v4",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian Splatting",
    "authors": [
      "Yifeng Yang",
      "Hengyu Liu",
      "Chenxin Li",
      "Yining Sun",
      "Wuyang Li",
      "Yifan Liu",
      "Yiyang Lin",
      "Yixuan Yuan",
      "Nanyang Ye"
    ],
    "abstract": "With the rapid development of 3D reconstruction technology, the widespread distribution of 3D data has become a future trend. While traditional visual data (such as images and videos) and NeRF-based formats already have mature techniques for copyright protection, steganographic techniques for the emerging 3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address this, we propose ConcealGS, an innovative method for embedding implicit information into 3D-GS. By introducing the knowledge distillation and gradient optimization strategy based on 3D-GS, ConcealGS overcomes the limitations of NeRF-based models and enhances the robustness of implicit information and the quality of 3D reconstruction. We evaluate ConcealGS in various potential application scenarios, and experimental results have demonstrated that ConcealGS not only successfully recovers implicit information but also has almost no impact on rendering quality, providing a new approach for embedding invisible and recoverable information into 3D models in the future.",
    "arxiv_url": "http://arxiv.org/abs/2501.03605v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03605v1",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs",
    "authors": [
      "Soonbin Lee",
      "Fangwen Shu",
      "Yago Sanchez",
      "Thomas Schierl",
      "Cornelius Hellge"
    ],
    "abstract": "3D Gaussian Splatting is a recognized method for 3D scene representation, known for its high rendering quality and speed. However, its substantial data requirements present challenges for practical applications. In this paper, we introduce an efficient compression technique that significantly reduces storage overhead by using compact representation. We propose a unified architecture that combines point cloud data and feature planes through a progressive tri-plane structure. Our method utilizes 2D feature planes, enabling continuous spatial representation. To further optimize these representations, we incorporate entropy modeling in the frequency domain, specifically designed for standard video codecs. We also propose channel-wise bit allocation to achieve a better trade-off between bitrate consumption and feature plane representation. Consequently, our model effectively leverages spatial correlations within the feature planes to enhance rate-distortion performance using standard, non-differentiable video codecs. Experimental results demonstrate that our method outperforms existing methods in data compactness while maintaining high rendering quality. Our project page is available at https://fraunhoferhhi.github.io/CodecGS",
    "arxiv_url": "http://arxiv.org/abs/2501.03399v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03399v1",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Masked Autoencoders",
    "authors": [
      "Jathushan Rajasegaran",
      "Xinlei Chen",
      "Rulilong Li",
      "Christoph Feichtenhofer",
      "Jitendra Malik",
      "Shiry Ginosar"
    ],
    "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae",
    "arxiv_url": "http://arxiv.org/abs/2501.03229v1",
    "pdf_url": "http://arxiv.org/pdf/2501.03229v1",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation",
    "authors": [
      "Wentian Qu",
      "Jiahe Li",
      "Jian Cheng",
      "Jian Shi",
      "Chenyu Meng",
      "Cuixia Ma",
      "Hongan Wang",
      "Xiaoming Deng",
      "Yinda Zhang"
    ],
    "abstract": "Understanding of bimanual hand-object interaction plays an important role in robotics and virtual reality. However, due to significant occlusions between hands and object as well as the high degree-of-freedom motions, it is challenging to collect and annotate a high-quality, large-scale dataset, which prevents further improvement of bimanual hand-object interaction-related baselines. In this work, we propose a new 3D Gaussian Splatting based data augmentation framework for bimanual hand-object interaction, which is capable of augmenting existing dataset to large-scale photorealistic data with various hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects and hands, and to deal with the rendering blur problem due to multi-resolution input images used, we design a super-resolution module. Second, we extend the single hand grasping pose optimization module for the bimanual hand object to generate various poses of bimanual hand-object interaction, which can significantly expand the pose distribution of the dataset. Third, we conduct an analysis for the impact of different aspects of the proposed data augmentation on the understanding of the bimanual hand-object interaction. We perform our data augmentation on two benchmarks, H2O and Arctic, and verify that our method can improve the performance of the baselines.",
    "arxiv_url": "http://arxiv.org/abs/2501.02845v1",
    "pdf_url": "http://arxiv.org/pdf/2501.02845v1",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yijin Li",
      "Fu-Yun Wang",
      "Hongsheng Li"
    ],
    "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
    "arxiv_url": "http://arxiv.org/abs/2501.02690v1",
    "pdf_url": "http://arxiv.org/pdf/2501.02690v1",
    "published_date": "2025-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "motion",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment",
    "authors": [
      "Wenyan Cong",
      "Hanqing Zhu",
      "Kevin Wang",
      "Jiahui Lei",
      "Colton Stearns",
      "Yuanhao Cai",
      "Dilin Wang",
      "Rakesh Ranjan",
      "Matt Feiszli",
      "Leonidas Guibas",
      "Zhangyang Wang",
      "Weiyao Wang",
      "Zhiwen Fan"
    ],
    "abstract": "Efficiently reconstructing 3D scenes from monocular video remains a core challenge in computer vision, vital for applications in virtual reality, robotics, and scene understanding. Recently, frame-by-frame progressive reconstruction without camera poses is commonly adopted, incurring high computational overhead and compounding errors when scaling to longer videos. To overcome these issues, we introduce VideoLifter, a novel video-to-3D pipeline that leverages a local-to-global strategy on a fragment basis, achieving both extreme efficiency and SOTA quality. Locally, VideoLifter leverages learnable 3D priors to register fragments, extracting essential information for subsequent 3D Gaussian initialization with enforced inter-fragment consistency and optimized efficiency. Globally, it employs a tree-based hierarchical merging method with key frame guidance for inter-fragment alignment, pairwise merging with Gaussian point pruning, and subsequent joint optimization to ensure global consistency while efficiently mitigating cumulative errors. This approach significantly accelerates the reconstruction process, reducing training time by over 82% while holding better visual quality than current SOTA methods.",
    "arxiv_url": "http://arxiv.org/abs/2501.01949v2",
    "pdf_url": "http://arxiv.org/pdf/2501.01949v2",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "head",
      "fast",
      "understanding",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
    "authors": [
      "Siyuan Huang",
      "Liliang Chen",
      "Pengfei Zhou",
      "Shengcong Chen",
      "Zhengkai Jiang",
      "Yue Hu",
      "Yue Liao",
      "Peng Gao",
      "Hongsheng Li",
      "Maoqing Yao",
      "Guanghui Ren"
    ],
    "abstract": "We introduce EnerVerse, a generative robotics foundation model that constructs and interprets embodied spaces. EnerVerse employs an autoregressive video diffusion framework to predict future embodied spaces from instructions, enhanced by a sparse context memory for long-term reasoning. To model the 3D robotics world, we propose Free Anchor Views (FAVs), a multi-view video representation offering flexible, task-adaptive perspectives to address challenges like motion ambiguity and environmental constraints. Additionally, we present EnerVerse-D, a data engine pipeline combining the generative model with 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the sim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world representations into physical actions via a policy head (EnerVerse-A), enabling robots to execute task instructions. EnerVerse-A achieves state-of-the-art performance in both simulation and real-world settings.",
    "arxiv_url": "http://arxiv.org/abs/2501.01895v2",
    "pdf_url": "http://arxiv.org/pdf/2501.01895v2",
    "published_date": "2025-01-03",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "head",
      "motion",
      "4d",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision",
    "authors": [
      "Alberta Longhini",
      "Marcel B√ºsching",
      "Bardienus P. Duisterhof",
      "Jens Lundell",
      "Jeffrey Ichnowski",
      "M√•rten Bj√∂rkman",
      "Danica Kragic"
    ],
    "abstract": "We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time.",
    "arxiv_url": "http://arxiv.org/abs/2501.01715v1",
    "pdf_url": "http://arxiv.org/pdf/2501.01715v1",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction",
    "authors": [
      "Chenhao Zhang",
      "Yuanping Cao",
      "Lei Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian primitives for high-quality scene representation and reconstruction. While existing 3DGS methods perform well in scenes with minor view variation, large view changes from cross-view data pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction based on multi-branch construction and fusion. Our method independently reconstructs models from different sets of views as multiple independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of multi-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2501.01695v2",
    "pdf_url": "http://arxiv.org/pdf/2501.01695v2",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping",
    "authors": [
      "Tengfei Wang",
      "Xin Wang",
      "Yongmao Hou",
      "Yiwei Xu",
      "Wendi Zhang",
      "Zongqian Zhan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-scale urban areas without downsampling the original image resolution. First, the Cross-modal model - Language Segment Anything is leveraged to segment building masks. Then, the segmented building regions is grouped into sub-regions according to the visibility check across registered images. The Gaussian kernels for these sub-regions are optimized in parallel with masked pixels. In addition, the normal loss is re-formulated for the detected edges of masks to alleviate the ambiguities in normal vectors on edges. Finally, to improve the optimization of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts for the complexity of the corresponding scenes, effectively minimizing the thread waiting time in the pixel-parallel rendering stage as well as the reconstruction lost. Extensive experiments are tested on various urban datasets, the results demonstrated the superior performance of our PG-SAG on building surface reconstruction, compared to several state-of-the-art 3DGS-based methods. Project Web:https://github.com/TFWang-9527/PG-SAG.",
    "arxiv_url": "http://arxiv.org/abs/2501.01677v1",
    "pdf_url": "http://arxiv.org/pdf/2501.01677v1",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/TFWang-9527/PG-SAG",
    "keywords": [
      "face",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic Reconstruction",
    "authors": [
      "Xuan Yu",
      "Yuxuan Xie",
      "Yili Liu",
      "Haojian Lu",
      "Rong Xiong",
      "Yiyi Liao",
      "Yue Wang"
    ],
    "abstract": "Open-vocabulary panoptic reconstruction offers comprehensive scene understanding, enabling advances in embodied robotics and photorealistic simulation. In this paper, we propose PanopticRecon++, an end-to-end method that formulates panoptic reconstruction through a novel cross-attention perspective. This perspective models the relationship between 3D instances (as queries) and the scene's 3D embedding field (as keys) through their attention map. Unlike existing methods that separate the optimization of queries and keys or overlook spatial proximity, PanopticRecon++ introduces learnable 3D Gaussians as instance queries. This formulation injects 3D spatial priors to preserve proximity while maintaining end-to-end optimizability. Moreover, this query formulation facilitates the alignment of 2D open-vocabulary instance IDs across frames by leveraging optimal linear assignment with instance masks rendered from the queries. Additionally, we ensure semantic-instance segmentation consistency by fusing query-based instance segmentation probabilities with semantic probabilities in a novel panoptic head supervised by a panoptic loss. During training, the number of instance query tokens dynamically adapts to match the number of objects. PanopticRecon++ shows competitive performance in terms of 3D and 2D segmentation and reconstruction performance on both simulation and real-world datasets, and demonstrates a user case as a robot simulator. Our project website is at: https://yuxuan1206.github.io/panopticrecon_pp/",
    "arxiv_url": "http://arxiv.org/abs/2501.01119v1",
    "pdf_url": "http://arxiv.org/pdf/2501.01119v1",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "head",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes",
    "authors": [
      "Jiwei Shan",
      "Zeyu Cai",
      "Cheng-Tai Hsieh",
      "Shing Shin Cheng",
      "Hesheng Wang"
    ],
    "abstract": "Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation, which reduces rendering speed. To address these challenges, we introduce EH-SurGS, an efficient and high-fidelity reconstruction algorithm for deformable surgical scenes. We propose a deformation modeling approach that incorporates the life cycle of 3D Gaussians, effectively capturing both regular and irreversible deformations, thus enhancing reconstruction quality. Additionally, we present an adaptive motion hierarchy strategy that distinguishes between static and deformable regions within the surgical scene. This strategy reduces the number of 3D Gaussians passing through the deformation field, thereby improving rendering speed. Extensive experiments demonstrate that our method surpasses existing state-of-the-art approaches in both reconstruction quality and rendering speed. Ablation studies further validate the effectiveness and necessity of our proposed components. We will open-source our code upon acceptance of the paper.",
    "arxiv_url": "http://arxiv.org/abs/2501.01101v1",
    "pdf_url": "http://arxiv.org/pdf/2501.01101v1",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy",
    "authors": [
      "Ao Gao",
      "Luosong Guo",
      "Tao Chen",
      "Zhao Wang",
      "Ying Tai",
      "Jian Yang",
      "Zhenyu Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2501.01003v2",
    "pdf_url": "http://arxiv.org/pdf/2501.01003v2",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting",
    "authors": [
      "Kyle Gao",
      "Liangzhi Li",
      "Hongjie He",
      "Dening Lu",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "Recently released open-source pre-trained foundational image segmentation and object detection models (SAM2+GroundingDINO) allow for geometrically consistent segmentation of objects of interest in multi-view 2D images. Users can use text-based or click-based prompts to segment objects of interest without requiring labeled training datasets. Gaussian Splatting allows for the learning of the 3D representation of a scene's geometry and radiance based on 2D images. Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and our improvements in mask refinement based on morphological operations and contour simplification, we created a pipeline to extract the 3D mesh of any building based on its name, address, or geographic coordinates.",
    "arxiv_url": "http://arxiv.org/abs/2501.00625v3",
    "pdf_url": "http://arxiv.org/pdf/2501.00625v3",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes",
    "authors": [
      "Jiawei Yang",
      "Jiahui Huang",
      "Yuxiao Chen",
      "Yan Wang",
      "Boyi Li",
      "Yurong You",
      "Apoorva Sharma",
      "Maximilian Igl",
      "Peter Karkus",
      "Danfei Xu",
      "Boris Ivanovic",
      "Yue Wang",
      "Marco Pavone"
    ],
    "abstract": "We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., \"amodal\") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2501.00602v1",
    "pdf_url": "http://arxiv.org/pdf/2501.00602v1",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "motion",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamDrive: Generative 4D Scene Modeling from Street View Images",
    "authors": [
      "Jiageng Mao",
      "Boyi Li",
      "Boris Ivanovic",
      "Yuxiao Chen",
      "Yan Wang",
      "Yurong You",
      "Chaowei Xiao",
      "Danfei Xu",
      "Marco Pavone",
      "Yue Wang"
    ],
    "abstract": "Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2501.00601v2",
    "pdf_url": "http://arxiv.org/pdf/2501.00601v2",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "geometry",
      "4d",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM",
    "authors": [
      "Runnan Chen",
      "Zhaoqing Wang",
      "Jiepeng Wang",
      "Yuexin Ma",
      "Mingming Gong",
      "Wenping Wang",
      "Tongliang Liu"
    ],
    "abstract": "Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (https://github.com/runnanchen/PanoSLAM)",
    "arxiv_url": "http://arxiv.org/abs/2501.00352v1",
    "pdf_url": "http://arxiv.org/pdf/2501.00352v1",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/runnanchen/PanoSLAM",
    "keywords": [
      "localization",
      "efficient",
      "robotics",
      "tracking",
      "efficient rendering",
      "semantic",
      "mapping",
      "understanding",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians",
    "authors": [
      "Yiwen Wang",
      "Siyuan Chen",
      "Ran Yi"
    ],
    "abstract": "3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.",
    "arxiv_url": "http://arxiv.org/abs/2501.00342v1",
    "pdf_url": "http://arxiv.org/pdf/2501.00342v1",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies",
    "authors": [
      "Runnan Chen",
      "Xiangyu Sun",
      "Zhaoqing Wang",
      "Youquan Liu",
      "Jiepeng Wang",
      "Lingdong Kong",
      "Jiankang Deng",
      "Mingming Gong",
      "Liang Pan",
      "Wenping Wang",
      "Tongliang Liu"
    ],
    "abstract": "Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \\textbf{OVGaussian}, a generalizable \\textbf{O}pen-\\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \\textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \\textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (https://github.com/runnanchen/OVGaussian).",
    "arxiv_url": "http://arxiv.org/abs/2501.00326v1",
    "pdf_url": "http://arxiv.org/pdf/2501.00326v1",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/runnanchen/OVGaussian",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
    "authors": [
      "Hyunsoo Cha",
      "Inhee Lee",
      "Hanbyul Joo"
    ],
    "abstract": "We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.",
    "arxiv_url": "http://arxiv.org/abs/2412.21206v1",
    "pdf_url": "http://arxiv.org/pdf/2412.21206v1",
    "published_date": "2024-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation",
    "authors": [
      "Yuanbo Yang",
      "Jiahao Shao",
      "Xinyang Li",
      "Yujun Shen",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation. Project page: https://freemty.github.io/project-prometheus/",
    "arxiv_url": "http://arxiv.org/abs/2412.21117v2",
    "pdf_url": "http://arxiv.org/pdf/2412.21117v2",
    "published_date": "2024-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences",
    "authors": [
      "Keng-Wei Chang",
      "Zi-Ming Wang",
      "Shang-Hong Lai"
    ],
    "abstract": "Reconstructing high-quality 3D models from sparse 2D images has garnered significant attention in computer vision. Recently, 3D Gaussian Splatting (3DGS) has gained prominence due to its explicit representation with efficient training speed and real-time rendering capabilities. However, existing methods still heavily depend on accurate camera poses for reconstruction. Although some recent approaches attempt to train 3DGS models without the Structure-from-Motion (SfM) preprocessing from monocular video datasets, these methods suffer from prolonged training times, making them impractical for many applications.   In this paper, we present an efficient framework that operates without any depth or matching model. Our approach initially uses SfM to quickly obtain rough camera poses within seconds, and then refines these poses by leveraging the dense representation in 3DGS. This framework effectively addresses the issue of long training times. Additionally, we integrate the densification process with joint refinement and propose a coarse-to-fine frequency-aware densification to reconstruct different levels of details. This approach prevents camera pose estimation from being trapped in local minima or drifting due to high-frequency signals. Our method significantly reduces training time from hours to minutes while achieving more accurate novel view synthesis and camera pose estimation compared to previous methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.20767v1",
    "pdf_url": "http://arxiv.org/pdf/2412.20767v1",
    "published_date": "2024-12-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives",
    "authors": [
      "Zeyu Yang",
      "Zijie Pan",
      "Xiatian Zhu",
      "Li Zhang",
      "Yu-Gang Jiang",
      "Philip H. S. Torr"
    ],
    "abstract": "Dynamic 3D scene representation and novel view synthesis from captured videos are crucial for enabling immersive experiences required by AR/VR and metaverse applications. However, this task is challenging due to the complexity of unconstrained real-world scenes and their temporal dynamics. In this paper, we frame dynamic scenes as a spatio-temporal 4D volume learning problem, offering a native explicit reformulation with minimal assumptions about motion, which serves as a versatile dynamic scene learning framework. Specifically, we represent a target dynamic scene using a collection of 4D Gaussian primitives with explicit geometry and appearance features, dubbed as 4D Gaussian splatting (4DGS). This approach can capture relevant information in space and time by fitting the underlying spatio-temporal volume. Modeling the spacetime as a whole with 4D Gaussians parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, our model can naturally learn view-dependent and time-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS model is the first solution that supports real-time rendering of high-resolution, photorealistic novel views for complex dynamic scenes. To enhance efficiency, we derive several compact variants that effectively reduce memory footprint and mitigate the risk of overfitting. Extensive experiments validate the superiority of 4DGS in terms of visual quality and efficiency across a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D generation, scene understanding) and scenarios (e.g., single object, indoor scenes, driving environments, synthetic and real data).",
    "arxiv_url": "http://arxiv.org/abs/2412.20720v1",
    "pdf_url": "http://arxiv.org/pdf/2412.20720v1",
    "published_date": "2024-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "motion",
      "understanding",
      "geometry",
      "4d",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
    "authors": [
      "Yifei Liu",
      "Zhihang Zhong",
      "Yifan Zhan",
      "Sheng Xu",
      "Xiao Sun"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian",
    "arxiv_url": "http://arxiv.org/abs/2412.20522v2",
    "pdf_url": "http://arxiv.org/pdf/2412.20522v2",
    "published_date": "2024-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/kaikai23/MaskGaussian",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis",
    "authors": [
      "Kaijun Deng",
      "Dezhi Zheng",
      "Jindong Xie",
      "Jinbao Wang",
      "Weicheng Xie",
      "Linlin Shen",
      "Siyang Song"
    ],
    "abstract": "Accurately synthesizing talking face videos and capturing fine facial features for individuals with long hair presents a significant challenge. To tackle these challenges in existing methods, we propose a decomposed per-embedding Gaussian fields (DEGSTalk), a 3D Gaussian Splatting (3DGS)-based talking face synthesis method for generating realistic talking faces with long hairs. Our DEGSTalk employs Deformable Pre-Embedding Gaussian Fields, which dynamically adjust pre-embedding Gaussian primitives using implicit expression coefficients. This enables precise capture of dynamic facial regions and subtle expressions. Additionally, we propose a Dynamic Hair-Preserving Portrait Rendering technique to enhance the realism of long hair motions in the synthesized videos. Results show that DEGSTalk achieves improved realism and synthesis quality compared to existing approaches, particularly in handling complex facial dynamics and hair preservation. Our code will be publicly available at https://github.com/CVI-SZU/DEGSTalk.",
    "arxiv_url": "http://arxiv.org/abs/2412.20148v1",
    "pdf_url": "http://arxiv.org/pdf/2412.20148v1",
    "published_date": "2024-12-28",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "https://github.com/CVI-SZU/DEGSTalk",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting",
    "authors": [
      "Atticus J. Zeller",
      "Haijuan Wu"
    ],
    "abstract": "We present GSplatLoc, a camera localization method that leverages the differentiable rendering capabilities of 3D Gaussian splatting for ultra-precise pose estimation. By formulating pose estimation as a gradient-based optimization problem that minimizes discrepancies between rendered depth maps from a pre-existing 3D Gaussian scene and observed depth images, GSplatLoc achieves translational errors within 0.01 cm and near-zero rotational errors on the Replica dataset - significantly outperforming existing methods. Evaluations on the Replica and TUM RGB-D datasets demonstrate the method's robustness in challenging indoor environments with complex camera motions. GSplatLoc sets a new benchmark for localization in dense mapping, with important implications for applications requiring accurate real-time localization, such as robotics and augmented reality.",
    "arxiv_url": "http://arxiv.org/abs/2412.20056v2",
    "pdf_url": "http://arxiv.org/pdf/2412.20056v2",
    "published_date": "2024-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "motion",
      "mapping",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction",
    "authors": [
      "Kai Xu",
      "Tze Ho Elden Tse",
      "Jizong Peng",
      "Angela Yao"
    ],
    "abstract": "We propose a novel framework for scene decomposition and static background reconstruction from everyday videos. By integrating the trained motion masks and modeling the static scene as Gaussian splats with dynamics-aware optimization, our method achieves more accurate background reconstruction results than previous works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods, DAS3R is more robust in complex motion scenarios, capable of handling videos where dynamic objects occupy a significant portion of the scene, and does not require camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates enhanced performance and robustness with a margin of more than 2 dB in PSNR. The project's webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}",
    "arxiv_url": "http://arxiv.org/abs/2412.19584v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19584v1",
    "published_date": "2024-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images",
    "authors": [
      "Xudong Cai",
      "Yongcai Wang",
      "Zhaoxin Fan",
      "Deng Haoran",
      "Shuo Wang",
      "Wanting Li",
      "Deying Li",
      "Lun Luo",
      "Minhang Wang",
      "Jintao Xu"
    ],
    "abstract": "Photo-realistic scene reconstruction from sparse-view, uncalibrated images is highly required in practice. Although some successes have been made, existing methods are either Sparse-View but require accurate camera parameters (i.e., intrinsic and extrinsic), or SfM-free but need densely captured images. To combine the advantages of both methods while addressing their respective weaknesses, we propose Dust to Tower (D2T), an accurate and efficient coarse-to-fine framework to optimize 3DGS and image poses simultaneously from sparse and uncalibrated images. Our key idea is to first construct a coarse model efficiently and subsequently refine it using warped and inpainted images at novel viewpoints. To do this, we first introduce a Coarse Construction Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA) module to refine the coarse depth maps by aligning their confident parts with estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting (WIGI) module is proposed to warp the training images to novel viewpoints by the refined depth maps, and inpainting is applied to fulfill the ``holes\" in the warped images caused by view-direction changes, providing high-quality supervision to further optimize the 3D model and the camera poses. Extensive experiments and ablation studies demonstrate the validity of D2T and its design choices, achieving state-of-the-art performance in both tasks of novel view synthesis and pose estimation while keeping high efficiency. Codes will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2412.19518v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19518v1",
    "published_date": "2024-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Radiance Fields from a Single Snapshot Compressive Image",
    "authors": [
      "Yunhao Li",
      "Xiang Liu",
      "Xiaodong Wang",
      "Xin Yuan",
      "Peidong Liu"
    ],
    "abstract": "In this paper, we explore the potential of Snapshot Compressive Imaging (SCI) technique for recovering the underlying 3D scene structure from a single temporal compressed image. SCI is a cost-effective method that enables the recording of high-dimensional data, such as hyperspectral or temporal information, into a single image using low-cost 2D imaging sensors. To achieve this, a series of specially designed 2D masks are usually employed, reducing storage and transmission requirements and offering potential privacy protection. Inspired by this, we take one step further to recover the encoded 3D scene information leveraging powerful 3D scene representation capabilities of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we formulate the physical imaging process of SCI as part of the training of NeRF, allowing us to exploit its impressive performance in capturing complex scene structures. In addition, we further integrate the popular 3D Gaussian Splatting (3DGS) framework and propose SCISplat to improve 3D scene reconstruction quality and training/rendering speed by explicitly optimizing point clouds into 3D Gaussian representations. To assess the effectiveness of our method, we conduct extensive evaluations using both synthetic data and real data captured by our SCI system. Experimental results demonstrate that our proposed approach surpasses the state-of-the-art methods in terms of image reconstruction and novel view synthesis. Moreover, our method also exhibits the ability to render high frame-rate multi-view consistent images in real time by leveraging SCI and the rendering capabilities of 3DGS. Codes will be available at: https://github.com/WU- CVGL/SCISplat.",
    "arxiv_url": "http://arxiv.org/abs/2412.19483v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19483v1",
    "published_date": "2024-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BeSplat: Gaussian Splatting from a Single Blurry Image and Event Stream",
    "authors": [
      "Gopi Raju Matta",
      "Reddypalli Trisha",
      "Kaushik Mitra"
    ],
    "abstract": "Novel view synthesis has been greatly enhanced by the development of radiance field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively addressed key challenges, such as long training times and slow rendering speeds, typically associated with Neural Radiance Fields (NeRF), while maintaining high-quality reconstructions. In this work (BeSplat), we demonstrate the recovery of sharp radiance field (Gaussian splats) from a single motion-blurred image and its corresponding event stream. Our method jointly learns the scene representation via Gaussian Splatting and recovers the camera motion through Bezier SE(3) formulation effectively, minimizing discrepancies between synthesized and real-world measurements of both blurry image and corresponding event stream. We evaluate our approach on both synthetic and real datasets, showcasing its ability to render view-consistent, sharp images from the learned radiance field and the estimated camera trajectory. To the best of our knowledge, ours is the first work to address this highly challenging ill-posed problem in a Gaussian Splatting framework with the effective incorporation of temporal information captured using the event stream.",
    "arxiv_url": "http://arxiv.org/abs/2412.19370v2",
    "pdf_url": "http://arxiv.org/pdf/2412.19370v2",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflective Gaussian Splatting",
    "authors": [
      "Yuxuan Yao",
      "Zixuan Zeng",
      "Chun Gu",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (Ref-Gaussian) framework characterized with two components: (I) Physically based deferred rendering that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) Gaussian-grounded inter-reflection that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing.",
    "arxiv_url": "http://arxiv.org/abs/2412.19282v2",
    "pdf_url": "http://arxiv.org/pdf/2412.19282v2",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "relighting",
      "lighting",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Editable Head Avatars with 3D Gaussian GANs",
    "authors": [
      "Guohao Li",
      "Hongyu Yang",
      "Yifang Men",
      "Di Huang",
      "Weixin Li",
      "Ruijie Yang",
      "Yunhong Wang"
    ],
    "abstract": "Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.",
    "arxiv_url": "http://arxiv.org/abs/2412.19149v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19149v1",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/liguohao96/EGG3D",
    "keywords": [
      "illumination",
      "animation",
      "head",
      "face",
      "deformation",
      "3d gaussian",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting",
    "authors": [
      "Siyu Jiao",
      "Haoye Dong",
      "Yuyang Yin",
      "Zequn Jie",
      "Yinlong Qian",
      "Yao Zhao",
      "Humphrey Shi",
      "Yunchao Wei"
    ],
    "abstract": "Recent works in 3D multimodal learning have made remarkable progress. However, typically 3D multimodal models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification.",
    "arxiv_url": "http://arxiv.org/abs/2412.19142v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19142v1",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "few-shot",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View Stereo",
    "authors": [
      "Byeonggwon Lee",
      "Junkyu Park",
      "Khang Truong Giang",
      "Sungho Jo",
      "Soohwan Song"
    ],
    "abstract": "This study addresses the challenge of online 3D model generation for neural rendering using an RGB image stream. Previous research has tackled this issue by incorporating Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS) as scene representations within dense SLAM methods. However, most studies focus primarily on estimating coarse 3D scenes rather than achieving detailed reconstructions. Moreover, depth estimation based solely on images is often ambiguous, resulting in low-quality 3D models that lead to inaccurate renderings. To overcome these limitations, we propose a novel framework for high-quality 3DGS modeling that leverages an online multi-view stereo (MVS) approach. Our method estimates MVS depth using sequential frames from a local time window and applies comprehensive depth refinement techniques to filter out outliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore, we introduce a parallelized backend module that optimizes the 3DGS model efficiently, ensuring timely updates with each new keyframe. Experimental results demonstrate that our method outperforms state-of-the-art dense SLAM methods, particularly excelling in challenging outdoor environments.",
    "arxiv_url": "http://arxiv.org/abs/2412.19130v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19130v1",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting",
    "authors": [
      "Chenghao Qian",
      "Yuhu Guo",
      "Wenjing Li",
      "Gustav Markkula"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods. See project page:https://jumponthemoon.github.io/weather-gs.",
    "arxiv_url": "http://arxiv.org/abs/2412.18862v3",
    "pdf_url": "http://arxiv.org/pdf/2412.18862v3",
    "published_date": "2024-12-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator",
    "authors": [
      "Rami Wilson"
    ],
    "abstract": "Modern autonomous vehicle simulators feature an ever-growing library of assets, including vehicles, buildings, roads, pedestrians, and more. While this level of customization proves beneficial when creating virtual urban environments, this process becomes cumbersome when intending to train within a digital twin or a duplicate of a real scene. Gaussian splatting emerged as a powerful technique in scene reconstruction and novel view synthesis, boasting high fidelity and rendering speeds. In this paper, we introduce GSAVS, an autonomous vehicle simulator that supports the creation and development of autonomous vehicle models. Every asset within the simulator is a 3D Gaussian splat, including the vehicles and the environment. However, the simulator runs within a classical 3D engine, rendering 3D Gaussian splats in real-time. This allows the simulator to utilize the photorealism that 3D Gaussian splatting boasts while providing the customization and ease of use of a classical 3D engine.",
    "arxiv_url": "http://arxiv.org/abs/2412.18816v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18816v1",
    "published_date": "2024-12-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtNVG: Content-Style Separated Artistic Neighboring-View Gaussian Stylization",
    "authors": [
      "Zixiao Gu",
      "Mengtian Li",
      "Ruhua Chen",
      "Zhongxia Ji",
      "Sichen Guo",
      "Zhenye Zhang",
      "Guangnan Ye",
      "Zuo Hu"
    ],
    "abstract": "As demand from the film and gaming industries for 3D scenes with target styles grows, the importance of advanced 3D stylization techniques increases. However, recent methods often struggle to maintain local consistency in color and texture throughout stylized scenes, which is essential for maintaining aesthetic coherence. To solve this problem, this paper introduces ArtNVG, an innovative 3D stylization framework that efficiently generates stylized 3D scenes by leveraging reference style images. Built on 3D Gaussian Splatting (3DGS), ArtNVG achieves rapid optimization and rendering while upholding high reconstruction quality. Our framework realizes high-quality 3D stylization by incorporating two pivotal techniques: Content-Style Separated Control and Attention-based Neighboring-View Alignment. Content-Style Separated Control uses the CSGO model and the Tile ControlNet to decouple the content and style control, reducing risks of information leakage. Concurrently, Attention-based Neighboring-View Alignment ensures consistency of local colors and textures across neighboring views, significantly improving visual quality. Extensive experiments validate that ArtNVG surpasses existing methods, delivering superior results in content preservation, style alignment, and local consistency.",
    "arxiv_url": "http://arxiv.org/abs/2412.18783v2",
    "pdf_url": "http://arxiv.org/pdf/2412.18783v2",
    "published_date": "2024-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors: Diverse-Resolution Training Outperforms Interpolation",
    "authors": [
      "Anselm Krainovic",
      "Stefan Ruschke",
      "Reinhard Heckel"
    ],
    "abstract": "Deep learning-based 3D imaging, in particular magnetic resonance imaging (MRI), is challenging because of limited availability of 3D training data. Therefore, 2D diffusion models trained on 2D slices are starting to be leveraged for 3D MRI reconstruction. However, as we show in this paper, existing methods pertain to a fixed voxel size, and performance degrades when the voxel size is varied, as it is often the case in clinical practice. In this paper, we propose and study several approaches for resolution-robust 3D MRI reconstruction with 2D diffusion priors. As a result of this investigation, we obtain a simple resolution-robust variational 3D reconstruction approach based on diffusion-guided regularization of randomly sampled 2D slices. This method provides competitive reconstruction quality compared to posterior sampling baselines. Towards resolving the sensitivity to resolution-shifts, we investigate state-of-the-art model-based approaches including Gaussian splatting, neural representations, and infinite-dimensional diffusion models, as well as a simple data-centric approach of training the diffusion model on several resolutions. Our experiments demonstrate that the model-based approaches fail to close the performance gap in 3D MRI. In contrast, the data-centric approach of training the diffusion model on various resolutions effectively provides a resolution-robust method without compromising accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2412.18584v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18584v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RSGaussian:3D Gaussian Splatting with LiDAR for Aerial Remote Sensing Novel View Synthesis",
    "authors": [
      "Yiling Yao",
      "Wenjuan Zhang",
      "Bing Zhang",
      "Bocheng Li",
      "Yaning Wang",
      "Bowen Wang"
    ],
    "abstract": "This study presents RSGaussian, an innovative novel view synthesis (NVS) method for aerial remote sensing scenes that incorporate LiDAR point cloud as constraints into the 3D Gaussian Splatting method, which ensures that Gaussians grow and split along geometric benchmarks, addressing the overgrowth and floaters issues occurs. Additionally, the approach introduces coordinate transformations with distortion parameters for camera models to achieve pixel-level alignment between LiDAR point clouds and 2D images, facilitating heterogeneous data fusion and achieving the high-precision geo-alignment required in aerial remote sensing. Depth and plane consistency losses are incorporated into the loss function to guide Gaussians towards real depth and plane representations, significantly improving depth estimation accuracy. Experimental results indicate that our approach has achieved novel view synthesis that balances photo-realistic visual quality and high-precision geometric estimation under aerial remote sensing datasets. Finally, we have also established and open-sourced a dense LiDAR point cloud dataset along with its corresponding aerial multi-view images, AIR-LONGYAN.",
    "arxiv_url": "http://arxiv.org/abs/2412.18380v1",
    "pdf_url": "http://arxiv.org/pdf/2412.18380v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlameGS: Reconstruct flame light field via Gaussian Splatting",
    "authors": [
      "Yunhao Shui",
      "Fuhao Zhang",
      "Can Gao",
      "Hao Xue",
      "Zhiyin Ma",
      "Gang Xun",
      "Xuesong Li"
    ],
    "abstract": "To address the time-consuming and computationally intensive issues of traditional ART algorithms for flame combustion diagnosis, inspired by flame simulation technology, we propose a novel representation method for flames. By modeling the luminous process of flames and utilizing 2D projection images for supervision, our experimental validation shows that this model achieves an average structural similarity index of 0.96 between actual images and predicted 2D projections, along with a Peak Signal-to-Noise Ratio of 39.05. Additionally, it saves approximately 34 times the computation time and about 10 times the memory compared to traditional algorithms.",
    "arxiv_url": "http://arxiv.org/abs/2412.19841v1",
    "pdf_url": "http://arxiv.org/pdf/2412.19841v1",
    "published_date": "2024-12-24",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FaceLift: Single Image to 3D Head with View Generation and GS-LRM",
    "authors": [
      "Weijie Lyu",
      "Yi Zhou",
      "Ming-Hsuan Yang",
      "Zhixin Shu"
    ],
    "abstract": "We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation. Project page: https://weijielyu.github.io/FaceLift.",
    "arxiv_url": "http://arxiv.org/abs/2412.17812v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17812v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "lighting",
      "face",
      "4d",
      "human",
      "ar",
      "animation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveGS: Active Scene Reconstruction Using Gaussian Splatting",
    "authors": [
      "Liren Jin",
      "Xingguang Zhong",
      "Yue Pan",
      "Jens Behley",
      "Cyrill Stachniss",
      "Marija Popoviƒá"
    ],
    "abstract": "Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an RGB-D camera on a mobile platform. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. At the core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the real-world applicability of our framework using an unmanned aerial vehicle.",
    "arxiv_url": "http://arxiv.org/abs/2412.17769v2",
    "pdf_url": "http://arxiv.org/pdf/2412.17769v2",
    "published_date": "2024-12-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance",
    "authors": [
      "Jingqiu Zhou",
      "Lue Fan",
      "Xuesong Chen",
      "Linjiang Huang",
      "Si Liu",
      "Hongsheng Li"
    ],
    "abstract": "In this paper, we present GaussianPainter, the first method to paint a point cloud into 3D Gaussians given a reference image. GaussianPainter introduces an innovative feed-forward approach to overcome the limitations of time-consuming test-time optimization in 3D Gaussian splatting. Our method addresses a critical challenge in the field: the non-uniqueness problem inherent in the large parameter space of 3D Gaussian splatting. This space, encompassing rotation, anisotropic scales, and spherical harmonic coefficients, introduces the challenge of rendering similar images from substantially different Gaussian fields. As a result, feed-forward networks face instability when attempting to directly predict high-quality Gaussian fields, struggling to converge on consistent parameters for a given output. To address this issue, we propose to estimate a surface normal for each point to determine its Gaussian rotation. This strategy enables the network to effectively predict the remaining Gaussian parameters in the constrained space. We further enhance our approach with an appearance injection module, incorporating reference image appearance into Gaussian fields via a multiscale triplane representation. Our method successfully balances efficiency and fidelity in 3D Gaussian generation, achieving high-quality, diverse, and robust 3D content creation from point clouds in a single forward pass.",
    "arxiv_url": "http://arxiv.org/abs/2412.17715v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17715v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding",
    "authors": [
      "Hao Li",
      "Roy Qin",
      "Zhengyu Zou",
      "Diqi He",
      "Bohan Li",
      "Bingquan Dai",
      "Dingewn Zhang",
      "Junwei Han"
    ],
    "abstract": "Applying Gaussian Splatting to perception tasks for 3D scene understanding is becoming increasingly popular. Most existing works primarily focus on rendering 2D feature maps from novel viewpoints, which leads to an imprecise 3D language field with outlier languages, ultimately failing to align objects in 3D space. By utilizing masked images for feature extraction, these approaches also lack essential contextual information, leading to inaccurate feature representation. To this end, we propose a Language-Embedded Surface Field (LangSurf), which accurately aligns the 3D language fields with the surface of objects, facilitating precise 2D and 3D segmentation with text query, widely expanding the downstream tasks such as removal and editing. The core of LangSurf is a joint training strategy that flattens the language Gaussian on the object surfaces using geometry supervision and contrastive losses to assign accurate language features to the Gaussians of objects. In addition, we also introduce the Hierarchical-Context Awareness Module to extract features at the image level for contextual information then perform hierarchical mask pooling using masks segmented by SAM to obtain fine-grained language features in different hierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic segmentation demonstrate that LangSurf outperforms the previous state-of-the-art method LangSplat by a large margin. As shown in Fig. 1, our method is capable of segmenting objects in 3D space, thus boosting the effectiveness of our approach in instance recognition, removal, and editing, which is also supported by comprehensive experiments. \\url{https://langsurf.github.io}.",
    "arxiv_url": "http://arxiv.org/abs/2412.17635v2",
    "pdf_url": "http://arxiv.org/pdf/2412.17635v2",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "face",
      "semantic",
      "understanding",
      "geometry",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction",
    "authors": [
      "Yuanyuan Gao",
      "Yalun Dai",
      "Hao Li",
      "Weicai Ye",
      "Junyi Chen",
      "Danpeng Chen",
      "Dingwen Zhang",
      "Tong He",
      "Guofeng Zhang",
      "Junwei Han"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene reconstruction. However, most existing GS-based surface reconstruction methods focus on 3D objects or limited scenes. Directly applying these methods to large-scale scene reconstruction will pose challenges such as high memory costs, excessive time consumption, and lack of geometric detail, which makes it difficult to implement in practical applications. To address these issues, we propose a multi-agent collaborative fast 3DGS surface reconstruction framework based on distributed learning for large-scale surface reconstruction. Specifically, we develop local model compression (LMC) and model aggregation schemes (MAS) to achieve high-quality surface representation of large scenes while reducing GPU memory consumption. Extensive experiments on Urban3d, MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast and scalable high-fidelity surface reconstruction and photorealistic rendering. Our project page is available at \\url{https://gyy456.github.io/CoSurfGS}.",
    "arxiv_url": "http://arxiv.org/abs/2412.17612v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17612v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "large scene",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "abstract": "Novel view synthesis (NVS) has shown significant promise for applications in cinematographic production, particularly through the exploitation of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D scenes, enabling the creation of new shots that are challenging to capture in the real world due to set topology or expensive equipment requirement. This innovation also offers cinematographic advantages such as smooth camera movements, virtual re-shoots, slow-motion effects, etc. This paper explores dynamic NVS with the aim of facilitating the model selection process. We showcase its potential through a short montage filmed using various NVS models.",
    "arxiv_url": "http://arxiv.org/abs/2412.17532v1",
    "pdf_url": "http://arxiv.org/pdf/2412.17532v1",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling",
    "authors": [
      "Hao Gui",
      "Lin Hu",
      "Rui Chen",
      "Mingxiao Huang",
      "Yuxin Yin",
      "Jin Yang",
      "Yong Wu",
      "Chen Liu",
      "Zhongxu Sun",
      "Xueyang Zhang",
      "Kun Zhan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.17378v4",
    "pdf_url": "http://arxiv.org/pdf/2412.17378v4",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSemSplat: Generalizable Semantic 3D Gaussian Splatting from Uncalibrated Image Pairs",
    "authors": [
      "Xingrui Wang",
      "Cuiling Lan",
      "Hanxin Zhu",
      "Zhibo Chen",
      "Yan Lu"
    ],
    "abstract": "Modeling and understanding the 3D world is crucial for various applications, from augmented reality to robotic navigation. Recent advancements based on 3D Gaussian Splatting have integrated semantic information from multi-view images into Gaussian primitives. However, these methods typically require costly per-scene optimization from dense calibrated images, limiting their practicality. In this paper, we consider the new task of generalizable 3D semantic field modeling from sparse, uncalibrated image pairs. Building upon the Splatt3R architecture, we introduce GSemSplat, a framework that learns open-vocabulary semantic representations linked to 3D Gaussians without the need for per-scene optimization, dense image collections or calibration. To ensure effective and reliable learning of semantic features in 3D space, we employ a dual-feature approach that leverages both region-specific and context-aware semantic features as supervision in the 2D space. This allows us to capitalize on their complementary strengths. Experimental results on the ScanNet++ dataset demonstrate the effectiveness and superiority of our approach compared to the traditional scene-specific method. We hope our work will inspire more research into generalizable 3D understanding.",
    "arxiv_url": "http://arxiv.org/abs/2412.16932v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16932v1",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting",
    "authors": [
      "Hanqing Jiang",
      "Xiaojun Xiang",
      "Han Sun",
      "Hongjie Li",
      "Liyang Zhou",
      "Xiaoyu Zhang",
      "Guofeng Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently attracted wide attentions in various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation, due to its photorealistic and efficient rendering performance. High-quality reconstrution of 3DGS relies on sufficient splats and a reasonable distribution of these splats to fit real geometric surface and texture details, which turns out to be a challenging problem. We present GeoTexDensifier, a novel geometry-texture-aware densification strategy to reconstruct high-quality Gaussian splats which better comply with the geometric structure and texture richness of the scene. Specifically, our GeoTexDensifier framework carries out an auxiliary texture-aware densification method to produce a denser distribution of splats in fully textured areas, while keeping sparsity in low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile, a geometry-aware splitting strategy takes depth and normal priors to guide the splitting sampling and filter out the noisy splats whose initial positions are far from the actual geometric surfaces they aim to fit, under a Validation of Depth Ratio Change checking. With the help of relative monocular depth prior, such geometry-aware validation can effectively reduce the influence of scattered Gaussians to the final rendering quality, especially in regions with weak textures or without sufficient training views. The texture-aware densification and geometry-aware splitting strategies are fully combined to obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier framework on various datasets and compare our Novel View Synthesis results to other state-of-the-art 3DGS approaches, with detailed quantitative and qualitative evaluations to demonstrate the effectiveness of our method in producing more photorealistic 3DGS models.",
    "arxiv_url": "http://arxiv.org/abs/2412.16809v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16809v1",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "efficient rendering",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity",
    "authors": [
      "Tianqi Shen",
      "Shaohua Liu",
      "Jiaqi Feng",
      "Ziye Ma",
      "Ning An"
    ],
    "abstract": "Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area.",
    "arxiv_url": "http://arxiv.org/abs/2412.16619v3",
    "pdf_url": "http://arxiv.org/pdf/2412.16619v3",
    "published_date": "2024-12-21",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "math.AT",
      "math.GT"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities",
    "authors": [
      "Suyoung Lee",
      "Jaeyoung Chung",
      "Kihoon Kim",
      "Jaeyoo Huh",
      "Gunhee Lee",
      "Minsoo Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "Feed-forward 3D Gaussian splatting (3DGS) models have gained significant popularity due to their ability to generate scenes immediately without needing per-scene optimization. Although omnidirectional images are becoming more popular since they reduce the computation required for image stitching to composite a holistic scene, existing feed-forward models are only designed for perspective images. The unique optical properties of omnidirectional images make it difficult for feature encoders to correctly understand the context of the image and make the Gaussian non-uniform in space, which hinders the image quality synthesized from novel views. We propose OmniSplat, a training-free fast feed-forward 3DGS generation framework for omnidirectional images. We adopt a Yin-Yang grid and decompose images based on it to reduce the domain gap between omnidirectional and perspective images. The Yin-Yang grid can use the existing CNN structure as it is, but its quasi-uniform characteristic allows the decomposed image to be similar to a perspective image, so it can exploit the strong prior knowledge of the learned feed-forward network. OmniSplat demonstrates higher reconstruction accuracy than existing feed-forward networks trained on perspective images. Our project page is available on: https://robot0321.github.io/omnisplat/index.html.",
    "arxiv_url": "http://arxiv.org/abs/2412.16604v2",
    "pdf_url": "http://arxiv.org/pdf/2412.16604v2",
    "published_date": "2024-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian Splatting Vacuum",
    "authors": [
      "JunEn Low",
      "Maximilian Adang",
      "Javier Yu",
      "Keiko Nagami",
      "Mac Schwager"
    ],
    "abstract": "We propose a new simulator, training approach, and policy architecture, collectively called SOUS VIDE, for end-to-end visual drone navigation. Our trained policies exhibit zero-shot sim-to-real transfer with robust real-world performance using only onboard perception and computation. Our simulator, called FiGS, couples a computationally simple drone dynamics model with a high visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly simulate drone flights producing photorealistic images at up to 130 fps. We use FiGS to collect 100k-300k image/state-action pairs from an expert MPC with privileged state and dynamics information, randomized over dynamics parameters and spatial disturbances. We then distill this expert MPC into an end-to-end visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net processes color image, optical flow and IMU data streams into low-level thrust and body rate commands at 20 Hz onboard a drone. Crucially, SV-Net includes a learned module for low-level control that adapts at runtime to variations in drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in ambient brightness, shifting or removing objects from the scene, and people moving aggressively through the drone's visual field. Code, data, and experiment videos can be found on our project page: https://stanfordmsl.github.io/SousVide/.",
    "arxiv_url": "http://arxiv.org/abs/2412.16346v2",
    "pdf_url": "http://arxiv.org/pdf/2412.16346v2",
    "published_date": "2024-12-20",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "gaussian splatting",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images",
    "authors": [
      "Jungho Lee",
      "Suhwan Cho",
      "Taeoh Kim",
      "Ho-Deok Jang",
      "Minhyeok Lee",
      "Geonho Cha",
      "Dongyoon Wee",
      "Dogyoon Lee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2412.16028v2",
    "pdf_url": "http://arxiv.org/pdf/2412.16028v2",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing",
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Zixuan Zeng",
      "Yuxuan Yao",
      "Li Zhang"
    ],
    "abstract": "In inverse rendering, accurately modeling visibility and indirect radiance for incident light is essential for capturing secondary effects. Due to the absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have either adopted a simplified rendering equation or used learnable parameters to approximate incident light, resulting in inaccurate material and lighting estimations. To this end, we introduce inter-reflective Gaussian splatting (IRGS) for inverse rendering. To capture inter-reflection, we apply the full rendering equation without simplification and compute incident radiance on the fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we present an efficient optimization scheme to handle the computational demands of Monte Carlo sampling for rendering equation evaluation. Furthermore, we introduce a novel strategy for querying the indirect radiance of incident light when relighting the optimized scenes. Extensive experiments on multiple standard benchmarks validate the effectiveness of IRGS, demonstrating its capability to accurately model complex inter-reflection effects.",
    "arxiv_url": "http://arxiv.org/abs/2412.15867v2",
    "pdf_url": "http://arxiv.org/pdf/2412.15867v2",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "reflection",
      "relighting",
      "lighting",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement with Automatic Pose Suggestion",
    "authors": [
      "Jotaro Sakamiya",
      "I-Chao Shen",
      "Jinsong Zhang",
      "Mustafa Doga Dogan",
      "Takeo Igarashi"
    ],
    "abstract": "Creating high-quality 3D avatars using 3D Gaussian Splatting (3DGS) from a monocular video benefits virtual reality and telecommunication applications. However, existing automatic methods exhibit artifacts under novel poses due to limited information in the input video. We propose AvatarPerfect, a novel system that allows users to iteratively refine 3DGS avatars by manually editing the rendered avatar images. In each iteration, our system suggests a new body and camera pose to help users identify and correct artifacts. The edited images are then used to update the current avatar, and our system suggests the next body and camera pose for further refinement. To investigate the effectiveness of AvatarPerfect, we conducted a user study comparing our method to an existing 3DGS editor SuperSplat, which allows direct manipulation of Gaussians without automatic pose suggestions. The results indicate that our system enables users to obtain higher quality refined 3DGS avatars than the existing 3DGS editor.",
    "arxiv_url": "http://arxiv.org/abs/2412.15609v1",
    "pdf_url": "http://arxiv.org/pdf/2412.15609v1",
    "published_date": "2024-12-20",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interactive Scene Authoring with Specialized Generative Primitives",
    "authors": [
      "Cl√©ment Jambon",
      "Changwoon Choi",
      "Dongsu Zhang",
      "Olga Sorkine-Hornung",
      "Young Min Kim"
    ],
    "abstract": "Generating high-quality 3D digital assets often requires expert knowledge of complex design tools. We introduce Specialized Generative Primitives, a generative framework that allows non-expert users to author high-quality 3D scenes in a seamless, lightweight, and controllable manner. Each primitive is an efficient generative model that captures the distribution of a single exemplar from the real world. With our framework, users capture a video of an environment, which we turn into a high-quality and explicit appearance model thanks to 3D Gaussian Splatting. Users then select regions of interest guided by semantically-aware features. To create a generative primitive, we adapt Generative Cellular Automata to single-exemplar training and controllable generation. We decouple the generative task from the appearance model by operating on sparse voxels and we recover a high-quality output with a subsequent sparse patch consistency step. Each primitive can be trained within 10 minutes and used to author new scenes interactively in a fully compositional manner. We showcase interactive sessions where various primitives are extracted from real-world scenes and controlled to create 3D assets and scenes in a few minutes. We also demonstrate additional capabilities of our primitives: handling various 3D representations to control generation, transferring appearances, and editing geometries.",
    "arxiv_url": "http://arxiv.org/abs/2412.16253v1",
    "pdf_url": "http://arxiv.org/pdf/2412.16253v1",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated Labeling for Large-Scale Driving Scene",
    "authors": [
      "Yixiong Huo",
      "Guangfeng Jiang",
      "Hongyang Wei",
      "Ji Liu",
      "Song Zhang",
      "Han Liu",
      "Xingliang Huang",
      "Mingjie Lu",
      "Jinzhang Peng",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "3D Gaussian Splatting (3D GS) has gained popularity due to its faster rendering speed and high-quality novel view synthesis. Some researchers have explored using 3D GS for reconstructing driving scenes. However, these methods often rely on various data types, such as depth maps, 3D boxes, and trajectories of moving objects. Additionally, the lack of annotations for synthesized images limits their direct application in downstream tasks. To address these issues, we propose EGSRAL, a 3D GS-based method that relies solely on training images without extra annotations. EGSRAL enhances 3D GS's capability to model both dynamic objects and static backgrounds and introduces a novel adaptor for auto labeling, generating corresponding annotations based on existing annotations. We also propose a grouping strategy for vanilla 3D GS to address perspective issues in rendering large-scale, complex scenes. Our method achieves state-of-the-art performance on multiple datasets without any extra annotation. For example, the PSNR metric reaches 29.04 on the nuScenes dataset. Moreover, our automated labeling can significantly improve the performance of 2D/3D detection tasks. Code is available at https://github.com/jiangxb98/EGSRAL.",
    "arxiv_url": "http://arxiv.org/abs/2412.15550v1",
    "pdf_url": "http://arxiv.org/pdf/2412.15550v1",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jiangxb98/EGSRAL",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction",
    "authors": [
      "Pou-Chun Kung",
      "Xianling Zhang",
      "Katherine A. Skinner",
      "Nikita Jaipuria"
    ],
    "abstract": "Photorealistic 3D scene reconstruction plays an important role in autonomous driving, enabling the generation of novel data from existing datasets to simulate safety-critical scenarios and expand training data without additional acquisition costs. Gaussian Splatting (GS) facilitates real-time, photorealistic rendering with an explicit 3D Gaussian representation of the scene, providing faster processing and more intuitive scene editing than the implicit Neural Radiance Fields (NeRFs). While extensive GS research has yielded promising advancements in autonomous driving applications, they overlook two critical aspects: First, existing methods mainly focus on low-speed and feature-rich urban scenes and ignore the fact that highway scenarios play a significant role in autonomous driving. Second, while LiDARs are commonplace in autonomous driving platforms, existing methods learn primarily from images and use LiDAR only for initial estimates or without precise sensor modeling, thus missing out on leveraging the rich depth information LiDAR offers and limiting the ability to synthesize LiDAR data. In this paper, we propose a novel GS method for dynamic scene synthesis and editing with improved scene reconstruction through LiDAR supervision and support for LiDAR rendering. Unlike prior works that are tested mostly on urban datasets, to the best of our knowledge, we are the first to focus on the more challenging and highly relevant highway scenes for autonomous driving, with sparse sensor views and monotone backgrounds. Visit our project page at: https://umautobots.github.io/lihi_gs",
    "arxiv_url": "http://arxiv.org/abs/2412.15447v2",
    "pdf_url": "http://arxiv.org/pdf/2412.15447v2",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction",
    "authors": [
      "Zhuowen Shen",
      "Yuan Liu",
      "Zhang Chen",
      "Zhong Li",
      "Jiepeng Wang",
      "Yongqing Liang",
      "Zhengming Yu",
      "Jingdong Zhang",
      "Yi Xu",
      "Scott Schaefer",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "Gaussian splatting has achieved impressive improvements for both novel-view synthesis and surface reconstruction from multi-view images. However, current methods still struggle to reconstruct high-quality surfaces from only sparse view input images using Gaussian splatting. In this paper, we propose a novel method called SolidGS to address this problem. We observed that the reconstructed geometry can be severely inconsistent across multi-views, due to the property of Gaussian function in geometry rendering. This motivates us to consolidate all Gaussians by adopting a more solid kernel function, which effectively improves the surface reconstruction quality. With the additional help of geometrical regularization and monocular normal estimation, our method achieves superior performance on the sparse view surface reconstruction than all the Gaussian splatting methods and neural field methods on the widely used DTU, Tanks-and-Temples, and LLFF datasets.",
    "arxiv_url": "http://arxiv.org/abs/2412.15400v1",
    "pdf_url": "http://arxiv.org/pdf/2412.15400v1",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars",
    "authors": [
      "Forrest Iandola",
      "Stanislav Pidhorskyi",
      "Igor Santesteban",
      "Divam Gupta",
      "Anuj Pahuja",
      "Nemanja Bartolovic",
      "Frank Yu",
      "Emanuel Garbin",
      "Tomas Simon",
      "Shunsuke Saito"
    ],
    "abstract": "Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset.",
    "arxiv_url": "http://arxiv.org/abs/2412.15171v3",
    "pdf_url": "http://arxiv.org/pdf/2412.15171v3",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "head",
      "high-fidelity",
      "vr",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination",
    "authors": [
      "Leonardo Barcellona",
      "Andrii Zadaianchuk",
      "Davide Allegro",
      "Samuele Papa",
      "Stefano Ghidoni",
      "Efstratios Gavves"
    ],
    "abstract": "A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world robotics applications. To overcome those challenges, we propose to rethink robot world models as learnable digital twins. We introduce DreMa, a new approach for constructing digital twins automatically using learned explicit representations of the real world and its dynamics, bridging the gap between traditional digital twins and world models. DreMa replicates the observed world and its structure by integrating Gaussian Splatting and physics simulators, allowing robots to imagine novel configurations of objects and to predict the future consequences of robot actions thanks to its compositionality. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page can be found in: https://dreamtomanipulate.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2412.14957v2",
    "pdf_url": "http://arxiv.org/pdf/2412.14957v2",
    "published_date": "2024-12-19",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting",
    "authors": [
      "Qianpu Sun",
      "Changyong Shu",
      "Sifan Zhou",
      "Zichen Yu",
      "Yan Chen",
      "Dawei Yang",
      "Yuan Chun"
    ],
    "abstract": "3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.",
    "arxiv_url": "http://arxiv.org/abs/2412.14579v1",
    "pdf_url": "http://arxiv.org/pdf/2412.14579v1",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation",
    "authors": [
      "Yongsung Kim",
      "Minjun Park",
      "Jooyoung Choi",
      "Sungroh Yoon"
    ],
    "abstract": "Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.",
    "arxiv_url": "http://arxiv.org/abs/2412.14568v1",
    "pdf_url": "http://arxiv.org/pdf/2412.14568v1",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians",
    "authors": [
      "Xiaobao Wei",
      "Peng Chen",
      "Ming Lu",
      "Hui Chen",
      "Feng Tian"
    ],
    "abstract": "Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: https://github.com/ucwxb/GraphAvatar",
    "arxiv_url": "http://arxiv.org/abs/2412.13983v1",
    "pdf_url": "http://arxiv.org/pdf/2412.13983v1",
    "published_date": "2024-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ucwxb/GraphAvatar",
    "keywords": [
      "head",
      "tracking",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting",
    "authors": [
      "Yuning Peng",
      "Haiping Wang",
      "Yuan Liu",
      "Chenglu Wen",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "abstract": "3D open-vocabulary scene understanding, which accurately perceives complex semantic properties of objects in space, has gained significant attention in recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP features into 3D Gaussian splatting, enabling open-vocabulary queries for renderings on arbitrary viewpoints. The main challenge of distilling 2D features for 3D fields lies in the multiview inconsistency of extracted 2D features, which provides unstable supervision for the 3D feature field. GAGS addresses this challenge with two novel strategies. First, GAGS associates the prompt point density of SAM with the camera distances, which significantly improves the multiview consistency of segmentation results. Second, GAGS further decodes a granularity factor to guide the distillation process and this granularity factor can be learned in a unsupervised manner to only select the multiview consistent 2D features in the distillation process. Experimental results on two datasets demonstrate significant performance and stability improvements of GAGS in visual grounding and semantic segmentation, with an inference speed 2$\\times$ faster than baseline methods. The code and additional results are available at https://pz0826.github.io/GAGS-Webpage/ .",
    "arxiv_url": "http://arxiv.org/abs/2412.13654v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13654v2",
    "published_date": "2024-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Radar-Inertial Odometry based on Gaussian Modeling and Multi-Hypothesis Scan Matching",
    "authors": [
      "Fernando Amodeo",
      "Luis Merino",
      "Fernando Caballero"
    ],
    "abstract": "4D millimeter-wave (mmWave) radars are sensors that provide robustness against adverse weather conditions (rain, snow, fog, etc.), and as such they are increasingly being used for odometry and SLAM applications. However, the noisy and sparse nature of the returned scan data proves to be a challenging obstacle for existing point cloud matching based solutions, especially those originally intended for more accurate sensors such as LiDAR. Inspired by visual odometry research around 3D Gaussian Splatting, in this paper we propose using freely positioned 3D Gaussians to create a summarized representation of a radar point cloud tolerant to sensor noise, and subsequently leverage its inherent probability distribution function for registration (similar to NDT). Moreover, we propose simultaneously optimizing multiple scan matching hypotheses in order to further increase the robustness of the system against local optima of the function. Finally, we fuse our Gaussian modeling and scan matching algorithms into an EKF radar-inertial odometry system designed after current best practices. Experiments using publicly available 4D radar datasets show that our Gaussian-based odometry is comparable to existing registration algorithms, outperforming them in several sequences.",
    "arxiv_url": "http://arxiv.org/abs/2412.13639v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13639v2",
    "published_date": "2024-12-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields",
    "authors": [
      "Tao Lu",
      "Ankit Dhiman",
      "R Srinath",
      "Emre Arslan",
      "Angela Xing",
      "Yuanbo Xiangli",
      "R Venkatesh Babu",
      "Srinath Sridhar"
    ],
    "abstract": "Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.",
    "arxiv_url": "http://arxiv.org/abs/2412.13547v1",
    "pdf_url": "http://arxiv.org/pdf/2412.13547v1",
    "published_date": "2024-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visualizing the Invisible: A Generative AR System for Intuitive Multi-Modal Sensor Data Presentation",
    "authors": [
      "Yunqi Guo",
      "Kaiyuan Hou",
      "Heming Fu",
      "Hongkai Chen",
      "Zhenyu Yan",
      "Guoliang Xing",
      "Xiaofan Jiang"
    ],
    "abstract": "Understanding sensor data can be difficult for non-experts because of the complexity and different semantic meanings of sensor modalities. This leads to a need for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we propose Vivar, a novel system that integrates multi-modal sensor data and presents 3D volumetric content for AR visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This approach accurately reflects value changes in multi-modal sensor information, ensuring that sensor variations are properly shown in visualization outcomes. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation, demonstrating 11x latency reduction without compromising quality. A user study involving over 503 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.",
    "arxiv_url": "http://arxiv.org/abs/2412.13509v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13509v2",
    "published_date": "2024-12-18",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures",
    "authors": [
      "Guoxing Sun",
      "Rishabh Dabral",
      "Heming Zhu",
      "Pascal Fua",
      "Christian Theobalt",
      "Marc Habermann"
    ],
    "abstract": "Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods. Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/",
    "arxiv_url": "http://arxiv.org/abs/2412.13183v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13183v2",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "deformation",
      "body",
      "geometry",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle Adjustment",
    "authors": [
      "Andrea Dunn Beltran",
      "Daniel Rho",
      "Stephen Pizer",
      "Marc Niethammer",
      "Roni Sengupta"
    ],
    "abstract": "Simultaneous Localization And Mapping (SLAM) from endoscopy videos can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Existing dense SLAM algorithms often assume distant and static lighting and optimize scene geometry and camera parameters by minimizing a photometric rendering loss, often called Photometric Bundle Adjustment. However, endoscopy videos exhibit dynamic near-field lighting due to the co-located light and camera moving extremely close to the surface. In addition, low texture surfaces in endoscopy videos cause photometric bundle adjustment of the existing SLAM frameworks to perform poorly compared to indoor/outdoor scenes. To mitigate this problem, we introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for low texture surfaces. Our proposed NFL-BA can be applied to any neural-rendering based SLAM framework. We show that by replacing traditional photometric bundle adjustment loss with our proposed NFL-BA results in improvement, using neural implicit SLAM and 3DGS SLAMs. In addition to producing state-of-the-art tracking and mapping results on colonoscopy C3VD dataset we also show improvement on real colonoscopy videos. See results at https://asdunnbe.github.io/NFL-BA/",
    "arxiv_url": "http://arxiv.org/abs/2412.13176v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13176v2",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "outdoor",
      "survey",
      "lighting",
      "face",
      "mapping",
      "geometry",
      "slam",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting for Efficient Satellite Image Photogrammetry",
    "authors": [
      "Luca Savant Aira",
      "Gabriele Facciolo",
      "Thibaud Ehret"
    ],
    "abstract": "Recently, Gaussian splatting has emerged as a strong alternative to NeRF, demonstrating impressive 3D modeling capabilities while requiring only a fraction of the training and rendering time. In this paper, we show how the standard Gaussian splatting framework can be adapted for remote sensing, retaining its high efficiency. This enables us to achieve state-of-the-art performance in just a few minutes, compared to the day-long optimization required by the best-performing NeRF-based Earth observation methods. The proposed framework incorporates remote-sensing improvements from EO-NeRF, such as radiometric correction and shadow modeling, while introducing novel components, including sparsity, view consistency, and opacity regularizations.",
    "arxiv_url": "http://arxiv.org/abs/2412.13047v2",
    "pdf_url": "http://arxiv.org/pdf/2412.13047v2",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "shadow",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel Reconstruction from Sparse-View Dynamic DSA Images",
    "authors": [
      "Zhentao Liu",
      "Ruyi Zha",
      "Huangxuan Zhao",
      "Hongdong Li",
      "Zhiming Cui"
    ],
    "abstract": "Reconstructing 3D vessel structures from sparse-view dynamic digital subtraction angiography (DSA) images enables accurate medical assessment while reducing radiation exposure. Existing methods often produce suboptimal results or require excessive computation time. In this work, we propose 4D radiative Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently. In detail, we represent the vessels with 4D radiative Gaussian kernels. Each kernel has time-invariant geometry parameters, including position, rotation, and scale, to model static vessel structures. The time-dependent central attenuation of each kernel is predicted from a compact neural network to capture the temporal varying response of contrast agent flow. We splat these Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize the model with real captured ones. The final 3D vessel volume is voxelized from the well-trained kernels. Moreover, we introduce accumulated attenuation pruning and bounded scaling activation to improve reconstruction quality. Extensive experiments on real-world patient data demonstrate that 4DRGS achieves impressive results in 5 minutes training, which is 32x faster than the state-of-the-art method. This underscores the potential of 4DRGS for real-world clinics.",
    "arxiv_url": "http://arxiv.org/abs/2412.12919v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12919v2",
    "published_date": "2024-12-17",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "geometry",
      "4d",
      "medical",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image",
    "authors": [
      "Wonseok Roh",
      "Hwanhee Jung",
      "Jong Wook Kim",
      "Seunggwan Lee",
      "Innfarn Yoo",
      "Andreas Lugmayr",
      "Seunggeun Chi",
      "Karthik Ramani",
      "Sangpil Kim"
    ],
    "abstract": "Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2412.12906v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12906v2",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyperGS: Hyperspectral 3D Gaussian Splatting",
    "authors": [
      "Christopher Thirgood",
      "Oscar Mendez",
      "Erin Chao Ling",
      "Jon Storey",
      "Simon Hadfield"
    ],
    "abstract": "We introduce HyperGS, a novel framework for Hyperspectral Novel View Synthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique. Our approach enables simultaneous spatial and spectral renderings by encoding material properties from multi-view 3D hyperspectral datasets. HyperGS reconstructs high-fidelity views from arbitrary perspectives with improved accuracy and speed, outperforming currently existing methods. To address the challenges of high-dimensional data, we perform view synthesis in a learned latent space, incorporating a pixel-wise adaptive density function and a pruning technique for increased training stability and efficiency. Additionally, we introduce the first HNVS benchmark, implementing a number of new baselines based on recent SOTA RGB-NVS techniques, alongside the small number of prior works on HNVS. We demonstrate HyperGS's robustness through extensive evaluation of real and simulated hyperspectral scenes with a 14db accuracy improvement upon previously published models.",
    "arxiv_url": "http://arxiv.org/abs/2412.12849v1",
    "pdf_url": "http://arxiv.org/pdf/2412.12849v1",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures",
    "authors": [
      "Sebastian Weiss",
      "Derek Bradley"
    ],
    "abstract": "Gaussian Splatting has recently emerged as the go-to representation for reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian primitives has further improved multi-view consistency and surface reconstruction accuracy. In this work we highlight the similarity between 2D Gaussian Splatting (2DGS) and billboards from traditional computer graphics. Both use flat semi-transparent 2D geometry that is positioned, oriented and scaled in 3D space. However 2DGS uses a solid color per splat and an opacity modulated by a Gaussian distribution, where billboards are more expressive, modulating the color with a uv-parameterized texture. We propose to unify these concepts by presenting Gaussian Billboards, a modification of 2DGS to add spatially-varying color achieved using per-splat texture interpolation. The result is a mixture of the two representations, which benefits from both the robust scene optimization power of 2DGS and the expressiveness of texture mapping. We show that our method can improve the sharpness and quality of the scene representation in a wide range of qualitative and quantitative evaluations compared to the original 2DGS implementation.",
    "arxiv_url": "http://arxiv.org/abs/2412.12734v1",
    "pdf_url": "http://arxiv.org/pdf/2412.12734v1",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "mapping",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
    "authors": [
      "Qi Wu",
      "Janick Martinez Esturo",
      "Ashkan Mirzaei",
      "Nicolas Moenne-Loccoz",
      "Zan Gojcic"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing the particles instead, but, this comes at the cost of significantly slower rendering. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation. The source code is available at: https://github.com/nv-tlabs/3dgrut.",
    "arxiv_url": "http://arxiv.org/abs/2412.12507v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12507v2",
    "published_date": "2024-12-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "https://github.com/nv-tlabs/3dgrut",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ray tracing",
      "reflection",
      "lighting",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting",
    "authors": [
      "Cheng Zhang",
      "Haofei Xu",
      "Qianyi Wu",
      "Camilo Cruz Gambardella",
      "Dinh Phung",
      "Jianfei Cai"
    ],
    "abstract": "With the advent of portable 360{\\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 $\\times$ 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets. Code is available at https://github.com/chengzhag/PanSplat.",
    "arxiv_url": "http://arxiv.org/abs/2412.12096v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12096v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chengzhag/PanSplat",
    "keywords": [
      "robotics",
      "efficient",
      "head",
      "vr",
      "autonomous driving",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wonderland: Navigating 3D Scenes from a Single Image",
    "authors": [
      "Hanwen Liang",
      "Junli Cao",
      "Vidit Goel",
      "Guocheng Qian",
      "Sergei Korolev",
      "Demetri Terzopoulos",
      "Konstantinos N. Plataniotis",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "How can one efficiently generate high-quality, wide-scope 3D scenes from arbitrary single images? Existing methods suffer several drawbacks, such as requiring multi-view data, time-consuming per-scene optimization, distorted geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D scene reconstruction pipeline overcomes these limitations to tackle the aforesaid challenge. Specifically, we introduce a large-scale reconstruction model that leverages latents from a video diffusion model to predict 3D Gaussian Splattings of scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that encode multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive learning strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets affirm that our model significantly outperforms existing single-view 3D scene generation methods, especially with out-of-domain images. Thus, we demonstrate for the first time that a 3D reconstruction model can effectively be built upon the latent space of a diffusion model in order to realize efficient 3D scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2412.12091v2",
    "pdf_url": "http://arxiv.org/pdf/2412.12091v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-ProCams: Gaussian Splatting-based Projector-Camera Systems",
    "authors": [
      "Qingyue Deng",
      "Jijiang Li",
      "Haibin Ling",
      "Bingyao Huang"
    ],
    "abstract": "We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GS-ProCams significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional colocated light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the target surface's geometry and materials represented by Gaussians, and global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It is also 600 times faster and uses only 1/10 of the GPU memory.",
    "arxiv_url": "http://arxiv.org/abs/2412.11762v1",
    "pdf_url": "http://arxiv.org/pdf/2412.11762v1",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "efficient",
      "global illumination",
      "fast",
      "face",
      "mapping",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable Radial Kernel Splatting",
    "authors": [
      "Yi-Hua Huang",
      "Ming-Xian Lin",
      "Yang-Tian Sun",
      "Ziyi Yang",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ],
    "abstract": "Recently, Gaussian splatting has emerged as a robust technique for representing 3D scenes, enabling real-time rasterization and high-fidelity rendering. However, Gaussians' inherent radial symmetry and smoothness constraints limit their ability to represent complex shapes, often requiring thousands of primitives to approximate detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature. iven DRK's planar nature, we further develop accurate ray-primitive intersection computation for depth sorting and introduce efficient kernel culling strategies for improved rasterization efficiency. Extensive experiments demonstrate that DRK outperforms existing methods in both representation efficiency and rendering quality, achieving state-of-the-art performance while dramatically reducing primitive count.",
    "arxiv_url": "http://arxiv.org/abs/2412.11752v2",
    "pdf_url": "http://arxiv.org/pdf/2412.11752v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro Radiance Field Rendering from a Single Sweep",
    "authors": [
      "Jingqian Wu",
      "Shuo Zhu",
      "Chutian Wang",
      "Boxin Shi",
      "Edmund Y. Lam"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the potential of using 3D Gaussian primitives for high-speed, high-fidelity, and cost-efficient novel view synthesis from continuously calibrated input views. However, conventional methods require high-frame-rate dense and high-quality sharp images, which are time-consuming and inefficient to capture, especially in dynamic environments. Event cameras, with their high temporal resolution and ability to capture asynchronous brightness changes, offer a promising alternative for more reliable scene reconstruction without motion blur. In this paper, we propose SweepEvGS, a novel hardware-integrated method that leverages event cameras for robust and accurate novel view synthesis across various imaging settings from a single sweep. SweepEvGS utilizes the initial static frame with dense event streams captured during a single camera sweep to effectively reconstruct detailed scene views. We also introduce different real-world hardware imaging systems for real-world data collection and evaluation for future research. We validate the robustness and efficiency of SweepEvGS through experiments in three different imaging settings: synthetic objects, real-world macro-level, and real-world micro-level view synthesis. Our results demonstrate that SweepEvGS surpasses existing methods in visual rendering quality, rendering speed, and computational efficiency, highlighting its potential for dynamic practical applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.11579v1",
    "pdf_url": "http://arxiv.org/pdf/2412.11579v1",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "lighting",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting",
    "authors": [
      "Dong In Lee",
      "Hyeongcheol Park",
      "Jiyoung Seo",
      "Eunbyung Park",
      "Hyunje Park",
      "Ha Dam Baek",
      "Sangheon Shin",
      "Sangmin Kim",
      "Sangpil Kim"
    ],
    "abstract": "Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR/VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel text-driven 3D scene editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric structure inherent to 3DGS. Additionally, our AGT utilizes the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local editing. Through extensive qualitative and quantitative evaluations, EditSplat achieves state-of-the-art performance, establishing a new benchmark for text-driven 3D scene editing.",
    "arxiv_url": "http://arxiv.org/abs/2412.11520v2",
    "pdf_url": "http://arxiv.org/pdf/2412.11520v2",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
    "authors": [
      "Xinli Xu",
      "Wenhang Ge",
      "Dicong Qiu",
      "ZhiFei Chen",
      "Dongyu Yan",
      "Zhuoyun Liu",
      "Haoyu Zhao",
      "Hanfeng Zhao",
      "Shunsi Zhang",
      "Junwei Liang",
      "Ying-Cong Chen"
    ],
    "abstract": "Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on \\href{https://Gaussian-Property.github.io}{this https URL}.",
    "arxiv_url": "http://arxiv.org/abs/2412.11258v1",
    "pdf_url": "http://arxiv.org/pdf/2412.11258v1",
    "published_date": "2024-12-15",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "robotics",
      "lighting",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DCSEG: Decoupled 3D Open-Set Segmentation using Gaussian Splatting",
    "authors": [
      "Luis Wiedmann",
      "Luca Wiehe",
      "David Rozenberszki"
    ],
    "abstract": "Open-set 3D segmentation represents a major point of interest for multiple downstream robotics and augmented/virtual reality applications. We present a decoupled 3D segmentation pipeline to ensure modularity and adaptability to novel 3D representations as well as semantic segmentation foundation models. We first reconstruct a scene with 3D Gaussians and learn class-agnostic features through contrastive supervision from a 2D instance proposal network. These 3D features are then clustered to form coarse object- or part-level masks. Finally, we match each 3D cluster to class-aware masks predicted by a 2D open-vocabulary segmentation model, assigning semantic labels without retraining the 3D representation. Our decoupled design (1) provides a plug-and-play interface for swapping different 2D or 3D modules, (2) ensures multi-object instance segmentation at no extra cost, and (3) leverages rich 3D geometry for robust scene understanding. We evaluate on synthetic and real-world indoor datasets, demonstrating improved performance over comparable NeRF-based pipelines on mIoU and mAcc, particularly for challenging or long-tail classes. We also show how varying the 2D backbone affects the final segmentation, highlighting the modularity of our framework. These results confirm that decoupling 3D mask proposal and semantic classification can deliver flexible, efficient, and open-vocabulary 3D segmentation.",
    "arxiv_url": "http://arxiv.org/abs/2412.10972v2",
    "pdf_url": "http://arxiv.org/pdf/2412.10972v2",
    "published_date": "2024-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "lighting",
      "face",
      "semantic",
      "understanding",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction",
    "authors": [
      "Sicheng Zuo",
      "Wenzhao Zheng",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction is important for autonomous driving due to its comprehensive perception of the surroundings. To incorporate sequential inputs, most existing methods fuse representations from previous frames to infer the current 3D occupancy. However, they fail to consider the continuity of driving scenarios and ignore the strong prior provided by the evolution of 3D scenes (e.g., only dynamic objects move). In this paper, we propose a world-model-based framework to exploit the scene evolution for perception. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors: 1) ego motion alignment of static scenes; 2) local movements of dynamic objects; and 3) completion of newly-observed scenes. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit these priors and infer the scene evolution in the 3D Gaussian space considering the current RGB observation. We evaluate the effectiveness of our framework on the widely used nuScenes dataset. Our GaussianWorld improves the performance of the single-frame counterpart by over 2% in mIoU without introducing additional computations. Code: https://github.com/zuosc19/GaussianWorld.",
    "arxiv_url": "http://arxiv.org/abs/2412.10373v1",
    "pdf_url": "http://arxiv.org/pdf/2412.10373v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/zuosc19/GaussianWorld",
    "keywords": [
      "autonomous driving",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianAD: Gaussian-Centric End-to-End Autonomous Driving",
    "authors": [
      "Wenzhao Zheng",
      "Junjie Wu",
      "Yao Zheng",
      "Sicheng Zuo",
      "Zixun Xie",
      "Longchao Yang",
      "Yong Pan",
      "Zhihui Hao",
      "Peng Jia",
      "Xianpeng Lang",
      "Shanghang Zhang"
    ],
    "abstract": "Vision-based autonomous driving shows great potential due to its satisfactory performance and low costs. Most existing methods adopt dense representations (e.g., bird's eye view) or sparse representations (e.g., instance boxes) for decision-making, which suffer from the trade-off between comprehensiveness and efficiency. This paper explores a Gaussian-centric end-to-end autonomous driving (GaussianAD) framework and exploits 3D semantic Gaussians to extensively yet sparsely describe the scene. We initialize the scene with uniform 3D Gaussians and use surrounding-view images to progressively refine them to obtain the 3D Gaussian scene representation. We then use sparse convolutions to efficiently perform 3D perception (e.g., 3D detection, semantic map construction). We predict 3D flows for the Gaussians with dynamic semantics and plan the ego trajectory accordingly with an objective of future scene forecasting. Our GaussianAD can be trained in an end-to-end manner with optional perception labels when available. Extensive experiments on the widely used nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on various tasks including motion planning, 3D occupancy prediction, and 4D occupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.",
    "arxiv_url": "http://arxiv.org/abs/2412.10371v1",
    "pdf_url": "http://arxiv.org/pdf/2412.10371v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "https://github.com/wzzheng/GaussianAD",
    "keywords": [
      "efficient",
      "autonomous driving",
      "motion",
      "semantic",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians",
    "authors": [
      "Siyun Liang",
      "Sen Wang",
      "Kunyi Li",
      "Michael Niemeyer",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.",
    "arxiv_url": "http://arxiv.org/abs/2412.10231v1",
    "pdf_url": "http://arxiv.org/pdf/2412.10231v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
    "authors": [
      "Jiapeng Tang",
      "Davide Davoli",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Matthias Niessner"
    ],
    "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.",
    "arxiv_url": "http://arxiv.org/abs/2412.10209v2",
    "pdf_url": "http://arxiv.org/pdf/2412.10209v2",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views",
    "authors": [
      "Liang Zhao",
      "Zehan Bao",
      "Yi Xie",
      "Hong Chen",
      "Yaohui Chen",
      "Weifu Li"
    ],
    "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field, achieving both panoptic and interactive segmentation of 3D scenes. However, existing methodologies often overlook the critical need for reconstructing specified targets with complex structures from sparse views. To address this issue, we introduce TSGaussian, a novel framework that combines semantic constraints with depth priors to avoid geometry degradation in challenging novel view synthesis tasks. Our approach prioritizes computational resources on designated targets while minimizing background allocation. Bounding boxes from YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask predictions, ensuring semantic accuracy and cost efficiency. TSGaussian effectively clusters 3D gaussians by introducing a compact identity encoding for each Gaussian ellipsoid and incorporating 3D spatial consistency regularization. Leveraging these modules, we propose a pruning strategy to effectively reduce redundancy in 3D gaussians. Extensive experiments demonstrate that TSGaussian outperforms state-of-the-art methods on three standard datasets and a new challenging dataset we collected, achieving superior results in novel view synthesis of specific objects. Code is available at: https://github.com/leon2000-ai/TSGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2412.10051v1",
    "pdf_url": "http://arxiv.org/pdf/2412.10051v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/leon2000-ai/TSGaussian",
    "keywords": [
      "sparse view",
      "semantic",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2412.09982v2",
    "pdf_url": "http://arxiv.org/pdf/2412.09982v2",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian Splatting",
    "authors": [
      "Lizhi Bai",
      "Chunqi Tian",
      "Jun Yang",
      "Siyu Zhang",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a promising technique for high-quality 3D rendering, leading to increasing interest in integrating 3DGS into realism SLAM systems. However, existing methods face challenges such as Gaussian primitives redundancy, forgetting problem during continuous optimization, and difficulty in initializing primitives in monocular case due to lack of depth information. In order to achieve efficient and photorealistic mapping, we propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian primitives optimization and consists of three key components. Firstly, we propose an efficient incremental mapping approach to achieve a compact and accurate representation of the scene through adaptive sampling and Gaussian primitives filtering. Secondly, a dynamic window optimization method is proposed to mitigate the forgetting problem and improve map consistency. Finally, for the monocular case, a monocular keyframe initialization method based on sparse point cloud is proposed to improve the initialization accuracy of Gaussian primitives, which provides a geometric basis for subsequent optimization. The results of numerous experiments demonstrate that RP-SLAM achieves state-of-the-art map rendering accuracy while ensuring real-time performance and model compactness.",
    "arxiv_url": "http://arxiv.org/abs/2412.09868v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09868v1",
    "published_date": "2024-12-13",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
    "authors": [
      "Xiaohao Xu",
      "Feng Xue",
      "Shibo Zhao",
      "Yike Pan",
      "Sebastian Scherer",
      "Xiaonan Huang"
    ],
    "abstract": "Real-time multi-agent collaboration for ego-motion estimation and high-fidelity 3D reconstruction is vital for scalable spatial intelligence. However, traditional methods produce sparse, low-detail maps, while recent dense mapping approaches struggle with high latency. To overcome these challenges, we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. Through Intra-Agent Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian splats within an agent. For global alignment, parallelized Inter-Agent Gaussian Consensus, which asynchronously aligns and optimizes local maps by regularizing multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D rendering, enabling rapid inter-agent Gaussian association and alignment. MAC-Ego3D bridges local precision and global coherence, delivering higher efficiency, largely reducing localization error, and improving mapping fidelity. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15x increase in inference speed, order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 dB. Our code will be made publicly available at https://github.com/Xiaohao-Xu/MAC-Ego3D .",
    "arxiv_url": "http://arxiv.org/abs/2412.09723v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09723v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Xiaohao-Xu/MAC-Ego3D",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "motion",
      "mapping",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields",
    "authors": [
      "Sean Wu",
      "Shamik Basu",
      "Tim Broedermann",
      "Luc Van Gool",
      "Christos Sakaridis"
    ],
    "abstract": "We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at https://github.com/s3anwu/pbrnerf",
    "arxiv_url": "http://arxiv.org/abs/2412.09680v2",
    "pdf_url": "http://arxiv.org/pdf/2412.09680v2",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/s3anwu/pbrnerf",
    "keywords": [
      "illumination",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Representing Long Volumetric Video with Temporal Gaussian Hierarchy",
    "authors": [
      "Zhen Xu",
      "Yinghao Xu",
      "Zhiyuan Yu",
      "Sida Peng",
      "Jiaming Sun",
      "Hujun Bao",
      "Xiaowei Zhou"
    ],
    "abstract": "This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1~2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling minutes of volumetric video data while maintaining state-of-the-art rendering quality. Our project page is available at: https://zju3dv.github.io/longvolcap.",
    "arxiv_url": "http://arxiv.org/abs/2412.09608v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09608v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting",
    "authors": [
      "Yue Chen",
      "Xingyu Chen",
      "Anpei Chen",
      "Gerard Pons-Moll",
      "Yuliang Xiu"
    ],
    "abstract": "Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry ($\\boldsymbol{x}, \\alpha, \\Sigma$) and texture ($\\boldsymbol{c}$) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis. Code and data will be made available at https://fanegg.github.io/Feat2GS/.",
    "arxiv_url": "http://arxiv.org/abs/2412.09606v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09606v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors",
    "authors": [
      "Yabo Chen",
      "Chen Yang",
      "Jiemin Fang",
      "Xiaopeng Zhang",
      "Lingxi Xie",
      "Wei Shen",
      "Wenrui Dai",
      "Hongkai Xiong",
      "Qi Tian"
    ],
    "abstract": "Single-image 3D reconstruction remains a fundamental challenge in computer vision due to inherent geometric ambiguities and limited viewpoint information. Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D priors learned from large-scale video data. However, leveraging these priors effectively faces three key challenges: (1) degradation in quality across large camera motions, (2) difficulties in achieving precise camera control, and (3) geometric distortions inherent to the diffusion process that damage 3D consistency. We address these challenges by proposing LiftImage3D, a framework that effectively releases LVDMs' generative priors while ensuring 3D consistency. Specifically, we design an articulated trajectory strategy to generate video frames, which decomposes video sequences with large camera motions into ones with controllable small motions. Then we use robust neural matching models, i.e. MASt3R, to calibrate the camera poses of generated frames and produce corresponding point clouds. Finally, we propose a distortion-aware 3D Gaussian splatting representation, which can learn independent distortions between frames and output undistorted canonical Gaussians. Extensive experiments demonstrate that LiftImage3D achieves state-of-the-art performance on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and generalizes well to diverse in-the-wild images, from cartoon illustrations to complex real-world scenes.",
    "arxiv_url": "http://arxiv.org/abs/2412.09597v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09597v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction",
    "authors": [
      "Jiale Xu",
      "Shenghua Gao",
      "Ying Shan"
    ],
    "abstract": "Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable of generating high-quality 3D Gaussians from uncalibrated sparse-view images and recovering their camera parameters in mere seconds. FreeSplatter is built upon a streamlined transformer architecture, comprising sequential self-attention blocks that facilitate information exchange among multi-view image tokens and decode them into pixel-wise 3D Gaussian primitives. The predicted Gaussian primitives are situated in a unified reference frame, allowing for high-fidelity 3D modeling and instant camera parameter estimation using off-the-shelf solvers. To cater to both object-centric and scene-level reconstruction, we train two model variants of FreeSplatter on extensive datasets. In both scenarios, FreeSplatter outperforms state-of-the-art baselines in terms of reconstruction quality and pose estimation accuracy. Furthermore, we showcase FreeSplatter's potential in enhancing the productivity of downstream applications, such as text/image-to-3D content creation.",
    "arxiv_url": "http://arxiv.org/abs/2412.09573v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09573v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing",
    "authors": [
      "Xueting Li",
      "Ye Yuan",
      "Shalini De Mello",
      "Gilles Daviet",
      "Jonathan Leaf",
      "Miles Macklin",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.",
    "arxiv_url": "http://arxiv.org/abs/2412.09545v2",
    "pdf_url": "http://arxiv.org/pdf/2412.09545v2",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency",
    "authors": [
      "Dongyue Lu",
      "Lingdong Kong",
      "Tianxin Huang",
      "Gim Hee Lee"
    ],
    "abstract": "Identifying affordance regions on 3D objects from semantic cues is essential for robotics and human-machine interaction. However, existing 3D affordance learning methods struggle with generalization and robustness due to limited annotated data and a reliance on 3D backbones focused on geometric encoding, which often lack resilience to real-world noise and data corruption. We propose GEAL, a novel framework designed to enhance the generalization and robustness of 3D affordance learning by leveraging large-scale pre-trained 2D models. We employ a dual-branch architecture with Gaussian splatting to establish consistent mappings between 3D point clouds and 2D representations, enabling realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion module and a 2D-3D consistency alignment module further strengthen cross-modal alignment and knowledge transfer, allowing the 3D branch to benefit from the rich semantics and generalization capacity of 2D models. To holistically assess the robustness, we introduce two new corruption-based benchmarks: PIAD-C and LASO-C. Extensive experiments on public datasets and our benchmarks show that GEAL consistently outperforms existing methods across seen and novel object categories, as well as corrupted data, demonstrating robust and adaptable affordance prediction under diverse conditions. Code and corruption datasets have been made publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2412.09511v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09511v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "semantic",
      "mapping",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting",
    "authors": [
      "Haotian Mao",
      "Zhuoxiong Xu",
      "Siyue Wei",
      "Yule Quan",
      "Nianchen Deng",
      "Xubo Yang"
    ],
    "abstract": "Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has shown immense potential in VR content creation due to its high-quality rendering and efficient production process. However, existing physics-based interaction systems for 3DGS can only perform simple and non-realistic simulations or demand extensive user input for complex scenes, primarily due to the absence of scene understanding. In this paper, we propose LIVE-GS, a highly realistic interactive VR system powered by LLM. After object-aware GS reconstruction, we prompt GPT-4o to analyze the physical properties of objects in the scene, which are used to guide physical simulations consistent with real phenomena. We also design a GPT-assisted GS inpainting module to fill the unseen area covered by manipulative objects. To perform a precise segmentation of Gaussian kernels, we propose a feature-mask segmentation strategy. To enable rich interaction, we further propose a computationally efficient physical simulation framework through an PBD-based unified interpolation method, supporting various physical forms such as rigid body, soft body, and granular materials. Our experimental results show that with the help of LLM's understanding and enhancement of scenes, our VR system can support complex and realistic interactions without additional manual design and annotation.",
    "arxiv_url": "http://arxiv.org/abs/2412.09176v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09176v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "body",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving",
    "authors": [
      "Hao Lu",
      "Tianshuo Xu",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Wei Zhan",
      "Dalong Du",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Yingcong Chen"
    ],
    "abstract": "Photorealistic 4D reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. However, most existing methods perform this task offline and rely on time-consuming iterative processes, limiting their practical applications. To this end, we introduce the Large 4D Gaussian Reconstruction Model (DrivingRecon), a generalizable driving scene reconstruction model, which directly predicts 4D Gaussian from surround view videos. To better integrate the surround-view images, the Prune and Dilate Block (PD-Block) is proposed to eliminate overlapping Gaussian points between adjacent views and remove redundant background points. To enhance cross-temporal information, dynamic and static decoupling is tailored to better learn geometry and motion features. Experimental results demonstrate that DrivingRecon significantly improves scene reconstruction quality and novel view synthesis compared to existing methods. Furthermore, we explore applications of DrivingRecon in model pre-training, vehicle adaptation, and scene editing. Our code is available at https://github.com/EnVision-Research/DriveRecon.",
    "arxiv_url": "http://arxiv.org/abs/2412.09043v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09043v1",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/EnVision-Research/DriveRecon",
    "keywords": [
      "autonomous driving",
      "motion",
      "geometry",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis",
    "authors": [
      "Yifan Xie",
      "Tao Feng",
      "Xin Zhang",
      "Xiangyang Luo",
      "Zixuan Guo",
      "Weijiang Yu",
      "Heng Chang",
      "Fei Ma",
      "Fei Richard Yu"
    ],
    "abstract": "Talking head synthesis with arbitrary speech audio is a crucial challenge in the field of digital humans. Recently, methods based on radiance fields have received increasing attention due to their ability to synthesize high-fidelity and identity-consistent talking heads from just a few minutes of training video. However, due to the limited scale of the training data, these methods often exhibit poor performance in audio-lip synchronization and visual quality. In this paper, we propose a novel 3D Gaussian-based method called PointTalk, which constructs a static 3D Gaussian field of the head and deforms it in sync with the audio. It also incorporates an audio-driven dynamic lip point cloud as a critical component of the conditional information, thereby facilitating the effective synthesis of talking heads. Specifically, the initial step involves generating the corresponding lip point cloud from the audio signal and capturing its topological structure. The design of the dynamic difference encoder aims to capture the subtle nuances inherent in dynamic lip movements more effectively. Furthermore, we integrate the audio-point enhancement module, which not only ensures the synchronization of the audio signal with the corresponding lip point cloud within the feature space, but also facilitates a deeper understanding of the interrelations among cross-modal conditional features. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking head synthesis compared to previous methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.08504v1",
    "pdf_url": "http://arxiv.org/pdf/2412.08504v1",
    "published_date": "2024-12-11",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.GR",
      "cs.MM",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "understanding",
      "3d gaussian",
      "human",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views",
    "authors": [
      "Kangjie Chen",
      "BingQuan Dai",
      "Minghan Qin",
      "Dongbin Zhang",
      "Peihao Li",
      "Yingshuang Zou",
      "Haoqian Wang"
    ],
    "abstract": "3D semantic field learning is crucial for applications like autonomous navigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from limited viewpoints is essential. Existing methods struggle under sparse view conditions, relying on inefficient per-scene multi-view optimizations, which are impractical for many real-world tasks. To address this, we propose SLGaussian, a feed-forward method for constructing 3D semantic fields from sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring consistent SAM segmentations through video tracking and using low-dimensional indexing for high-dimensional CLIP features, SLGaussian efficiently embeds language information in 3D space, offering a robust solution for accurate 3D scene understanding under sparse view conditions. In experiments on two-view sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets, SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy, and mIoU. Moreover, our model achieves scene inference in under 30 seconds and open-vocabulary querying in just 0.011 seconds per query.",
    "arxiv_url": "http://arxiv.org/abs/2412.08331v1",
    "pdf_url": "http://arxiv.org/pdf/2412.08331v1",
    "published_date": "2024-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "localization",
      "efficient",
      "robotics",
      "tracking",
      "vr",
      "fast",
      "semantic",
      "understanding",
      "segmentation",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion Models",
    "authors": [
      "Kevin Miao",
      "Harsh Agrawal",
      "Qihang Zhang",
      "Federico Semeraro",
      "Marco Cavallo",
      "Jiatao Gu",
      "Alexander Toshev"
    ],
    "abstract": "Generating high-quality 3D content requires models capable of learning robust distributions of complex scenes and the real-world objects within them. Recent Gaussian-based 3D reconstruction techniques have achieved impressive results in recovering high-fidelity 3D assets from sparse input images by predicting 3D Gaussians in a feed-forward manner. However, these techniques often lack the extensive priors and expressiveness offered by Diffusion Models. On the other hand, 2D Diffusion Models, which have been successfully applied to denoise multiview images, show potential for generating a wide range of photorealistic 3D outputs but still fall short on explicit 3D priors and consistency. In this work, we aim to bridge these two approaches by introducing DSplats, a novel method that directly denoises multiview images using Gaussian Splat-based Reconstructors to produce a diverse array of realistic 3D assets. To harness the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent Diffusion Model into the reconstructor backbone to predict a set of 3D Gaussians. Additionally, the explicit 3D representation embedded in the denoising network provides a strong inductive bias, ensuring geometrically consistent novel view generation. Our qualitative and quantitative experiments demonstrate that DSplats not only produces high-quality, spatially consistent outputs, but also sets a new standard in single-image to 3D reconstruction. When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.",
    "arxiv_url": "http://arxiv.org/abs/2412.09648v1",
    "pdf_url": "http://arxiv.org/pdf/2412.09648v1",
    "published_date": "2024-12-11",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProGDF: Progressive Gaussian Differential Field for Controllable and Flexible 3D Editing",
    "authors": [
      "Yian Zhao",
      "Wanshi Xu",
      "Yang Wu",
      "Weiheng Huang",
      "Zhongqian Sun",
      "Wei Yang"
    ],
    "abstract": "3D editing plays a crucial role in editing and reusing existing 3D assets, thereby enhancing productivity. Recently, 3DGS-based methods have gained increasing attention due to their efficient rendering and flexibility. However, achieving desired 3D editing results often requires multiple adjustments in an iterative loop, resulting in tens of minutes of training time cost for each attempt and a cumbersome trial-and-error cycle for users. This in-the-loop training paradigm results in a poor user experience. To address this issue, we introduce the concept of process-oriented modelling for 3D editing and propose the Progressive Gaussian Differential Field (ProGDF), an out-of-loop training approach that requires only a single training session to provide users with controllable editing capability and variable editing results through a user-friendly interface in real-time. ProGDF consists of two key components: Progressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS introduces the progressive constraint to extract the diverse intermediate results of the editing process and employs rendering quality regularization to improve the quality of these results. Based on these intermediate results, GDF leverages a lightweight neural network to model the editing process. Extensive results on two novel applications, namely controllable 3D editing and flexible fine-grained 3D manipulation, demonstrate the effectiveness, practicality and flexibility of the proposed ProGDF.",
    "arxiv_url": "http://arxiv.org/abs/2412.08152v1",
    "pdf_url": "http://arxiv.org/pdf/2412.08152v1",
    "published_date": "2024-12-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "face",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion-Based Attention Warping for Consistent 3D Scene Editing",
    "authors": [
      "Eyal Gomel",
      "Lior Wolf"
    ],
    "abstract": "We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \\url{https://attention-warp.github.io}",
    "arxiv_url": "http://arxiv.org/abs/2412.07984v1",
    "pdf_url": "http://arxiv.org/pdf/2412.07984v1",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GASP: Gaussian Avatars with Synthetic Priors",
    "authors": [
      "Jack Saunders",
      "Charlie Hewitt",
      "Yanan Jian",
      "Marek Kowalski",
      "Tadas Baltrusaitis",
      "Yiye Chen",
      "Darren Cosker",
      "Virginia Estellers",
      "Nicholas Gyde",
      "Vinay P. Namboodiri",
      "Benjamin E Lundell"
    ],
    "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://microsoft.github.io/GASP/) for results.",
    "arxiv_url": "http://arxiv.org/abs/2412.07739v1",
    "pdf_url": "http://arxiv.org/pdf/2412.07739v1",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "high quality",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proc-GS: Procedural Building Generation for City Assembly with 3D Gaussians",
    "authors": [
      "Yixuan Li",
      "Xingjian Ran",
      "Linning Xu",
      "Tao Lu",
      "Mulin Yu",
      "Zhenzhi Wang",
      "Yuanbo Xiangli",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "Buildings are primary components of cities, often featuring repeated elements such as windows and doors. Traditional 3D building asset creation is labor-intensive and requires specialized skills to develop design rules. Recent generative models for building creation often overlook these patterns, leading to low visual fidelity and limited scalability. Drawing inspiration from procedural modeling techniques used in the gaming and visual effects industry, our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting (3D-GS) framework, leveraging their advantages in high-fidelity rendering and efficient asset management from both worlds. By manipulating procedural code, we can streamline this process and generate an infinite variety of buildings. This integration significantly reduces model size by utilizing shared foundational assets, enabling scalable generation with precise control over building assembly. We showcase the potential for expansive cityscape generation while maintaining high rendering fidelity and precise control on both real and synthetic cases.",
    "arxiv_url": "http://arxiv.org/abs/2412.07660v1",
    "pdf_url": "http://arxiv.org/pdf/2412.07660v1",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Faster and Better 3D Splatting via Group Training",
    "authors": [
      "Chengbo Wang",
      "Guozheng Ma",
      "Yifei Xue",
      "Yizhen Lao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2412.07608v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07608v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery",
    "authors": [
      "Yanzhe Lyu",
      "Kai Cheng",
      "Xin Kang",
      "Xuejin Chen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis reveals that the 3D-GS densification operation lacks adaptiveness and faces a dilemma between geometry coverage and detail recovery. To address this, we introduce a novel densification operation, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.07494v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07494v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EventSplat: 3D Gaussian Splatting from Moving Event Cameras for Real-time Rendering",
    "authors": [
      "Toshiya Yura",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ],
    "abstract": "We introduce a method for using event camera data in novel view synthesis via Gaussian Splatting. Event cameras offer exceptional temporal resolution and a high dynamic range. Leveraging these capabilities allows us to effectively address the novel view synthesis challenge in the presence of fast camera motion. For initialization of the optimization process, our approach uses prior knowledge encoded in an event-to-video model. We also use spline interpolation for obtaining high quality poses along the event camera trajectory. This enhances the reconstruction quality from fast-moving cameras while overcoming the computational limitations traditionally associated with event-based Neural Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that our results achieve higher visual fidelity and better performance than existing event-based NeRF approaches while being an order of magnitude faster to render.",
    "arxiv_url": "http://arxiv.org/abs/2412.07293v2",
    "pdf_url": "http://arxiv.org/pdf/2412.07293v2",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "high quality",
      "3d gaussian",
      "real-time rendering",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds",
    "authors": [
      "Zhenggang Tang",
      "Yuchen Fan",
      "Dilin Wang",
      "Hongyu Xu",
      "Rakesh Ranjan",
      "Alexander Schwing",
      "Zhicheng Yan"
    ],
    "abstract": "Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2412.06974v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06974v1",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "head",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video",
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang",
      "Mingyang Chen",
      "Xiaopeng Fan",
      "Zifei Yan",
      "Wangmeng Zuo"
    ],
    "abstract": "Recent 4D reconstruction methods have yielded impressive results but rely on sharp videos as supervision. However, motion blur often occurs in videos due to camera shake and object movement, while existing methods render blurry results when using such videos for reconstructing 4D models. Although a few NeRF-based approaches attempted to address the problem, they struggled to produce high-quality results, due to the inaccuracy in estimating continuous dynamic representations within the exposure time. Encouraged by recent works in 3D motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest taking 3DGS as the scene representation manner, and propose the first 4D Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry monocular video, named Deblur4DGS. Specifically, we transform continuous dynamic representations estimation within an exposure time into the exposure time estimation. Moreover, we introduce exposure regularization to avoid trivial solutions, as well as multi-frame and multi-resolution consistency ones to alleviate artifacts. Furthermore, to better represent objects with large motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view synthesis, Deblur4DGS can be applied to improve blurry video from multiple perspectives, including deblurring, frame interpolation, and video stabilization. Extensive experiments on the above four tasks show that Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes are available at https://github.com/ZcsrenlongZ/Deblur4DGS.",
    "arxiv_url": "http://arxiv.org/abs/2412.06424v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06424v1",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ZcsrenlongZ/Deblur4DGS",
    "keywords": [
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes",
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Luyang Tang",
      "Ronggang Wang"
    ],
    "abstract": "Reconstructing dynamic scenes from video sequences is a highly promising task in the multimedia domain. While previous methods have made progress, they often struggle with slow rendering and managing temporal complexities such as significant motion and object appearance/disappearance. In this paper, we propose SaRO-GS as a novel dynamic scene representation capable of achieving real-time rendering while effectively handling temporal complexities in dynamic scenes. To address the issue of slow rendering speed, we adopt a Gaussian primitive-based representation and optimize the Gaussians in 4D space, which facilitates real-time rendering with the assistance of 3D Gaussian Splatting. Additionally, to handle temporally complex dynamic scenes, we introduce a Scale-aware Residual Field. This field considers the size information of each Gaussian primitive while encoding its residual feature and aligns with the self-splitting behavior of Gaussian primitives. Furthermore, we propose an Adaptive Optimization Schedule, which assigns different optimization strategies to Gaussian primitives based on their distinct temporal properties, thereby expediting the reconstruction of dynamic regions. Through evaluations on monocular and multi-view datasets, our method has demonstrated state-of-the-art performance. Please see our project page at https://yjb6.github.io/SaRO-GS.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.06299v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06299v1",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "4d",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects",
    "authors": [
      "Shi Qiu",
      "Binzhu Xie",
      "Qixuan Liu",
      "Pheng-Ann Heng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted significant attention for its potential to revolutionize 3D representation, rendering, and interaction. Despite the rapid growth of 3DGS research, its direct application to Extended Reality (XR) remains underexplored. Although many studies recognize the potential of 3DGS for XR, few have explicitly focused on or demonstrated its effectiveness within XR environments. In this paper, we aim to synthesize innovations in 3DGS that show specific potential for advancing XR research and development. We conduct a comprehensive review of publicly available 3DGS papers, with a focus on those referencing XR-related concepts. Additionally, we perform an in-depth analysis of innovations explicitly relevant to XR and propose a taxonomy to highlight their significance. Building on these insights, we propose several prospective XR research areas where 3DGS can make promising contributions, yet remain rarely touched. By investigating the intersection of 3DGS and XR, this paper provides a roadmap to push the boundaries of XR using cutting-edge 3DGS techniques.",
    "arxiv_url": "http://arxiv.org/abs/2412.06257v2",
    "pdf_url": "http://arxiv.org/pdf/2412.06257v2",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatter-360: Generalizable 360$^{\\circ}$ Gaussian Splatting for Wide-baseline Panoramic Images",
    "authors": [
      "Zheng Chen",
      "Chenming Wu",
      "Zhelun Shen",
      "Chen Zhao",
      "Weicai Ye",
      "Haocheng Feng",
      "Errui Ding",
      "Song-Hai Zhang"
    ],
    "abstract": "Wide-baseline panoramic images are frequently used in applications like VR and simulations to minimize capturing labor costs and storage needs. However, synthesizing novel views from these panoramic images in real time remains a significant challenge, especially due to panoramic imagery's high resolution and inherent distortions. Although existing 3D Gaussian splatting (3DGS) methods can produce photo-realistic views under narrow baselines, they often overfit the training views when dealing with wide-baseline panoramic images due to the difficulty in learning precise geometry from sparse 360$^{\\circ}$ views. This paper presents \\textit{Splatter-360}, a novel end-to-end generalizable 3DGS framework designed to handle wide-baseline panoramic images. Unlike previous approaches, \\textit{Splatter-360} performs multi-view matching directly in the spherical domain by constructing a spherical cost volume through a spherical sweep algorithm, enhancing the network's depth perception and geometry estimation. Additionally, we introduce a 3D-aware bi-projection encoder to mitigate the distortions inherent in panoramic images and integrate cross-view attention to improve feature interactions across multiple viewpoints. This enables robust 3D-aware feature representations and real-time rendering capabilities. Experimental results on the HM3D~\\cite{hm3d} and Replica~\\cite{replica} demonstrate that \\textit{Splatter-360} significantly outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat, DepthSplat, and HiSplat) in both synthesis quality and generalization performance for wide-baseline panoramic images. Code and trained models are available at \\url{https://3d-aigc.github.io/Splatter-360/}.",
    "arxiv_url": "http://arxiv.org/abs/2412.06250v1",
    "pdf_url": "http://arxiv.org/pdf/2412.06250v1",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "authors": [
      "Seungtae Nam",
      "Xiangyu Sun",
      "Gyeongjin Kang",
      "Younggeun Lee",
      "Seungjun Oh",
      "Eunbyung Park"
    ],
    "abstract": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.",
    "arxiv_url": "http://arxiv.org/abs/2412.06234v3",
    "pdf_url": "http://arxiv.org/pdf/2412.06234v3",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation",
    "authors": [
      "Zipeng Qi",
      "Hao Chen",
      "Haotian Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "In this paper, we propose a novel semantic splatting approach based on Gaussian Splatting to achieve efficient and low-latency. Our method projects the RGB attributes and semantic features of point clouds onto the image plane, simultaneously rendering RGB images and semantic segmentation results. Leveraging the explicit structure of point clouds and a one-time rendering strategy, our approach significantly enhances efficiency during optimization and rendering. Additionally, we employ SAM2 to generate pseudo-labels for boundary regions, which often lack sufficient supervision, and introduce two-level aggregation losses at the 2D feature map and 3D spatial levels to improve the view-consistent and spatial continuity.",
    "arxiv_url": "http://arxiv.org/abs/2412.05969v2",
    "pdf_url": "http://arxiv.org/pdf/2412.05969v2",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting and Meshing",
    "authors": [
      "Jianing Zhang",
      "Yuchao Zheng",
      "Ziwei Li",
      "Qionghai Dai",
      "Xiaoyun Yuan"
    ],
    "abstract": "Gaussian splatting has gained attention for its efficient representation and rendering of 3D scenes using continuous Gaussian primitives. However, it struggles with sparse-view inputs due to limited geometric and photometric information, causing ambiguities in depth, shape, and texture.   we propose GBR: Generative Bundle Refinement, a method for high-fidelity Gaussian splatting and meshing using only 4-6 input views. GBR integrates a neural bundle adjustment module to enhance geometry accuracy and a generative depth refinement module to improve geometry fidelity. More specifically, the neural bundle adjustment module integrates a foundation network to produce initial 3D point maps and point matches from unposed images, followed by bundle adjustment optimization to improve multiview consistency and point cloud accuracy. The generative depth refinement module employs a diffusion-based strategy to enhance geometric details and fidelity while preserving the scale. Finally, for Gaussian splatting optimization, we propose a multimodal loss function incorporating depth and normal consistency, geometric regularization, and pseudo-view supervision, providing robust guidance under sparse-view conditions. Experiments on widely used datasets show that GBR significantly outperforms existing methods under sparse-view inputs. Additionally, GBR demonstrates the ability to reconstruct and render large-scale real-world scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable details using only 6 views.",
    "arxiv_url": "http://arxiv.org/abs/2412.05908v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05908v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SizeGS: Size-aware Compression of 3D Gaussians with Hierarchical Mixed Precision Quantization",
    "authors": [
      "Shuzhao Xie",
      "Jiahang Liu",
      "Weixiang Zhang",
      "Shijia Ge",
      "Sicheng Pan",
      "Chen Tang",
      "Yunpeng Bai",
      "Zhi Wang"
    ],
    "abstract": "Effective compression technology is crucial for 3DGS to adapt to varying storage and transmission conditions. However, existing methods fail to address size constraints while maintaining optimal quality. In this paper, we introduce SizeGS, a framework that compresses 3DGS within a specified size budget while optimizing visual quality. We start with a size estimator to establish a clear relationship between file size and hyperparameters. Leveraging this estimator, we incorporate mixed precision quantization (MPQ) into 3DGS attributes, structuring MPQ in two hierarchical level -- inter-attribute and intra-attribute -- to optimize visual quality under the size constraint. At the inter-attribute level, we assign bit-widths to each attribute channel by formulating the combinatorial optimization as a 0-1 integer linear program, which can be efficiently solved. At the intra-attribute level, we divide each attribute channel into blocks of vectors, quantizing each vector based on the optimal bit-width derived at the inter-attribute level. Dynamic programming determines block lengths. Using the size estimator and MPQ, we develop a calibrated algorithm to identify optimal hyperparameters in just 10 minutes, achieving a 1.69$\\times$ efficiency increase with quality comparable to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.05808v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05808v1",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "dynamic",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes",
    "authors": [
      "Saqib Javed",
      "Ahmad Jarrar Khan",
      "Corentin Dumery",
      "Chen Zhao",
      "Mathieu Salzmann"
    ],
    "abstract": "Recent advancements in high-fidelity dynamic scene reconstruction have leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene representation. However, to make these methods viable for real-time applications such as AR/VR, gaming, and rendering on low-power devices, substantial reductions in memory usage and improvements in rendering efficiency are required. While many state-of-the-art methods prioritize lightweight implementations, they struggle in handling scenes with complex motions or long sequences. In this work, we introduce Temporally Compressed 3D Gaussian Splatting (TC3DGS), a novel technique designed specifically to effectively compress dynamic 3D Gaussian representations. TC3DGS selectively prunes Gaussians based on their temporal relevance and employs gradient-aware mixed-precision quantization to dynamically compress Gaussian parameters. It additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a post-processing step to further reduce storage by interpolating Gaussian trajectories across frames. Our experiments across multiple datasets demonstrate that TC3DGS achieves up to 67$\\times$ compression with minimal or no degradation in visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2412.05700v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05700v1",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "high-fidelity",
      "vr",
      "motion",
      "lightweight",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via Universal Watermarking",
    "authors": [
      "Yuqi Tan",
      "Xiang Liu",
      "Shuzhao Xie",
      "Bin Chen",
      "Shu-Tao Xia",
      "Zhi Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene representation, providing rapid rendering speeds and high fidelity. As 3DGS gains prominence, safeguarding its intellectual property becomes increasingly crucial since 3DGS could be used to imitate unauthorized scene creations and raise copyright issues. Existing watermarking methods for implicit NeRFs cannot be directly applied to 3DGS due to its explicit representation and real-time rendering process, leaving watermarking for 3DGS largely unexplored. In response, we propose WATER-GS, a novel method designed to protect 3DGS copyrights through a universal watermarking strategy. First, we introduce a pre-trained watermark decoder, treating raw 3DGS generative modules as potential watermark encoders to ensure imperceptibility. Additionally, we implement novel 3D distortion layers to enhance the robustness of the embedded watermark against common real-world distortions of point cloud data. Comprehensive experiments and ablation studies demonstrate that WATER-GS effectively embeds imperceptible and robust watermarks into 3DGS without compromising rendering efficiency and quality. Our experiments indicate that the 3D distortion layers can yield up to a 20% improvement in accuracy rate. Notably, our method is adaptable to different 3DGS variants, including 3DGS compression frameworks and 2D Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2412.05695v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05695v1",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis",
    "authors": [
      "Diwen Wan",
      "Yuxiang Wang",
      "Ruijie Lu",
      "Gang Zeng"
    ],
    "abstract": "While novel view synthesis for dynamic scenes has made significant progress, capturing skeleton models of objects and re-posing them remains a challenging task. To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates. Our approach utilizes 3D Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model. Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints. Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects. Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images.",
    "arxiv_url": "http://arxiv.org/abs/2412.05570v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05570v1",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation",
    "authors": [
      "Wenqing Wang",
      "Yun Fu"
    ],
    "abstract": "Text-to-3D generation is a valuable technology in virtual reality and digital content creation. While recent works have pushed the boundaries of text-to-3D generation, producing high-fidelity 3D objects with inefficient prompts and simulating their physics-grounded motion accurately still remain unsolved challenges. To address these challenges, we present an innovative framework that utilizes the Large Language Model (LLM)-refined prompts and diffusion priors-guided Gaussian Splatting (GS) for generating 3D models with accurate appearances and geometric structures. We also incorporate a continuum mechanics-based deformation map and color regularization to synthesize vivid physics-grounded motion for the generated 3D Gaussians, adhering to the conservation of mass and momentum. By integrating text-to-3D generation with physics-grounded motion synthesis, our framework renders photo-realistic 3D objects that exhibit physics-aware motion, accurately reflecting the behaviors of the objects under various forces and constraints across different materials. Extensive experiments demonstrate that our approach achieves high-quality 3D generations with realistic physics-grounded motion.",
    "arxiv_url": "http://arxiv.org/abs/2412.05560v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05560v1",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiant: Large-scale 3D Gaussian Rendering based on Hierarchical Framework",
    "authors": [
      "Haosong Peng",
      "Tianyu Qi",
      "Yufeng Zhan",
      "Hao Li",
      "Yalun Dai",
      "Yuanqing Xia"
    ],
    "abstract": "With the advancement of computer vision, the recently emerged 3D Gaussian Splatting (3DGS) has increasingly become a popular scene reconstruction algorithm due to its outstanding performance. Distributed 3DGS can efficiently utilize edge devices to directly train on the collected images, thereby offloading computational demands and enhancing efficiency. However, traditional distributed frameworks often overlook computational and communication challenges in real-world environments, hindering large-scale deployment and potentially posing privacy risks. In this paper, we propose Radiant, a hierarchical 3DGS algorithm designed for large-scale scene reconstruction that considers system heterogeneity, enhancing the model performance and training efficiency. Via extensive empirical study, we find that it is crucial to partition the regions for each edge appropriately and allocate varying camera positions to each device for image collection and training. The core of Radiant is partitioning regions based on heterogeneous environment information and allocating workloads to each device accordingly. Furthermore, we provide a 3DGS model aggregation algorithm that enhances the quality and ensures the continuity of models' boundaries. Finally, we develop a testbed, and experiments demonstrate that Radiant improved reconstruction quality by up to 25.7\\% and reduced up to 79.6\\% end-to-end latency.",
    "arxiv_url": "http://arxiv.org/abs/2412.05546v1",
    "pdf_url": "http://arxiv.org/pdf/2412.05546v1",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Extrapolated Urban View Synthesis Benchmark",
    "authors": [
      "Xiangyu Han",
      "Zhen Jia",
      "Boyi Li",
      "Yan Wang",
      "Boris Ivanovic",
      "Yurong You",
      "Lingjie Liu",
      "Yue Wang",
      "Marco Pavone",
      "Chen Feng",
      "Yiming Li"
    ],
    "abstract": "Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial capability that generates diverse unseen viewpoints to accommodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic rendering at real-time speeds and have been widely used in modeling large-scale driving scenes. However, their performance is commonly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapolation, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we leverage publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct both quantitative and qualitative evaluations of state-of-the-art NVS methods across different evaluation settings. Our results show that current NVS methods are prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry cannot fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large-scale training. We will release the data to help advance self-driving and urban robotics simulation technology.",
    "arxiv_url": "http://arxiv.org/abs/2412.05256v3",
    "pdf_url": "http://arxiv.org/pdf/2412.05256v3",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "lighting",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MixedGaussianAvatar: Realistically and Geometrically Accurate Head Avatar via Mixed 2D-3D Gaussian Splatting",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Qingpo Wuwu",
      "Xinyi Wang",
      "Xingyu Xiao",
      "Ming Lu"
    ],
    "abstract": "Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: https://github.com/ChenVoid/MGA/.",
    "arxiv_url": "http://arxiv.org/abs/2412.04955v2",
    "pdf_url": "http://arxiv.org/pdf/2412.04955v2",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ChenVoid/MGA/",
    "keywords": [
      "head",
      "high-fidelity",
      "nerf",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction",
    "authors": [
      "Jixuan Fan",
      "Wanhua Li",
      "Yifei Han",
      "Yansong Tang"
    ],
    "abstract": "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS_Page/",
    "arxiv_url": "http://arxiv.org/abs/2412.04887v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04887v1",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "large scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D Gaussian Splatting Approach",
    "authors": [
      "Chaozheng Wen",
      "Jingwen Tong",
      "Yingdong Hu",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a long-standing challenge. This issue has been escalated due to denser network deployment, larger antenna arrays, and broader bandwidth in next-generation networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting (3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. While WRF-GS demonstrates remarkable effectiveness, it faces limitations in capturing high-frequency signal variations caused by complex multipath effects. To overcome these limitations, we propose WRF-GS+, an enhanced framework that integrates electromagnetic wave physics into the neural network design. WRF-GS+ leverages deformable 3D Gaussians to model both static and dynamic components of the WRF, significantly improving its ability to characterize signal variations. In addition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS modeling process and improving computational efficiency. Experimental results demonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial spectrum synthesis, including ray tracing and other deep-learning approaches. Notably, WRF-GS+ achieves state-of-the-art performance in the received signal strength indication (RSSI) and channel state information (CSI) prediction tasks, surpassing existing methods by more than 0.7 dB and 3.36 dB, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2412.04832v4",
    "pdf_url": "http://arxiv.org/pdf/2412.04832v4",
    "published_date": "2024-12-06",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pushing Rendering Boundaries: Hard Gaussian Splatting",
    "authors": [
      "Qingshan Xu",
      "Jiequan Cui",
      "Xuanyu Yi",
      "Yuxuan Wang",
      "Yuan Zhou",
      "Yew-Soon Ong",
      "Hanwang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive Novel View Synthesis (NVS) results in a real-time rendering manner. During training, it relies heavily on the average magnitude of view-space positional gradients to grow Gaussians to reduce rendering loss. However, this average operation smooths the positional gradients from different viewpoints and rendering errors from different pixels, hindering the growth and optimization of many defective Gaussians. This leads to strong spurious artifacts in some areas. To address this problem, we propose Hard Gaussian Splatting, dubbed HGS, which considers multi-view significant positional gradients and rendering errors to grow hard Gaussians that fill the gaps of classical Gaussian Splatting on 3D scenes, thus achieving superior NVS results. In detail, we present positional gradient driven HGS, which leverages multi-view significant positional gradients to uncover hard Gaussians. Moreover, we propose rendering error guided HGS, which identifies noticeable pixel rendering errors and potentially over-large Gaussians to jointly mine hard Gaussians. By growing and optimizing these hard Gaussians, our method helps to resolve blurring and needle-like artifacts. Experiments on various datasets demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2412.04826v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04826v1",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Turbo3D: Ultra-fast Text-to-3D Generation",
    "authors": [
      "Hanzhe Hu",
      "Tianwei Yin",
      "Fujun Luan",
      "Yiwei Hu",
      "Hao Tan",
      "Zexiang Xu",
      "Sai Bi",
      "Shubham Tulsiani",
      "Kai Zhang"
    ],
    "abstract": "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.",
    "arxiv_url": "http://arxiv.org/abs/2412.04470v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04470v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos",
    "authors": [
      "Sharath Girish",
      "Tianye Li",
      "Amrita Mazumdar",
      "Abhinav Shrivastava",
      "David Luebke",
      "Shalini De Mello"
    ],
    "abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel framework for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residuals between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity framework, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at 350 FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen",
    "arxiv_url": "http://arxiv.org/abs/2412.04469v1",
    "pdf_url": "http://arxiv.org/pdf/2412.04469v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering",
    "authors": [
      "Cheng Sun",
      "Jaesung Choe",
      "Charles Loop",
      "Wei-Chiu Ma",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on adaptive sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to adaptively and explicitly allocate sparse voxels to different levels of detail within scenes, faithfully reproducing scene details with $65536^3$ grid resolution while achieving high rendering frame rates. Second, we customize a rasterizer for efficient adaptive sparse voxels rendering. We render voxels in the correct depth order by using ray direction-dependent Morton ordering, which avoids the well-known popping artifact found in Gaussian splatting. Our method improves the previous neural-free voxel model by over 4db PSNR and more than 10x FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our voxel representation is seamlessly compatible with grid-based 3D processing techniques such as Volume Fusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future extensions and applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.04459v3",
    "pdf_url": "http://arxiv.org/pdf/2412.04459v3",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "4d",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular Dynamic Gaussian Splatting: Fast, Brittle, and Scene Complexity Rules",
    "authors": [
      "Yiqing Liang",
      "Mikhail Okunev",
      "Mikaela Angelina Uy",
      "Runfeng Li",
      "Leonidas Guibas",
      "James Tompkin",
      "Adam W. Harley"
    ],
    "abstract": "Gaussian splatting methods are emerging as a popular approach for converting multi-view image data into scene representations that allow view synthesis. In particular, there is interest in enabling view synthesis for dynamic scenes using only monocular input data -- an ill-posed and challenging problem. The fast pace of work in this area has produced multiple simultaneous papers that claim to work best, which cannot all be true. In this work, we organize, benchmark, and analyze many Gaussian-splatting-based methods, providing apples-to-apples comparisons that prior works have lacked. We use multiple existing datasets and a new instructive synthetic dataset designed to isolate factors that affect reconstruction quality. We systematically categorize Gaussian splatting methods into specific motion representation types and quantify how their differences impact performance. Empirically, we find that their rank order is well-defined in synthetic data, but the complexity of real-world data currently overwhelms the differences. Furthermore, the fast rendering speed of all Gaussian-based methods comes at the cost of brittleness in optimization. We summarize our experiments into a list of findings that can help to further progress in this lively problem setting.",
    "arxiv_url": "http://arxiv.org/abs/2412.04457v2",
    "pdf_url": "http://arxiv.org/pdf/2412.04457v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars",
    "authors": [
      "Shota Sasaki",
      "Jane Wu",
      "Ko Nishino"
    ],
    "abstract": "This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes ``movement-dependent'' cloth deformation via physical simulation, rather than merely relying on ``pose-dependent'' rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes' Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.04433v2",
    "pdf_url": "http://arxiv.org/pdf/2412.04433v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents which demands to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through the local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Experiments demonstrate that our EmbodiedOcc outperforms existing local prediction methods and accomplishes the embodied occupancy prediction with high accuracy and strong expandability. Code: https://github.com/YkiWu/EmbodiedOcc.",
    "arxiv_url": "http://arxiv.org/abs/2412.04380v2",
    "pdf_url": "http://arxiv.org/pdf/2412.04380v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/YkiWu/EmbodiedOcc",
    "keywords": [
      "efficient",
      "semantic",
      "understanding",
      "3d gaussian",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models",
    "authors": [
      "Yifan Lu",
      "Xuanchi Ren",
      "Jiawei Yang",
      "Tianchang Shen",
      "Zhangjie Wu",
      "Jun Gao",
      "Yue Wang",
      "Siheng Chen",
      "Mike Chen",
      "Sanja Fidler",
      "Jiahui Huang"
    ],
    "abstract": "We present InfiniCube, a scalable method for generating unbounded dynamic 3D driving scenes with high fidelity and controllability. Previous methods for scene generation either suffer from limited scales or lack geometric and appearance consistency along generated sequences. In contrast, we leverage the recent advancements in scalable 3D representation and video models to achieve large dynamic scene generation that allows flexible controls through HD maps, vehicle bounding boxes, and text descriptions. First, we construct a map-conditioned sparse-voxel-based 3D generative model to unleash its power for unbounded voxel world generation. Then, we re-purpose a video model and ground it on the voxel world through a set of carefully designed pixel-aligned guidance buffers, synthesizing a consistent appearance. Finally, we propose a fast feed-forward approach that employs both voxel and pixel branches to lift the dynamic videos to dynamic 3D Gaussians with controllable objects. Our method can generate controllable and realistic 3D driving scenes, and extensive experiments validate the effectiveness and superiority of our model.",
    "arxiv_url": "http://arxiv.org/abs/2412.03934v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03934v1",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-View Pose-Agnostic Change Localization with Zero Labels",
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Suenderhauf",
      "Dimity Miller"
    ],
    "abstract": "Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn an additional change channel in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7x and 1.5x improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations.",
    "arxiv_url": "http://arxiv.org/abs/2412.03911v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03911v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "lighting",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction",
    "authors": [
      "Xuesong Li",
      "Jinguang Tong",
      "Jie Hong",
      "Vivien Rolland",
      "Lars Petersson"
    ],
    "abstract": "Dynamic scene reconstruction from monocular video is critical for real-world applications. This paper tackles the dual challenges of dynamic novel-view synthesis and 3D geometry reconstruction by introducing a hybrid framework: Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both modules can leverage each other for both tasks. During training, depth maps generated by the deformable Gaussian splatting module guide the ray sampling for faster processing and provide depth supervision within the dynamic neural surface module to improve geometry reconstruction. Simultaneously, the dynamic neural surface directs the distribution of Gaussian primitives around the surface, enhancing rendering quality. To further refine depth supervision, we introduce a depth-filtering process on depth maps derived from Gaussian rasterization. Extensive experiments on public datasets demonstrate that DGNS achieves state-of-the-art performance in both novel-view synthesis and 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.03910v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03910v2",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "geometry",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.",
    "arxiv_url": "http://arxiv.org/abs/2412.03844v4",
    "pdf_url": "http://arxiv.org/pdf/2412.03844v4",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos",
    "authors": [
      "Hanxue Liang",
      "Jiawei Ren",
      "Ashkan Mirzaei",
      "Antonio Torralba",
      "Ziwei Liu",
      "Igor Gilitschenski",
      "Sanja Fidler",
      "Cengiz Oztireli",
      "Huan Ling",
      "Zan Gojcic",
      "Jiahui Huang"
    ],
    "abstract": "Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.",
    "arxiv_url": "http://arxiv.org/abs/2412.03526v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03526v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter",
    "authors": [
      "Hermes McGriff",
      "Renato Martins",
      "Nicolas Andreff",
      "Cedric Demonceaux"
    ],
    "abstract": "This paper presents a dense depth estimation approach from light-field (LF) images that is able to compensate for strong rolling shutter (RS) effects. Our method estimates RS compensated views and dense RS compensated disparity maps. We present a two-stage method based on a 2D Gaussians Splatting that allows for a ``render and compare\" strategy with a point cloud formulation. In the first stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D shape that is related to the scene target shape ``up to a motion\". In the second stage, the deformation of the 3D shape is computed by estimating an admissible camera motion. We demonstrate the effectiveness and advantages of this approach through several experiments conducted for different scenes and types of motions. Due to lack of suitable datasets for evaluation, we also present a new carefully designed synthetic dataset of RS LF images. The source code, trained models and dataset will be made publicly available at: https://github.com/ICB-Vision-AI/DenseRSLF",
    "arxiv_url": "http://arxiv.org/abs/2412.03518v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03518v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ICB-Vision-AI/DenseRSLF",
    "keywords": [
      "motion",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UrbanGS: Semantic-Guided Gaussian Splatting for Urban Scene Reconstruction",
    "authors": [
      "Ziwen Li",
      "Jiaxin Huang",
      "Runnan Chen",
      "Yunlong Che",
      "Yandong Guo",
      "Tongliang Liu",
      "Fakhri Karray",
      "Mingming Gong"
    ],
    "abstract": "Reconstructing urban scenes is challenging due to their complex geometries and the presence of potentially dynamic objects. 3D Gaussian Splatting (3DGS)-based methods have shown strong performance, but existing approaches often incorporate manual 3D annotations to improve dynamic object modeling, which is impractical due to high labeling costs. Some methods leverage 4D Gaussian Splatting (4DGS) to represent the entire scene, but they treat static and dynamic objects uniformly, leading to unnecessary updates for static elements and ultimately degrading reconstruction quality. To address these issues, we propose UrbanGS, which leverages 2D semantic maps and an existing dynamic Gaussian approach to distinguish static objects from the scene, enabling separate processing of definite static and potentially dynamic elements. Specifically, for definite static regions, we enforce global consistency to prevent unintended changes in dynamic Gaussian and introduce a K-nearest neighbor (KNN)-based regularization to improve local coherence on low-textured ground surfaces. Notably, for potentially dynamic objects, we aggregate temporal information using learnable time embeddings, allowing each Gaussian to model deformations over time. Extensive experiments on real-world datasets demonstrate that our approach outperforms state-of-the-art methods in reconstruction quality and efficiency, accurately preserving static content while capturing dynamic elements.",
    "arxiv_url": "http://arxiv.org/abs/2412.03473v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03473v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "deformation",
      "4d",
      "3d gaussian",
      "ar",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
    "authors": [
      "Wanting Zhang",
      "Haodong Xiang",
      "Zhichao Liao",
      "Xiansong Lai",
      "Xinghui Li",
      "Long Zeng"
    ],
    "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.03428v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03428v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volumetrically Consistent 3D Gaussian Rasterization",
    "authors": [
      "Chinmay Talegaonkar",
      "Yash Belhe",
      "Ravi Ramamoorthi",
      "Nicholas Antipa"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds. However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy. We show that the core approximations in splatting are unnecessary, even within a rasterizer; We instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically. We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS. This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points. Our code is publicly available at: https://github.com/chinmay0301ucsd/Vol3DGS",
    "arxiv_url": "http://arxiv.org/abs/2412.03378v3",
    "pdf_url": "http://arxiv.org/pdf/2412.03378v3",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chinmay0301ucsd/Vol3DGS",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGSST: Scaling Gaussian Splatting StyleTransfer",
    "authors": [
      "Bruno Galerne",
      "Jianling Wang",
      "Lara Raad",
      "Jean-Michel Morel"
    ],
    "abstract": "Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering. 3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality. This work introduces SGSST: Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes. We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons.",
    "arxiv_url": "http://arxiv.org/abs/2412.03371v2",
    "pdf_url": "http://arxiv.org/pdf/2412.03371v2",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF and Gaussian Splatting SLAM in the Wild",
    "authors": [
      "Fabian Schmidt",
      "Markus Enzweiler",
      "Abhinav Valada"
    ],
    "abstract": "Navigating outdoor environments with visual Simultaneous Localization and Mapping (SLAM) systems poses significant challenges due to dynamic scenes, lighting variations, and seasonal changes, requiring robust solutions. While traditional SLAM methods struggle with adaptability, deep learning-based approaches and emerging neural radiance fields as well as Gaussian Splatting-based SLAM methods, offer promising alternatives. However, these methods have primarily been evaluated in controlled indoor environments with stable conditions, leaving a gap in understanding their performance in unstructured and variable outdoor settings. This study addresses this gap by evaluating these methods in natural outdoor environments, focusing on camera tracking accuracy, robustness to environmental factors, and computational efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate that neural SLAM methods achieve superior robustness, particularly under challenging conditions such as low light, but at a high computational cost. At the same time, traditional methods perform the best across seasons but are highly sensitive to variations in lighting conditions. The code of the benchmark is publicly available at https://github.com/iis-esslingen/nerf-3dgs-benchmark.",
    "arxiv_url": "http://arxiv.org/abs/2412.03263v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03263v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/iis-esslingen/nerf-3dgs-benchmark",
    "keywords": [
      "localization",
      "tracking",
      "outdoor",
      "lighting",
      "mapping",
      "understanding",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting",
    "authors": [
      "Yijia Guo",
      "Wenkai Huang",
      "Yang Li",
      "Gaolei Li",
      "Hang Zhang",
      "Liwen Hu",
      "Jianhua Li",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience. Codes and data will be released at https://water-gs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.03121v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03121v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos",
    "authors": [
      "Yoonwoo Jeong",
      "Junmyeong Lee",
      "Hoseung Choi",
      "Minsu Cho"
    ],
    "abstract": "Dynamic view synthesis (DVS) has advanced remarkably in recent years, achieving high-fidelity rendering while reducing computational costs. Despite the progress, optimizing dynamic neural fields from casual videos remains challenging, as these videos do not provide direct 3D information, such as camera trajectories or the underlying scene geometry. In this work, we present RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual videos. It effectively learns motion and underlying geometry of scenes by separating dynamic and static primitives, and ensures that the learned motion and geometry are physically plausible by incorporating motion and geometric regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig, that provides extensive camera and object motion along with simultaneous multi-view captures, features that are absent in previous benchmarks. Experimental results demonstrate that the proposed method significantly outperforms previous pose-free dynamic neural fields and achieves competitive rendering quality compared to existing pose-free static neural fields. The code and data are publicly available at https://rodygs.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2412.03077v1",
    "pdf_url": "http://arxiv.org/pdf/2412.03077v1",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects",
    "authors": [
      "Abdurrahman Zeybey",
      "Mehmet Ergezer",
      "Tommy Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has advanced radiance field reconstruction, enabling high-quality view synthesis and fast rendering in 3D modeling. While adversarial attacks on object detection models are well-studied for 2D images, their impact on 3D models remains underexplored. This work introduces the Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate adversarial noise targeting the CLIP vision-language model. M-IFGSM specifically alters the object of interest by focusing perturbations on masked regions, degrading the performance of CLIP's zero-shot object detection capability when applied to 3D models. Using eight objects from the Common Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces the accuracy and confidence of the model, with adversarial noise being nearly imperceptible to human observers. The top-1 accuracy in original model renders drops from 95.4\\% to 12.5\\% for train images and from 91.2\\% to 35.4\\% for test images, with confidence levels reflecting this shift from true classification to misclassification, underscoring the risks of adversarial attacks on 3D models in applications such as autonomous driving, robotics, and surveillance. The significance of this research lies in its potential to expose vulnerabilities in modern 3D vision models, including radiance fields, prompting the development of more robust defenses and security measures in critical real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.02803v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02803v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "autonomous driving",
      "fast",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction",
    "authors": [
      "Lingteng Qiu",
      "Shenhao Zhu",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Junfei Zhang",
      "Chao Xu",
      "Zhe Li",
      "Weihao Yuan",
      "Liefeng Bo",
      "Guanying Chen",
      "Zilong Dong"
    ],
    "abstract": "Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.",
    "arxiv_url": "http://arxiv.org/abs/2412.02684v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02684v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "real-time rendering",
      "3d reconstruction",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark",
    "authors": [
      "Haidong Xu",
      "Meishan Zhang",
      "Hao Ju",
      "Zhedong Zheng",
      "Erik Cambria",
      "Min Zhang",
      "Hao Fei"
    ],
    "abstract": "Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states.",
    "arxiv_url": "http://arxiv.org/abs/2412.02508v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02508v2",
    "published_date": "2024-12-03",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "mapping",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex Motions via Relay Gaussians",
    "authors": [
      "Qiankun Gao",
      "Yanmin Wu",
      "Chengxiang Wen",
      "Jiarui Meng",
      "Luyang Tang",
      "Jie Chen",
      "Ronggang Wang",
      "Jian Zhang"
    ],
    "abstract": "Reconstructing dynamic scenes with large-scale and complex motions remains a significant challenge. Recent techniques like Neural Radiance Fields and 3D Gaussian Splatting (3DGS) have shown promise but still struggle with scenes involving substantial movement. This paper proposes RelayGS, a novel method based on 3DGS, specifically designed to represent and reconstruct highly dynamic scenes. Our RelayGS learns a complete 4D representation with canonical 3D Gaussians and a compact motion field, consisting of three stages. First, we learn a fundamental 3DGS from all frames, ignoring temporal scene variations, and use a learnable mask to separate the highly dynamic foreground from the minimally moving background. Second, we replicate multiple copies of the decoupled foreground Gaussians from the first stage, each corresponding to a temporal segment, and optimize them using pseudo-views constructed from multiple frames within each segment. These Gaussians, termed Relay Gaussians, act as explicit relay nodes, simplifying and breaking down large-scale motion trajectories into smaller, manageable segments. Finally, we jointly learn the scene's temporal motion and refine the canonical Gaussians learned from the first two stages. We conduct thorough experiments on two dynamic scene datasets featuring large and complex motions, where our RelayGS outperforms state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs real-world basketball game scenes in a much more complete and coherent manner, whereas previous methods usually struggle to capture the complex motion of players. Code will be publicly available at https://github.com/gqk/RelayGS",
    "arxiv_url": "http://arxiv.org/abs/2412.02493v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02493v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/gqk/RelayGS",
    "keywords": [
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TimeWalker: Personalized Neural Space for Lifelong Head Avatars",
    "authors": [
      "Dongwei Pan",
      "Yang Li",
      "Hongsheng Li",
      "Kwan-Yee Lin"
    ],
    "abstract": "We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.",
    "arxiv_url": "http://arxiv.org/abs/2412.02421v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02421v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "head",
      "motion",
      "deformation",
      "human",
      "ar",
      "animation",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Realistic Surgical Simulation from Monocular Videos",
    "authors": [
      "Kailing Wang",
      "Chen Yang",
      "Keyang Zhao",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "abstract": "This paper tackles the challenge of automatically performing realistic surgical simulations from readily available surgical videos. Recent efforts have successfully integrated physically grounded dynamics within 3D Gaussians to perform high-fidelity simulations in well-reconstructed simulation environments from static scenes. However, they struggle with the geometric inconsistency in reconstructing simulation environments and unrealistic physical deformations in simulations of soft tissues when it comes to dynamic and complex surgical processes. In this paper, we propose SurgiSim, a novel automatic simulation system to overcome these limitations. To build a surgical simulation environment, we maintain a canonical 3D scene composed of 3D Gaussians coupled with a deformation field to represent a dynamic surgical scene. This process involves a multi-stage optimization with trajectory and anisotropic regularization, enhancing the geometry consistency of the canonical scene, which serves as the simulation environment. To achieve realistic physical simulations in this environment, we implement a Visco-Elastic deformation model based on the Maxwell model, effectively restoring the complex deformations of tissues. Additionally, we infer the physical parameters of tissues by minimizing the discrepancies between the input video and simulation results guided by estimated tissue motion, ensuring realistic simulation outcomes. Experiments on various surgical scenarios and interactions demonstrate SurgiSim's ability to perform realistic simulation of soft tissues among surgical procedures, showing its enormous potential for enhancing surgical training, planning, and robotic surgery systems. The project page is at https://namaenashibot.github.io/SurgiSim/.",
    "arxiv_url": "http://arxiv.org/abs/2412.02359v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02359v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos",
    "authors": [
      "Zhiyuan Chen",
      "Fan Lu",
      "Guo Yu",
      "Bin Li",
      "Sanqing Qu",
      "Yuan Huang",
      "Changhong Fu",
      "Guang Chen"
    ],
    "abstract": "Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is crucial for robotic manipulation. However, existing approaches typically rely on accurate depth information, which is non-trivial to obtain in real-world scenarios. Although depth estimation algorithms can be employed, geometric inaccuracy can lead to failures in RGBD-based pose tracking methods. To address this challenge, we introduce GSGTrack, a novel RGB-based pose tracking framework that jointly optimizes geometry and pose. Specifically, we adopt 3D Gaussian Splatting to create an optimizable 3D representation, which is learned simultaneously with a graph-based geometry optimization to capture the object's appearance features and refine its geometry. However, the joint optimization process is susceptible to perturbations from noisy pose and geometry data. Thus, we propose an object silhouette loss to address the issue of pixel-wise loss being overly sensitive to pose noise during tracking. To mitigate the geometric ambiguities caused by inaccurate depth information, we propose a geometry-consistent image pair selection strategy, which filters out low-confidence pairs and ensures robust geometric optimization. Extensive experiments on the OnePose and HO3D datasets demonstrate the effectiveness of GSGTrack in both 6DoF pose tracking and object reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2412.02267v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02267v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-robot autonomous 3D reconstruction using Gaussian splatting with Semantic guidance",
    "authors": [
      "Jing Zeng",
      "Qi Ye",
      "Tianle Liu",
      "Yang Xu",
      "Jin Li",
      "Jinming Xu",
      "Liang Li",
      "Jiming Chen"
    ],
    "abstract": "Implicit neural representations and 3D Gaussian splatting (3DGS) have shown great potential for scene reconstruction. Recent studies have expanded their applications in autonomous reconstruction through task assignment methods. However, these methods are mainly limited to single robot, and rapid reconstruction of large-scale scenes remains challenging. Additionally, task-driven planning based on surface uncertainty is prone to being trapped in local optima. To this end, we propose the first 3DGS-based centralized multi-robot autonomous 3D reconstruction framework. To further reduce time cost of task generation and improve reconstruction quality, we integrate online open-vocabulary semantic segmentation with surface uncertainty of 3DGS, focusing view sampling on regions with high instance uncertainty. Finally, we develop a multi-robot collaboration strategy with mode and task assignments improving reconstruction quality while ensuring planning efficiency. Our method demonstrates the highest reconstruction quality among all planning methods and superior planning efficiency compared to existing multi-robot methods. We deploy our method on multiple robots, and results show that it can effectively plan view paths and reconstruct scenes with high quality.",
    "arxiv_url": "http://arxiv.org/abs/2412.02249v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02249v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "segmentation",
      "high quality",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseLGS: Sparse View Language Embedded Gaussian Splatting",
    "authors": [
      "Jun Hu",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu",
      "Juyong Zhang"
    ],
    "abstract": "Recently, several studies have combined Gaussian Splatting to obtain scene representations with language embeddings for open-vocabulary 3D scene understanding. While these methods perform well, they essentially require very dense multi-view inputs, limiting their applicability in real-world scenarios. In this work, we propose SparseLGS to address the challenge of 3D scene understanding with pose-free and sparse view input images. Our method leverages a learning-based dense stereo model to handle pose-free and sparse inputs, and a three-step region matching approach to address the multi-view semantic inconsistency problem, which is especially important for sparse inputs. Different from directly learning high-dimensional CLIP features, we extract low-dimensional information and build bijections to avoid excessive learning and storage costs. We introduce a reconstruction loss during semantic training to improve Gaussian positions and shapes. To the best of our knowledge, we are the first to address the 3D semantic field problem with sparse pose-free inputs. Experimental results show that SparseLGS achieves comparable quality when reconstructing semantic fields with fewer inputs (3-4 views) compared to previous SOTA methods with dense input. Besides, when using the same sparse input, SparseLGS leads significantly in quality and heavily improves the computation speed (5$\\times$speedup). Project page: https://ustc3dv.github.io/SparseLGS",
    "arxiv_url": "http://arxiv.org/abs/2412.02245v2",
    "pdf_url": "http://arxiv.org/pdf/2412.02245v2",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "semantic",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "How to Use Diffusion Priors under Sparse Views?",
    "authors": [
      "Qisen Wang",
      "Yifan Zhao",
      "Jiawei Ma",
      "Jia Li"
    ],
    "abstract": "Novel view synthesis under sparse views has been a long-term important challenge in 3D reconstruction. Existing works mainly rely on introducing external semantic or depth priors to supervise the optimization of 3D representations. However, the diffusion model, as an external prior that can directly provide visual supervision, has always underperformed in sparse-view 3D reconstruction using Score Distillation Sampling (SDS) due to the low information entropy of sparse views compared to text, leading to optimization challenges caused by mode deviation. To this end, we present a thorough analysis of SDS from the mode-seeking perspective and propose Inline Prior Guided Score Matching (IPSM), which leverages visual inline priors provided by pose relationships between viewpoints to rectify the rendered image distribution and decomposes the original optimization objective of SDS, thereby offering effective diffusion visual guidance without any fine-tuning or pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts 3D Gaussian Splatting as the backbone and supplements depth and geometry consistency regularization based on IPSM to further improve inline priors and rectified distribution. Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality. The code is released at https://github.com/iCVTEAM/IPSM.",
    "arxiv_url": "http://arxiv.org/abs/2412.02225v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02225v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/iCVTEAM/IPSM",
    "keywords": [
      "sparse view",
      "sparse-view",
      "semantic",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from Sparse Multi-View RGB Images",
    "authors": [
      "Junqiu Yu",
      "Xinlin Ren",
      "Yongchong Gu",
      "Haitao Lin",
      "Tianyu Wang",
      "Yi Zhu",
      "Hang Xu",
      "Yu-Gang Jiang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ],
    "abstract": "Language-guided robotic grasping is a rapidly advancing field where robots are instructed using human language to grasp specific objects. However, existing methods often depend on dense camera views and struggle to quickly update scenes, limiting their effectiveness in changeable environments.   In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping system that operates efficiently with sparse-view RGB images and handles scene updates fastly. Our system builds upon and significantly enhances existing computer vision modules in robotic learning. Specifically, SparseGrasp utilizes DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian Splatting (3DGS), maintaining high fidelity even under sparse supervision. Importantly, SparseGrasp incorporates semantic awareness from recent vision foundation models. To further improve processing efficiency, we repurpose Principal Component Analysis (PCA) to compress features from 2D models. Additionally, we introduce a novel render-and-compare strategy that ensures rapid scene updates, enabling multi-turn grasping in changeable environments.   Experimental results show that SparseGrasp significantly outperforms state-of-the-art methods in terms of both speed and adaptability, providing a robust solution for multi-turn grasping in changeable environment.",
    "arxiv_url": "http://arxiv.org/abs/2412.02140v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02140v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "semantic",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Object Carver: Object-Compositional Gaussian Splatting with surfaces completion",
    "authors": [
      "Liu Liu",
      "Xinjie Wang",
      "Jiaxiong Qiu",
      "Tianwei Lin",
      "Xiaolin Zhou",
      "Zhizhong Su"
    ],
    "abstract": "3D scene reconstruction is a foundational problem in computer vision. Despite recent advancements in Neural Implicit Representations (NIR), existing methods often lack editability and compositional flexibility, limiting their use in scenarios requiring high interactivity and object-level manipulation. In this paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and scalable framework for object-compositional 3D scene reconstruction. GOC leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors and multi-view geometry regularization, to achieve high-quality and flexible reconstruction. Furthermore, we propose a zero-shot Object Surface Completion (OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved surfaces, ensuring object completeness even in occluded areas. Experimental results demonstrate that GOC improves reconstruction efficiency and geometric fidelity. It holds promise for advancing the practical application of digital twins in embodied AI, AR/VR, and interactive simulation environments.",
    "arxiv_url": "http://arxiv.org/abs/2412.02075v1",
    "pdf_url": "http://arxiv.org/pdf/2412.02075v1",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Planar Gaussian Splatting",
    "authors": [
      "Farhad G. Zanjani",
      "Hong Cai",
      "Hanno Ackermann",
      "Leila Mirvakhabova",
      "Fatih Porikli"
    ],
    "abstract": "This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering approach to learn the 3D geometry and parse the 3D planes of a scene, directly from multiple RGB images. The PGS leverages Gaussian primitives to model the scene and employ a hierarchical Gaussian mixture approach to group them. Similar Gaussians are progressively merged probabilistically in the tree-structured Gaussian mixtures to identify distinct 3D plane instances and form the overall 3D scene geometry. In order to enable the grouping, the Gaussian primitives contain additional parameters, such as plane descriptors derived by lifting 2D masks from a general 2D segmentation model and surface normals. Experiments show that the proposed PGS achieves state-of-the-art performance in 3D planar reconstruction without requiring either 3D plane labels or depth supervision. In contrast to existing supervised methods that have limited generalizability and struggle under domain shift, PGS maintains its performance across datasets thanks to its neural rendering and scene-specific optimization mechanism, while also being significantly faster than existing optimization-based approaches.",
    "arxiv_url": "http://arxiv.org/abs/2412.01931v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01931v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "fast",
      "segmentation",
      "geometry",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering",
    "authors": [
      "Yunzhou Song",
      "Heguang Lin",
      "Jiahui Lei",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "abstract": "Recent advancements in neural rendering, particularly 2D Gaussian Splatting (2DGS), have shown promising results for jointly reconstructing fine appearance and geometry by leveraging 2D Gaussian surfels. However, current methods face significant challenges when rendering at arbitrary viewpoints, such as anti-aliasing for down-sampled rendering, and texture detail preservation for high-resolution rendering. We proposed a novel method to align the 2D surfels with texture maps and augment it with per-ray depth sorting and fisher-based pruning for rendering consistency and efficiency. With correct order, per-surfel texture maps significantly improve the capabilities to capture fine details. Additionally, to render high-fidelity details in varying viewpoints, we designed a frustum-based sampling method to mitigate the aliasing artifacts. Experimental results on benchmarks and our custom texture-rich dataset demonstrate that our method surpasses existing techniques, particularly in detail preservation and anti-aliasing.",
    "arxiv_url": "http://arxiv.org/abs/2412.01823v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01823v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Occam's LGS: An Efficient Approach for Language Gaussian Splatting",
    "authors": [
      "Jiahuan Cheng",
      "Jan-Nico Zaech",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene representation, offering efficient, high-quality reconstruction and rendering. A key reason for its success is the simplicity of representing scenes with sets of Gaussians, making it interpretable and adaptable. To enhance understanding beyond visual representation, recent approaches extend Gaussian Splatting with semantic vision-language features, enabling open-set tasks. Typically, these language features are aggregated from multiple 2D views, however, existing methods rely on cumbersome techniques, resulting in high computational costs and longer training times.   In this work, we show that the complicated pipelines for language 3D Gaussian Splatting are simply unnecessary. Instead, we follow a probabilistic formulation of Language Gaussian Splatting and apply Occam's razor to the task at hand, leading to a highly efficient weighted multi-view feature aggregation technique. Doing so offers us state-of-the-art results with a speed-up of two orders of magnitude without any compression, allowing for easy scene manipulation. Project Page: https://insait-institute.github.io/OccamLGS/",
    "arxiv_url": "http://arxiv.org/abs/2412.01807v2",
    "pdf_url": "http://arxiv.org/pdf/2412.01807v2",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion",
    "authors": [
      "Kai He",
      "Chin-Hsuan Wu",
      "Igor Gilitschenski"
    ],
    "abstract": "Recent advances in 3D representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have greatly improved realistic scene modeling and novel-view synthesis. However, achieving controllable and consistent editing in dynamic 3D scenes remains a significant challenge. Previous work is largely constrained by its editing backbones, resulting in inconsistent edits and limited controllability. In our work, we introduce a novel framework that first fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of the scene based on deformable 3D Gaussians. Our fine-tuning enables the model to \"learn\" the editing ability from a single edited reference image, transforming the complex task of dynamic scene editing into a simple 2D image editing process. By directly learning editing regions and styles from the reference, our approach enables consistent and precise local edits without the need for tracking desired editing regions, effectively addressing key challenges in dynamic scene editing. Then, our two-stage optimization progressively edits the trained dynamic scene, using a designed edited image buffer to accelerate convergence and improve temporal consistency. Compared to state-of-the-art methods, our approach offers more flexible and controllable local scene editing, achieving high-quality and consistent results.",
    "arxiv_url": "http://arxiv.org/abs/2412.01792v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01792v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale Aerial-to-Ground Scenes",
    "authors": [
      "Lihan Jiang",
      "Kerui Ren",
      "Mulin Yu",
      "Linning Xu",
      "Junting Dong",
      "Tao Lu",
      "Feng Zhao",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "Seamless integration of both aerial and street view images remains a significant challenge in neural scene reconstruction and rendering. Existing methods predominantly focus on single domain, limiting their applications in immersive environments, which demand extensive free view exploration with large view changes both horizontally and vertically. We introduce Horizon-GS, a novel approach built upon Gaussian Splatting techniques, tackles the unified reconstruction and rendering for aerial and street views. Our method addresses the key challenges of combining these perspectives with a new training strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes. We also curate a high-quality aerial-to-ground views dataset encompassing both synthetic and real-world scene to advance further research. Experiments across diverse urban scene datasets confirm the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2412.01745v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01745v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving",
    "authors": [
      "Hongyu Zhou",
      "Longzhong Lin",
      "Jiabao Wang",
      "Yichong Lu",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yue Wang",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, We tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.",
    "arxiv_url": "http://arxiv.org/abs/2412.01718v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01718v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "lighting",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Driving Scene Synthesis on Free-form Trajectories with Generative Prior",
    "authors": [
      "Zeyu Yang",
      "Zijie Pan",
      "Yuankun Yang",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "Driving scene synthesis along free-form trajectories is essential for driving simulations to enable closed-loop evaluation of end-to-end driving policies. While existing methods excel at novel view synthesis on recorded trajectories, they face challenges with novel trajectories due to limited views of driving videos and the vastness of driving environments. To tackle this challenge, we propose a novel free-form driving view synthesis approach, dubbed DriveX, by leveraging video generative prior to optimize a 3D model across a variety of trajectories. Concretely, we crafted an inverse problem that enables a video diffusion model to be utilized as a prior for many-trajectory optimization of a parametric 3D model (e.g., Gaussian splatting). To seamlessly use the generative prior, we iteratively conduct this process during optimization. Our resulting model can produce high-fidelity virtual driving environments outside the recorded trajectory, enabling free-form trajectory driving simulation. Beyond real driving scenes, DriveX can also be utilized to simulate virtual driving worlds from AI-generated videos.",
    "arxiv_url": "http://arxiv.org/abs/2412.01717v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01717v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Models with Anisotropic Gaussian Splatting for Image Inpainting",
    "authors": [
      "Jacob Fein-Ashley",
      "Benjamin Fein-Ashley"
    ],
    "abstract": "Image inpainting is a fundamental task in computer vision, aiming to restore missing or corrupted regions in images realistically. While recent deep learning approaches have significantly advanced the state-of-the-art, challenges remain in maintaining structural continuity and generating coherent textures, particularly in large missing areas. Diffusion models have shown promise in generating high-fidelity images but often lack the structural guidance necessary for realistic inpainting. We propose a novel inpainting method that combines diffusion models with anisotropic Gaussian splatting to capture both local structures and global context effectively. By modeling missing regions using anisotropic Gaussian functions that adapt to local image gradients, our approach provides structural guidance to the diffusion-based inpainting network. The Gaussian splat maps are integrated into the diffusion process, enhancing the model's ability to generate high-fidelity and structurally coherent inpainting results. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques, producing visually plausible results with enhanced structural integrity and texture realism.",
    "arxiv_url": "http://arxiv.org/abs/2412.01682v3",
    "pdf_url": "http://arxiv.org/pdf/2412.01682v3",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting",
    "authors": [
      "Ziyang Yan",
      "Lei Li",
      "Yihua Shao",
      "Siyu Chen",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Hao Zhao",
      "Fabio Remondino"
    ],
    "abstract": "The creation of 3D scenes has traditionally been both labor-intensive and costly, requiring designers to meticulously configure 3D assets and environments. Recent advancements in generative AI, including text-to-3D and image-to-3D methods, have dramatically reduced the complexity and cost of this process. However, current techniques for editing complex 3D scenes continue to rely on generally interactive multi-step, 2D-to-3D projection methods and diffusion-based techniques, which often lack precision in control and hamper real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based paradigm for real-time, precise editing of intricate 3D scenes using Gaussian Splatting. Unlike conventional methods, 3DSceneEditor operates through a streamlined 3D pipeline, enabling direct manipulation of Gaussians for efficient, high-quality edits based on input prompts.The proposed framework (i) integrates a pre-trained instance segmentation model for semantic labeling; (ii) employs a zero-shot grounding approach with CLIP to align target objects with user prompts; and (iii) applies scene modifications, such as object addition, repositioning, recoloring, replacing, and deletion directly on Gaussians. Extensive experimental results show that 3DSceneEditor achieves superior editing precision and speed with respect to current SOTA 3D scene editing approaches, establishing a new benchmark for efficient and interactive 3D scene customization.",
    "arxiv_url": "http://arxiv.org/abs/2412.01583v2",
    "pdf_url": "http://arxiv.org/pdf/2412.01583v2",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SfM-Free 3D Gaussian Splatting via Hierarchical Training",
    "authors": [
      "Bo Ji",
      "Angela Yao"
    ],
    "abstract": "Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera poses and a sparse point cloud, obtained from structure-from-motion (SfM) preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free 3DGS (SFGS) method for video input, eliminating the need for known camera poses and SfM preprocessing. Our approach introduces a hierarchical training strategy that trains and merges multiple 3D Gaussian representations -- each optimized for specific scene regions -- into a single, unified 3DGS model representing the entire scene. To compensate for large camera motions, we leverage video frame interpolation models. Additionally, we incorporate multi-source supervision to reduce overfitting and enhance representation. Experimental results reveal that our approach significantly surpasses state-of-the-art SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with a top gain of 3.90dB. The code is available at https://github.com/jibo27/3DGS_Hierarchical_Training.",
    "arxiv_url": "http://arxiv.org/abs/2412.01553v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01553v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jibo27/3DGS_Hierarchical_Training",
    "keywords": [
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GFreeDet: Exploiting Gaussian Splatting and Foundation Models for Model-free Unseen Object Detection in the BOP Challenge 2024",
    "authors": [
      "Xingyu Liu",
      "Gu Wang",
      "Chengxi Li",
      "Yingyue Li",
      "Chenyangguang Zhang",
      "Ziqin Huang",
      "Xiangyang Ji"
    ],
    "abstract": "We present GFreeDet, an unseen object detection approach that leverages Gaussian splatting and vision Foundation models under model-free setting. Unlike existing methods that rely on predefined CAD templates, GFreeDet reconstructs objects directly from reference videos using Gaussian splatting, enabling robust detection of novel objects without prior 3D models. Evaluated on the BOP-H3 benchmark, GFreeDet achieves comparable performance to CAD-based methods, demonstrating the viability of model-free detection for mixed reality (MR) applications. Notably, GFreeDet won the best overall method and the best fast method awards in the model-free 2D detection track at BOP Challenge 2024.",
    "arxiv_url": "http://arxiv.org/abs/2412.01552v4",
    "pdf_url": "http://arxiv.org/pdf/2412.01552v4",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting",
    "authors": [
      "Yufeng Jin",
      "Vignesh Prasad",
      "Snehal Jauhri",
      "Mathias Franzius",
      "Georgia Chalvatzaki"
    ],
    "abstract": "Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \\& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.",
    "arxiv_url": "http://arxiv.org/abs/2412.01543v2",
    "pdf_url": "http://arxiv.org/pdf/2412.01543v2",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "tracking",
      "autonomous driving",
      "fast",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
    "authors": [
      "Jianfeng Xiang",
      "Zelong Lv",
      "Sicheng Xu",
      "Yu Deng",
      "Ruicheng Wang",
      "Bowen Zhang",
      "Dong Chen",
      "Xin Tong",
      "Jiaolong Yang"
    ],
    "abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.",
    "arxiv_url": "http://arxiv.org/abs/2412.01506v3",
    "pdf_url": "http://arxiv.org/pdf/2412.01506v3",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting with Multi-View Geometric Consistency",
    "authors": [
      "Zhuoxiao Li",
      "Shanliang Yao",
      "Yong Yue",
      "Wufan Zhao",
      "Rongjun Qin",
      "Angel F. Garcia-Fernandez",
      "Andrew Levers",
      "Xiaohui Zhu"
    ],
    "abstract": "While Gaussian Splatting (GS) demonstrates efficient and high-quality scene rendering and small area surface extraction ability, it falls short in handling large-scale aerial image surface extraction tasks. To overcome this, we present ULSR-GS, a framework dedicated to high-fidelity surface extraction in ultra-large-scale scenes, addressing the limitations of existing GS-based mesh extraction methods. Specifically, we propose a point-to-photo partitioning approach combined with a multi-view optimal view matching principle to select the best training images for each sub-region. Additionally, during training, ULSR-GS employs a densification strategy based on multi-view geometric consistency to enhance surface extraction details. Experimental results demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on large-scale aerial photogrammetry benchmark datasets, significantly improving surface extraction accuracy in complex urban environments. Project page: https://ulsrgs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2412.01402v2",
    "pdf_url": "http://arxiv.org/pdf/2412.01402v2",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid Gaussian Splatting",
    "authors": [
      "Zhenzhong Cao",
      "Chenyang Zhao",
      "Qianyi Zhang",
      "Jinzheng Guang",
      "Yinuo Song Jingtai Liu"
    ],
    "abstract": "High-quality reconstruction is crucial for dense SLAM. Recent popular approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and semantic reconstruction of scenes. However, these methods often overlook issues of detail and consistency in different parts of the scene. To address this, we propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level pyramid gaussian splatting, which enables high-quality dense reconstruction of scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level pyramid gaussian splatting method that restores scene details by extracting multi-level image pyramids for gaussian splatting training, ensuring consistency in RGB, depth, and semantic reconstructions. Additionally, we design a tightly-coupled multi-features reconstruction optimization mechanism, allowing the reconstruction accuracy of RGB, depth, and semantic maps to mutually enhance each other during the rendering optimization process. Extensive quantitative, qualitative, and ablation experiments on the Replica and ScanNet public datasets demonstrate that our proposed method outperforms current state-of-the-art methods. The open-source code will be available at: https://github.com/zhenzhongcao/RGBDS-SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2412.01217v2",
    "pdf_url": "http://arxiv.org/pdf/2412.01217v2",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zhenzhongcao/RGBDS-SLAM",
    "keywords": [
      "semantic",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
    "authors": [
      "Jun Xiang",
      "Yudong Guo",
      "Leipeng Hu",
      "Boyang Guo",
      "Yancheng Yuan",
      "Juyong Zhang"
    ],
    "abstract": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.",
    "arxiv_url": "http://arxiv.org/abs/2412.01106v1",
    "pdf_url": "http://arxiv.org/pdf/2412.01106v1",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "avatar",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
    "authors": [
      "Youjia Zhang",
      "Anpei Chen",
      "Yumin Wan",
      "Zikai Song",
      "Junqing Yu",
      "Yawei Luo",
      "Wei Yang"
    ],
    "abstract": "In this paper, we introduce Ref-GS, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. Ref-GS builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. Next, we introduce a spherical Mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading. Additionally, we propose a simple yet efficient geometry-lighting factorization that connects geometry and lighting via the vector outer product, significantly reducing renderer overhead when integrating volumetric attributes. Our method achieves superior photorealistic rendering for a range of open-world scenes while also accurately recovering geometry.",
    "arxiv_url": "http://arxiv.org/abs/2412.00905v2",
    "pdf_url": "http://arxiv.org/pdf/2412.00905v2",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "lighting",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair",
    "authors": [
      "Weihang Li",
      "Weirong Chen",
      "Shenhan Qian",
      "Jiajie Chen",
      "Daniel Cremers",
      "Haoang Li"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.",
    "arxiv_url": "http://arxiv.org/abs/2412.00851v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00851v1",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "high-fidelity",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality",
    "authors": [
      "Zhaofeng Luo",
      "Zhitong Cui",
      "Shijian Luo",
      "Mengyu Chu",
      "Minchen Li"
    ],
    "abstract": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables intuitive creation and manipulation of elastoplastic objects in Virtual Reality (VR). By customizing the Material Point Method (MPM) for real-time simulation of hand-induced large deformations and enhancing 3D Gaussian Splatting for seamless rendering, VR-Doh provides an interactive and immersive 3D modeling experience. Users can naturally sculpt, deform, and edit objects through both contact- and gesture-based hand-object interactions. To achieve real-time performance, our system incorporates localized simulation techniques, particle-level collision handling, and the decoupling of physical and appearance representations, ensuring smooth and responsive interactions. VR-Doh supports both object creation and editing, enabling diverse modeling tasks such as designing food items, characters, and interlocking structures, all resulting in simulation-ready assets. User studies with both novice and experienced participants highlight the system's intuitive design, immersive feedback, and creative potential. Compared to existing geometric modeling tools, VR-Doh offers enhanced accessibility and natural interaction, making it a powerful tool for creative exploration in VR.",
    "arxiv_url": "http://arxiv.org/abs/2412.00814v4",
    "pdf_url": "http://arxiv.org/pdf/2412.00814v4",
    "published_date": "2024-12-01",
    "categories": [
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "deformation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ChatSplat: 3D Conversational Gaussian Splatting",
    "authors": [
      "Hanlin Chen",
      "Fangyin Wei",
      "Gim Hee Lee"
    ],
    "abstract": "Humans naturally interact with their 3D surroundings using language, and modeling 3D language fields for scene understanding and interaction has gained growing interest. This paper introduces ChatSplat, a system that constructs a 3D language field, enabling rich chat-based interaction within 3D space. Unlike existing methods that primarily use CLIP-derived language features focused solely on segmentation, ChatSplat facilitates interaction on three levels: objects, views, and the entire 3D scene. For view-level interaction, we designed an encoder that encodes the rendered feature map of each view into tokens, which are then processed by a large language model (LLM) for conversation. At the scene level, ChatSplat combines multi-view tokens, enabling interactions that consider the entire scene. For object-level interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's pixel-wise language embedding that implicitly includes mask and embedding. Here, we explicitly decouple the language embedding into separate mask and feature map representations, allowing more flexible object-level interaction. To address the challenge of learning 3D Gaussians posed by the complex and diverse distribution of language embeddings used in the LLM, we introduce a learnable normalization technique to standardize these embeddings, facilitating effective learning. Extensive experimental results demonstrate that ChatSplat supports multi-level interactions -- object, view, and scene -- within 3D space, enhancing both understanding and engagement.",
    "arxiv_url": "http://arxiv.org/abs/2412.00734v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00734v1",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction with Gaussian Splatting",
    "authors": [
      "Phu Pham",
      "Damon Conover",
      "Aniket Bera"
    ],
    "abstract": "We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based SLAM methods often fall short in sparse view settings and during large camera movements due to their reliance on gradient descent-based optimization, which is both slow and inaccurate. FlashSLAM addresses these limitations by combining 3DGS with a fast vision-based camera tracking technique, utilizing a pretrained feature matching model and point cloud registration for precise pose estimation in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without costly iterative rendering. In sparse settings, our method achieves up to a 92% improvement in average tracking accuracy over previous methods. Additionally, it accounts for noise in depth sensors, enhancing robustness when using unspecialized devices such as smartphones. Extensive experiments show that FlashSLAM performs reliably across both sparse and dense settings, in synthetic and real-world environments. Evaluations on benchmark datasets highlight its superior accuracy and efficiency, establishing FlashSLAM as a versatile and high-performance solution for SLAM, advancing the state-of-the-art in 3D reconstruction across diverse applications.",
    "arxiv_url": "http://arxiv.org/abs/2412.00682v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00682v1",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "tracking",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Lesson in Splats: Teacher-Guided Diffusion for 3D Gaussian Splats Generation with 2D Supervision",
    "authors": [
      "Chensheng Peng",
      "Ido Sobol",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Chenfeng Xu",
      "Or Litany"
    ],
    "abstract": "We introduce a diffusion model for Gaussian Splats, SplatDiffusion, to enable generation of three-dimensional structures from single images, addressing the ill-posed nature of lifting 2D inputs to 3D. Existing methods rely on deterministic, feed-forward predictions, which limit their ability to handle the inherent ambiguity of 3D inference from 2D data. Diffusion models have recently shown promise as powerful generative models for 3D data, including Gaussian splats; however, standard diffusion frameworks typically require the target signal and denoised signal to be in the same modality, which is challenging given the scarcity of 3D data. To overcome this, we propose a novel training strategy that decouples the denoised modality from the supervision modality. By using a deterministic model as a noisy teacher to create the noised signal and transitioning from single-step to multi-step denoising supervised by an image rendering loss, our approach significantly enhances performance compared to the deterministic teacher. Additionally, our method is flexible, as it can learn from various 3D Gaussian Splat (3DGS) teachers with minimal adaptation; we demonstrate this by surpassing the performance of two different deterministic models as teachers, highlighting the potential generalizability of our framework. Our approach further incorporates a guidance mechanism to aggregate information from multiple views, enhancing reconstruction quality when more than one view is available. Experimental results on object-level and scene-level datasets demonstrate the effectiveness of our framework.",
    "arxiv_url": "http://arxiv.org/abs/2412.00623v2",
    "pdf_url": "http://arxiv.org/pdf/2412.00623v2",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives",
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Geng Lin",
      "Vasu Singla",
      "Matthias Zwicker",
      "Tom Goldstein"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS to substantially improve rendering speed. These improvements also yield the ancillary benefits of reduced model size and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\\mathit{6.71\\times}$ across scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.",
    "arxiv_url": "http://arxiv.org/abs/2412.00578v2",
    "pdf_url": "http://arxiv.org/pdf/2412.00578v2",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects",
    "authors": [
      "Amir Barda",
      "Matheus Gadelha",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Amit H. Bermano",
      "Thibault Groueix"
    ],
    "abstract": "We propose a generative technique to edit 3D shapes, represented as meshes, NeRFs, or Gaussian Splats, in approximately 3 seconds, without the need for running an SDS type of optimization. Our key insight is to cast 3D editing as a multiview image inpainting problem, as this representation is generic and can be mapped back to any 3D representation using the bank of available Large Reconstruction Models. We explore different fine-tuning strategies to obtain both multiview generation and inpainting capabilities within the same diffusion model. In particular, the design of the inpainting mask is an important factor of training an inpainting model, and we propose several masking strategies to mimic the types of edits a user would perform on a 3D shape. Our approach takes 3D generative editing from hours to seconds and produces higher-quality results compared to previous works.",
    "arxiv_url": "http://arxiv.org/abs/2412.00518v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00518v1",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LineGS : 3D Line Segment Representation on 3D Gaussian Splatting",
    "authors": [
      "Chenggang Yang",
      "Yuang Shi"
    ],
    "abstract": "Abstract representations of 3D scenes play a crucial role in computer vision, enabling a wide range of applications such as mapping, localization, surface reconstruction, and even advanced tasks like SLAM and rendering. Among these representations, line segments are widely used because of their ability to succinctly capture the structural features of a scene. However, existing 3D reconstruction methods often face significant challenges. Methods relying on 2D projections suffer from instability caused by errors in multi-view matching and occlusions, while direct 3D approaches are hampered by noise and sparsity in 3D point cloud data. This paper introduces LineGS, a novel method that combines geometry-guided 3D line reconstruction with a 3D Gaussian splatting model to address these challenges and improve representation ability. The method leverages the high-density Gaussian point distributions along the edge of the scene to refine and optimize initial line segments generated from traditional geometric approaches. By aligning these segments with the underlying geometric features of the scene, LineGS achieves a more precise and reliable representation of 3D structures. The results show significant improvements in both geometric accuracy and model compactness compared to baseline methods.",
    "arxiv_url": "http://arxiv.org/abs/2412.00477v3",
    "pdf_url": "http://arxiv.org/pdf/2412.00477v3",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "face",
      "mapping",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D Boundary Precision",
    "authors": [
      "Zehao Li",
      "Wenwei Han",
      "Yujun Cai",
      "Hao Jiang",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Zhaoqi Wang"
    ],
    "abstract": "While 3D Gaussian Splatting enables high-quality real-time rendering, existing Gaussian-based frameworks for 3D semantic segmentation still face significant challenges in boundary recognition accuracy. To address this, we propose a novel 3DGS-based framework named GradiSeg, incorporating Identity Encoding to construct a deeper semantic understanding of scenes. Our approach introduces two key modules: Identity Gradient Guided Densification (IGD) and Local Adaptive K-Nearest Neighbors (LA-KNN). The IGD module supervises gradients of Identity Encoding to refine Gaussian distributions along object boundaries, aligning them closely with boundary contours. Meanwhile, the LA-KNN module employs position gradients to adaptively establish locality-aware propagation of Identity Encodings, preventing irregular Gaussian spreads near boundaries. We validate the effectiveness of our method through comprehensive experiments. Results show that GradiSeg effectively addresses boundary-related issues, significantly improving segmentation accuracy without compromising scene reconstruction quality. Furthermore, our method's robust segmentation capability and decoupled Identity Encoding representation make it highly suitable for various downstream scene editing tasks, including 3D object removal, swapping and so on.",
    "arxiv_url": "http://arxiv.org/abs/2412.00392v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00392v1",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "face",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting with State-Space Modeling",
    "authors": [
      "Junli Deng",
      "Yihao Luo"
    ],
    "abstract": "Dynamic scene rendering has taken a leap forward with the rise of 4D Gaussian Splatting, but there's still one elusive challenge: how to make 3D Gaussians move through time as naturally as they would in the real world, all while keeping the motion smooth and consistent. In this paper, we unveil a fresh approach that blends state-space modeling with Wasserstein geometry, paving the way for a more fluid and coherent representation of dynamic scenes. We introduce a State Consistency Filter that merges prior predictions with the current observations, enabling Gaussians to stay true to their way over time. We also employ Wasserstein distance regularization to ensure smooth, consistent updates of Gaussian parameters, reducing motion artifacts. Lastly, we leverage Wasserstein geometry to capture both translational motion and shape deformations, creating a more physically plausible model for dynamic scenes. Our approach guides Gaussians along their natural way in the Wasserstein space, achieving smoother, more realistic motion and stronger temporal coherence. Experimental results show significant improvements in rendering quality and efficiency, outperforming current state-of-the-art techniques.",
    "arxiv_url": "http://arxiv.org/abs/2412.00333v3",
    "pdf_url": "http://arxiv.org/pdf/2412.00333v3",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "geometry",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting",
    "authors": [
      "Zixuan Chen",
      "Guangcong Wang",
      "Jiahao Zhu",
      "Jianhuang Lai",
      "Xiaohua Xie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for various applications. However, considering security, capacity, invisibility, and training efficiency, the copyright of 3DGS assets is not well protected as existing watermarking methods are unsuited for its rendering pipeline. In this paper, we propose GuardSplat, an innovative and efficient framework for watermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline for optimizing the message decoder with minimal costs. The key objective is to achieve high-accuracy extraction by leveraging CLIP's aligning capability and rich representations, demonstrating exceptional capacity and efficiency. 2) We tailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS, seamlessly embedding messages into the SH features of each 3D Gaussian while preserving the original 3D structure. This enables watermarking 3DGS assets with minimal fidelity trade-offs and prevents malicious users from removing the watermarks from the model files, meeting the demands for invisibility and security. 3) We present an Anti-distortion Message Extraction module to improve robustness against various distortions. Experiments demonstrate that GuardSplat outperforms state-of-the-art and achieves fast optimization speed. Project page is at https://narcissusex.github.io/GuardSplat, and Code is at https://github.com/NarcissusEx/GuardSplat.",
    "arxiv_url": "http://arxiv.org/abs/2411.19895v5",
    "pdf_url": "http://arxiv.org/pdf/2411.19895v5",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "github_url": "https://github.com/NarcissusEx/GuardSplat",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering",
    "authors": [
      "Yihao Wang",
      "Marcus Klasson",
      "Matias Turkulainen",
      "Shuzhe Wang",
      "Juho Kannala",
      "Arno Solin"
    ],
    "abstract": "Gaussian splatting enables fast novel view synthesis in static 3D environments. However, reconstructing real-world environments remains challenging as distractors or occluders break the multi-view consistency assumption required for accurate 3D reconstruction. Most existing methods rely on external semantic information from pre-trained models, introducing additional computational overhead as pre-processing steps or during optimization. In this work, we propose a novel method, DeSplat, that directly separates distractors and static scene elements purely based on volume rendering of Gaussian primitives. We initialize Gaussians within each camera view for reconstructing the view-specific distractors to separately model the static 3D scene and distractors in the alpha compositing stages. DeSplat yields an explicit scene separation of static elements and distractors, achieving comparable results to prior distractor-free approaches without sacrificing rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data sets for distractor-free novel view synthesis. See the project website at https://aaltoml.github.io/desplat/.",
    "arxiv_url": "http://arxiv.org/abs/2411.19756v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19756v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "fast",
      "semantic",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TexGaussian: Generating High-quality PBR Material via Octree-based 3D Gaussian Splatting",
    "authors": [
      "Bojun Xiong",
      "Jialun Liu",
      "Jiakui Hu",
      "Chenming Wu",
      "Jinbo Wu",
      "Xing Liu",
      "Chen Zhao",
      "Errui Ding",
      "Zhouhui Lian"
    ],
    "abstract": "Physically Based Rendering (PBR) materials play a crucial role in modern graphics, enabling photorealistic rendering across diverse environment maps. Developing an effective and efficient algorithm that is capable of automatically generating high-quality PBR materials rather than RGB texture for 3D meshes can significantly streamline the 3D content creation. Most existing methods leverage pre-trained 2D diffusion models for multi-view image synthesis, which often leads to severe inconsistency between the generated textures and input 3D meshes. This paper presents TexGaussian, a novel method that uses octant-aligned 3D Gaussian Splatting for rapid PBR material generation. Specifically, we place each 3D Gaussian on the finest leaf node of the octree built from the input 3D mesh to render the multi-view images not only for the albedo map but also for roughness and metallic. Moreover, our model is trained in a regression manner instead of diffusion denoising, capable of generating the PBR material for a 3D mesh in a single feed-forward process. Extensive experiments on publicly available benchmarks demonstrate that our method synthesizes more visually pleasing PBR materials and runs faster than previous methods in both unconditional and text-conditional scenarios, exhibiting better consistency with the given geometry. Our code and trained models are available at https://3d-aigc.github.io/TexGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2411.19654v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19654v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tortho-Gaussian: Splatting True Digital Orthophoto Maps",
    "authors": [
      "Xin Wang",
      "Wendi Zhang",
      "Hong Xie",
      "Haibin Ai",
      "Qiangqiang Yuan",
      "Zongqian Zhan"
    ],
    "abstract": "True Digital Orthophoto Maps (TDOMs) are essential products for digital twins and Geographic Information Systems (GIS). Traditionally, TDOM generation involves a complex set of traditional photogrammetric process, which may deteriorate due to various challenges, including inaccurate Digital Surface Model (DSM), degenerated occlusion detections, and visual artifacts in weak texture regions and reflective surfaces, etc. To address these challenges, we introduce TOrtho-Gaussian, a novel method inspired by 3D Gaussian Splatting (3DGS) that generates TDOMs through orthogonal splatting of optimized anisotropic Gaussian kernel. More specifically, we first simplify the orthophoto generation by orthographically splatting the Gaussian kernels onto 2D image planes, formulating a geometrically elegant solution that avoids the need for explicit DSM and occlusion detection. Second, to produce TDOM of large-scale area, a divide-and-conquer strategy is adopted to optimize memory usage and time efficiency of training and rendering for 3DGS. Lastly, we design a fully anisotropic Gaussian kernel that adapts to the varying characteristics of different regions, particularly improving the rendering quality of reflective surfaces and slender structures. Extensive experimental evaluations demonstrate that our method outperforms existing commercial software in several aspects, including the accuracy of building boundaries, the visual quality of low-texture regions and building facades. These results underscore the potential of our approach for large-scale urban scene reconstruction, offering a robust alternative for enhancing TDOM quality and scalability.",
    "arxiv_url": "http://arxiv.org/abs/2411.19594v1",
    "pdf_url": "http://arxiv.org/pdf/2411.19594v1",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splashing: Direct Volumetric Rendering Underwater",
    "authors": [
      "Nir Mualem",
      "Roy Amoyal",
      "Oren Freifeld",
      "Derya Akkaynak"
    ],
    "abstract": "In underwater images, most useful features are occluded by water. The extent of the occlusion depends on imaging geometry and can vary even across a sequence of burst images. As a result, 3D reconstruction methods robust on in-air scenes, like Neural Radiance Field methods (NeRFs) or 3D Gaussian Splatting (3DGS), fail on underwater scenes. While a recent underwater adaptation of NeRFs achieved state-of-the-art results, it is impractically slow: reconstruction takes hours and its rendering rate, in frames per second (FPS), is less than 1. Here, we present a new method that takes only a few minutes for reconstruction and renders novel underwater scenes at 140 FPS. Named Gaussian Splashing, our method unifies the strengths and speed of 3DGS with an image formation model for capturing scattering, introducing innovations in the rendering and depth estimation procedures and in the 3DGS loss function. Despite the complexities of underwater adaptation, our method produces images at unparalleled speeds with superior details. Moreover, it reveals distant scene details with far greater clarity than other methods, dramatically improving reconstructed and rendered images. We demonstrate results on existing datasets and a new dataset we have collected.   Additional visual results are available at: https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/ .",
    "arxiv_url": "http://arxiv.org/abs/2411.19588v1",
    "pdf_url": "http://arxiv.org/pdf/2411.19588v1",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bootstraping Clustering of Gaussians for View-consistent 3D Scene Understanding",
    "authors": [
      "Wenbo Zhang",
      "Lu Zhang",
      "Ping Hu",
      "Liqian Ma",
      "Yunzhi Zhuge",
      "Huchuan Lu"
    ],
    "abstract": "Injecting semantics into 3D Gaussian Splatting (3DGS) has recently garnered significant attention. While current approaches typically distill 3D semantic features from 2D foundational models (e.g., CLIP and SAM) to facilitate novel view segmentation and semantic understanding, their heavy reliance on 2D supervision can undermine cross-view semantic consistency and necessitate complex data preparation processes, therefore hindering view-consistent scene understanding. In this work, we present FreeGS, an unsupervised semantic-embedded 3DGS framework that achieves view-consistent 3D scene understanding without the need for 2D labels. Instead of directly learning semantic features, we introduce the IDentity-coupled Semantic Field (IDSF) into 3DGS, which captures both semantic representations and view-consistent instance indices for each Gaussian. We optimize IDSF with a two-step alternating strategy: semantics help to extract coherent instances in 3D space, while the resulting instances regularize the injection of stable semantics from 2D space. Additionally, we adopt a 2D-3D joint contrastive loss to enhance the complementarity between view-consistent 3D geometry and rich semantics during the bootstrapping process, enabling FreeGS to uniformly perform tasks such as novel-view semantic segmentation, object selection, and 3D object detection. Extensive experiments on LERF-Mask, 3D-OVS, and ScanNet datasets demonstrate that FreeGS performs comparably to state-of-the-art methods while avoiding the complex data preprocessing workload. Our code is publicly available at https://github.com/wb014/FreeGS.",
    "arxiv_url": "http://arxiv.org/abs/2411.19551v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19551v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/wb014/FreeGS",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration",
    "authors": [
      "Chaojun Ni",
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Chen Liu",
      "Yuyin Chen",
      "Yida Wang",
      "Xueyang Zhang",
      "Yifei Zhan",
      "Kun Zhan",
      "Peng Jia",
      "Xianpeng Lang",
      "Xingang Wang",
      "Wenjun Mei"
    ],
    "abstract": "Closed-loop simulation is crucial for end-to-end autonomous driving. Existing sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes based on conditions that closely mirror training data distributions. However, these methods struggle with rendering novel trajectories, such as lane changes. Recent works have demonstrated that integrating world model knowledge alleviates these issues. Despite their efficiency, these approaches still encounter difficulties in the accurate representation of more complex maneuvers, with multi-lane shifts being a notable example. Therefore, we introduce ReconDreamer, which enhances driving scene reconstruction through incremental integration of world model knowledge. Specifically, DriveRestorer is proposed to mitigate artifacts via online restoration. This is complemented by a progressive data update strategy designed to ensure high-quality rendering for more complex maneuvers. To the best of our knowledge, ReconDreamer is the first method to effectively render in large maneuvers. Experimental results demonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU, NTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%. Furthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large maneuver rendering, as verified by a relative improvement of 195.87% in the NTA-IoU metric and a comprehensive user study.",
    "arxiv_url": "http://arxiv.org/abs/2411.19548v1",
    "pdf_url": "http://arxiv.org/pdf/2411.19548v1",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "nerf",
      "4d",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "T-3DGS: Removing Transient Objects for 3D Scene Reconstruction",
    "authors": [
      "Alexander Markin",
      "Vadim Pryadilshchikov",
      "Artem Komarichev",
      "Ruslan Rakhimov",
      "Peter Wonka",
      "Evgeny Burnaev"
    ],
    "abstract": "Transient objects in video sequences can significantly degrade the quality of 3D scene reconstructions. To address this challenge, we propose T-3DGS, a novel framework that robustly filters out transient distractors during 3D reconstruction using Gaussian Splatting. Our framework consists of two steps. First, we employ an unsupervised classification network that distinguishes transient objects from static scene elements by leveraging their distinct training dynamics within the reconstruction process. Second, we refine these initial detections by integrating an off-the-shelf segmentation method with a bidirectional tracking module, which together enhance boundary accuracy and temporal coherence. Evaluations on both sparsely and densely captured video datasets demonstrate that T-3DGS significantly outperforms state-of-the-art approaches, enabling high-fidelity 3D reconstructions in challenging, real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2412.00155v2",
    "pdf_url": "http://arxiv.org/pdf/2412.00155v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction",
    "authors": [
      "Jiepeng Wang",
      "Yuan Liu",
      "Peng Wang",
      "Cheng Lin",
      "Junhui Hou",
      "Xin Li",
      "Taku Komura",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting has achieved impressive performance in novel view synthesis with real-time rendering capabilities. However, reconstructing high-quality surfaces with fine details using 3D Gaussians remains a challenging task. In this work, we introduce GausSurf, a novel approach to high-quality surface reconstruction by employing geometry guidance from multi-view consistency in texture-rich areas and normal priors in texture-less areas of a scene. We observe that a scene can be mainly divided into two primary regions: 1) texture-rich and 2) texture-less areas. To enforce multi-view consistency at texture-rich areas, we enhance the reconstruction quality by incorporating a traditional patch-match based Multi-View Stereo (MVS) approach to guide the geometry optimization in an iterative scheme. This scheme allows for mutual reinforcement between the optimization of Gaussians and patch-match refinement, which significantly improves the reconstruction results and accelerates the training process. Meanwhile, for the texture-less areas, we leverage normal priors from a pre-trained normal estimation model to guide optimization. Extensive experiments on the DTU and Tanks and Temples datasets demonstrate that our method surpasses state-of-the-art methods in terms of reconstruction quality and computation time.",
    "arxiv_url": "http://arxiv.org/abs/2411.19454v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19454v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RF-3DGS: Wireless Channel Modeling with Radio Radiance Field and 3D Gaussian Splatting",
    "authors": [
      "Lihao Zhang",
      "Haijian Sun",
      "Samuel Berweger",
      "Camillo Gentile",
      "Rose Qingyang Hu"
    ],
    "abstract": "Precisely modeling radio propagation in complex environments has been a significant challenge, especially with the advent of 5G and beyond networks, where managing massive antenna arrays demands more detailed information. Traditional methods, such as empirical models and ray tracing, often fall short, either due to insufficient details or because of challenges for real-time applications. Inspired by the newly proposed 3D Gaussian Splatting method in the computer vision domain, which outperforms other methods in reconstructing optical radiance fields, we propose RF-3DGS, a novel approach that enables precise site-specific reconstruction of radio radiance fields from sparse samples. RF-3DGS can render radio spatial spectra at arbitrary positions within 2 ms following a brief 3-minute training period, effectively identifying dominant propagation paths. Furthermore, RF-3DGS can provide fine-grained Spatial Channel State Information (Spatial-CSI) of these paths, including the channel gain, the delay, the angle of arrival (AoA), and the angle of departure (AoD). Our experiments, calibrated through real-world measurements, demonstrate that RF-3DGS not only significantly improves reconstruction quality, training efficiency, and rendering speed compared to state-of-the-art methods, but also holds great potential for supporting wireless communication and advanced applications such as Integrated Sensing and Communication (ISAC). Code and dataset will be available at https://github.com/SunLab-UGA/RF-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2411.19420v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19420v2",
    "published_date": "2024-11-29",
    "categories": [
      "cs.NI"
    ],
    "github_url": "https://github.com/SunLab-UGA/RF-3DGS",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "ray tracing"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAMa: Material-aware 3D Selection and Segmentation",
    "authors": [
      "Michael Fischer",
      "Iliyan Georgiev",
      "Thibault Groueix",
      "Vladimir G. Kim",
      "Tobias Ritschel",
      "Valentin Deschaintre"
    ],
    "abstract": "Decomposing 3D assets into material parts is a common task for artists and creators, yet remains a highly manual process. In this work, we introduce Select Any Material (SAMa), a material selection approach for various 3D representations. Building on the recently introduced SAM2 video selection model, we extend its capabilities to the material domain. We leverage the model's cross-view consistency to create a 3D-consistent intermediate material-similarity representation in the form of a point cloud from a sparse set of views. Nearest-neighbour lookups in this similarity cloud allow us to efficiently reconstruct accurate continuous selection masks over objects' surfaces that can be inspected from any view. Our method is multiview-consistent by design, alleviating the need for contrastive learning or feature-field pre-processing, and performs optimization-free selection in seconds. Our approach works on arbitrary 3D representations and outperforms several strong baselines in terms of selection accuracy and multiview consistency. It enables several compelling applications, such as replacing the diffuse-textured materials on a text-to-3D output, or selecting and editing materials on NeRFs and 3D-Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2411.19322v1",
    "pdf_url": "http://arxiv.org/pdf/2411.19322v1",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation",
    "authors": [
      "Yichong Lu",
      "Yichi Cai",
      "Shangzhan Zhang",
      "Hongyu Zhou",
      "Haoji Hu",
      "Huimin Yu",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that generates highly controllable and photorealistic 3D vehicle digital twins from a single urban image, leveraging a large collection of free 3D CAD models and handcrafted materials. To achieve this, we propose a novel pipeline that follows a retrieval-optimization manner, adapting to observational data while preserving fine-grained expert-designed priors for both geometry and material. This enables vehicles' realistic 360-degree rendering, background insertion, material transfer, relighting, and component manipulation. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications.",
    "arxiv_url": "http://arxiv.org/abs/2411.19292v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19292v2",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "autonomous driving",
      "relighting",
      "lighting",
      "geometry",
      "ar",
      "urban scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SADG: Segment Any Dynamic Gaussian Without Object Trackers",
    "authors": [
      "Yun-Jin Li",
      "Mariia Gladkova",
      "Yan Xia",
      "Daniel Cremers"
    ],
    "abstract": "Understanding dynamic 3D scenes is fundamental for various applications, including extended reality (XR) and autonomous driving. Effectively integrating semantic information into 3D reconstruction enables holistic representation that opens opportunities for immersive and interactive applications. We introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines dynamic Gaussian Splatting representation and semantic information without reliance on object IDs. In contrast to existing works, we do not rely on supervision based on object identities to enable consistent segmentation of dynamic 3D objects. To this end, we propose to learn semantically-aware features by leveraging masks generated from the Segment Anything Model (SAM) and utilizing our novel contrastive learning objective based on hard pixel mining. The learned Gaussian features can be effectively clustered without further post-processing. This enables fast computation for further object-level editing, such as object removal, composition, and style transfer by manipulating the Gaussians in the scene. We further extend several dynamic novel-view datasets with segmentation benchmarks to enable testing of learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks and demonstrate the superior performance of our approach in segmenting objects within dynamic scenes along with its effectiveness for further downstream editing tasks.",
    "arxiv_url": "http://arxiv.org/abs/2411.19290v1",
    "pdf_url": "http://arxiv.org/pdf/2411.19290v1",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "fast",
      "semantic",
      "understanding",
      "segmentation",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones",
    "authors": [
      "Xuqian Ren",
      "Matias Turkulainen",
      "Jiepeng Wang",
      "Otto Seiskari",
      "Iaroslav Melekhov",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "abstract": "Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools. Our code is released in https://xuqianren.github.io/ags_mesh_website/.",
    "arxiv_url": "http://arxiv.org/abs/2411.19271v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19271v2",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unleashing the Power of Data Synthesis in Visual Localization",
    "authors": [
      "Sihang Li",
      "Siqi Tan",
      "Bowen Chang",
      "Jing Zhang",
      "Chen Feng",
      "Yiming Li"
    ],
    "abstract": "Visual localization, which estimates a camera's pose within a known scene, is a long-standing challenge in vision and robotics. Recent end-to-end methods that directly regress camera poses from query images have gained attention for fast inference. However, existing methods often struggle to generalize to unseen views. In this work, we aim to unleash the power of data synthesis to promote the generalizability of pose regression. Specifically, we lift real 2D images into 3D Gaussian Splats with varying appearance and deblurring abilities, which are then used as a data engine to synthesize more posed images. To fully leverage the synthetic data, we build a two-branch joint training pipeline, with an adversarial discriminator to bridge the syn-to-real gap. Experiments on established benchmarks show that our method outperforms state-of-the-art end-to-end approaches, reducing translation and rotation errors by 50% and 21.6% on indoor datasets, and 35.56% and 38.7% on outdoor datasets. We also validate the effectiveness of our method in dynamic driving scenarios under varying weather conditions. Notably, as data synthesis scales up, our method exhibits a growing ability to interpolate and extrapolate training data for localizing unseen views. Project Page: https://ai4ce.github.io/RAP/",
    "arxiv_url": "http://arxiv.org/abs/2412.00138v1",
    "pdf_url": "http://arxiv.org/pdf/2412.00138v1",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "outdoor",
      "fast",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for 3D Instance-Level Perception",
    "authors": [
      "Haijie Li",
      "Yanmin Wu",
      "Jiarui Meng",
      "Qiankun Gao",
      "Zhiyao Zhang",
      "Ronggang Wang",
      "Jian Zhang"
    ],
    "abstract": "3D scene understanding has become an essential area of research with applications in autonomous driving, robotics, and augmented reality. Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful approach, combining explicit modeling with neural adaptability to provide efficient and detailed scene representations. However, three major challenges remain in leveraging 3DGS for scene understanding: 1) an imbalance between appearance and semantics, where dense Gaussian usage for fine-grained texture modeling does not align with the minimal requirements for semantic attributes; 2) inconsistencies between appearance and semantics, as purely appearance-based Gaussians often misrepresent object boundaries; and 3) reliance on top-down instance segmentation methods, which struggle with uneven category distributions, leading to over- or under-segmentation. In this work, we propose InstanceGaussian, a method that jointly learns appearance and semantic features while adaptively aggregating instances. Our contributions include: i) a novel Semantic-Scaffold-GS representation balancing appearance and semantics to improve feature representations and boundary delineation; ii) a progressive appearance-semantic joint training strategy to enhance stability and segmentation accuracy; and iii) a bottom-up, category-agnostic instance aggregation approach that addresses segmentation challenges through farthest point sampling and connected component analysis. Our approach achieves state-of-the-art performance in category-agnostic, open-vocabulary 3D point-level segmentation, highlighting the effectiveness of the proposed representation and training strategies. Project page: https://lhj-git.github.io/InstanceGaussian/",
    "arxiv_url": "http://arxiv.org/abs/2411.19235v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19235v2",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "autonomous driving",
      "lighting",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes",
    "authors": [
      "Thomas Wimmer",
      "Michael Oechsle",
      "Michael Niemeyer",
      "Federico Tombari"
    ],
    "abstract": "State-of-the-art novel view synthesis methods achieve impressive results for multi-view captures of static 3D scenes. However, the reconstructed scenes still lack \"liveliness,\" a key component for creating engaging 3D experiences. Recently, novel video diffusion models generate realistic videos with complex motion and enable animations of 2D images, however they cannot naively be used to animate 3D scenes as they lack multi-view consistency. To breathe life into the static world, we propose Gaussians2Life, a method for animating parts of high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is to leverage powerful video diffusion models as the generative component of our model and to combine these with a robust technique to lift 2D videos into meaningful 3D motion. We find that, in contrast to prior work, this enables realistic animations of complex, pre-existing 3D scenes and further enables the animation of a large variety of object classes, while related work is mostly focused on prior-based character animation, or single 3D objects. Our model enables the creation of consistent, immersive 3D experiences for arbitrary scenes.",
    "arxiv_url": "http://arxiv.org/abs/2411.19233v2",
    "pdf_url": "http://arxiv.org/pdf/2411.19233v2",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors",
    "authors": [
      "Rui Xu",
      "Wenyue Chen",
      "Jiepeng Wang",
      "Yuan Liu",
      "Peng Wang",
      "Lin Gao",
      "Shiqing Xin",
      "Taku Komura",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "Gaussian Splattings demonstrate impressive results in multi-view reconstruction based on Gaussian explicit representations. However, the current Gaussian primitives only have a single view-dependent color and an opacity to represent the appearance and geometry of the scene, resulting in a non-compact representation. In this paper, we introduce a new method called SuperGaussians that utilizes spatially varying colors and opacity in a single Gaussian primitive to improve its representation ability. We have implemented bilinear interpolation, movable kernels, and even tiny neural networks as spatially varying functions. Quantitative and qualitative experimental results demonstrate that all three functions outperform the baseline, with the best movable kernels achieving superior novel view synthesis performance on multiple datasets, highlighting the strong potential of spatially varying functions.",
    "arxiv_url": "http://arxiv.org/abs/2411.18966v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18966v1",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "geometry",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RIGI: Rectifying Image-to-3D Generation Inconsistency via Uncertainty-aware Learning",
    "authors": [
      "Jiacheng Wang",
      "Zhedong Zheng",
      "Wei Xu",
      "Ping Liu"
    ],
    "abstract": "Given a single image of a target object, image-to-3D generation aims to reconstruct its texture and geometric shape. Recent methods often utilize intermediate media, such as multi-view images or videos, to bridge the gap between input image and the 3D target, thereby guiding the generation of both shape and texture. However, inconsistencies in the generated multi-view snapshots frequently introduce noise and artifacts along object boundaries, undermining the 3D reconstruction process. To address this challenge, we leverage 3D Gaussian Splatting (3DGS) for 3D reconstruction, and explicitly integrate uncertainty-aware learning into the reconstruction process. By capturing the stochasticity between two Gaussian models, we estimate an uncertainty map, which is subsequently used for uncertainty-aware regularization to rectify the impact of inconsistencies. Specifically, we optimize both Gaussian models simultaneously, calculating the uncertainty map by evaluating the discrepancies between rendered images from identical viewpoints. Based on the uncertainty map, we apply adaptive pixel-wise loss weighting to regularize the models, reducing reconstruction intensity in high-uncertainty regions. This approach dynamically detects and mitigates conflicts in multi-view labels, leading to smoother results and effectively reducing artifacts. Extensive experiments show the effectiveness of our method in improving 3D generation quality by reducing inconsistencies and artifacts.",
    "arxiv_url": "http://arxiv.org/abs/2411.18866v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18866v1",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling",
    "authors": [
      "Brian Chao",
      "Hung-Yu Tseng",
      "Lorenzo Porzi",
      "Chen Gao",
      "Tuotuo Li",
      "Qinbo Li",
      "Ayush Saraf",
      "Jia-Bin Huang",
      "Johannes Kopf",
      "Gordon Wetzstein",
      "Changil Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2411.18625v2",
    "pdf_url": "http://arxiv.org/pdf/2411.18625v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
    "authors": [
      "Rundi Wu",
      "Ruiqi Gao",
      "Ben Poole",
      "Alex Trevithick",
      "Changxi Zheng",
      "Jonathan T. Barron",
      "Aleksander Holynski"
    ],
    "abstract": "We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: https://cat-4d.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2411.18613v2",
    "pdf_url": "http://arxiv.org/pdf/2411.18613v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "4d",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSpeech: Audio-Driven Gaussian Avatars",
    "authors": [
      "Shivangi Aneja",
      "Artem Sevastopolsky",
      "Tobias Kirschstein",
      "Justus Thies",
      "Angela Dai",
      "Matthias Nie√üner"
    ],
    "abstract": "We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity animation sequences of photo-realistic, personalized 3D human head avatars from spoken audio. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple speech signal with 3D Gaussian splatting to create realistic, temporally coherent motion sequences. We propose a compact and efficient 3DGS-based avatar representation that generates expression-dependent color and leverages wrinkle- and perceptually-based losses to synthesize facial details, including wrinkles that occur with different expressions. To enable sequence modeling of 3D Gaussian splats with audio, we devise an audio-conditioned transformer model capable of extracting lip and expression features directly from audio input. Due to the absence of high-quality datasets of talking humans in correspondence with audio, we captured a new large-scale multi-view dataset of audio-visual sequences of talking humans with native English accents and diverse facial geometry. GaussianSpeech consistently achieves state-of-the-art performance with visually natural motion at real time rendering rates, while encompassing diverse facial expressions and styles.",
    "arxiv_url": "http://arxiv.org/abs/2411.18675v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18675v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "motion",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "compact",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a Single Image",
    "authors": [
      "Han Yan",
      "Mingrui Zhang",
      "Yang Li",
      "Chao Ma",
      "Pan Ji"
    ],
    "abstract": "We present PhyCAGE, the first approach for physically plausible compositional 3D asset generation from a single image. Given an input image, we first generate consistent multi-view images for components of the assets. These images are then fitted with 3D Gaussian Splatting representations. To ensure that the Gaussians representing objects are physically compatible with each other, we introduce a Physical Simulation-Enhanced Score Distillation Sampling (PSE-SDS) technique to further optimize the positions of the Gaussians. It is achieved by setting the gradient of the SDS loss as the initial velocity of the physical simulation, allowing the simulator to act as a physics-guided optimizer that progressively corrects the Gaussians' positions to a physically compatible state. Experimental results demonstrate that the proposed method can generate physically plausible compositional 3D assets given a single image.",
    "arxiv_url": "http://arxiv.org/abs/2411.18548v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18548v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Point Cloud Unsupervised Pre-training via 3D Gaussian Splatting",
    "authors": [
      "Hao Liu",
      "Minglin Chen",
      "Yanni Ma",
      "Haihong Xiao",
      "Ying He"
    ],
    "abstract": "Pre-training on large-scale unlabeled datasets contribute to the model achieving powerful performance on 3D vision tasks, especially when annotations are limited. However, existing rendering-based self-supervised frameworks are computationally demanding and memory-intensive during pre-training due to the inherent nature of volume rendering. In this paper, we propose an efficient framework named GS$^3$ to learn point cloud representation, which seamlessly integrates fast 3D Gaussian Splatting into the rendering-based framework. The core idea behind our framework is to pre-train the point cloud encoder by comparing rendered RGB images with real RGB images, as only Gaussian points enriched with learned rich geometric and appearance information can produce high-quality renderings. Specifically, we back-project the input RGB-D images into 3D space and use a point cloud encoder to extract point-wise features. Then, we predict 3D Gaussian points of the scene from the learned point cloud features and uses a tile-based rasterizer for image rendering. Finally, the pre-trained point cloud encoder can be fine-tuned to adapt to various downstream 3D tasks, including high-level perception tasks such as 3D segmentation and detection, as well as low-level tasks such as 3D scene reconstruction. Extensive experiments on downstream tasks demonstrate the strong transferability of the pre-trained point cloud encoder and the effectiveness of our self-supervised learning framework. In addition, our GS$^3$ framework is highly efficient, achieving approximately 9$\\times$ pre-training speedup and less than 0.25$\\times$ memory cost compared to the previous rendering-based framework Ponder.",
    "arxiv_url": "http://arxiv.org/abs/2411.18667v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18667v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression",
    "authors": [
      "Lei Liu",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Dong Xu"
    ],
    "abstract": "In this work, we propose a novel compression framework for 3D Gaussian Splatting (3DGS) data. Building on anchor-based 3DGS methodologies, our approach compresses all attributes within each anchor by introducing a novel Hybrid Entropy Model for 3D Gaussian Splatting (HEMGS) to achieve hybrid lossy-lossless compression. It consists of three main components: a variable-rate predictor, a hyperprior network, and an autoregressive network. First, unlike previous methods that adopt multiple models to achieve multi-rate lossy compression, thereby increasing training overhead, our variable-rate predictor enables variable-rate compression with a single model and a hyperparameter $\\lambda$ by producing a learned Quantization Step feature for versatile lossy compression. Second, to improve lossless compression, the hyperprior network captures both scene-agnostic and scene-specific features to generate a prior feature, while the autoregressive network employs an adaptive context selection algorithm with flexible receptive fields to produce a contextual feature. By integrating these two features, HEMGS can accurately estimate the distribution of the current coding element within each attribute, enabling improved entropy coding and reduced storage. We integrate HEMGS into a compression framework, and experimental results on four benchmarks indicate that HEMGS achieves about a 40% average reduction in size while maintaining rendering quality over baseline methods and achieving state-of-the-art compression results.",
    "arxiv_url": "http://arxiv.org/abs/2411.18473v2",
    "pdf_url": "http://arxiv.org/pdf/2411.18473v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Surface Priors for Editable Gaussian Splatting",
    "authors": [
      "Jakub Szymkowiak",
      "Weronika Jakubowska",
      "Dawid Malarz",
      "Weronika Smolak-Dy≈ºewska",
      "Maciej Ziƒôba",
      "Przemyslaw Musialski",
      "Wojtek Pa≈Çubicki",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "In computer graphics and vision, recovering easily modifiable scene appearance from image data is crucial for applications such as content creation. We introduce a novel method that integrates 3D Gaussian Splatting with an implicit surface representation, enabling intuitive editing of recovered scenes through mesh manipulation. Starting with a set of input images and camera poses, our approach reconstructs the scene surface using a neural signed distance field. This neural surface acts as a geometric prior guiding the training of Gaussian Splatting components, ensuring their alignment with the scene geometry. To facilitate editing, we encode the visual and geometric information into a lightweight triangle soup proxy. Edits applied to the mesh extracted from the neural surface propagate seamlessly through this intermediate structure to update the recovered appearance. Unlike previous methods relying on the triangle soup proxy representation, our approach supports a wider range of modifications and fully leverages the mesh topology, enabling a more flexible and intuitive editing process. The complete source code for this project can be accessed at: https://github.com/WJakubowska/NeuralSurfacePriors.",
    "arxiv_url": "http://arxiv.org/abs/2411.18311v2",
    "pdf_url": "http://arxiv.org/pdf/2411.18311v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/WJakubowska/NeuralSurfacePriors",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
    "authors": [
      "Zhiyang Guo",
      "Jinxu Xiang",
      "Kai Ma",
      "Wengang Zhou",
      "Houqiang Li",
      "Ran Zhang"
    ],
    "abstract": "3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed. More demos and code are available at https://jasongzy.github.io/Make-It-Animatable/.",
    "arxiv_url": "http://arxiv.org/abs/2411.18197v3",
    "pdf_url": "http://arxiv.org/pdf/2411.18197v3",
    "published_date": "2024-11-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmileSplat: Generalizable Gaussian Splats for Unconstrained Sparse Images",
    "authors": [
      "Yanyan Li",
      "Yixin Fang",
      "Federico Tombari",
      "Gim Hee Lee"
    ],
    "abstract": "Sparse Multi-view Images can be Learned to predict explicit radiance fields via Generalizable Gaussian Splatting approaches, which can achieve wider application prospects in real-life when ground-truth camera parameters are not required as inputs. In this paper, a novel generalizable Gaussian Splatting method, SmileSplat, is proposed to reconstruct pixel-aligned Gaussian surfels for diverse scenarios only requiring unconstrained sparse multi-view images. First, Gaussian surfels are predicted based on the multi-head Gaussian regression decoder, which can are represented with less degree-of-freedom but have better multi-view consistency. Furthermore, the normal vectors of Gaussian surfel are enhanced based on high-quality of normal priors. Second, the Gaussians and camera parameters (both extrinsic and intrinsic) are optimized to obtain high-quality Gaussian radiance fields for novel view synthesis tasks based on the proposed Bundle-Adjusting Gaussian Splatting module. Extensive experiments on novel view rendering and depth map prediction tasks are conducted on public datasets, demonstrating that the proposed method achieves state-of-the-art performance in various 3D vision tasks. More information can be found on our project page (https://yanyan-li.github.io/project/gs/smilesplat)",
    "arxiv_url": "http://arxiv.org/abs/2411.18072v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18072v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GLS: Geometry-aware 3D Language Gaussian Splatting",
    "authors": [
      "Jiaxiong Qiu",
      "Liu Liu",
      "Zhizhong Su",
      "Tianwei Lin"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has achieved significant performance on indoor surface reconstruction and open-vocabulary segmentation. This paper presents GLS, a unified framework of surface reconstruction and open-vocabulary segmentation based on 3DGS. GLS extends two fields by exploring the correlation between them. For indoor surface reconstruction, we introduce surface normal prior as a geometric cue to guide the rendered normal, and use the normal error to optimize the rendered depth. For open-vocabulary segmentation, we employ 2D CLIP features to guide instance features and utilize DEVA masks to enhance their view consistency. Extensive experiments demonstrate the effectiveness of jointly optimizing surface reconstruction and open-vocabulary segmentation, where GLS surpasses state-of-the-art approaches of each task on MuSHRoom, ScanNet++, and LERF-OVS datasets. Code will be available at https://github.com/JiaxiongQ/GLS.",
    "arxiv_url": "http://arxiv.org/abs/2411.18066v1",
    "pdf_url": "http://arxiv.org/pdf/2411.18066v1",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/JiaxiongQ/GLS",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction",
    "authors": [
      "Wei Zhang",
      "Qing Cheng",
      "David Skuddis",
      "Niclas Zeller",
      "Daniel Cremers",
      "Norbert Haala"
    ],
    "abstract": "We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2411.17982v2",
    "pdf_url": "http://arxiv.org/pdf/2411.17982v2",
    "published_date": "2024-11-27",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting",
    "authors": [
      "Christian Homeyer",
      "Leon Begiristain",
      "Christoph Schn√∂rr"
    ],
    "abstract": "Recent progress in scene synthesis makes standalone SLAM systems purely based on optimizing hyperprimitives with a Rendering objective possible. However, the tracking performance still lacks behind traditional and end-to-end SLAM systems. An optimal trade-off between robustness, speed and accuracy has not yet been reached, especially for monocular video. In this paper, we introduce a SLAM system based on an end-to-end Tracker and extend it with a Renderer based on recent 3D Gaussian Splatting techniques. Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering results on common SLAM benchmarks. We implemented multiple building blocks of modern SLAM systems to run in parallel, allowing for fast inference on common consumer GPU's. Recent progress in monocular depth prediction and camera calibration allows our system to achieve strong results even on in-the-wild data without known camera intrinsics. Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}.",
    "arxiv_url": "http://arxiv.org/abs/2411.17660v2",
    "pdf_url": "http://arxiv.org/pdf/2411.17660v2",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ChenHoy/DROID-Splat",
    "keywords": [
      "tracking",
      "fast",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distractor-free Generalizable 3D Gaussian Splatting",
    "authors": [
      "Yanqi Bao",
      "Jing Liao",
      "Jing Huo",
      "Yang Gao"
    ],
    "abstract": "We present DGGS, a novel framework that addresses the previously unexplored challenge: $\\textbf{Distractor-free Generalizable 3D Gaussian Splatting}$ (3DGS). It mitigates 3D inconsistency and training instability caused by distractor data in the cross-scenes generalizable train setting while enabling feedforward inference for 3DGS and distractor masks from references in the unseen scenes. To achieve these objectives, DGGS proposes a scene-agnostic reference-based mask prediction and refinement module during the training phase, effectively eliminating the impact of distractor on training stability. Moreover, we combat distractor-induced artifacts and holes at inference time through a novel two-stage inference framework for references scoring and re-selection, complemented by a distractor pruning mechanism that further removes residual distractor 3DGS-primitive influences. Extensive feedforward experiments on the real and our synthetic data show DGGS's reconstruction capability when dealing with novel distractor scenes. Moreover, our generalizable mask prediction even achieves an accuracy superior to existing scene-specific training methods. Homepage is https://github.com/bbbbby-99/DGGS.",
    "arxiv_url": "http://arxiv.org/abs/2411.17605v2",
    "pdf_url": "http://arxiv.org/pdf/2411.17605v2",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/bbbbby-99/DGGS",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters",
    "authors": [
      "Mingze Sun",
      "Junhao Chen",
      "Junting Dong",
      "Yurun Chen",
      "Xinyu Jiang",
      "Shiwei Mao",
      "Puhua Jiang",
      "Jingbo Wang",
      "Bo Dai",
      "Ruqi Huang"
    ],
    "abstract": "Recent advances in generative models have enabled high-quality 3D character reconstruction from multi-modal. However, animating these generated characters remains a challenging task, especially for complex elements like garments and hair, due to the lack of large-scale datasets and effective rigging methods. To address this gap, we curate AnimeRig, a large-scale dataset with detailed skeleton and skinning annotations. Building upon this, we propose DRiVE, a novel framework for generating and rigging 3D human characters with intricate structures. Unlike existing methods, DRiVE utilizes a 3D Gaussian representation, facilitating efficient animation and high-quality rendering. We further introduce GSDiff, a 3D Gaussian-based diffusion module that predicts joint positions as spatial distributions, overcoming the limitations of regression-based approaches. Extensive experiments demonstrate that DRiVE achieves precise rigging results, enabling realistic dynamics for clothing and hair, and surpassing previous methods in both quality and versatility. The code and dataset will be made public for academic use upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2411.17423v1",
    "pdf_url": "http://arxiv.org/pdf/2411.17423v1",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting",
    "authors": [
      "Gyeongjin Kang",
      "Jisang Yoo",
      "Jihyeon Park",
      "Seungtae Nam",
      "Hyeonsoo Im",
      "Sangheon Shin",
      "Sangpil Kim",
      "Eunbyung Park"
    ],
    "abstract": "We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at https://gynjn.github.io/selfsplat/",
    "arxiv_url": "http://arxiv.org/abs/2411.17190v5",
    "pdf_url": "http://arxiv.org/pdf/2411.17190v5",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysMotion: Physics-Grounded Dynamics From a Single Image",
    "authors": [
      "Xiyang Tan",
      "Ying Jiang",
      "Xuan Li",
      "Zeshun Zong",
      "Tianyi Xie",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "abstract": "We introduce PhysMotion, a novel framework that leverages principled physics-based simulations to guide intermediate 3D representations generated from a single image and input conditions (e.g., applied force and torque), producing high-quality, physically plausible video generation. By utilizing continuum mechanics-based simulations as a prior knowledge, our approach addresses the limitations of traditional data-driven generative models and result in more consistent physically plausible motions. Our framework begins by reconstructing a feed-forward 3D Gaussian from a single image through geometry optimization. This representation is then time-stepped using a differentiable Material Point Method (MPM) with continuum mechanics-based elastoplasticity models, which provides a strong foundation for realistic dynamics, albeit at a coarse level of detail. To enhance the geometry, appearance and ensure spatiotemporal consistency, we refine the initial simulation using a text-to-image (T2I) diffusion model with cross-frame attention, resulting in a physically plausible video that retains intricate details comparable to the input image. We conduct comprehensive qualitative and quantitative evaluations to validate the efficacy of our method. Our project page is available at: https://supertan0204.github.io/physmotion_website/.",
    "arxiv_url": "http://arxiv.org/abs/2411.17189v2",
    "pdf_url": "http://arxiv.org/pdf/2411.17189v2",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Scaffold Gaussian Splatting for Memory Efficient Dynamic Scene Reconstruction",
    "authors": [
      "Woong Oh Cho",
      "In Cho",
      "Seoha Kim",
      "Jeongmin Bae",
      "Youngjung Uh",
      "Seon Joo Kim"
    ],
    "abstract": "Existing 4D Gaussian methods for dynamic scene reconstruction offer high visual fidelity and fast rendering. However, these methods suffer from excessive memory and storage demands, which limits their practical deployment. This paper proposes a 4D anchor-based framework that retains visual quality and rendering speed of 4D Gaussians while significantly reducing storage costs. Our method extends 3D scaffolding to 4D space, and leverages sparse 4D grid-aligned anchors with compressed feature vectors. Each anchor models a set of neural 4D Gaussians, each of which represent a local spatiotemporal region. In addition, we introduce a temporal coverage-aware anchor growing strategy to effectively assign additional anchors to under-reconstructed dynamic regions. Our method adjusts the accumulated gradients based on Gaussians' temporal coverage, improving reconstruction quality in dynamic regions. To reduce the number of anchors, we further present enhanced formulations of neural 4D Gaussians. These include the neural velocity, and the temporal opacity derived from a generalized Gaussian distribution. Experimental results demonstrate that our method achieves state-of-the-art visual quality and 97.8% storage reduction over 4DGS.",
    "arxiv_url": "http://arxiv.org/abs/2411.17044v1",
    "pdf_url": "http://arxiv.org/pdf/2411.17044v1",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction",
    "authors": [
      "Kunyi Li",
      "Michael Niemeyer",
      "Zeyu Chen",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2411.16898v2",
    "pdf_url": "http://arxiv.org/pdf/2411.16898v2",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence",
    "authors": [
      "Zequn Chen",
      "Jiezhi Yang",
      "Heng Yang"
    ],
    "abstract": "We present PreF3R, Pose-Free Feed-forward 3D Reconstruction from an image sequence of variable length. Unlike previous approaches, PreF3R removes the need for camera calibration and reconstructs the 3D Gaussian field within a canonical coordinate frame directly from a sequence of unposed images, enabling efficient novel-view rendering. We leverage DUSt3R's ability for pair-wise 3D structure reconstruction, and extend it to sequential multi-view input via a spatial memory network, eliminating the need for optimization-based global alignment. Additionally, PreF3R incorporates a dense Gaussian parameter prediction head, which enables subsequent novel-view synthesis with differentiable rasterization. This allows supervising our model with the combination of photometric loss and pointmap regression loss, enhancing both photorealism and structural accuracy. Given a sequence of ordered images, PreF3R incrementally reconstructs the 3D Gaussian field at 20 FPS, therefore enabling real-time novel-view rendering. Empirical experiments demonstrate that PreF3R is an effective solution for the challenging task of pose-free feed-forward novel-view synthesis, while also exhibiting robust generalization to unseen scenes.",
    "arxiv_url": "http://arxiv.org/abs/2411.16877v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16877v1",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Georg Hess",
      "Carl Lindstr√∂m",
      "Maryam Fatemi",
      "Christoffer Petersson",
      "Lennart Svensson"
    ],
    "abstract": "Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude. See https://research.zenseact.com/publications/splatad/ for our project page.",
    "arxiv_url": "http://arxiv.org/abs/2411.16816v3",
    "pdf_url": "http://arxiv.org/pdf/2411.16816v3",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Jiho Jang",
      "Jin-Young Kim",
      "Soonwoo Kwon",
      "Changick Kim"
    ],
    "abstract": "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.",
    "arxiv_url": "http://arxiv.org/abs/2411.16443v3",
    "pdf_url": "http://arxiv.org/pdf/2411.16443v3",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction",
    "authors": [
      "Ziyu Zhang",
      "Binbin Huang",
      "Hanqing Jiang",
      "Liyang Zhou",
      "Xiaojun Xiang",
      "Shunhan Shen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its superior rendering quality and speed over Neural Radiance Fields (NeRF). To address 3DGS's limitations in surface representation, 2D Gaussian Splatting (2DGS) introduced disks as scene primitives to model and reconstruct geometries from multi-view images, offering view-consistent geometry. However, the disk's first-order linear approximation often leads to over-smoothed results. We propose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks with quadric surfaces, enhancing geometric fitting, whose code will be open-sourced. QGS defines Gaussian distributions in non-Euclidean space, allowing primitives to capture more complex textures. As a second-order surface approximation, QGS also renders spatial curvature to guide the normal consistency term, to effectively reduce over-smoothing. Moreover, QGS is a generalized version of 2DGS that achieves more accurate and detailed reconstructions, as verified by experiments on DTU and TNT, demonstrating its effectiveness in surpassing current state-of-the-art methods in geometry reconstruction. Our code willbe released as open source.",
    "arxiv_url": "http://arxiv.org/abs/2411.16392v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16392v1",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM",
    "authors": [
      "Vladimir Yugay",
      "Theo Gevers",
      "Martin R. Oswald"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) systems with novel view synthesis capabilities are widely used in computer vision, with applications in augmented reality, robotics, and autonomous driving. However, existing approaches are limited to single-agent operation. Recent work has addressed this problem using a distributed neural scene representation. Unfortunately, existing methods are slow, cannot accurately render real-world data, are restricted to two agents, and have limited tracking accuracy. In contrast, we propose a rigidly deformable 3D Gaussian-based scene representation that dramatically speeds up the system. However, improving tracking accuracy and reconstructing a globally consistent map from multiple agents remains challenging due to trajectory drift and discrepancies across agents' observations. Therefore, we propose new tracking and map-merging mechanisms and integrate loop closure in the Gaussian-based SLAM pipeline. We evaluate MAGiC-SLAM on synthetic and real-world datasets and find it more accurate and faster than the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2411.16785v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16785v1",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "tracking",
      "autonomous driving",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction",
    "authors": [
      "Wenhao Xu",
      "Wenming Weng",
      "Yueyi Zhang",
      "Ruikang Xu",
      "Zhiwei Xiong"
    ],
    "abstract": "Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediate motion information due to the low temporal resolution of RGB cameras. To address this, we introduce the first approach combining event cameras, which capture high-temporal-resolution, continuous motion data, with deformable 3D-GS for dynamic scene reconstruction. We observe that threshold modeling for events plays a crucial role in achieving high-quality reconstruction. Therefore, we propose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcing process that greatly improves both 3D reconstruction and threshold modeling. Moreover, we introduce a Dynamic-Static Decomposition strategy that first identifies dynamic areas by exploiting the inability of static Gaussians to represent motions, then applies a buffer-based soft decomposition to separate dynamic and static areas. This strategy accelerates rendering by avoiding unnecessary deformation in static areas, and focuses on dynamic areas to enhance fidelity. Additionally, we contribute the first event-inclusive 4D benchmark with synthetic and real-world dynamic scenes, on which our method achieves state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2411.16180v2",
    "pdf_url": "http://arxiv.org/pdf/2411.16180v2",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "4d",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NovelGS: Consistent Novel-view Denoising via Large Gaussian Reconstruction Model",
    "authors": [
      "Jinpeng Liu",
      "Jiale Xu",
      "Weihao Cheng",
      "Yiming Gao",
      "Xintao Wang",
      "Ying Shan",
      "Yansong Tang"
    ],
    "abstract": "We introduce NovelGS, a diffusion model for Gaussian Splatting (GS) given sparse-view images. Recent works leverage feed-forward networks to generate pixel-aligned Gaussians, which could be fast rendered. Unfortunately, the method was unable to produce satisfactory results for areas not covered by the input images due to the formulation of these methods. In contrast, we leverage the novel view denoising through a transformer-based network to generate 3D Gaussians. Specifically, by incorporating both conditional views and noisy target views, the network predicts pixel-aligned Gaussians for each view. During training, the rendered target and some additional views of the Gaussians are supervised. During inference, the target views are iteratively rendered and denoised from pure noise. Our approach demonstrates state-of-the-art performance in addressing the multi-view image reconstruction challenge. Due to generative modeling of unseen regions, NovelGS effectively reconstructs 3D objects with consistent and sharp textures. Experimental results on publicly available datasets indicate that NovelGS substantially surpasses existing image-to-3D frameworks, both qualitatively and quantitatively. We also demonstrate the potential of NovelGS in generative tasks, such as text-to-3D and image-to-3D, by integrating it with existing multiview diffusion models. We will make the code publicly accessible.",
    "arxiv_url": "http://arxiv.org/abs/2411.16779v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16779v1",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context",
    "authors": [
      "Wangze Xu",
      "Yifan Zhan",
      "Zhihang Zhong",
      "Xiao Sun"
    ],
    "abstract": "3D human avatars, through the use of canonical radiance fields and per-frame observed warping, enable high-fidelity rendering and animating. However, existing methods, which rely on either spatial SMPL(-X) poses or temporal embeddings, respectively suffer from coarse rendering quality or limited animation flexibility. To address these challenges, we propose GAST, a framework that unifies 3D human modeling with 3DGS by hierarchically integrating both spatial and temporal information. Specifically, we design a sequential conditioning framework for the non-rigid warping of the human body, under whose guidance more accurate 3D Gaussians can be obtained in the observation space. Moreover, the explicit properties of Gaussians allow us to embed richer sequential information, encompassing both the coarse sequence of human poses and finer per-vertex motion details. These sequence conditions are further sampled across different temporal scales, in a coarse-to-fine manner, ensuring unbiased inputs for non-rigid warping. Experimental results demonstrate that our method combined with hierarchical spatio-temporal modeling surpasses concurrent baselines, delivering both high-quality rendering and flexible animating capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2411.16768v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16768v1",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "high-fidelity",
      "motion",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation",
    "authors": [
      "Guangzhao Dai",
      "Jian Zhao",
      "Yuantao Chen",
      "Yusen Qin",
      "Hao Zhao",
      "Guosen Xie",
      "Yazhou Yao",
      "Xiangbo Shu",
      "Xuelong Li"
    ],
    "abstract": "Vision-and-Language Navigation (VLN), where an agent follows instructions to reach a target destination, has recently seen significant advancements. In contrast to navigation in discrete environments with predefined trajectories, VLN in Continuous Environments (VLN-CE) presents greater challenges, as the agent is free to navigate any unobstructed location and is more vulnerable to visual occlusions or blind spots. Recent approaches have attempted to address this by imagining future environments, either through predicted future visual images or semantic features, rather than relying solely on current observations. However, these RGB-based and feature-based methods lack intuitive appearance-level information or high-level semantic complexity crucial for effective navigation. To overcome these limitations, we introduce a novel, generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables agents to better explore future environments by unitedly rendering high-fidelity 360 visual images and semantic features. UnitedVLN employs two key schemes: search-then-query sampling and separate-then-united rendering, which facilitate efficient exploitation of neural primitives, helping to integrate both appearance and semantic information for more robust navigation. Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art methods on existing VLN-CE benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2411.16053v2",
    "pdf_url": "http://arxiv.org/pdf/2411.16053v2",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using Depth-Enhanced Diffusion Priors",
    "authors": [
      "Soumava Paul",
      "Prakhar Kaushik",
      "Alan Yuille"
    ],
    "abstract": "In this work, we introduce a generative approach for pose-free (without camera parameters) reconstruction of 360 scenes from a sparse set of 2D images. Pose-free scene reconstruction from incomplete, pose-free observations is usually regularized with depth estimation or 3D foundational priors. While recent advances have enabled sparse-view reconstruction of large complex scenes (with high degree of foreground and background detail) with known camera poses using view-conditioned generative priors, these methods cannot be directly adapted for the pose-free setting when ground-truth poses are not available during evaluation. To address this, we propose an image-to-image generative model designed to inpaint missing details and remove artifacts in novel view renders and depth maps of a 3D scene. We introduce context and geometry conditioning using Feature-wise Linear Modulation (FiLM) modulation layers as a lightweight alternative to cross-attention and also propose a novel confidence measure for 3D Gaussian splat representations to allow for better detection of these artifacts. By progressively integrating these novel views in a Gaussian-SLAM-inspired process, we achieve a multi-view-consistent 3D representation. Evaluations on the MipNeRF360 and DL3DV-10K benchmark datasets demonstrate that our method surpasses existing pose-free techniques and performs competitively with state-of-the-art posed (precomputed camera parameters are given) reconstruction methods in complex 360 scenes.",
    "arxiv_url": "http://arxiv.org/abs/2411.15966v2",
    "pdf_url": "http://arxiv.org/pdf/2411.15966v2",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PG-SLAM: Photo-realistic and Geometry-aware RGB-D SLAM in Dynamic Environments",
    "authors": [
      "Haoang Li",
      "Xiangqi Meng",
      "Xingxing Zuo",
      "Zhe Liu",
      "Hesheng Wang",
      "Daniel Cremers"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) has achieved impressive performance in static environments. However, SLAM in dynamic environments remains an open question. Many methods directly filter out dynamic objects, resulting in incomplete scene reconstruction and limited accuracy of camera localization. The other works express dynamic objects by point clouds, sparse joints, or coarse meshes, which fails to provide a photo-realistic representation. To overcome the above limitations, we propose a photo-realistic and geometry-aware RGB-D SLAM method by extending Gaussian splatting. Our method is composed of three main modules to 1) map the dynamic foreground including non-rigid humans and rigid items, 2) reconstruct the static background, and 3) localize the camera. To map the foreground, we focus on modeling the deformations and/or motions. We consider the shape priors of humans and exploit geometric and appearance constraints of humans and items. For background mapping, we design an optimization strategy between neighboring local maps by integrating appearance constraint into geometric alignment. As to camera localization, we leverage both static background and dynamic foreground to increase the observations for noise compensation. We explore the geometric and appearance constraints by associating 3D Gaussians with 2D optical flows and pixel patches. Experiments on various real-world datasets demonstrate that our method outperforms state-of-the-art approaches in terms of camera localization and scene representation. Source codes will be publicly available upon paper acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2411.15800v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15800v1",
    "published_date": "2024-11-24",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "deformation",
      "mapping",
      "geometry",
      "3d gaussian",
      "human",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZeroGS: Training 3D Gaussian Splatting from Unposed Images",
    "authors": [
      "Yu Chen",
      "Rolandos Alexandros Potamias",
      "Evangelos Ververas",
      "Jifei Song",
      "Jiankang Deng",
      "Gim Hee Lee"
    ],
    "abstract": "Neural radiance fields (NeRF) and 3D Gaussian Splatting (3DGS) are popular techniques to reconstruct and render photo-realistic images. However, the pre-requisite of running Structure-from-Motion (SfM) to get camera poses limits their completeness. While previous methods can reconstruct from a few unposed images, they are not applicable when images are unordered or densely captured. In this work, we propose ZeroGS to train 3DGS from hundreds of unposed and unordered images. Our method leverages a pretrained foundation model as the neural scene representation. Since the accuracy of the predicted pointmaps does not suffice for accurate image registration and high-fidelity image rendering, we propose to mitigate the issue by initializing and finetuning the pretrained model from a seed image. Images are then progressively registered and added to the training buffer, which is further used to train the model. We also propose to refine the camera poses and pointmaps by minimizing a point-to-camera ray consistency loss across multiple views. Experiments on the LLFF dataset, the MipNeRF360 dataset, and the Tanks-and-Temples dataset show that our method recovers more accurate camera poses than state-of-the-art pose-free NeRF/3DGS methods, and even renders higher quality images than 3DGS with COLMAP poses. Our project page is available at https://aibluefisher.github.io/ZeroGS.",
    "arxiv_url": "http://arxiv.org/abs/2411.15779v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15779v1",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bundle Adjusted Gaussian Avatars Deblurring",
    "authors": [
      "Muyao Niu",
      "Yifan Zhan",
      "Qingtian Zhu",
      "Zhuoxiao Li",
      "Wei Wang",
      "Zhihang Zhong",
      "Xiao Sun",
      "Yinqiang Zheng"
    ],
    "abstract": "The development of 3D human avatars from multi-view videos represents a significant yet challenging task in the field. Recent advancements, including 3D Gaussian Splattings (3DGS), have markedly progressed this domain. Nonetheless, existing techniques necessitate the use of high-quality sharp images, which are often impractical to obtain in real-world settings due to variations in human motion speed and intensity. In this study, we attempt to explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video footage in an end-to-end manner. Our approach encompasses a 3D-aware, physics-oriented model of blur formation attributable to human movement, coupled with a 3D human motion model to clarify ambiguities found in motion-induced blurry images. This methodology facilitates the concurrent learning of avatar model parameters and the refinement of sub-frame motion parameters from a coarse initialization. We have established benchmarks for this task through a synthetic dataset derived from existing multi-view captures, alongside a real-captured dataset acquired through a 360-degree synchronous hybrid-exposure camera system. Comprehensive evaluations demonstrate that our model surpasses existing baselines.",
    "arxiv_url": "http://arxiv.org/abs/2411.16758v1",
    "pdf_url": "http://arxiv.org/pdf/2411.16758v1",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and Precise Editing with Diffusion Models",
    "authors": [
      "Yangyang Qian",
      "Yuan Sun",
      "Yu Guo"
    ],
    "abstract": "Generating and editing dynamic 3D head avatars are crucial tasks in virtual reality and film production. However, existing methods often suffer from facial distortions, inaccurate head movements, and limited fine-grained editing capabilities. To address these challenges, we present DynamicAvatars, a dynamic model that generates photorealistic, moving 3D head avatars from video clips and parameters associated with facial positions and expressions. Our approach enables precise editing through a novel prompt-based editing model, which integrates user-provided prompts with guiding parameters derived from large language models (LLMs). To achieve this, we propose a dual-tracking framework based on Gaussian Splatting and introduce a prompt preprocessing module to enhance editing stability. By incorporating a specialized GAN algorithm and connecting it to our control module, which generates precise guiding parameters from LLMs, we successfully address the limitations of existing methods. Additionally, we develop a dynamic editing strategy that selectively utilizes specific training datasets to improve the efficiency and adaptability of the model for dynamic editing tasks.",
    "arxiv_url": "http://arxiv.org/abs/2411.15732v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15732v1",
    "published_date": "2024-11-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "tracking",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian Supervision",
    "authors": [
      "Baixin Xu",
      "Jiangbei Hu",
      "Jiaze Li",
      "Ying He"
    ],
    "abstract": "Surface reconstruction from multi-view images is a core challenge in 3D vision. Recent studies have explored signed distance fields (SDF) within Neural Radiance Fields (NeRF) to achieve high-fidelity surface reconstructions. However, these approaches often suffer from slow training and rendering speeds compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques attempt to fuse depth information to extract geometry from 3DGS, but frequently result in incomplete reconstructions and fragmented surfaces. In this paper, we introduce GSurf, a novel end-to-end method for learning a signed distance field directly from Gaussian primitives. The continuous and smooth nature of SDF addresses common issues in the 3DGS family, such as holes resulting from noisy or missing depth data. By using Gaussian splatting for rendering, GSurf avoids the redundant volume rendering typically required in other GS and SDF integrations. Consequently, GSurf achieves faster training and rendering speeds while delivering 3D reconstruction quality comparable to neural implicit surface methods, such as VolSDF and NeuS. Experimental results across various benchmark datasets demonstrate the effectiveness of our method in producing high-fidelity 3D reconstructions.",
    "arxiv_url": "http://arxiv.org/abs/2411.15723v3",
    "pdf_url": "http://arxiv.org/pdf/2411.15723v3",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting",
    "authors": [
      "Xiaobao Wei",
      "Qingpo Wuwu",
      "Zhongyu Zhao",
      "Zhuangzhe Wu",
      "Nan Huang",
      "Ming Lu",
      "Ningning MA",
      "Shanghang Zhang"
    ],
    "abstract": "Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed EMD is a plug-and-play approach applicable to various baseline methods. We also propose tailored training strategies to apply EMD to both supervised and self-supervised baselines. Through comprehensive experimentation, we illustrate the effectiveness of our approach with various established baselines. The code will be released at: https://qingpowuwu.github.io/emdgaussian.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2411.15582v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15582v1",
    "published_date": "2024-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "motion",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving",
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Zhuoyang Sun",
      "Yingjie Victor Chen",
      "Mei Chen"
    ],
    "abstract": "Most existing Dynamic Gaussian Splatting methods for complex dynamic urban scenarios rely on accurate object-level supervision from expensive manual labeling, limiting their scalability in real-world applications. In this paper, we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without requiring tracked 3D bounding boxes, enabling accurate dynamic scene reconstruction and novel view RGB/depth/flow synthesis. SplatFlow designs a unified framework to seamlessly integrate time-dependent 4D Gaussian representation within NMFF, where NMFF is a set of implicit functions to model temporal motions of both LiDAR points and Gaussians as continuous motion flow fields. Leveraging NMFF, SplatFlow effectively decomposes static background and dynamic objects, representing them with 3D and 4D Gaussian primitives, respectively. NMFF also models the correspondences of each 4D Gaussian across time, which aggregates temporal features to enhance cross-view consistency of dynamic components. SplatFlow further improves dynamic object identification by distilling features from 2D foundation models into 4D space-time representation. Comprehensive evaluations conducted on the Waymo and KITTI Datasets validate SplatFlow's state-of-the-art (SOTA) performance for both image reconstruction and novel view synthesis in dynamic urban scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2411.15482v2",
    "pdf_url": "http://arxiv.org/pdf/2411.15482v2",
    "published_date": "2024-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "motion",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gassidy: Gaussian Splatting SLAM in Dynamic Environments",
    "authors": [
      "Long Wen",
      "Shixin Li",
      "Yu Zhang",
      "Yuhong Huang",
      "Jianjie Lin",
      "Fengjunjie Pan",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) allows flexible adjustments to scene representation, enabling continuous optimization of scene quality during dense visual simultaneous localization and mapping (SLAM) in static environments. However, 3DGS faces challenges in handling environmental disturbances from dynamic objects with irregular movement, leading to degradation in both camera tracking accuracy and map reconstruction quality. To address this challenge, we develop an RGB-D dense SLAM which is called Gaussian Splatting SLAM in Dynamic Environments (Gassidy). This approach calculates Gaussians to generate rendering loss flows for each environmental component based on a designed photometric-geometric loss function. To distinguish and filter environmental disturbances, we iteratively analyze rendering loss flows to detect features characterized by changes in loss values between dynamic objects and static components. This process ensures a clean environment for accurate scene reconstruction. Compared to state-of-the-art SLAM methods, experimental results on open datasets show that Gassidy improves camera tracking precision by up to 97.9% and enhances map quality by up to 6%.",
    "arxiv_url": "http://arxiv.org/abs/2411.15476v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15476v1",
    "published_date": "2024-11-23",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "face",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion",
    "authors": [
      "Runfa Blark Li",
      "Keito Suzuki",
      "Bang Du",
      "Ki Myung Brian Lee",
      "Nikolay Atanasov",
      "Truong Nguyen"
    ],
    "abstract": "A signed distance function (SDF) is a useful representation for continuous-space geometry and many related operations, including rendering, collision checking, and mesh generation. Hence, reconstructing SDF from image observations accurately and efficiently is a fundamental problem. Recently, neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering, have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF enables continuous-space SDF reconstruction with better geometric and photometric accuracy. However, the accuracy and convergence speed of scene-level SDF reconstruction require further improvements for many applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit representation with excellent rendering quality and speed, several works have focused on improving SDF-NeRF by introducing consistency losses on depth and surface normals between 3DGS and SDF-NeRF. However, loss-level connections alone lead to incremental improvements. We propose a novel neural implicit SDF called \"SplatSDF\" to fuse 3DGSandSDF-NeRF at an architecture level with significant boosts to geometric and photometric accuracy and convergence speed. Our SplatSDF relies on 3DGS as input only during training, and keeps the same complexity and efficiency as the original SDF-NeRF during inference. Our method outperforms state-of-the-art SDF-NeRF models on geometric and photometric evaluation by the time of submission.",
    "arxiv_url": "http://arxiv.org/abs/2411.15468v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15468v1",
    "published_date": "2024-11-23",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations",
    "authors": [
      "Yuan Ren",
      "Guile Wu",
      "Runhao Li",
      "Zheyuan Yang",
      "Yibo Liu",
      "Xingxin Chen",
      "Tongtong Cao",
      "Bingbing Liu"
    ],
    "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.",
    "arxiv_url": "http://arxiv.org/abs/2411.15355v2",
    "pdf_url": "http://arxiv.org/pdf/2411.15355v2",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "fast",
      "semantic",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural 4D Evolution under Large Topological Changes from 2D Images",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Tiago Novello",
      "Asen Nachkov",
      "Thomas Probst",
      "Danda Paudel"
    ],
    "abstract": "In the literature, it has been shown that the evolution of the known explicit 3D surface to the target one can be learned from 2D images using the instantaneous flow field, where the known and target 3D surfaces may largely differ in topology. We are interested in capturing 4D shapes whose topology changes largely over time. We encounter that the straightforward extension of the existing 3D-based method to the desired 4D case performs poorly.   In this work, we address the challenges in extending 3D neural evolution to 4D under large topological changes by proposing two novel modifications. More precisely, we introduce (i) a new architecture to discretize and encode the deformation and learn the SDF and (ii) a technique to impose the temporal consistency. (iii) Also, we propose a rendering scheme for color prediction based on Gaussian splatting. Furthermore, to facilitate learning directly from 2D images, we propose a learning framework that can disentangle the geometry and appearance from RGB images. This method of disentanglement, while also useful for the 4D evolution problem that we are concentrating on, is also novel and valid for static scenes. Our extensive experiments on various data provide awesome results and, most importantly, open a new approach toward reconstructing challenging scenes with significant topological changes and deformations. Our source code and the dataset are publicly available at https://github.com/insait-institute/N4DE.",
    "arxiv_url": "http://arxiv.org/abs/2411.15018v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15018v1",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "I.4.5; I.3.5"
    ],
    "github_url": "https://github.com/insait-institute/N4DE",
    "keywords": [
      "face",
      "deformation",
      "geometry",
      "4d",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Abdullah Hamdi",
      "Adrien Deliege",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "Recent advances in radiance field reconstruction, such as 3D Gaussian Splatting (3DGS), have achieved high-quality novel view synthesis and fast rendering by representing scenes with compositions of Gaussian primitives. However, 3D Gaussians present several limitations for scene reconstruction. Accurately capturing hard edges is challenging without significantly increasing the number of Gaussians, creating a large memory footprint. Moreover, they struggle to represent flat surfaces, as they are diffused in space. Without hand-crafted regularizers, they tend to disperse irregularly around the actual surface. To circumvent these issues, we introduce a novel method, named 3D Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for modeling geometrically-meaningful radiance fields from multi-view images. Smooth convex shapes offer greater flexibility than Gaussians, allowing for a better representation of 3D scenes with hard edges and dense volumes using fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and Temples, and Deep Blending. Specifically, our method attains an improvement of up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high rendering speeds and reducing the number of required primitives. Our results highlight the potential of 3D Convex Splatting to become the new standard for high-quality scene reconstruction and novel view synthesis. Project page: convexsplatting.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2411.14974v3",
    "pdf_url": "http://arxiv.org/pdf/2411.14974v3",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction",
    "authors": [
      "Zhening Liu",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Rui Song",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "The recent development of 3D Gaussian Splatting (3DGS) has led to great interest in 4D dynamic spatial reconstruction. Existing approaches mainly rely on full-length multi-view videos, while there has been limited exploration of online reconstruction methods that enable on-the-fly training and per-timestep streaming. Current 3DGS-based streaming methods treat the Gaussian primitives uniformly and constantly renew the densified Gaussians, thereby overlooking the difference between dynamic and static features as well as neglecting the temporal continuity in the scene. To address these limitations, we propose a novel three-stage pipeline for iterative streamable 4D dynamic spatial reconstruction. Our pipeline comprises a selective inheritance stage to preserve temporal continuity, a dynamics-aware shift stage to distinguish dynamic and static primitives and optimize their movements, and an error-guided densification stage to accommodate emerging objects. Our method achieves state-of-the-art performance in online 4D reconstruction, demonstrating the fastest on-the-fly training, superior representation quality, and real-time rendering capability. Project page: https://www.liuzhening.top/DASS",
    "arxiv_url": "http://arxiv.org/abs/2411.14847v2",
    "pdf_url": "http://arxiv.org/pdf/2411.14847v2",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving",
    "authors": [
      "Haiming Zhang",
      "Wending Zhou",
      "Yiyao Zhu",
      "Xu Yan",
      "Jiantao Gao",
      "Dongfeng Bai",
      "Yingjie Cai",
      "Bingbing Liu",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "This paper introduces VisionPAD, a novel self-supervised pre-training paradigm designed for vision-centric algorithms in autonomous driving. In contrast to previous approaches that employ neural rendering with explicit depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to reconstruct multi-view representations using only images as supervision. Specifically, we introduce a self-supervised method for voxel velocity estimation. By warping voxels to adjacent frames and supervising the rendered outputs, the model effectively learns motion cues in the sequential data. Furthermore, we adopt a multi-frame photometric consistency approach to enhance geometric perception. It projects adjacent frames to the current frame based on rendered depths and relative poses, boosting the 3D geometric representation through pure image supervision. Extensive experiments on autonomous driving datasets demonstrate that VisionPAD significantly improves performance in 3D object detection, occupancy prediction and map segmentation, surpassing state-of-the-art pre-training strategies by a considerable margin.",
    "arxiv_url": "http://arxiv.org/abs/2411.14716v2",
    "pdf_url": "http://arxiv.org/pdf/2411.14716v2",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "motion",
      "segmentation",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation",
    "authors": [
      "Zhuoman Liu",
      "Weicai Ye",
      "Yan Luximon",
      "Pengfei Wan",
      "Di Zhang"
    ],
    "abstract": "Realistic simulation of dynamic scenes requires accurately capturing diverse material properties and modeling complex object interactions grounded in physical principles. However, existing methods are constrained to basic material types with limited predictable parameters, making them insufficient to represent the complexity of real-world materials. We introduce PhysFlow, a novel approach that leverages multi-modal foundation models and video diffusion to achieve enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to identify material types and initialize material parameters through image queries, while simultaneously inferring 3D Gaussian splats for detailed scene representation. We further refine these material parameters using video diffusion with a differentiable Material Point Method (MPM) and optical flow guidance rather than render loss or Score Distillation Sampling (SDS) loss. This integrated framework enables accurate prediction and realistic simulation of dynamic interactions in real-world scenarios, advancing both accuracy and flexibility in physics-based simulations.",
    "arxiv_url": "http://arxiv.org/abs/2411.14423v4",
    "pdf_url": "http://arxiv.org/pdf/2411.14423v4",
    "published_date": "2024-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "4d",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation and Reconstruction",
    "authors": [
      "Yuanhao Cai",
      "He Zhang",
      "Kai Zhang",
      "Yixun Liang",
      "Mengwei Ren",
      "Fujun Luan",
      "Qing Liu",
      "Soo Ye Kim",
      "Jianming Zhang",
      "Zhifei Zhang",
      "Yuqian Zhou",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "abstract": "Existing feedforward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric cases. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object generation and scene reconstruction from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generality of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that DiffusionGS yields improvements of 2.20 dB/23.25 and 1.34 dB/19.16 in PSNR/FID for objects and scenes than the state-of-the-art methods, without depth estimator. Plus, our method enjoys over 5$\\times$ faster speed ($\\sim$6s on an A100 GPU). Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive results.",
    "arxiv_url": "http://arxiv.org/abs/2411.14384v3",
    "pdf_url": "http://arxiv.org/pdf/2411.14384v3",
    "published_date": "2024-11-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching",
    "authors": [
      "Arjun P S",
      "Andrew Melnik",
      "Gora Chand Nandi"
    ],
    "abstract": "Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods",
    "arxiv_url": "http://arxiv.org/abs/2411.14322v2",
    "pdf_url": "http://arxiv.org/pdf/2411.14322v2",
    "published_date": "2024-11-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NexusSplats: Efficient 3D Gaussian Splatting in the Wild",
    "authors": [
      "Yuzhou Tang",
      "Dejun Xu",
      "Yongjie Hou",
      "Zhenzhong Wang",
      "Min Jiang"
    ],
    "abstract": "Photorealistic 3D reconstruction of unstructured real-world scenes remains challenging due to complex illumination variations and transient occlusions. Existing methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) struggle with inefficient light decoupling and structure-agnostic occlusion handling. To address these limitations, we propose NexusSplats, an approach tailored for efficient and high-fidelity 3D scene reconstruction under complex lighting and occlusion conditions. In particular, NexusSplats leverages a hierarchical light decoupling strategy that performs centralized appearance learning, efficiently and effectively decoupling varying lighting conditions. Furthermore, a structure-aware occlusion handling mechanism is developed, establishing a nexus between 3D and 2D structures for fine-grained occlusion handling. Experimental results demonstrate that NexusSplats achieves state-of-the-art rendering quality and reduces the number of total parameters by 65.4\\%, leading to 2.7$\\times$ faster reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2411.14514v5",
    "pdf_url": "http://arxiv.org/pdf/2411.14514v5",
    "published_date": "2024-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "efficient",
      "high-fidelity",
      "lighting",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
    "authors": [
      "Ola Shorinwa",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
    "arxiv_url": "http://arxiv.org/abs/2411.13753v2",
    "pdf_url": "http://arxiv.org/pdf/2411.13753v2",
    "published_date": "2024-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "fast",
      "semantic",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating 3D-Consistent Videos from Unposed Internet Photos",
    "authors": [
      "Gene Chou",
      "Kai Zhang",
      "Sai Bi",
      "Hao Tan",
      "Zexiang Xu",
      "Fujun Luan",
      "Bharath Hariharan",
      "Noah Snavely"
    ],
    "abstract": "We address the problem of generating videos from unposed internet photos. A handful of input images serve as keyframes, and our model interpolates between them to simulate a path moving between the cameras. Given random images, a model's ability to capture underlying geometry, recognize scene identity, and relate frames in terms of camera position and orientation reflects a fundamental understanding of 3D structure and scene layout. However, existing video models such as Luma Dream Machine fail at this task. We design a self-supervised method that takes advantage of the consistency of videos and variability of multiview internet photos to train a scalable, 3D-aware video model without any 3D annotations such as camera parameters. We validate that our method outperforms all baselines in terms of geometric and appearance consistency. We also show our model benefits applications that enable camera control, such as 3D Gaussian Splatting. Our results suggest that we can scale up scene-level 3D learning using only 2D data such as videos and multiview internet photos.",
    "arxiv_url": "http://arxiv.org/abs/2411.13549v1",
    "pdf_url": "http://arxiv.org/pdf/2411.13549v1",
    "published_date": "2024-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting",
    "authors": [
      "Xiaobao Wei",
      "Peng Chen",
      "Guangyu Li",
      "Ming Lu",
      "Hui Chen",
      "Feng Tian"
    ],
    "abstract": "Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, a high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. By leveraging the unstructured nature of 3DGS, we develop a novel eye representation for rigid eye rotation based on the target gaze direction. To enhance synthesis generalization across various subjects, we integrate an expression-conditional module to guide the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. We also demonstrate that existing gaze estimation methods can leverage GazeGaussian to improve their generalization performance. The code will be available at: https://ucwxb.github.io/GazeGaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2411.12981v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12981v1",
    "published_date": "2024-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "nerf",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization",
    "authors": [
      "Hao Ju",
      "Shaofei Huang",
      "Si Liu",
      "Zhedong Zheng"
    ],
    "abstract": "Existing approaches to drone visual geo-localization predominantly adopt the image-based setting, where a single drone-view snapshot is matched with images from other platforms. Such task formulation, however, underutilizes the inherent video output of the drone and is sensitive to occlusions and viewpoint disparity. To address these limitations, we formulate a new video-based drone geo-localization task and propose the Video2BEV paradigm. This paradigm transforms the video into a Bird's Eye View (BEV), simplifying the subsequent \\textbf{inter-platform} matching process. In particular, we employ Gaussian Splatting to reconstruct a 3D scene and obtain the BEV projection. Different from the existing transform methods, \\eg, polar transform, our BEVs preserve more fine-grained details without significant distortion. To facilitate the discriminative \\textbf{intra-platform} representation learning, our Video2BEV paradigm also incorporates a diffusion-based module for generating hard negative samples. To validate our approach, we introduce UniV, a new video-based geo-localization dataset that extends the image-based University-1652 dataset. UniV features flight paths at $30^\\circ$ and $45^\\circ$ elevation angles with increased frame rates of up to 10 frames per second (FPS). Extensive experiments on the UniV dataset show that our Video2BEV paradigm achieves competitive recall rates and outperforms conventional video-based methods. Compared to other competitive methods, our proposed approach exhibits robustness at lower elevations with more occlusions.",
    "arxiv_url": "http://arxiv.org/abs/2411.13610v3",
    "pdf_url": "http://arxiv.org/pdf/2411.13610v3",
    "published_date": "2024-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy",
    "authors": [
      "Joanna Kaleta",
      "Weronika Smolak-Dy≈ºewska",
      "Dawid Malarz",
      "Diego Dall'Alba",
      "Przemys≈Çaw Korzeniowski",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Endoscopic procedures are crucial for colorectal cancer diagnosis, and three-dimensional reconstruction of the environment for real-time novel-view synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework that leverages 3D Gaussian Splatting within a physically based, relightable model tailored for the complex acquisition conditions in endoscopy, such as restricted camera rotations and strong view-dependent illumination. By exploiting the connection between the camera and light source, our approach introduces a relighting model to capture the intricate interactions between light and tissue using physically based rendering and MLP. Existing methods often produce artifacts and inconsistencies under these conditions, which PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes light angles and normal vectors, achieving stable reconstructions even with limited training camera rotations. We benchmarked our framework using a publicly available dataset and a newly introduced dataset with wider camera rotations. Our methods demonstrated superior image quality compared to baseline approaches.",
    "arxiv_url": "http://arxiv.org/abs/2411.12510v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12510v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image",
    "authors": [
      "Zixu Wang",
      "Hao Yang",
      "Yu Guo",
      "Fei Wang"
    ],
    "abstract": "Snapshot Compressive Imaging (SCI) offers a possibility for capturing information in high-speed dynamic scenes, requiring efficient reconstruction method to recover scene information. Despite promising results, current deep learning-based and NeRF-based reconstruction methods face challenges: 1) deep learning-based reconstruction methods struggle to maintain 3D structural consistency within scenes, and 2) NeRF-based reconstruction methods still face limitations in handling dynamic scenes. To address these challenges, we propose SCIGS, a variant of 3DGS, and develop a primitive-level transformation network that utilizes camera pose stamps and Gaussian primitive coordinates as embedding vectors. This approach resolves the necessity of camera pose in vanilla 3DGS and enhances multi-view 3D structural consistency in dynamic scenes by utilizing transformed primitives. Additionally, a high-frequency filter is introduced to eliminate the artifacts generated during the transformation. The proposed SCIGS is the first to reconstruct a 3D explicit scene from a single compressed image, extending its application to dynamic 3D scenes. Experiments on both static and dynamic scenes demonstrate that SCIGS not only enhances SCI decoding but also outperforms current state-of-the-art methods in reconstructing dynamic 3D scenes from a single compressed image. The code will be made available upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2411.12471v2",
    "pdf_url": "http://arxiv.org/pdf/2411.12471v2",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting",
    "authors": [
      "Haoyu Zhao",
      "Hao Wang",
      "Xingyue Zhao",
      "Hao Fei",
      "Hongqiu Wang",
      "Chengjiang Long",
      "Hua Zou"
    ],
    "abstract": "Recent advancements in 3D generation models have opened new possibilities for simulating dynamic 3D object movements and customizing behaviors, yet creating this content remains challenging. Current methods often require manual assignment of precise physical properties for simulations or rely on video generation models to predict them, which is computationally intensive. In this paper, we rethink the usage of multi-modal large language model (MLLM) in physics-based simulation, and present Sim Anything, a physics-based approach that endows static 3D objects with interactive dynamics. We begin with detailed scene reconstruction and object-level 3D open-vocabulary segmentation, progressing to multi-view image in-painting. Inspired by human visual reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to predict mean physical properties of objects in a zero-shot manner. Based on the mean values and the object's geometry, the Material Property Distribution Prediction model (MPDP) model then estimates the full distribution, reformulating the problem as probability distribution estimation to reduce computational costs. Finally, we simulate objects in an open-world scene with particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy, efficiently capturing complex deformations and significantly reducing computational costs. Extensive experiments and user studies demonstrate our Sim Anything achieves more realistic motion than state-of-the-art methods within 2 minutes on a single GPU.",
    "arxiv_url": "http://arxiv.org/abs/2411.12789v2",
    "pdf_url": "http://arxiv.org/pdf/2411.12789v2",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "segmentation",
      "geometry",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPretrain: A Simple Unified 3D Gaussian Representation for Visual Pre-training in Autonomous Driving",
    "authors": [
      "Shaoqing Xu",
      "Fang Li",
      "Shengyin Jiang",
      "Ziying Song",
      "Li Liu",
      "Zhi-xin Yang"
    ],
    "abstract": "Self-supervised learning has made substantial strides in image processing, while visual pre-training for autonomous driving is still in its infancy. Existing methods often focus on learning geometric scene information while neglecting texture or treating both aspects separately, hindering comprehensive scene understanding. In this context, we are excited to introduce GaussianPretrain, a novel pre-training paradigm that achieves a holistic understanding of the scene by uniformly integrating geometric and texture representations. Conceptualizing 3D Gaussian anchors as volumetric LiDAR points, our method learns a deepened understanding of scenes to enhance pre-training performance with detailed spatial structure and texture, achieving that 40.6% faster than NeRF-based method UniPAD with 70% GPU memory only. We demonstrate the effectiveness of GaussianPretrain across multiple 3D perception tasks, showing significant performance improvements, such as a 7.05% increase in NDS for 3D object detection, boosts mAP by 1.9% in HD map construction and 0.8% improvement on Occupancy prediction. These significant gains highlight GaussianPretrain's theoretical innovation and strong practical potential, promoting visual pre-training development for autonomous driving. Source code will be available at https://github.com/Public-BOTs/GaussianPretrain",
    "arxiv_url": "http://arxiv.org/abs/2411.12452v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12452v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Public-BOTs/GaussianPretrain",
    "keywords": [
      "autonomous driving",
      "fast",
      "understanding",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting",
    "authors": [
      "Joji Joseph",
      "Bharadwaj Amrutur",
      "Shalabh Bhatnagar"
    ],
    "abstract": "We introduce a training-free method for feature field rendering in Gaussian splatting. Our approach back-projects 2D features into pre-trained 3D Gaussians, using a weighted sum based on each Gaussian's influence in the final rendering. While most training-based feature field rendering methods excel at 2D segmentation but perform poorly at 3D segmentation without post-processing, our method achieves high-quality results in both 2D and 3D segmentation. Experimental results demonstrate that our approach is fast, scalable, and offers performance comparable to training-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2411.15193v1",
    "pdf_url": "http://arxiv.org/pdf/2411.15193v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels",
    "authors": [
      "Haodong Chen",
      "Runnan Chen",
      "Qiang Qu",
      "Zhaoqing Wang",
      "Tongliang Liu",
      "Xiaoming Chen",
      "Yuk Ying Chung"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training images, improving rendering consistency, and optimizing density control, yet the role of kernel design remains underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of the causes of these artifacts, limiting detail capture in high-frequency regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which replaces Gaussian kernels with linear kernels to achieve sharper and more precise results, particularly in high-frequency regions. Through evaluations on three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along with a 30% FPS improvement over baseline 3DGS. The implementation will be made publicly available upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2411.12440v3",
    "pdf_url": "http://arxiv.org/pdf/2411.12440v3",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification",
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ],
    "abstract": "In this study, we explore the essential challenge of fast scene optimization for Gaussian Splatting. Through a thorough analysis of the geometry modeling process, we reveal that dense point clouds can be effectively reconstructed early in optimization through Gaussian representations. This insight leads to our approach of aggressive Gaussian densification, which provides a more efficient alternative to conventional progressive densification methods. By significantly increasing the number of critical Gaussians, we enhance the model capacity to capture dense scene geometry at the early stage of optimization. This strategy is seamlessly integrated into the Mini-Splatting densification and simplification framework, enabling rapid convergence without compromising quality. Additionally, we introduce visibility culling within Gaussian Splatting, leveraging per-view Gaussian importance as precomputed visibility to accelerate the optimization process. Our Mini-Splatting2 achieves a balanced trade-off among optimization time, the number of Gaussians, and rendering quality, establishing a strong baseline for future Gaussian-Splatting-based works. Our work sets the stage for more efficient, high-quality 3D scene modeling in real-world applications, and the code will be made available no matter acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2411.12788v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12788v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
    "authors": [
      "Renxiang Xiao",
      "Wei Liu",
      "Yushuai Chen",
      "Liang Hu"
    ],
    "abstract": "We present LiV-GS, a LiDAR-visual SLAM system in outdoor environments that leverages 3D Gaussian as a differentiable spatial representation. Notably, LiV-GS is the first method that directly aligns discrete and sparse LiDAR data with continuous differentiable Gaussian maps in large-scale outdoor scenes, overcoming the limitation of fixed resolution in traditional LiDAR mapping. The system aligns point clouds with Gaussian maps using shared covariance attributes for front-end tracking and integrates the normal orientation into the loss function to refines the Gaussian map. To reliably and stably update Gaussians outside the LiDAR field of view, we introduce a novel conditional Gaussian constraint that aligns these Gaussians closely with the nearest reliable ones. The targeted adjustment enables LiV-GS to achieve fast and accurate mapping with novel view synthesis at a rate of 7.98 FPS. Extensive comparative experiments demonstrate LiV-GS's superior performance in SLAM, image rendering and mapping. The successful cross-modal radar-LiDAR localization highlights the potential of LiV-GS for applications in cross-modal semantic positioning and object segmentation with Gaussian maps.",
    "arxiv_url": "http://arxiv.org/abs/2411.12185v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12185v1",
    "published_date": "2024-11-19",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "outdoor",
      "fast",
      "semantic",
      "mapping",
      "segmentation",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sketch-guided Cage-based 3D Gaussian Splatting Deformation",
    "authors": [
      "Tianhao Xie",
      "Noam Aigerman",
      "Eugene Belilovsky",
      "Tiberiu Popa"
    ],
    "abstract": "3D Gaussian Splatting (GS) is one of the most promising novel 3D representations that has received great interest in computer graphics and computer vision. While various systems have introduced editing capabilities for 3D GS, such as those guided by text prompts, fine-grained control over deformation remains an open challenge. In this work, we present a novel sketch-guided 3D GS deformation system that allows users to intuitively modify the geometry of a 3D GS model by drawing a silhouette sketch from a single viewpoint. Our approach introduces a new deformation method that combines cage-based deformations with a variant of Neural Jacobian Fields, enabling precise, fine-grained control. Additionally, it leverages large-scale 2D diffusion priors and ControlNet to ensure the generated deformations are semantically plausible. Through a series of experiments, we demonstrate the effectiveness of our method and showcase its ability to animate static 3D GS models as one of its key applications.",
    "arxiv_url": "http://arxiv.org/abs/2411.12168v2",
    "pdf_url": "http://arxiv.org/pdf/2411.12168v2",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FruitNinja: 3D Object Interior Texture Generation with Gaussian Splatting",
    "authors": [
      "Fangyu Wu",
      "Yuhao Chen"
    ],
    "abstract": "In the real world, objects reveal internal textures when sliced or cut, yet this behavior is not well-studied in 3D generation tasks today. For example, slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no available dataset captures an object's full internal structure and collecting data from all slices is impractical, generative methods become the obvious approach. However, current 3D generation and inpainting methods often focus on visible appearance and overlook internal textures. To bridge this gap, we introduce FruitNinja, the first method to generate internal textures for 3D objects undergoing geometric and topological changes. Our approach produces objects via 3D Gaussian Splatting (3DGS) with both surface and interior textures synthesized, enabling real-time slicing and rendering without additional optimization. FruitNinja leverages a pre-trained diffusion model to progressively inpaint cross-sectional views and applies voxel-grid-based smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS strategy overcomes 3DGS limitations by employing densely distributed opaque Gaussians, avoiding biases toward larger particles that destabilize training and sharp color transitions for fine-grained textures. Experimental results show that FruitNinja substantially outperforms existing approaches, showcasing unmatched visual quality in real-time rendered internal views across arbitrary geometry manipulations.",
    "arxiv_url": "http://arxiv.org/abs/2411.12089v2",
    "pdf_url": "http://arxiv.org/pdf/2411.12089v2",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator",
    "authors": [
      "Xinhai Li",
      "Jialin Li",
      "Ziheng Zhang",
      "Rui Zhang",
      "Fan Jia",
      "Tiancai Wang",
      "Haoqiang Fan",
      "Kuo-Kun Tseng",
      "Ruiping Wang"
    ],
    "abstract": "Efficient acquisition of real-world embodied data has been increasingly critical. However, large-scale demonstrations captured by remote operation tend to take extremely high costs and fail to scale up the data size in an efficient manner. Sampling the episodes under a simulated environment is a promising way for large-scale collection while existing simulators fail to high-fidelity modeling on texture and physics. To address these limitations, we introduce the RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting and the physics engine. RoboGSim mainly includes four parts: Gaussian Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine. It can synthesize the simulated data with novel views, objects, trajectories, and scenes. RoboGSim also provides an online, reproducible, and safe evaluation for different manipulation policies. The real2sim and sim2real transfer experiments show a high consistency in the texture and physics. Moreover, the effectiveness of synthetic data is validated under the real-world manipulated tasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison on policy learning. More information can be found on our project page https://robogsim.github.io/ .",
    "arxiv_url": "http://arxiv.org/abs/2411.11839v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11839v1",
    "published_date": "2024-11-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction",
    "authors": [
      "DaDong Jiang",
      "Zhihui Ke",
      "Xiaobo Zhou",
      "Zhi Hou",
      "Xianghui Yang",
      "Wenbo Hu",
      "Tie Qiu",
      "Chunchao Guo"
    ],
    "abstract": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent methods extend 3D Gaussian Splatting to dynamic scenes via additional deformation fields and apply explicit constraints like motion flow to guide the deformation. However, they learn motion changes from individual timestamps independently, making it challenging to reconstruct complex scenes, particularly when dealing with violent movement, extreme-shaped geometries, or reflective surfaces. To address the above issue, we design a plug-and-play module called TimeFormer to enable existing deformable 3D Gaussians reconstruction methods with the ability to implicitly model motion patterns from a learning perspective. Specifically, TimeFormer includes a Cross-Temporal Transformer Encoder, which adaptively learns the temporal relationships of deformable 3D Gaussians. Furthermore, we propose a two-stream optimization strategy that transfers the motion knowledge learned from TimeFormer to the base stream during the training phase. This allows us to remove TimeFormer during inference, thereby preserving the original rendering speed. Extensive experiments in the multi-view and monocular dynamic scenes validate qualitative and quantitative improvement brought by TimeFormer. Project Page: https://patrickddj.github.io/TimeFormer/",
    "arxiv_url": "http://arxiv.org/abs/2411.11941v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11941v1",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views",
    "authors": [
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Hanzhang Tu",
      "Ruizhi Shao",
      "Boning Liu",
      "Shengping Zhang",
      "Liqiang Nie",
      "Yebin Liu"
    ],
    "abstract": "Differentiable rendering techniques have recently shown promising results for free-viewpoint video synthesis of characters. However, such methods, either Gaussian Splatting or neural implicit rendering, typically necessitate per-subject optimization which does not meet the requirement of real-time rendering in an interactive application. We propose a generalizable Gaussian Splatting approach for high-resolution image rendering under a sparse-view camera setting. To this end, we introduce Gaussian parameter maps defined on the source views and directly regress Gaussian properties for instant novel view synthesis without any fine-tuning or optimization. We train our Gaussian parameter regression module on human-only data or human-scene data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable with both depth and rendering supervision or with only rendering supervision. We further introduce a regularization term and an epipolar attention mechanism to preserve geometry consistency between two source views, especially when neglecting depth supervision. Experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2411.11363v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11363v1",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes",
    "authors": [
      "Chensheng Peng",
      "Chengwei Zhang",
      "Yixiao Wang",
      "Chenfeng Xu",
      "Yichen Xie",
      "Wenzhao Zheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective static-dynamic decomposition and high-fidelity surface reconstruction in complex driving scenarios. Our approach employs a two-stage optimization pipeline of dynamic street Gaussians. In the first stage, we extract 2D motion masks based on the observation that 3D Gaussian Splatting inherently can reconstruct only the static regions in dynamic environments. These extracted 2D motion priors are then mapped into the Gaussian space in a differentiable manner, leveraging an efficient formulation of dynamic Gaussians in the second stage. Combined with the introduced geometric regularizations, our method are able to address the over-fitting issues caused by data sparsity in autonomous driving, reconstructing physically plausible Gaussians that align with object surfaces rather than floating in air. Furthermore, we introduce temporal cross-view consistency to ensure coherence across time and viewpoints, resulting in high-quality surface reconstruction. Comprehensive experiments demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior self-supervised arts and achieving accuracy comparable to methods relying on external 3D bounding box annotations. Code is available at \\url{https://github.com/chengweialan/DeSiRe-GS}",
    "arxiv_url": "http://arxiv.org/abs/2411.11921v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11921v1",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chengweialan/DeSiRe-GS",
    "keywords": [
      "efficient",
      "high-fidelity",
      "autonomous driving",
      "motion",
      "face",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VeGaS: Video Gaussian Splatting",
    "authors": [
      "Weronika Smolak-Dy≈ºewska",
      "Dawid Malarz",
      "Kornel Howil",
      "Jan Kaczmarczyk",
      "Marcin Mazur",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.",
    "arxiv_url": "http://arxiv.org/abs/2411.11024v1",
    "pdf_url": "http://arxiv.org/pdf/2411.11024v1",
    "published_date": "2024-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/gmum/VeGaS",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Direct and Explicit 3D Generation from a Single Image",
    "authors": [
      "Haoyu Wu",
      "Meher Gitika Karumuri",
      "Chuhang Zou",
      "Seungbae Bang",
      "Yuelong Li",
      "Dimitris Samaras",
      "Sunil Hadap"
    ],
    "abstract": "Current image-to-3D approaches suffer from high computational costs and lack scalability for high-resolution outputs. In contrast, we introduce a novel framework to directly generate explicit surface geometry and texture using multi-view 2D depth and RGB images along with 3D Gaussian features using a repurposed Stable Diffusion model. We introduce a depth branch into U-Net for efficient and high quality multi-view, cross-domain generation and incorporate epipolar attention into the latent-to-pixel decoder for pixel-level multi-view consistency. By back-projecting the generated depth pixels into 3D space, we create a structured 3D representation that can be either rendered via Gaussian splatting or extracted to high-quality meshes, thereby leveraging additional novel view synthesis loss to further improve our performance. Extensive experiments demonstrate that our method surpasses existing baselines in geometry and texture quality while achieving significantly faster generation time.",
    "arxiv_url": "http://arxiv.org/abs/2411.10947v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10947v1",
    "published_date": "2024-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "high quality",
      "3d gaussian",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment",
    "authors": [
      "Mangyu Kong",
      "Jaewon Lee",
      "Seongwon Lee",
      "Euntai Kim"
    ],
    "abstract": "We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic SLAM framework built on the foundation of Gaussian Splatting. While recent advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene representation, most approaches assume a static environment, making them vulnerable to photometric and geometric inconsistencies caused by dynamic objects. To address these challenges, we integrate Gaussian Splatting SLAM with a robust filtering process to handle dynamic objects throughout the entire pipeline, including Gaussian insertion and keyframe selection. Within this framework, to further improve the accuracy of dynamic object removal, we introduce a robust mask generation method that enforces photometric consistency across keyframes, reducing noise from inaccurate segmentation and artifacts such as shadows. Additionally, we propose the loop-aware window selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops between the current and past frames, facilitating joint optimization of the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art performance in both camera tracking and novel view synthesis on various dynamic SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2411.10722v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10722v1",
    "published_date": "2024-11-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "shadow",
      "3d gaussian",
      "gaussian splatting",
      "slam",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction",
    "authors": [
      "Yutao Tang",
      "Yuxiang Guo",
      "Deming Li",
      "Cheng Peng"
    ],
    "abstract": "Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve photorealistic rendering; however, such capability is limited in sparse-view scenarios due to sparse initialization and over-fitting floaters. Recent progress in depth estimation and alignment can provide dense point cloud with few views; however, the resulting pose accuracy is suboptimal. In this work, we present SPARS3R, which combines the advantages of accurate pose estimation from Structure-from-Motion and dense point cloud from depth estimation. To this end, SPARS3R first performs a Global Fusion Alignment process that maps a prior dense point cloud to a sparse point cloud from Structure-from-Motion based on triangulated correspondences. RANSAC is applied during this process to distinguish inliers and outliers. SPARS3R then performs a second, Semantic Outlier Alignment step, which extracts semantically coherent regions around the outliers and performs local alignment in these regions. Along with several improvements in the evaluation process, we demonstrate that SPARS3R can achieve photorealistic rendering with sparse images and significantly outperforms existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2411.12592v1",
    "pdf_url": "http://arxiv.org/pdf/2411.12592v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "semantic",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods",
    "authors": [
      "Yifu Tao",
      "Miguel √Ångel Mu√±oz-Ba√±√≥n",
      "Lintong Zhang",
      "Jiahao Wang",
      "Lanke Frank Tarimo Fu",
      "Maurice Fallon"
    ],
    "abstract": "This paper introduces a large-scale multi-modal dataset captured in and around well-known landmarks in Oxford using a custom-built multi-sensor perception unit as well as a millimetre-accurate map from a Terrestrial LiDAR Scanner (TLS). The perception unit includes three synchronised global shutter colour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all precisely calibrated. We also establish benchmarks for tasks involving localisation, reconstruction, and novel-view synthesis, which enable the evaluation of Simultaneous Localisation and Mapping (SLAM) methods, Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as radiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting. To evaluate 3D reconstruction the TLS 3D models are used as ground truth. Localisation ground truth is computed by registering the mobile LiDAR scans to the TLS 3D models. Radiance field methods are evaluated not only with poses sampled from the input trajectory, but also from viewpoints that are from trajectories which are distant from the training poses. Our evaluation demonstrates a key limitation of state-of-the-art radiance field methods: we show that they tend to overfit to the training poses/images and do not generalise well to out-of-sequence poses. They also underperform in 3D reconstruction compared to MVS systems using the same visual inputs. Our dataset and benchmarks are intended to facilitate better integration of radiance field methods and SLAM systems. The raw and processed data, along with software for parsing and evaluation, can be accessed at https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.",
    "arxiv_url": "http://arxiv.org/abs/2411.10546v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10546v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ],
    "abstract": "Spike cameras, as an innovative neuromorphic camera that captures scenes with the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Previous spike-based 3D reconstruction approaches often employ a casecased pipeline: starting with high-quality image reconstruction from spike streams based on established spike-to-image reconstruction algorithms, then progressing to camera pose estimation and 3D reconstruction. However, this cascaded approach suffers from substantial cumulative errors, where quality limitations of initial image reconstructions negatively impact pose estimation, ultimately degrading the fidelity of the 3D reconstruction. To address these issues, we propose a synergistic optimization framework, \\textbf{USP-Gaussian}, that unifies spike-based image reconstruction, pose correction, and Gaussian splatting into an end-to-end framework. Leveraging the multi-view consistency afforded by 3DGS and the motion capture capability of the spike camera, our framework enables a joint iterative optimization that seamlessly integrates information between the spike-to-image network and 3DGS. Experiments on synthetic datasets with accurate poses demonstrate that our method surpasses previous approaches by effectively eliminating cascading errors. Moreover, we integrate pose optimization to achieve robust 3D reconstruction in real-world scenarios with inaccurate initial poses, outperforming alternative methods by effectively reducing noise and preserving fine texture details. Our code, data and trained models will be available at \\url{https://github.com/chenkang455/USP-Gaussian}.",
    "arxiv_url": "http://arxiv.org/abs/2411.10504v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10504v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/chenkang455/USP-Gaussian",
    "keywords": [
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Density Control for 3D Gaussian Splatting",
    "authors": [
      "Xiaobin Deng",
      "Changyu Diao",
      "Min Li",
      "Ruohan Yu",
      "Duanqing Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated outstanding performance in novel view synthesis, achieving a balance between rendering quality and real-time performance. 3DGS employs Adaptive Density Control (ADC) to increase the number of Gaussians. However, the clone and split operations within ADC are not sufficiently efficient, impacting optimization speed and detail recovery. Additionally, overfitted Gaussians that affect rendering quality may exist, and the original ADC is unable to remove them. To address these issues, we propose two key innovations: (1) Long-Axis Split, which precisely controls the position, shape, and opacity of child Gaussians to minimize the difference before and after splitting. (2) Recovery-Aware Pruning, which leverages differences in recovery speed after resetting opacity to prune overfitted Gaussians, thereby improving generalization performance. Experimental results show that our method significantly enhances rendering quality. Code is available at https://github.com/XiaoBin2001/EDC.",
    "arxiv_url": "http://arxiv.org/abs/2411.10133v3",
    "pdf_url": "http://arxiv.org/pdf/2411.10133v3",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/XiaoBin2001/EDC",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSEditPro: 3D Gaussian Splatting Editing with Attention-based Progressive Localization",
    "authors": [
      "Yanhao Sun",
      "RunZe Tian",
      "Xiao Han",
      "XinYao Liu",
      "Yan Zhang",
      "Kai Xu"
    ],
    "abstract": "With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D representations like Neural Radiance Fields (NeRF), many text-driven generative editing methods based on NeRF have appeared. However, the implicit encoding of geometric and textural information poses challenges in accurately locating and controlling objects during editing. Recently, significant advancements have been made in the editing methods of 3D Gaussian Splatting, a real-time rendering technology that relies on explicit representation. However, these methods still suffer from issues including inaccurate localization and limited manipulation over editing. To tackle these challenges, we propose GSEditPro, a novel 3D scene editing framework which allows users to perform various creative and precise editing using text prompts only. Leveraging the explicit nature of the 3D Gaussian distribution, we introduce an attention-based progressive localization module to add semantic labels to each Gaussian during rendering. This enables precise localization on editing areas by classifying Gaussians based on their relevance to the editing prompts derived from cross-attention layers of the T2I model. Furthermore, we present an innovative editing optimization method based on 3D Gaussian Splatting, obtaining stable and refined editing results through the guidance of Score Distillation Sampling and pseudo ground truth. We prove the efficacy of our method through extensive experiments.",
    "arxiv_url": "http://arxiv.org/abs/2411.10033v1",
    "pdf_url": "http://arxiv.org/pdf/2411.10033v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "semantic",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video",
    "authors": [
      "Jingxuan Chen"
    ],
    "abstract": "Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar's superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model's benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.",
    "arxiv_url": "http://arxiv.org/abs/2411.09952v1",
    "pdf_url": "http://arxiv.org/pdf/2411.09952v1",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "https://github.com/J-X-Chen/GGAvatar/",
    "keywords": [
      "lighting",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RenderBender: A Survey on Adversarial Attacks Using Differentiable Rendering",
    "authors": [
      "Matthew Hull",
      "Haoran Wang",
      "Matthew Lau",
      "Alec Helbling",
      "Mansi Phute",
      "Chao Zhang",
      "Zsolt Kira",
      "Willian Lunardi",
      "Martin Andreoni",
      "Wenke Lee",
      "Polo Chau"
    ],
    "abstract": "Differentiable rendering techniques like Gaussian Splatting and Neural Radiance Fields have become powerful tools for generating high-fidelity models of 3D objects and scenes. Their ability to produce both physically plausible and differentiable models of scenes are key ingredient needed to produce physically plausible adversarial attacks on DNNs. However, the adversarial machine learning community has yet to fully explore these capabilities, partly due to differing attack goals (e.g., misclassification, misdetection) and a wide range of possible scene manipulations used to achieve them (e.g., alter texture, mesh). This survey contributes the first framework that unifies diverse goals and tasks, facilitating easy comparison of existing work, identifying research gaps, and highlighting future directions - ranging from expanding attack goals and tasks to account for new modalities, state-of-the-art models, tools, and pipelines, to underscoring the importance of studying real-world threats in complex scenes.",
    "arxiv_url": "http://arxiv.org/abs/2411.09749v2",
    "pdf_url": "http://arxiv.org/pdf/2411.09749v2",
    "published_date": "2024-11-14",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "survey",
      "lighting",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment for Accelerated 3D Mesh Reconstruction",
    "authors": [
      "Shengchao Zhao",
      "Yundong Li"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS), which lead to high-quality novel view synthesis and accelerated rendering, have remarkably improved the quality of radiance field reconstruction. However, the extraction of mesh from a massive number of minute 3D Gaussian points remains great challenge due to the large volume of Gaussians and difficulty of representation of sharp signals caused by their inherent low-pass characteristics. To address this issue, we propose DyGASR, which utilizes generalized exponential function instead of traditional 3D Gaussian to decrease the number of particles and dynamically optimize the representation of the captured signal. In addition, it is observed that reconstructing mesh with Generalized Exponential Splatting(GES) without modifications frequently leads to failures since the generalized exponential distribution centroids may not precisely align with the scene surface. To overcome this, we adopt Sugar's approach and introduce Generalized Surface Regularization (GSR), which reduces the smallest scaling vector of each point cloud to zero and ensures normal alignment perpendicular to the surface, facilitating subsequent Poisson surface mesh reconstruction. Additionally, we propose a dynamic resolution adjustment strategy that utilizes a cosine schedule to gradually increase image resolution from low to high during the training stage, thus avoiding constant full resolution, which significantly boosts the reconstruction speed. Our approach surpasses existing 3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations on various scene datasets, demonstrating a 25\\% increase in speed, and a 30\\% reduction in memory usage.",
    "arxiv_url": "http://arxiv.org/abs/2411.09156v2",
    "pdf_url": "http://arxiv.org/pdf/2411.09156v2",
    "published_date": "2024-11-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization",
    "authors": [
      "Mijeong Kim",
      "Jongwoo Lim",
      "Bohyung Han"
    ],
    "abstract": "Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2411.08879v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08879v1",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "few-shot",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards More Accurate Fake Detection on Images Generated from Advanced Generative and Neural Rendering Models",
    "authors": [
      "Chengdong Dong",
      "Vijayakumar Bhagavatula",
      "Zhenyu Zhou",
      "Ajay Kumar"
    ],
    "abstract": "The remarkable progress in neural-network-driven visual data generation, especially with neural rendering techniques like Neural Radiance Fields and 3D Gaussian splatting, offers a powerful alternative to GANs and diffusion models. These methods can produce high-fidelity images and lifelike avatars, highlighting the need for robust detection methods. In response, an unsupervised training technique is proposed that enables the model to extract comprehensive features from the Fourier spectrum magnitude, thereby overcoming the challenges of reconstructing the spectrum due to its centrosymmetric properties. By leveraging the spectral domain and dynamically combining it with spatial domain information, we create a robust multimodal detector that demonstrates superior generalization capabilities in identifying challenging synthetic images generated by the latest image synthesis techniques. To address the absence of a 3D neural rendering-based fake image database, we develop a comprehensive database that includes images generated by diverse neural rendering techniques, providing a robust foundation for evaluating and advancing detection methods.",
    "arxiv_url": "http://arxiv.org/abs/2411.08642v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08642v1",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "3d gaussian",
      "ar",
      "avatar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis",
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ],
    "abstract": "We present billboard Splatting (BBSplat) - a novel approach for novel view synthesis based on textured geometric primitives. BBSplat represents the scene as a set of optimizable textured planar primitives with learnable RGB textures and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The proposed primitives close the rendering quality gap between 2D and 3D Gaussian Splatting (GS), enabling the accurate extraction of 3D mesh as in the 2DGS framework. Additionally, the explicit nature of planar primitives enables the use of the ray-tracing effects in rasterization. Our novel regularization term encourages textures to have a sparser structure, enabling an efficient compression that leads to a reduction in the storage space of the model up to x17 times compared to 3DGS. Our experiments show the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU, and Mip-NeRF-360. Namely, we achieve a state-of-the-art PSNR of 29.72 for DTU at Full HD resolution.",
    "arxiv_url": "http://arxiv.org/abs/2411.08508v4",
    "pdf_url": "http://arxiv.org/pdf/2411.08508v4",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "3d gaussian",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Biomass phenotyping of oilseed rape through UAV multi-view oblique imaging with 3DGS and SAM model",
    "authors": [
      "Yutao Shen",
      "Hongyu Zhou",
      "Xin Yang",
      "Xuqi Lu",
      "Ziyue Guo",
      "Lixi Jiang",
      "Yong He",
      "Haiyan Cen"
    ],
    "abstract": "Biomass estimation of oilseed rape is crucial for optimizing crop productivity and breeding strategies. While UAV-based imaging has advanced high-throughput phenotyping, current methods often rely on orthophoto images, which struggle with overlapping leaves and incomplete structural information in complex field environments. This study integrates 3D Gaussian Splatting (3DGS) with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass estimation of oilseed rape. UAV multi-view oblique images from 36 angles were used to perform 3D reconstruction, with the SAM module enhancing point cloud segmentation. The segmented point clouds were then converted into point cloud volumes, which were fitted to ground-measured biomass using linear regression. The results showed that 3DGS (7k and 30k iterations) provided high accuracy, with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times of 7 and 49 minutes, respectively. This performance exceeded that of structure from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating superior efficiency. The SAM module achieved high segmentation accuracy, with a mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980. Additionally, a comparison of biomass extraction models found the point cloud volume model to be the most accurate, with an determination coefficient (R2) of 0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and individual crop volume models. This study highlights the potential of combining 3DGS with multi-view UAV imaging for improved biomass phenotyping.",
    "arxiv_url": "http://arxiv.org/abs/2411.08453v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08453v1",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization",
    "authors": [
      "Yueming Xu",
      "Haochen Jiang",
      "Zhongyang Xiao",
      "Jianfeng Feng",
      "Li Zhang"
    ],
    "abstract": "Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.",
    "arxiv_url": "http://arxiv.org/abs/2411.08373v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08373v1",
    "published_date": "2024-11-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "high-fidelity",
      "tracking",
      "motion",
      "face",
      "mapping",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation",
    "authors": [
      "Peng Wang",
      "Lingzhe Zhao",
      "Yin Zhang",
      "Shiyu Zhao",
      "Peidong Liu"
    ],
    "abstract": "Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras' local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach. Code is available at https://github.com/WU-CVGL/MBA-SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2411.08279v1",
    "pdf_url": "http://arxiv.org/pdf/2411.08279v1",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/WU-CVGL/MBA-SLAM",
    "keywords": [
      "localization",
      "efficient",
      "motion",
      "lighting",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Projecting Gaussian Ellipsoids While Avoiding Affine Projection Approximation",
    "authors": [
      "Han Qi",
      "Tao Cai",
      "Xiyue Han"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its real-time rendering speed and state-of-the-art rendering quality. However, during the rendering process, the use of the Jacobian of the affine approximation of the projection transformation leads to inevitable errors, resulting in blurriness, artifacts and a lack of scene consistency in the final rendered images. To address this issue, we introduce an ellipsoid-based projection method to calculate the projection of Gaussian ellipsoid onto the image plane, which is the primitive of 3D Gaussian Splatting. As our proposed ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera origins inside them or parts lying below $z=0$ plane in the camera space, we designed a pre-filtering strategy. Experiments over multiple widely adopted benchmark datasets show that our ellipsoid-based projection method can enhance the rendering quality of 3D Gaussian Splatting and its extensions.",
    "arxiv_url": "http://arxiv.org/abs/2411.07579v3",
    "pdf_url": "http://arxiv.org/pdf/2411.07579v3",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCut: Interactive segmentation via graph cut for 3D Gaussian Splatting",
    "authors": [
      "Umangi Jain",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ],
    "abstract": "We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts intuitive user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians. Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and further refine these coarse estimates using our graph construction. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any additional segmentation-aware training.",
    "arxiv_url": "http://arxiv.org/abs/2411.07555v1",
    "pdf_url": "http://arxiv.org/pdf/2411.07555v1",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D Gaussian Splatting",
    "authors": [
      "Qiankun Gao",
      "Jiarui Meng",
      "Chengxiang Wen",
      "Jie Chen",
      "Jian Zhang"
    ],
    "abstract": "The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed HiCoM, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about $20\\%$ and reduces the data storage by $85\\%$, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to $<2$ seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness.",
    "arxiv_url": "http://arxiv.org/abs/2411.07541v2",
    "pdf_url": "http://arxiv.org/pdf/2411.07541v2",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering",
    "authors": [
      "Zhihao Liang",
      "Hongdong Li",
      "Kui Jia",
      "Kailing Guo",
      "Qi Zhang"
    ],
    "abstract": "Recovering the intrinsic physical attributes of a scene from images, generally termed as the inverse rendering problem, has been a central and challenging task in computer vision and computer graphics. In this paper, we present GUS-IR, a novel framework designed to address the inverse rendering problem for complicated scenes featuring rough and glossy surfaces. This paper starts by analyzing and comparing two prominent shading techniques popularly used for inverse rendering, forward shading and deferred shading, effectiveness in handling complex materials. More importantly, we propose a unified shading solution that combines the advantages of both techniques for better decomposition. In addition, we analyze the normal modeling in 3D Gaussian Splatting (3DGS) and utilize the shortest axis as normal for each particle in GUS-IR, along with a depth-related regularization, resulting in improved geometric representation and better shape reconstruction. Furthermore, we enhance the probe-based baking scheme proposed by GS-IR to achieve more accurate ambient occlusion modeling to better handle indirect illumination. Extensive experiments have demonstrated the superior performance of GUS-IR in achieving precise intrinsic decomposition and geometric representation, supporting many downstream tasks (such as relighting, retouching) in computer vision, graphics, and extended reality.",
    "arxiv_url": "http://arxiv.org/abs/2411.07478v1",
    "pdf_url": "http://arxiv.org/pdf/2411.07478v1",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Hierarchical Compression Technique for 3D Gaussian Splatting Compression",
    "authors": [
      "He Huang",
      "Wenjie Huang",
      "Qi Yang",
      "Yiling Xu",
      "Zhu li"
    ],
    "abstract": "3D Gaussian Splatting (GS) demonstrates excellent rendering quality and generation speed in novel view synthesis. However, substantial data size poses challenges for storage and transmission, making 3D GS compression an essential technology. Current 3D GS compression research primarily focuses on developing more compact scene representations, such as converting explicit 3D GS data into implicit forms. In contrast, compression of the GS data itself has hardly been explored. To address this gap, we propose a Hierarchical GS Compression (HGSC) technique. Initially, we prune unimportant Gaussians based on importance scores derived from both global and local significance, effectively reducing redundancy while maintaining visual quality. An Octree structure is used to compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical attribute compression strategy by employing a KD-tree to partition the 3D GS into multiple blocks. We apply farthest point sampling to select anchor primitives within each block and others as non-anchor primitives with varying Levels of Details (LoDs). Anchor primitives serve as reference points for predicting non-anchor primitives across different LoDs to reduce spatial redundancy. For anchor primitives, we use the region adaptive hierarchical transform to achieve near-lossless compression of various attributes. For non-anchor primitives, each is predicted based on the k-nearest anchor primitives. To further minimize prediction errors, the reconstructed LoD and anchor primitives are combined to form new anchor primitives to predict the next LoD. Our method notably achieves superior compression quality and a significant data size reduction of over 4.5 times compared to the state-of-the-art compression method on small scenes datasets.",
    "arxiv_url": "http://arxiv.org/abs/2411.06976v2",
    "pdf_url": "http://arxiv.org/pdf/2411.06976v2",
    "published_date": "2024-11-11",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction",
    "authors": [
      "Decai Chen",
      "Brianne Oberson",
      "Ingo Feldmann",
      "Oliver Schreer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "3D Gaussian Splatting has recently achieved notable success in novel view synthesis for dynamic scenes and geometry reconstruction in static scenes. Building on these advancements, early methods have been developed for dynamic surface reconstruction by globally optimizing entire sequences. However, reconstructing dynamic scenes with significant topology changes, emerging or disappearing objects, and rapid movements remains a substantial challenge, particularly for long sequences. To address these issues, we propose AT-GS, a novel method for reconstructing high-quality dynamic surfaces from multi-view videos through per-frame incremental optimization. To avoid local minima across frames, we introduce a unified and adaptive gradient-aware densification strategy that integrates the strengths of conventional cloning and splitting techniques. Additionally, we reduce temporal jittering in dynamic surfaces by ensuring consistency in curvature maps across consecutive frames. Our method achieves superior accuracy and temporal coherence in dynamic surface reconstruction, delivering high-fidelity space-time novel view synthesis, even in complex and challenging scenes. Extensive experiments on diverse multi-view video datasets demonstrate the effectiveness of our approach, showing clear advantages over baseline methods. Project page: \\url{https://fraunhoferhhi.github.io/AT-GS}",
    "arxiv_url": "http://arxiv.org/abs/2411.06602v1",
    "pdf_url": "http://arxiv.org/pdf/2411.06602v1",
    "published_date": "2024-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFormer: Point Transformer for Robust 3D Gaussian Splatting",
    "authors": [
      "Yutong Chen",
      "Marko Mihajlovic",
      "Xiyi Chen",
      "Yiming Wang",
      "Sergey Prokudin",
      "Siyu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks.",
    "arxiv_url": "http://arxiv.org/abs/2411.06390v3",
    "pdf_url": "http://arxiv.org/pdf/2411.06390v3",
    "published_date": "2024-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Through the Curved Cover: Synthesizing Cover Aberrated Scenes with Refractive Field",
    "authors": [
      "Liuyue Xie",
      "Jiancong Guo",
      "Laszlo A. Jeni",
      "Zhiheng Jia",
      "Mingyang Li",
      "Yunwen Zhou",
      "Chao Guo"
    ],
    "abstract": "Recent extended reality headsets and field robots have adopted covers to protect the front-facing cameras from environmental hazards and falls. The surface irregularities on the cover can lead to optical aberrations like blurring and non-parametric distortions. Novel view synthesis methods like NeRF and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with optical aberrations. To address this challenge, we introduce SynthCover to enable novel view synthesis through protective covers for downstream extended reality applications. SynthCover employs a Refractive Field that estimates the cover's geometry, enabling precise analytical calculation of refracted rays. Experiments on synthetic and real-world scenes demonstrate our method's ability to accurately model scenes viewed through protective covers, achieving a significant improvement in rendering quality compared to prior methods. We also show that the model can adjust well to various cover geometries with synthetic sequences captured with covers of different surface curvatures. To motivate further studies on this problem, we provide the benchmarked dataset containing real and synthetic walkable scenes captured with protective cover optical aberrations.",
    "arxiv_url": "http://arxiv.org/abs/2411.06365v1",
    "pdf_url": "http://arxiv.org/pdf/2411.06365v1",
    "published_date": "2024-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AI-Driven Stylization of 3D Environments",
    "authors": [
      "Yuanbo Chen",
      "Yixiao Kang",
      "Yukun Song",
      "Cyrus Vachha",
      "Sining Huang"
    ],
    "abstract": "In this system, we discuss methods to stylize a scene of 3D primitive objects into a higher fidelity 3D scene using novel 3D representations like NeRFs and 3D Gaussian Splatting. Our approach leverages existing image stylization systems and image-to-3D generative models to create a pipeline that iteratively stylizes and composites 3D objects into scenes. We show our results on adding generated objects into a scene and discuss limitations.",
    "arxiv_url": "http://arxiv.org/abs/2411.06067v1",
    "pdf_url": "http://arxiv.org/pdf/2411.06067v1",
    "published_date": "2024-11-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSpa: An \"Optimizing-Sparsifying\" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting",
    "authors": [
      "Yangming Zhang",
      "Wenqi Jia",
      "Wei Niu",
      "Miao Yin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient \"optimizing-sparsifying\" solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10$\\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://noodle-lab.github.io/gaussianspa/.",
    "arxiv_url": "http://arxiv.org/abs/2411.06019v3",
    "pdf_url": "http://arxiv.org/pdf/2411.06019v3",
    "published_date": "2024-11-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering",
    "authors": [
      "Junxi Jin",
      "Xiulai Li",
      "Haiping Huang",
      "Lianjun Liu",
      "Yujie Sun",
      "Logan Liu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in real-time, high-quality 3D scene rendering. However, it faces several challenges, including Gaussian redundancy, limited ability to capture view-dependent effects, and difficulties in handling complex lighting and specular reflections. Additionally, methods that use spherical harmonics for color representation often struggle to effectively capture anisotropic components, especially when modeling view-dependent colors under complex lighting conditions, leading to insufficient contrast and unnatural color saturation. To address these limitations, we introduce PEP-GS, a perceptually-enhanced framework that dynamically predicts Gaussian attributes, including opacity, color, and covariance. We replace traditional spherical harmonics with a Hierarchical Granular-Structural Attention mechanism, which enables more accurate modeling of complex view-dependent color effects. By employing a stable and interpretable framework for opacity and covariance estimation, PEP-GS avoids the removal of essential Gaussians prematurely, ensuring a more accurate scene representation. Furthermore, perceptual optimization is applied to the final rendered images, enhancing perceptual consistency across different views and ensuring high-quality renderings with improved texture fidelity and fine-scale detail preservation. Experimental results demonstrate that PEP-GS outperforms state-of-the-art methods, particularly in challenging scenarios involving view-dependent effects and fine-scale details.",
    "arxiv_url": "http://arxiv.org/abs/2411.05731v3",
    "pdf_url": "http://arxiv.org/pdf/2411.05731v3",
    "published_date": "2024-11-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProEdit: Simple Progression is All You Need for High-Quality 3D Scene Editing",
    "authors": [
      "Jun-Kun Chen",
      "Yu-Xiong Wang"
    ],
    "abstract": "This paper proposes ProEdit - a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel progressive manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model's large feasible output space (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high quality and efficiency in performing each subtask. Extensive evaluation shows that our ProEdit achieves state-of-the-art results in various scenes and challenging editing tasks, all through a simple framework without any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, ProEdit also provides a new way to control, preview, and select the \"aggressivity\" of editing operation during the editing process.",
    "arxiv_url": "http://arxiv.org/abs/2411.05006v1",
    "pdf_url": "http://arxiv.org/pdf/2411.05006v1",
    "published_date": "2024-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views",
    "authors": [
      "Yuedong Chen",
      "Chuanxia Zheng",
      "Haofei Xu",
      "Bohan Zhuang",
      "Andrea Vedaldi",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ],
    "abstract": "We introduce MVSplat360, a feed-forward approach for 360{\\deg} novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360{\\deg} NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. The video results are available on our project page: https://donydchen.github.io/mvsplat360.",
    "arxiv_url": "http://arxiv.org/abs/2411.04924v1",
    "pdf_url": "http://arxiv.org/pdf/2411.04924v1",
    "published_date": "2024-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Discretized Gaussian Representation for Tomographic Reconstruction",
    "authors": [
      "Shaokai Wu",
      "Yuxiang Lu",
      "Wei Ji",
      "Suizhi Huang",
      "Fengyu Yang",
      "Shalayiding Sirejiding",
      "Qichen He",
      "Jing Tong",
      "Yanbiao Ji",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "abstract": "Computed Tomography (CT) is a widely used imaging technique that provides detailed cross-sectional views of objects. Over the past decade, Deep Learning-based Reconstruction (DLR) methods have led efforts to enhance image quality and reduce noise, yet they often require large amounts of data and are computationally intensive. Inspired by recent advancements in scene reconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting (3DGS) techniques for CT reconstruction. However, these methods are not ideal for direct 3D volume reconstruction. In this paper, we propose a novel Discretized Gaussian Representation (DGR) for CT reconstruction, which directly reconstructs the 3D volume using a set of discretized Gaussian functions in an end-to-end manner. To further enhance computational efficiency, we introduce a Fast Volume Reconstruction technique that aggregates the contributions of these Gaussians into a discretized volume in a highly parallelized fashion. Our extensive experiments on both real-world and synthetic datasets demonstrate that DGR achieves superior reconstruction quality and significantly improved computational efficiency compared to existing DLR and instance reconstruction methods. Our code has been provided for review purposes and will be made publicly available upon publication.",
    "arxiv_url": "http://arxiv.org/abs/2411.04844v3",
    "pdf_url": "http://arxiv.org/pdf/2411.04844v3",
    "published_date": "2024-11-07",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian Splatting",
    "authors": [
      "Jilan Mei",
      "Junbo Li",
      "Cai Meng"
    ],
    "abstract": "This paper proposes a new method for accurate and robust 6D pose estimation of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose can utilize the reconstruction results without requiring a high-quality CAD model, which means it only requires segmented RGBD images as input. Specifically, GS2Pose employs a two-stage structure consisting of coarse estimation followed by refined estimation. In the coarse stage, a lightweight U-Net network with a polarization attention mechanism, called Pose-Net, is designed. By using the 3DGS model for supervised training, Pose-Net can generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose formulates a pose regression algorithm following the idea of reprojection or Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that refines the coarse pose by comparing the input images with the rendered images. GS-Refiner also selectively updates parameters in the 3DGS model to achieve environmental adaptation, thereby enhancing the algorithm's robustness and flexibility to illuminative variation, occlusion, and other challenging disruptive factors. GS2Pose was evaluated through experiments conducted on the LineMod dataset, where it was compared with similar algorithms, yielding highly competitive results. The code for GS2Pose will soon be released on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2411.03807v3",
    "pdf_url": "http://arxiv.org/pdf/2411.03807v3",
    "published_date": "2024-11-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement",
    "authors": [
      "Ziqi Lu",
      "Jianbo Ye",
      "John Leonard"
    ],
    "abstract": "We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS's novel view rendering and EfficientSAM's zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D change masks and object transformations. Our method can accurately identify changes in cluttered environments using sparse (as few as one) post-change images within as little as 18s. It does not rely on depth input, user instructions, pre-defined object classes, or object models -- An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.",
    "arxiv_url": "http://arxiv.org/abs/2411.03706v2",
    "pdf_url": "http://arxiv.org/pdf/2411.03706v2",
    "published_date": "2024-11-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/520xyxyzq/3DGS-CD",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis",
    "authors": [
      "Rui Peng",
      "Wangze Xu",
      "Luyang Tang",
      "Liwei Liao",
      "Jianbo Jiao",
      "Ronggang Wang"
    ],
    "abstract": "Despite the substantial progress of novel view synthesis, existing methods, either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian Splatting (3DGS), suffer significant degradation when the input becomes sparse. Numerous efforts have been introduced to alleviate this problem, but they still struggle to synthesize satisfactory results efficiently, especially in the large scene. In this paper, we propose SCGaussian, a Structure Consistent Gaussian Splatting method using matching priors to learn 3D consistent scene structure. Considering the high interdependence of Gaussian attributes, we optimize the scene structure in two folds: rendering geometry and, more importantly, the position of Gaussian primitives, which is hard to be directly constrained in the vanilla 3DGS due to the non-structure property. To achieve this, we present a hybrid Gaussian representation. Besides the ordinary non-structure Gaussian primitives, our model also consists of ray-based Gaussian primitives that are bound to matching rays and whose optimization of their positions is restricted along the ray. Thus, we can utilize the matching correspondence to directly enforce the position of these Gaussian primitives to converge to the surface points where rays intersect. Extensive experiments on forward-facing, surrounding, and complex large scenes show the effectiveness of our approach with state-of-the-art performance and high efficiency. Code is available at https://github.com/prstrive/SCGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2411.03637v1",
    "pdf_url": "http://arxiv.org/pdf/2411.03637v1",
    "published_date": "2024-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/prstrive/SCGaussian",
    "keywords": [
      "efficient",
      "few-shot",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "large scene",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting",
    "authors": [
      "Michael B√ºttner",
      "Jonathan Francis",
      "Helge Rhodin",
      "Andrew Melnik"
    ],
    "abstract": "This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.",
    "arxiv_url": "http://arxiv.org/abs/2411.03555v1",
    "pdf_url": "http://arxiv.org/pdf/2411.03555v1",
    "published_date": "2024-11-05",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features",
    "authors": [
      "Arnab Dey",
      "Cheng-You Lu",
      "Andrew I. Comport",
      "Srinath Sridhar",
      "Chin-Teng Lin",
      "Jean Martinet"
    ],
    "abstract": "Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2411.03086v1",
    "pdf_url": "http://arxiv.org/pdf/2411.03086v1",
    "published_date": "2024-11-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting",
    "authors": [
      "Huibin Zhao",
      "Weipeng Guan",
      "Peng Lu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the complementary characteristics of LiDAR and image sensors to capture both geometric structures and visual details of 3D scenes. To this end, the 3D Gaussians are initialized from colourized LiDAR points and optimized using differentiable rendering. In order to achieve high-fidelity mapping, we introduce a pyramid-based training approach to effectively learn multi-level features and incorporate depth loss derived from LiDAR measurements to improve geometric feature perception. Through well-designed strategies for Gaussian-Map expansion, keyframe selection, thread management, and custom CUDA acceleration, our framework achieves real-time photo-realistic mapping. Numerical experiments are performed to evaluate the superior performance of our method compared to state-of-the-art 3D reconstruction systems.",
    "arxiv_url": "http://arxiv.org/abs/2411.02703v1",
    "pdf_url": "http://arxiv.org/pdf/2411.02703v1",
    "published_date": "2024-11-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "mapping",
      "3d gaussian",
      "acceleration",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Modeling Uncertainty in 3D Gaussian Splatting through Continuous Semantic Splatting",
    "authors": [
      "Joey Wilson",
      "Marcelino Almeida",
      "Min Sun",
      "Sachit Mahajan",
      "Maani Ghaffari",
      "Parker Ewen",
      "Omid Ghasemalizadeh",
      "Cheng-Hao Kuo",
      "Arnie Sen"
    ],
    "abstract": "In this paper, we present a novel algorithm for probabilistically updating and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although previous methods have introduced algorithms which learn to rasterize features in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which presents a challenge for safety-critical robotic applications. To address this gap, we propose a method which advances the literature of continuous semantic mapping from voxels to ellipsoids, combining the precise structure of 3D-GS with the ability to quantify uncertainty of probabilistic robotic maps. Given a set of images, our algorithm performs a probabilistic semantic update directly on the 3D ellipsoids to obtain an expectation and variance through the use of conjugate priors. We also propose a probabilistic rasterization which returns per-pixel segmentation predictions with quantifiable uncertainty. We compare our method with similar probabilistic voxel-based methods to verify our extension to 3D ellipsoids, and perform ablation studies on uncertainty quantification and temporal smoothing.",
    "arxiv_url": "http://arxiv.org/abs/2411.02547v1",
    "pdf_url": "http://arxiv.org/pdf/2411.02547v1",
    "published_date": "2024-11-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "mapping",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatOverflow: Asynchronous Hardware Troubleshooting",
    "authors": [
      "Amritansh Kwatra",
      "Tobias Weinberg",
      "Ilan Mandel",
      "Ritik Batra",
      "Peter He",
      "Francois Guimbretiere",
      "Thijs Roumen"
    ],
    "abstract": "As tools for designing and manufacturing hardware become more accessible, smaller producers can develop and distribute novel hardware. However, processes for supporting end-user hardware troubleshooting or routine maintenance aren't well defined. As a result, providing technical support for hardware remains ad-hoc and challenging to scale. Inspired by patterns that helped scale software troubleshooting, we propose a workflow for asynchronous hardware troubleshooting: SplatOverflow.   SplatOverflow creates a novel boundary object, the SplatOverflow scene, that users reference to communicate about hardware. A scene comprises a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD model. The splat captures the current state of the hardware, and the registered CAD model acts as a referential anchor for troubleshooting instructions. With SplatOverflow, remote maintainers can directly address issues and author instructions in the user's workspace. Workflows containing multiple instructions can easily be shared between users and recontextualized in new environments.   In this paper, we describe the design of SplatOverflow, the workflows it enables, and its utility to different kinds of users. We also validate that non-experts can use SplatOverflow to troubleshoot common problems with a 3D printer in a usability study.   Project Page: https://amritkwatra.com/research/splatoverflow.",
    "arxiv_url": "http://arxiv.org/abs/2411.02332v3",
    "pdf_url": "http://arxiv.org/pdf/2411.02332v3",
    "published_date": "2024-11-04",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training",
    "authors": [
      "Ruihong Yin",
      "Vladimir Yugay",
      "Yue Li",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "abstract": "The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2411.02229v2",
    "pdf_url": "http://arxiv.org/pdf/2411.02229v2",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes",
    "authors": [
      "Gaochao Song",
      "Chong Cheng",
      "Hao Wang"
    ],
    "abstract": "In this paper we present a novel method for efficient and effective 3D surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF) based works typically require extensive training and rendering time due to the adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS) uses an explicit and discrete representation, hence the reconstructed surface is built by the huge number of Gaussian primitives, which leads to excessive memory consumption and rough surface details in sparse Gaussian areas. To address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which establish a continuous scene representation based on discrete 3DGS through kernel regression. The GVKF integrates fast 3DGS rasterization and highly effective scene implicit representations, achieving high-fidelity open scene surface reconstruction. Experiments on challenging scene datasets demonstrate the efficiency and effectiveness of our proposed GVKF, featuring with high reconstruction quality, real-time rendering speed, significant savings in storage and training memory consumption.",
    "arxiv_url": "http://arxiv.org/abs/2411.01853v3",
    "pdf_url": "http://arxiv.org/pdf/2411.01853v3",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes with 4D Gaussian Splatting",
    "authors": [
      "Fengze Li",
      "Jishuai He",
      "Jieming Ma",
      "Zhijing Wu"
    ],
    "abstract": "Dynamic scene reconstruction is essential in robotic minimally invasive surgery, providing crucial spatial information that enhances surgical precision and outcomes. However, existing methods struggle to address the complex, temporally dynamic nature of endoscopic scenes. This paper presents ST-Endo4DGS, a novel framework that models the spatio-temporal volume of dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS) primitives, parameterized by anisotropic ellipses with flexible 4D rotations. This approach enables precise representation of deformable tissue dynamics, capturing intricate spatial and temporal correlations in real time. Additionally, we extend spherindrical harmonics to represent time-evolving appearance, achieving realistic adaptations to lighting and view changes. A new endoscopic normal alignment constraint (ENAC) further enhances geometric fidelity by aligning rendered normals with depth-derived geometry. Extensive evaluations show that ST-Endo4DGS outperforms existing methods in both visual quality and real-time performance, establishing a new state-of-the-art in dynamic scene reconstruction for endoscopic surgery.",
    "arxiv_url": "http://arxiv.org/abs/2411.01218v1",
    "pdf_url": "http://arxiv.org/pdf/2411.01218v1",
    "published_date": "2024-11-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes",
    "authors": [
      "Yang Liu",
      "Chuanchen Luo",
      "Zhongkai Mao",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10$\\times$ compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.",
    "arxiv_url": "http://arxiv.org/abs/2411.00771v2",
    "pdf_url": "http://arxiv.org/pdf/2411.00771v2",
    "published_date": "2024-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud Understanding",
    "authors": [
      "Jincen Jiang",
      "Qianyu Zhou",
      "Yuhang Li",
      "Xinkui Zhao",
      "Meili Wang",
      "Lizhuang Ma",
      "Jian Chang",
      "Jian Jun Zhang",
      "Xuequan Lu"
    ],
    "abstract": "In this paper, we present PCoTTA, an innovative, pioneering framework for Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding, enhancing the model's transferability towards the continually changing target domain. We introduce a multi-task setting for PCoTTA, which is practical and realistic, handling multiple tasks within one unified model during the continual adaptation. Our PCoTTA involves three key components: automatic prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and contrastive prototype repulsion (CPR). Firstly, APM is designed to automatically mix the source prototypes with the learnable prototypes with a similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS dynamically shifts the testing sample toward the source domain, mitigating error accumulation in an online manner. In addition, CPR is proposed to pull the nearest learnable prototype close to the testing feature and push it away from other prototypes, making each prototype distinguishable during the adaptation. Experimental comparisons lead to a new benchmark, demonstrating PCoTTA's superiority in boosting the model's transferability towards the continually changing target domain.",
    "arxiv_url": "http://arxiv.org/abs/2411.00632v1",
    "pdf_url": "http://arxiv.org/pdf/2411.00632v1",
    "published_date": "2024-11-01",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Aquatic-GS: A Hybrid 3D Representation for Underwater Scenes",
    "authors": [
      "Shaohua Liu",
      "Junzhe Lu",
      "Zuoya Gu",
      "Jiajun Li",
      "Yue Deng"
    ],
    "abstract": "Representing underwater 3D scenes is a valuable yet complex task, as attenuation and scattering effects during underwater imaging significantly couple the information of the objects and the water. This coupling presents a significant challenge for existing methods in effectively representing both the objects and the water medium simultaneously. To address this challenge, we propose Aquatic-GS, a hybrid 3D representation approach for underwater scenes that effectively represents both the objects and the water medium. Specifically, we construct a Neural Water Field (NWF) to implicitly model the water parameters, while extending the latest 3D Gaussian Splatting (3DGS) to model the objects explicitly. Both components are integrated through a physics-based underwater image formation model to represent complex underwater scenes. Moreover, to construct more precise scene geometry and details, we design a Depth-Guided Optimization (DGO) mechanism that uses a pseudo-depth map as auxiliary guidance. After optimization, Aquatic-GS enables the rendering of novel underwater viewpoints and supports restoring the true appearance of underwater scenes, as if the water medium were absent. Extensive experiments on both simulated and real-world datasets demonstrate that Aquatic-GS surpasses state-of-the-art underwater 3D representation methods, achieving better rendering quality and real-time rendering performance with a 410x increase in speed. Furthermore, regarding underwater image restoration, Aquatic-GS outperforms representative dewatering methods in color correction, detail recovery, and stability. Our models, code, and datasets can be accessed at https://aquaticgs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2411.00239v2",
    "pdf_url": "http://arxiv.org/pdf/2411.00239v2",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis",
    "authors": [
      "Chen Zhao",
      "Xuan Wang",
      "Tong Zhang",
      "Saqib Javed",
      "Mathieu Salzmann"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A $\\mathbf{\\Delta}$-model and a $\\mathbf{\\Sigma}$-model are jointly trained on the available images. The $\\mathbf{\\Delta}$-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the $\\mathbf{\\Sigma}$-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the $\\mathbf{\\Sigma}$-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html.",
    "arxiv_url": "http://arxiv.org/abs/2411.00144v3",
    "pdf_url": "http://arxiv.org/pdf/2411.00144v3",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "head",
      "few-shot",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "URAvatar: Universal Relightable Gaussian Codec Avatars",
    "authors": [
      "Junxuan Li",
      "Chen Cao",
      "Gabriel Schwartz",
      "Rawal Khirodkar",
      "Christian Richardt",
      "Tomas Simon",
      "Yaser Sheikh",
      "Shunsuke Saito"
    ],
    "abstract": "We present a new approach to creating photorealistic and relightable head avatars from a phone scan with unknown illumination. The reconstructed avatars can be animated and relit in real time with the global illumination of diverse environments. Unlike existing approaches that estimate parametric reflectance parameters via inverse rendering, our approach directly models learnable radiance transfer that incorporates global light transport in an efficient manner for real-time rendering. However, learning such a complex light transport that can generalize across identities is non-trivial. A phone scan in a single environment lacks sufficient information to infer how the head would appear in general environments. To address this, we build a universal relightable avatar model represented by 3D Gaussians. We train on hundreds of high-quality multi-view human scans with controllable point lights. High-resolution geometric guidance further enhances the reconstruction accuracy and generalization. Once trained, we finetune the pretrained model on a phone scan using inverse rendering to obtain a personalized relightable avatar. Our experiments establish the efficacy of our design, outperforming existing approaches while retaining real-time rendering capability.",
    "arxiv_url": "http://arxiv.org/abs/2410.24223v1",
    "pdf_url": "http://arxiv.org/pdf/2410.24223v1",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "relightable",
      "global illumination",
      "light transport",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "illumination",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images",
    "authors": [
      "Botao Ye",
      "Sifei Liu",
      "Haofei Xu",
      "Xueting Li",
      "Marc Pollefeys",
      "Ming-Hsuan Yang",
      "Songyou Peng"
    ],
    "abstract": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. Code and trained models are available at https://noposplat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2410.24207v1",
    "pdf_url": "http://arxiv.org/pdf/2410.24207v1",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering",
    "authors": [
      "Kai Ye",
      "Chong Gao",
      "Guanbin Li",
      "Wenzheng Chen",
      "Baoquan Chen"
    ],
    "abstract": "We consider the problem of physically-based inverse rendering using 3D Gaussian Splatting (3DGS) representations. While recent 3DGS methods have achieved remarkable results in novel view synthesis (NVS), accurately capturing high-fidelity geometry, physically interpretable materials and lighting remains challenging, as it requires precise geometry modeling to provide accurate surface normals, along with physically-based rendering (PBR) techniques to ensure correct material and lighting disentanglement. Previous 3DGS methods resort to approximating surface normals, but often struggle with noisy local geometry, leading to inaccurate normal estimation and suboptimal material-lighting decomposition. In this paper, we introduce GeoSplatting, a novel hybrid representation that augments 3DGS with explicit geometric guidance and differentiable PBR equations. Specifically, we bridge isosurface and 3DGS together, where we first extract isosurface mesh from a scalar field, then convert it into 3DGS points and formulate PBR equations for them in a fully differentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry, enabling precise surface normal modeling, which facilitates the use of PBR frameworks for material decomposition. This approach further maintains the efficiency and quality of NVS from 3DGS while ensuring accurate geometry from the isosurface. Comprehensive evaluations across diverse datasets demonstrate the superiority of GeoSplatting, consistently outperforming existing methods both quantitatively and qualitatively.",
    "arxiv_url": "http://arxiv.org/abs/2410.24204v2",
    "pdf_url": "http://arxiv.org/pdf/2410.24204v2",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting",
    "authors": [
      "Xiufeng Huang",
      "Ruiqi Li",
      "Yiu-ming Cheung",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D assets. To protect the copyright of these assets, digital watermarking techniques can be applied to embed ownership information discreetly within 3DGS models. However, existing watermarking methods for meshes, point clouds, and implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS models use explicit 3D Gaussians with distinct structures and do not rely on neural networks. Naively embedding the watermark on a pre-trained 3DGS can cause obvious distortion in rendered images. In our work, we propose an uncertainty-based method that constrains the perturbation of model parameters to achieve invisible watermarking for 3DGS. At the message decoding stage, the copyright messages can be reliably extracted from both 3D Gaussians and 2D rendered images even under various forms of 3D and 2D distortions. We conduct extensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate the effectiveness of our proposed method, demonstrating state-of-the-art performance on both message decoding accuracy and view synthesis quality.",
    "arxiv_url": "http://arxiv.org/abs/2410.23718v1",
    "pdf_url": "http://arxiv.org/pdf/2410.23718v1",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM",
    "authors": [
      "Xiaomeng Wang",
      "Nan Wang",
      "Guofeng Zhang"
    ],
    "abstract": "In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a modular code design and a multi-process running mechanism, providing highly reusable foundational modules such as unified dataset management, 3d visualization, algorithm configuration, and metrics evaluation. It can help developers quickly build a complete SLAM system, flexibly combine different algorithm modules, and conduct standardized benchmarking for accuracy and efficiency comparison. Within this framework, we integrate several state-of-the-art SLAM algorithms with different types, including NeRF and 3DGS based SLAM, and even odometry or reconstruction algorithms, which demonstrates the flexibility and extensibility. We also conduct a comprehensive comparison and evaluation of these integrated algorithms, analyzing the characteristics of each. Finally, we contribute all the code, configuration and data to the open-source community, which aims to promote the widespread research and development of SLAM technology within the open-source ecosystem.",
    "arxiv_url": "http://arxiv.org/abs/2410.23690v1",
    "pdf_url": "http://arxiv.org/pdf/2410.23690v1",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring",
    "authors": [
      "Dongwoo Lee",
      "Joonkyu Park",
      "Kyoung Mu Lee"
    ],
    "abstract": "To train a deblurring network, an appropriate dataset with paired blurry and sharp images is essential. Existing datasets collect blurry images either synthetically by aggregating consecutive sharp frames or using sophisticated camera systems to capture real blur. However, these methods offer limited diversity in blur types (blur trajectories) or require extensive human effort to reconstruct large-scale datasets, failing to fully reflect real-world blur scenarios. To address this, we propose GS-Blur, a dataset of synthesized realistic blurry images created using a novel approach. To this end, we first reconstruct 3D scenes from multi-view images using 3D Gaussian Splatting (3DGS), then render blurry images by moving the camera view along the randomly generated motion trajectories. By adopting various camera trajectories in reconstructing our GS-Blur, our dataset contains realistic and diverse types of blur, offering a large-scale dataset that generalizes well to real-world blur. Using GS-Blur with various deblurring methods, we demonstrate its ability to generalize effectively compared to previous synthetic or real blur datasets, showing significant improvements in deblurring performance.",
    "arxiv_url": "http://arxiv.org/abs/2410.23658v1",
    "pdf_url": "http://arxiv.org/pdf/2410.23658v1",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ELMGS: Enhancing memory and computation scaLability through coMpression for 3D Gaussian Splatting",
    "authors": [
      "Muhammad Salman Ali",
      "Sung-Ho Bae",
      "Enzo Tartaglione"
    ],
    "abstract": "3D models have recently been popularized by the potentiality of end-to-end training offered first by Neural Radiance Fields and most recently by 3D Gaussian Splatting models. The latter has the big advantage of naturally providing fast training convergence and high editability. However, as the research around these is still in its infancy, there is still a gap in the literature regarding the model's scalability. In this work, we propose an approach enabling both memory and computation scalability of such models. More specifically, we propose an iterative pruning strategy that removes redundant information encoded in the model. We also enhance compressibility for the model by including in the optimization strategy a differentiable quantization and entropy coding estimator. Our results on popular benchmarks showcase the effectiveness of the proposed approach and open the road to the broad deployability of such a solution even on resource-constrained devices.",
    "arxiv_url": "http://arxiv.org/abs/2410.23213v1",
    "pdf_url": "http://arxiv.org/pdf/2410.23213v1",
    "published_date": "2024-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis",
    "authors": [
      "Zhiyuan Min",
      "Yawei Luo",
      "Jianwen Sun",
      "Yi Yang"
    ],
    "abstract": "Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from sparse-view observations in a feed-forward inference manner, eliminating the need for scene-specific retraining required in conventional 3DGS. However, existing methods rely heavily on epipolar priors, which can be unreliable in complex realworld scenes, particularly in non-overlapping and occluded regions. In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints. To enhance multiview feature extraction with 3D perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view completion pre-training on large-scale datasets. Additionally, we introduce an Iterative Cross-view Gaussians Alignment method to ensure consistent depth scales across different views. Our eFreeSplat represents an innovative approach for generalizable novel view synthesis. Different from the existing pure geometry-free methods, eFreeSplat focuses more on achieving epipolar-free feature matching and encoding by providing 3D priors through cross-view pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks using the RealEstate10K and ACID datasets. Extensive experiments demonstrate that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar priors, achieving superior geometry reconstruction and novel view synthesis quality. Project page: https://tatakai1.github.io/efreesplat/.",
    "arxiv_url": "http://arxiv.org/abs/2410.22817v2",
    "pdf_url": "http://arxiv.org/pdf/2410.22817v2",
    "published_date": "2024-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images",
    "authors": [
      "Qi Song",
      "Ziyuan Luo",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed \"geometry cloaks\", into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak. Our project is available at https://qsong2001.github.io/geometry_cloak.",
    "arxiv_url": "http://arxiv.org/abs/2410.22705v1",
    "pdf_url": "http://arxiv.org/pdf/2410.22705v1",
    "published_date": "2024-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Sunghwan Hong",
      "Jaewoo Jung",
      "Heeseong Shin",
      "Jisang Han",
      "Jiaolong Yang",
      "Chong Luo",
      "Seungryong Kim"
    ],
    "abstract": "We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices.",
    "arxiv_url": "http://arxiv.org/abs/2410.22128v1",
    "pdf_url": "http://arxiv.org/pdf/2410.22128v1",
    "published_date": "2024-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lightweight",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeGaussian: Annotation-free Controllable 3D Gaussian Splats with Flow Derivatives",
    "authors": [
      "Qizhi Chen",
      "Delin Qu",
      "Junli Liu",
      "Yiwen Tang",
      "Haoming Song",
      "Dong Wang",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Reconstructing controllable Gaussian splats from monocular video is a challenging task due to its inherently insufficient constraints. Widely adopted approaches supervise complex interactions with additional masks and control signal annotations, limiting their real-world applications. In this paper, we propose an annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian motion from optical flow and camera motion using novel dynamic Gaussian constraints. By establishing a connection between 2D flows and 3D Gaussian dynamic control, our method enables self-supervised optimization and continuity of dynamic Gaussian motions from flow priors. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state with a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Quantitative and qualitative evaluations on extensive experiments demonstrate the state-of-the-art visual performance and control capability of our method. Project page: https://freegaussian.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2410.22070v2",
    "pdf_url": "http://arxiv.org/pdf/2410.22070v2",
    "published_date": "2024-10-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian Splatting",
    "authors": [
      "Yuetao Li",
      "Zijia Kuang",
      "Ting Li",
      "Guyue Zhou",
      "Shaohui Zhang",
      "Zike Yan"
    ],
    "abstract": "We propose ActiveSplat, an autonomous high-fidelity reconstruction system leveraging Gaussian splatting. Taking advantage of efficient and realistic rendering, the system establishes a unified framework for online mapping, viewpoint selection, and path planning. The key to ActiveSplat is a hybrid map representation that integrates both dense information about the environment and a sparse abstraction of the workspace. Therefore, the system leverages sparse topology for efficient viewpoint sampling and path planning, while exploiting view-dependent dense prediction for viewpoint selection, facilitating efficient decision-making with promising accuracy and completeness. A hierarchical planning strategy based on the topological map is adopted to mitigate repetitive trajectories and improve local granularity given limited budgets, ensuring high-fidelity reconstruction with photorealistic view synthesis. Extensive experiments and ablation studies validate the efficacy of the proposed method in terms of reconstruction accuracy, data coverage, and exploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.",
    "arxiv_url": "http://arxiv.org/abs/2410.21955v1",
    "pdf_url": "http://arxiv.org/pdf/2410.21955v1",
    "published_date": "2024-10-29",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "mapping",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps",
    "authors": [
      "Yating Xu",
      "Chen Li",
      "Gim Hee Lee"
    ],
    "abstract": "The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection. Previous method relies on NeRF for geometry reasoning. However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance. In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection. To circumvent the requirement for a large number of depth planes for accurate depth prediction, we design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume. We select multiple locations that score top in the probability volume for each pixel and use their probability score to indicate the confidence. We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets are conducted to show the superiority of our model. Our code is available at https://github.com/Pixie8888/MVSDet.",
    "arxiv_url": "http://arxiv.org/abs/2410.21566v1",
    "pdf_url": "http://arxiv.org/pdf/2410.21566v1",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Pixie8888/MVSDet",
    "keywords": [
      "efficient",
      "head",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Grid4D: 4D Decomposed Hash Encoding for High-Fidelity Dynamic Gaussian Splatting",
    "authors": [
      "Jiawei Xu",
      "Zexin Fan",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "Recently, Gaussian splatting has received more and more attention in the field of static scene rendering. Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models. However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality. To tackle these problems, we propose Grid4D, a dynamic scene rendering model based on Gaussian splatting and employing a novel explicit encoding method for the 4D input through the hash encoding. Different from plane-based explicit representations, we decompose the 4D encoding into one spatial and three temporal 3D hash encodings without the low-rank assumption. Additionally, we design a novel attention module that generates the attention scores in a directional range to aggregate the spatial and temporal features. The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features. Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction. Our experiments demonstrate that Grid4D significantly outperforms the state-of-the-art models in visual quality and rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2410.20815v3",
    "pdf_url": "http://arxiv.org/pdf/2410.20815v3",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with Gaussian Splatting for Enhanced Human Avatars",
    "authors": [
      "Xiaonuo Dongye",
      "Hanzhi Guo",
      "Le Luo",
      "Haiyan Jiang",
      "Yihua Bao",
      "Zeyu Tian",
      "Dongdong Weng"
    ],
    "abstract": "With the advancement of virtual reality, the demand for 3D human avatars is increasing. The emergence of Gaussian Splatting technology has enabled the rendering of Gaussian avatars with superior visual quality and reduced computational costs. Despite numerous methods researchers propose for implementing drivable Gaussian avatars, limited attention has been given to balancing visual quality and computational costs. In this paper, we introduce LoDAvatar, a method that introduces levels of detail into Gaussian avatars through hierarchical embedding and selective detail enhancement methods. The key steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian optimization, and selective detail enhancement. We conducted experiments involving Gaussian avatars at various levels of detail, employing both objective assessments and subjective evaluations. The outcomes indicate that incorporating levels of detail into Gaussian avatars can decrease computational costs during rendering while upholding commendable visual quality, thereby enhancing runtime frame rates. We advocate adopting LoDAvatar to render multiple dynamic Gaussian avatars or extensive Gaussian scenes to balance visual quality and computational costs.",
    "arxiv_url": "http://arxiv.org/abs/2410.20789v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20789v1",
    "published_date": "2024-10-28",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians",
    "authors": [
      "Chongjian Ge",
      "Chenfeng Xu",
      "Yuanfeng Ji",
      "Chensheng Peng",
      "Masayoshi Tomizuka",
      "Ping Luo",
      "Mingyu Ding",
      "Varun Jampani",
      "Wei Zhan"
    ],
    "abstract": "Recent breakthroughs in text-guided image generation have significantly advanced the field of 3D generation. While generating a single high-quality 3D object is now feasible, generating multiple objects with reasonable interactions within a 3D space, a.k.a. compositional 3D generation, presents substantial challenges. This paper introduces CompGS, a novel generative framework that employs 3D Gaussian Splatting (GS) for efficient, compositional text-to-3D content generation. To achieve this goal, two core designs are proposed: (1) 3D Gaussians Initialization with 2D compositionality: We transfer the well-established 2D compositionality to initialize the Gaussian parameters on an entity-by-entity basis, ensuring both consistent 3D priors for each entity and reasonable interactions among multiple entities; (2) Dynamic Optimization: We propose a dynamic strategy to optimize 3D Gaussians using Score Distillation Sampling (SDS) loss. CompGS first automatically decomposes 3D Gaussians into distinct entity parts, enabling optimization at both the entity and composition levels. Additionally, CompGS optimizes across objects of varying scales by dynamically adjusting the spatial parameters of each entity, enhancing the generation of fine-grained details, particularly in smaller entities. Qualitative comparisons and quantitative evaluations on T3Bench demonstrate the effectiveness of CompGS in generating compositional 3D objects with superior image quality and semantic alignment over existing methods. CompGS can also be easily extended to controllable 3D editing, facilitating scene generation. We hope CompGS will provide new insights to the compositional 3D generation. Project page: https://chongjiange.github.io/compgs.html.",
    "arxiv_url": "http://arxiv.org/abs/2410.20723v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20723v1",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODGS: 3D Scene Reconstruction from Omnidirectional Images with 3D Gaussian Splattings",
    "authors": [
      "Suyoung Lee",
      "Jaeyoung Chung",
      "Jaeyoo Huh",
      "Kyoung Mu Lee"
    ],
    "abstract": "Omnidirectional (or 360-degree) images are increasingly being used for 3D applications since they allow the rendering of an entire scene with a single image. Existing works based on neural radiance fields demonstrate successful 3D reconstruction quality on egocentric videos, yet they suffer from long training and rendering times. Recently, 3D Gaussian splatting has gained attention for its fast optimization and real-time rendering. However, directly using a perspective rasterizer to omnidirectional images results in severe distortion due to the different optical properties between two image domains. In this work, we present ODGS, a novel rasterization pipeline for omnidirectional images, with geometric interpretation. For each Gaussian, we define a tangent plane that touches the unit sphere and is perpendicular to the ray headed toward the Gaussian center. We then leverage a perspective camera rasterizer to project the Gaussian onto the corresponding tangent plane. The projected Gaussians are transformed and combined into the omnidirectional image, finalizing the omnidirectional rasterization process. This interpretation reveals the implicit assumptions within the proposed pipeline, which we verify through mathematical proofs. The entire rasterization process is parallelized using CUDA, achieving optimization and rendering speeds 100 times faster than NeRF-based methods. Our comprehensive experiments highlight the superiority of ODGS by delivering the best reconstruction and perceptual quality across various datasets. Additionally, results on roaming datasets demonstrate that ODGS restores fine details effectively, even when reconstructing large 3D scenes. The source code is available on our project page (https://github.com/esw0116/ODGS).",
    "arxiv_url": "http://arxiv.org/abs/2410.20686v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20686v1",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/esw0116/ODGS",
    "keywords": [
      "head",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Normal-GS: 3D Gaussian Splatting with Normal-Involved Rendering",
    "authors": [
      "Meng Wei",
      "Qianyi Wu",
      "Jianmin Zheng",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ],
    "abstract": "Rendering and reconstruction are long-standing topics in computer vision and graphics. Achieving both high rendering quality and accurate geometry is a challenge. Recent advancements in 3D Gaussian Splatting (3DGS) have enabled high-fidelity novel view synthesis at real-time speeds. However, the noisy and discrete nature of 3D Gaussian primitives hinders accurate surface estimation. Previous attempts to regularize 3D Gaussian normals often degrade rendering quality due to the fundamental disconnect between normal vectors and the rendering pipeline in 3DGS-based methods. Therefore, we introduce Normal-GS, a novel approach that integrates normal vectors into the 3DGS rendering pipeline. The core idea is to model the interaction between normals and incident lighting using the physically-based rendering equation. Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV). To optimize memory usage and simplify optimization, we employ an anchor-based 3DGS to implicitly encode locally-shared IDIVs. Additionally, Normal-GS leverages optimized normals and Integrated Directional Encoding (IDE) to accurately model specular effects, enhancing both rendering quality and surface normal precision. Extensive experiments demonstrate that Normal-GS achieves near state-of-the-art visual quality while obtaining accurate surface normals and preserving real-time rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2410.20593v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20593v1",
    "published_date": "2024-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Fields in Robotics: A Survey",
    "authors": [
      "Muhammad Zubair Irshad",
      "Mauro Comi",
      "Yen-Chen Lin",
      "Nick Heppert",
      "Abhinav Valada",
      "Rares Ambrus",
      "Zsolt Kira",
      "Jonathan Tremblay"
    ],
    "abstract": "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io",
    "arxiv_url": "http://arxiv.org/abs/2410.20220v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20220v1",
    "published_date": "2024-10-26",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "autonomous driving",
      "survey",
      "lighting",
      "semantic",
      "geometry",
      "3d reconstruction",
      "ar",
      "nerf",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCube: Instant Large-Scale Scene Reconstruction using VoxSplats",
    "authors": [
      "Xuanchi Ren",
      "Yifan Lu",
      "Hanxue Liang",
      "Zhangjie Wu",
      "Huan Ling",
      "Mike Chen",
      "Sanja Fidler",
      "Francis Williams",
      "Jiahui Huang"
    ],
    "abstract": "We present SCube, a novel method for reconstructing large-scale 3D scenes (geometry, appearance, and semantics) from a sparse set of posed images. Our method encodes reconstructed scenes using a novel representation VoxSplat, which is a set of 3D Gaussians supported on a high-resolution sparse-voxel scaffold. To reconstruct a VoxSplat from images, we employ a hierarchical voxel latent diffusion model conditioned on the input images followed by a feedforward appearance prediction model. The diffusion model generates high-resolution grids progressively in a coarse-to-fine manner, and the appearance network predicts a set of Gaussians within each voxel. From as few as 3 non-overlapping input images, SCube can generate millions of Gaussians with a 1024^3 voxel grid spanning hundreds of meters in 20 seconds. Past works tackling scene reconstruction from images either rely on per-scene optimization and fail to reconstruct the scene away from input views (thus requiring dense view coverage as input) or leverage geometric priors based on low-resolution models, which produce blurry results. In contrast, SCube leverages high-resolution sparse networks and produces sharp outputs from few views. We show the superiority of SCube compared to prior art using the Waymo self-driving dataset on 3D reconstruction and demonstrate its applications, such as LiDAR simulation and text-to-scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2410.20030v1",
    "pdf_url": "http://arxiv.org/pdf/2410.20030v1",
    "published_date": "2024-10-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffGS: Functional Gaussian Splatting Diffusion",
    "authors": [
      "Junsheng Zhou",
      "Weiqi Zhang",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2410.19657v2",
    "pdf_url": "http://arxiv.org/pdf/2410.19657v2",
    "published_date": "2024-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robotic Learning in your Backyard: A Neural Simulator from Open Source Components",
    "authors": [
      "Liyou Zhou",
      "Oleg Sinavski",
      "Athanasios Polydoros"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting for fast and high-quality novel view synthesize has opened up the possibility to construct photo-realistic simulations from video for robotic reinforcement learning. While the approach has been demonstrated in several research papers, the software tools used to build such a simulator remain unavailable or proprietary. We present SplatGym, an open source neural simulator for training data-driven robotic control policies. The simulator creates a photorealistic virtual environment from a single video. It supports ego camera view generation, collision detection, and virtual object in-painting. We demonstrate training several visual navigation policies via reinforcement learning. SplatGym represents a notable first step towards an open-source general-purpose neural environment for robotic learning. It broadens the range of applications that can effectively utilise reinforcement learning by providing convenient and unrestricted tooling, and by eliminating the need for the manual development of conventional 3D environments.",
    "arxiv_url": "http://arxiv.org/abs/2410.19564v1",
    "pdf_url": "http://arxiv.org/pdf/2410.19564v1",
    "published_date": "2024-10-25",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene Intricacy Through Learned Bitwidth Quantization",
    "authors": [
      "Weihang Liu",
      "Xue Xian Zheng",
      "Jingyi Yu",
      "Xin Lou"
    ],
    "abstract": "The recent popular radiance field models, exemplified by Neural Radiance Fields (NeRF), Instant-NGP and 3D Gaussian Splatting, are designed to represent 3D content by that training models for each individual scene. This unique characteristic of scene representation and per-scene training distinguishes radiance field models from other neural models, because complex scenes necessitate models with higher representational capacity and vice versa. In this paper, we propose content-aware radiance fields, aligning the model complexity with the scene intricacies through Adversarial Content-Aware Quantization (A-CAQ). Specifically, we make the bitwidth of parameters differentiable and trainable, tailored to the unique characteristics of specific scenes and requirements. The proposed framework has been assessed on Instant-NGP, a well-known NeRF variant and evaluated using various datasets. Experimental results demonstrate a notable reduction in computational complexity, while preserving the requisite reconstruction and rendering quality, making it beneficial for practical deployment of radiance fields models. Codes are available at https://github.com/WeihangLiu2024/Content_Aware_NeRF.",
    "arxiv_url": "http://arxiv.org/abs/2410.19483v1",
    "pdf_url": "http://arxiv.org/pdf/2410.19483v1",
    "published_date": "2024-10-25",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/WeihangLiu2024/Content_Aware_NeRF",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArCSEM: Artistic Colorization of SEM Images via Gaussian Splatting",
    "authors": [
      "Takuma Nishimura",
      "Andreea Dogaru",
      "Martin Oeggerli",
      "Bernhard Egger"
    ],
    "abstract": "Scanning Electron Microscopes (SEMs) are widely renowned for their ability to analyze the surface structures of microscopic objects, offering the capability to capture highly detailed, yet only grayscale, images. To create more expressive and realistic illustrations, these images are typically manually colorized by an artist with the support of image editing software. This task becomes highly laborious when multiple images of a scanned object require colorization. We propose facilitating this process by using the underlying 3D structure of the microscopic scene to propagate the color information to all the captured images, from as little as one colorized view. We explore several scene representation techniques and achieve high-quality colorized novel view synthesis of a SEM scene. In contrast to prior work, there is no manual intervention or labelling involved in obtaining the 3D representation. This enables an artist to color a single or few views of a sequence and automatically retrieve a fully colored scene or video. Project page: https://ronly2460.github.io/ArCSEM",
    "arxiv_url": "http://arxiv.org/abs/2410.21310v1",
    "pdf_url": "http://arxiv.org/pdf/2410.21310v1",
    "published_date": "2024-10-25",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views",
    "authors": [
      "Xin Fei",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Jiwen Lu"
    ],
    "abstract": "We propose PixelGaussian, an efficient feed-forward framework for learning generalizable 3D Gaussian reconstruction from arbitrary views. Most existing methods rely on uniform pixel-wise Gaussian representations, which learn a fixed number of 3D Gaussians for each view and cannot generalize well to more input views. Differently, our PixelGaussian dynamically adapts both the Gaussian distribution and quantity based on geometric complexity, leading to more efficient representations and significant improvements in reconstruction quality. Specifically, we introduce a Cascade Gaussian Adapter to adjust Gaussian distribution according to local geometry complexity identified by a keypoint scorer. CGA leverages deformable attention in context-aware hypernetworks to guide Gaussian pruning and splitting, ensuring accurate representation in complex regions while reducing redundancy. Furthermore, we design a transformer-based Iterative Gaussian Refiner module that refines Gaussian representations through direct image-Gaussian interactions. Our PixelGaussian can effectively reduce Gaussian redundancy as input views increase. We conduct extensive experiments on the large-scale ACID and RealEstate10K datasets, where our method achieves state-of-the-art performance with good generalization to various numbers of views. Code: https://github.com/Barrybarry-Smith/PixelGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2410.18979v1",
    "pdf_url": "http://arxiv.org/pdf/2410.18979v1",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/Barrybarry-Smith/PixelGaussian",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation",
    "authors": [
      "Hansheng Chen",
      "Bokui Shen",
      "Yulin Liu",
      "Ruoxi Shi",
      "Linqi Zhou",
      "Connor Z. Lin",
      "Jiayuan Gu",
      "Hao Su",
      "Gordon Wetzstein",
      "Leonidas Guibas"
    ],
    "abstract": "Multi-view image diffusion models have significantly advanced open-domain 3D object generation. However, most existing models rely on 2D network architectures that lack inherent 3D biases, resulting in compromised geometric consistency. To address this challenge, we introduce 3D-Adapter, a plug-in module designed to infuse 3D geometry awareness into pretrained image diffusion models. Central to our approach is the idea of 3D feedback augmentation: for each denoising step in the sampling loop, 3D-Adapter decodes intermediate multi-view features into a coherent 3D representation, then re-encodes the rendered RGBD views to augment the pretrained base model through feature addition. We study two variants of 3D-Adapter: a fast feed-forward version based on Gaussian splatting and a versatile training-free version utilizing neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter not only greatly enhances the geometry quality of text-to-multi-view models such as Instant3D and Zero123++, but also enables high-quality 3D generation using the plain text-to-image Stable Diffusion. Furthermore, we showcase the broad application potential of 3D-Adapter by presenting high quality results in text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.",
    "arxiv_url": "http://arxiv.org/abs/2410.18974v2",
    "pdf_url": "http://arxiv.org/pdf/2410.18974v2",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "geometry",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sort-free Gaussian Splatting via Weighted Sum Rendering",
    "authors": [
      "Qiqi Hou",
      "Randall Rauwendaal",
      "Zifeng Li",
      "Hoang Le",
      "Farzad Farhadzadeh",
      "Fatih Porikli",
      "Alexei Bourd",
      "Amir Said"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering performance is constrained by its dependence on costly non-commutative alpha-blending operations. These operations mandate complex view dependent sorting operations that introduce computational overhead, especially on the resource-constrained platforms such as mobile phones. In this paper, we propose Weighted Sum Rendering, which approximates alpha blending with weighted sums, thereby removing the need for sorting. This simplifies implementation, delivers superior performance, and eliminates the \"popping\" artifacts caused by sorting. Experimental results show that optimizing a generalized Gaussian splatting formulation to the new differentiable rendering yields competitive image quality. The method was implemented and tested in a mobile device GPU, achieving on average $1.23\\times$ faster rendering.",
    "arxiv_url": "http://arxiv.org/abs/2410.18931v2",
    "pdf_url": "http://arxiv.org/pdf/2410.18931v2",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
    "authors": [
      "Mingtong Zhang",
      "Kaifeng Zhang",
      "Yunzhu Li"
    ],
    "abstract": "Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2410.18912v1",
    "pdf_url": "http://arxiv.org/pdf/2410.18912v1",
    "published_date": "2024-10-24",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis",
    "authors": [
      "Liang Han",
      "Junsheng Zhou",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Novel view synthesis from sparse inputs is a vital yet challenging task in 3D computer vision. Previous methods explore 3D Gaussian Splatting with neural priors (e.g. depth priors) as an additional supervision, demonstrating promising quality and efficiency compared to the NeRF based methods. However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields. In this paper, We propose a novel method for synthesizing novel views from sparse views with Gaussian Splatting that does not require external prior as supervision. Our key idea lies in exploring the self-supervisions inherent in the binocular stereo consistency between each pair of binocular images constructed with disparity-guided image warping. To this end, we additionally introduce a Gaussian opacity constraint which regularizes the Gaussian locations and avoids Gaussian redundancy for improving the robustness and efficiency of inferring 3D Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and Blender datasets demonstrate that our method significantly outperforms the state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2410.18822v2",
    "pdf_url": "http://arxiv.org/pdf/2410.18822v2",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian Splatting and Neural Points",
    "authors": [
      "Linus Franke",
      "Laura Fink",
      "Marc Stamminger"
    ],
    "abstract": "Recent advances in novel view synthesis have demonstrated impressive results in fast photorealistic scene rendering through differentiable point rendering, either via Gaussian Splatting (3DGS) [Kerbl and Kopanas et al. 2023] or neural point rendering [Aliev et al. 2020]. Unfortunately, these directions require either a large number of small Gaussians or expensive per-pixel post-processing for reconstructing fine details, which negatively impacts rendering performance. To meet the high performance demands of virtual reality (VR) systems, primitive or pixel counts therefore must be kept low, affecting visual quality.   In this paper, we propose a novel hybrid approach based on foveated rendering as a promising solution that combines the strengths of both point rendering directions regarding performance sweet spots. Analyzing the compatibility with the human visual system, we find that using a low-detailed, few primitive smooth Gaussian representation for the periphery is cheap to compute and meets the perceptual demands of peripheral vision. For the fovea only, we use neural points with a convolutional neural network for the small pixel footprint, which provides sharp, detailed output within the rendering budget. This combination also allows for synergistic method accelerations with point occlusion culling and reducing the demands on the neural network.   Our evaluation confirms that our approach increases sharpness and details compared to a standard VR-ready 3DGS configuration, and participants of a user study overwhelmingly preferred our method. Our system meets the necessary performance requirements for real-time VR interactions, ultimately enhancing the user's immersive experience.   The project page can be found at: https://lfranke.github.io/vr_splatting",
    "arxiv_url": "http://arxiv.org/abs/2410.17932v2",
    "pdf_url": "http://arxiv.org/pdf/2410.17932v2",
    "published_date": "2024-10-23",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "fast",
      "3d gaussian",
      "human",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting",
    "authors": [
      "Yu Wang",
      "Xiaobao Wei",
      "Ming Lu",
      "Guoliang Kang"
    ],
    "abstract": "Previous methods utilize the Neural Radiance Field (NeRF) for panoptic lifting, while their training and rendering speed are unsatisfactory. In contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due to its rapid training and rendering speed. However, unlike NeRF, the conventional 3DGS may not satisfy the basic smoothness assumption as it does not rely on any parameterized structures to render (e.g., MLPs). Consequently, the conventional 3DGS is, in nature, more susceptible to noisy 2D mask supervision. In this paper, we propose a new method called PLGS that enables 3DGS to generate consistent panoptic segmentation masks from noisy 2D segmentation masks while maintaining superior efficiency compared to NeRF-based methods. Specifically, we build a panoptic-aware structured 3D Gaussian model to introduce smoothness and design effective noise reduction strategies. For the semantic field, instead of initialization with structure from motion, we construct reliable semantic anchor points to initialize the 3D Gaussians. We then use these anchor points as smooth regularization during training. Additionally, we present a self-training approach using pseudo labels generated by merging the rendered masks with the noisy masks to enhance the robustness of PLGS. For the instance field, we project the 2D instance masks into 3D space and match them with oriented bounding boxes to generate cross-view consistent instance masks for supervision. Experiments on various benchmarks demonstrate that our method outperforms previous state-of-the-art methods in terms of both segmentation quality and speed.",
    "arxiv_url": "http://arxiv.org/abs/2410.17505v1",
    "pdf_url": "http://arxiv.org/pdf/2410.17505v1",
    "published_date": "2024-10-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multimodal LLM Guided Exploration and Active Mapping using Fisher Information",
    "authors": [
      "Wen Jiang",
      "Boshu Lei",
      "Katrina Ashton",
      "Kostas Daniilidis"
    ],
    "abstract": "We present an active mapping system that could plan for long-horizon exploration goals and short-term actions with a 3D Gaussian Splatting (3DGS) representation. Existing methods either did not take advantage of recent developments in multimodal Large Language Models (LLM) or did not consider challenges in localization uncertainty, which is critical in embodied agents. We propose employing multimodal LLMs for long-horizon planning in conjunction with detailed motion planning using our information-based algorithm. By leveraging high-quality view synthesis from our 3DGS representation, our method employs a multimodal LLM as a zero-shot planner for long-horizon exploration goals from the semantic perspective. We also introduce an uncertainty-aware path proposal and selection algorithm that balances the dual objectives of maximizing the information gain for the environment while minimizing the cost of localization errors. Experiments conducted on the Gibson and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2410.17422v2",
    "pdf_url": "http://arxiv.org/pdf/2410.17422v2",
    "published_date": "2024-10-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "semantic",
      "mapping",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
    "authors": [
      "Cheng-De Fan",
      "Chen-Wei Chang",
      "Yi-Ruei Liu",
      "Jie-Ying Lee",
      "Jiun-Long Huang",
      "Yu-Chee Tseng",
      "Yu-Lun Liu"
    ],
    "abstract": "We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to represent specular surfaces accurately. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy significantly enhancing scene geometry and specular color prediction. It is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.",
    "arxiv_url": "http://arxiv.org/abs/2410.17249v3",
    "pdf_url": "http://arxiv.org/pdf/2410.17249v3",
    "published_date": "2024-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "motion",
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
    "authors": [
      "Haian Jin",
      "Hanwen Jiang",
      "Hao Tan",
      "Kai Zhang",
      "Sai Bi",
      "Tianyuan Zhang",
      "Fujun Luan",
      "Noah Snavely",
      "Zexiang Xu"
    ],
    "abstract": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ .",
    "arxiv_url": "http://arxiv.org/abs/2410.17242v2",
    "pdf_url": "http://arxiv.org/pdf/2410.17242v2",
    "published_date": "2024-10-22",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "sparse-view",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "E-3DGS: Gaussian Splatting with Exposure and Motion Events",
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Yuhan Bao",
      "Zhenshan Bing",
      "Yiyi Liao",
      "Kailun Yang",
      "Kaiwei Wang"
    ],
    "abstract": "Achieving 3D reconstruction from images captured under optimal conditions has been extensively studied in the vision and imaging fields. However, in real-world scenarios, challenges such as motion blur and insufficient illumination often limit the performance of standard frame-based cameras in delivering high-quality images. To address these limitations, we incorporate a transmittance adjustment device at the hardware level, enabling event cameras to capture both motion and exposure events for diverse 3D reconstruction scenarios. Motion events (triggered by camera or object movement) are collected in fast-motion scenarios when the device is inactive, while exposure events (generated through controlled camera exposure) are captured during slower motion to reconstruct grayscale images for high-quality training and optimization of event-based 3D Gaussian Splatting (3DGS). Our framework supports three modes: High-Quality Reconstruction using exposure events, Fast Reconstruction relying on motion events, and Balanced Hybrid optimizing with initial exposure events followed by high-speed motion events. On the EventNeRF dataset, we demonstrate that exposure events significantly improve fine detail reconstruction compared to motion events and outperform frame-based cameras under challenging conditions such as low illumination and overexposure. Furthermore, we introduce EME-3D, a real-world 3D dataset with exposure events, motion events, camera calibration parameters, and sparse point clouds. Our method achieves faster and higher-quality reconstruction than event-based NeRF and is more cost-effective than methods combining event and RGB data. E-3DGS sets a new benchmark for event-based 3D reconstruction with robust performance in challenging conditions and lower hardware demands. The source code and dataset will be available at https://github.com/MasterHow/E-3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2410.16995v2",
    "pdf_url": "http://arxiv.org/pdf/2410.16995v2",
    "published_date": "2024-10-22",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "https://github.com/MasterHow/E-3DGS",
    "keywords": [
      "illumination",
      "motion",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization",
    "authors": [
      "Constantin Kleinbeck",
      "Hannah Schieber",
      "Klaus Engel",
      "Ralf Gutjahr",
      "Daniel Roth"
    ],
    "abstract": "In medical image visualization, path tracing of volumetric medical data like CT scans produces lifelike three-dimensional visualizations. Immersive VR displays can further enhance the understanding of complex anatomies. Going beyond the diagnostic quality of traditional 2D slices, they enable interactive 3D evaluation of anatomies, supporting medical education and planning. Rendering high-quality visualizations in real-time, however, is computationally intensive and impractical for compute-constrained devices like mobile headsets.   We propose a novel approach utilizing GS to create an efficient but static intermediate representation of CT scans. We introduce a layered GS representation, incrementally including different anatomical structures while minimizing overlap and extending the GS training to remove inactive Gaussians. We further compress the created model with clustering across layers.   Our approach achieves interactive frame rates while preserving anatomical structures, with quality adjustable to the target hardware. Compared to standard GS, our representation retains some of the explorative qualities initially enabled by immersive path tracing. Selective activation and clipping of layers are possible at rendering time, adding a degree of interactivity to otherwise static GS models. This could enable scenarios where high computational demands would otherwise prohibit using path-traced medical volumes.",
    "arxiv_url": "http://arxiv.org/abs/2410.16978v1",
    "pdf_url": "http://arxiv.org/pdf/2410.16978v1",
    "published_date": "2024-10-22",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "path tracing",
      "understanding",
      "medical",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MvDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors",
    "authors": [
      "Honghua Chen",
      "Yushi Lan",
      "Yongwei Chen",
      "Yifan Zhou",
      "Xingang Pan"
    ],
    "abstract": "Drag-based editing has become popular in 2D content creation, driven by the capabilities of image generative models. However, extending this technique to 3D remains a challenge. Existing 3D drag-based editing methods, whether employing explicit spatial transformations or relying on implicit latent optimization within limited-capacity 3D generative models, fall short in handling significant topology changes or generating new textures across diverse object categories. To overcome these limitations, we introduce MVDrag3D, a novel framework for more flexible and creative drag-based 3D editing that leverages multi-view generation and reconstruction priors. At the core of our approach is the usage of a multi-view diffusion model as a strong generative prior to perform consistent drag editing over multiple rendered views, which is followed by a reconstruction model that reconstructs 3D Gaussians of the edited object. While the initial 3D Gaussians may suffer from misalignment between different views, we address this via view-specific deformation networks that adjust the position of Gaussians to be well aligned. In addition, we propose a multi-view score function that distills generative priors from multiple views to further enhance the view consistency and visual quality. Extensive experiments demonstrate that MVDrag3D provides a precise, generative, and flexible solution for 3D drag-based editing, supporting more versatile editing effects across various object categories and 3D representations.",
    "arxiv_url": "http://arxiv.org/abs/2410.16272v1",
    "pdf_url": "http://arxiv.org/pdf/2410.16272v1",
    "published_date": "2024-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
    "authors": [
      "Xi Liu",
      "Chaoyi Zhou",
      "Siyu Huang"
    ],
    "abstract": "Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .",
    "arxiv_url": "http://arxiv.org/abs/2410.16266v1",
    "pdf_url": "http://arxiv.org/pdf/2410.16266v1",
    "published_date": "2024-10-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MSGField: A Unified Scene Representation Integrating Motion, Semantics, and Geometry for Robotic Manipulation",
    "authors": [
      "Yu Sheng",
      "Runfeng Lin",
      "Lidian Wang",
      "Quecheng Qiu",
      "YanYong Zhang",
      "Yu Zhang",
      "Bei Hua",
      "Jianmin Ji"
    ],
    "abstract": "Combining accurate geometry with rich semantics has been proven to be highly effective for language-guided robotic manipulation. Existing methods for dynamic scenes either fail to update in real-time or rely on additional depth sensors for simple scene editing, limiting their applicability in real-world. In this paper, we introduce MSGField, a representation that uses a collection of 2D Gaussians for high-quality reconstruction, further enhanced with attributes to encode semantic and motion information. Specially, we represent the motion field compactly by decomposing each primitive's motion into a combination of a limited set of motion bases. Leveraging the differentiable real-time rendering of Gaussian splatting, we can quickly optimize object motion, even for complex non-rigid motions, with image supervision from only two camera views. Additionally, we designed a pipeline that utilizes object priors to efficiently obtain well-defined semantics. In our challenging dataset, which includes flexible and extremely small objects, our method achieve a success rate of 79.2% in static and 63.3% in dynamic environments for language-guided manipulation. For specified object grasping, we achieve a success rate of 90%, on par with point cloud-based methods. Code and dataset will be released at:https://shengyu724.github.io/MSGField.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2410.15730v1",
    "pdf_url": "http://arxiv.org/pdf/2410.15730v1",
    "published_date": "2024-10-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "semantic",
      "geometry",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LucidFusion: Reconstructing 3D Gaussians with Arbitrary Unposed Images",
    "authors": [
      "Hao He",
      "Yixun Liang",
      "Luozhou Wang",
      "Yuanhao Cai",
      "Xinli Xu",
      "Hao-Xiang Guo",
      "Xiang Wen",
      "Yingcong Chen"
    ],
    "abstract": "Recent large reconstruction models have made notable progress in generating high-quality 3D objects from single images. However, current reconstruction methods often rely on explicit camera pose estimation or fixed viewpoints, restricting their flexibility and practical applicability. We reformulate 3D reconstruction as image-to-image translation and introduce the Relative Coordinate Map (RCM), which aligns multiple unposed images to a main view without pose estimation. While RCM simplifies the process, its lack of global 3D supervision can yield noisy outputs. To address this, we propose Relative Coordinate Gaussians (RCG) as an extension to RCM, which treats each pixel's coordinates as a Gaussian center and employs differentiable rasterization for consistent geometry and pose recovery. Our LucidFusion framework handles an arbitrary number of unposed inputs, producing robust 3D reconstructions within seconds and paving the way for more flexible, pose-free 3D pipelines.",
    "arxiv_url": "http://arxiv.org/abs/2410.15636v3",
    "pdf_url": "http://arxiv.org/pdf/2410.15636v3",
    "published_date": "2024-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fully Explicit Dynamic Gaussian Splatting",
    "authors": [
      "Junoh Lee",
      "Chang-Yeon Won",
      "Hyunjun Jung",
      "Inhwan Bae",
      "Hae-Gon Jeon"
    ],
    "abstract": "3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design a Explicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.",
    "arxiv_url": "http://arxiv.org/abs/2410.15629v2",
    "pdf_url": "http://arxiv.org/pdf/2410.15629v2",
    "published_date": "2024-10-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "fast",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting",
    "authors": [
      "Bohao Liao",
      "Wei Zhai",
      "Zengyu Wan",
      "Zhixin Cheng",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "abstract": "Scene reconstruction from casually captured videos has wide applications in real-world scenarios. With recent advancements in differentiable rendering techniques, several methods have attempted to simultaneously optimize scene representations (NeRF or 3DGS) and camera poses. Despite recent progress, existing methods relying on traditional camera input tend to fail in high-speed (or equivalently low-frame-rate) scenarios. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event camera to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, supervising the rendered views observed by the event stream. Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise manner to extract motion information by maximizing the contrast of the Image of Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on the Linear Event Generation Model (LEGM), the brightness information encoded in the IWE is also utilized to constrain the 3DGS in the gradient domain. Third, to mitigate the absence of color information of events, we introduce photometric bundle adjustment (PBA) to ensure view consistency across events and frames. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our project page is https://lbh666.github.io/ef-3dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2410.15392v3",
    "pdf_url": "http://arxiv.org/pdf/2410.15392v3",
    "published_date": "2024-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with Gaussian Splatting",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jin Wu",
      "Jun Ma"
    ],
    "abstract": "In this paper, we introduce GS-LIVM, a real-time photo-realistic LiDAR-Inertial-Visual mapping framework with Gaussian Splatting tailored for outdoor scenes. Compared to existing methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), our approach enables real-time photo-realistic mapping while ensuring high-quality image rendering in large-scale unbounded outdoor environments. In this work, Gaussian Process Regression (GPR) is employed to mitigate the issues resulting from sparse and unevenly distributed LiDAR observations. The voxel-based 3D Gaussians map representation facilitates real-time dense mapping in large outdoor environments with acceleration governed by custom CUDA kernels. Moreover, the overall framework is designed in a covariance-centered manner, where the estimated covariance is used to initialize the scale and rotation of 3D Gaussians, as well as update the parameters of the GPR. We evaluate our algorithm on several outdoor datasets, and the results demonstrate that our method achieves state-of-the-art performance in terms of mapping efficiency and rendering quality. The source code is available on GitHub.",
    "arxiv_url": "http://arxiv.org/abs/2410.17084v1",
    "pdf_url": "http://arxiv.org/pdf/2410.17084v1",
    "published_date": "2024-10-18",
    "categories": [
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "mapping",
      "3d gaussian",
      "acceleration",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scenes",
    "authors": [
      "Juliette Marrie",
      "Romain Menegaux",
      "Michael Arbel",
      "Diane Larlus",
      "Julien Mairal"
    ],
    "abstract": "We address the problem of extending the capabilities of vision foundation models such as DINO, SAM, and CLIP, to 3D tasks. Specifically, we introduce a novel method to uplift 2D image features into Gaussian Splatting representations of 3D scenes. Unlike traditional approaches that rely on minimizing a reconstruction loss, our method employs a simpler and more efficient feature aggregation technique, augmented by a graph diffusion mechanism. Graph diffusion refines 3D features, such as coarse segmentation masks, by leveraging 3D geometry and pairwise similarities induced by DINOv2. Our approach achieves performance comparable to the state of the art on multiple downstream tasks while delivering significant speed-ups. Notably, we obtain competitive segmentation results using generic DINOv2 features, despite DINOv2 not being trained on millions of annotated segmentation masks like SAM. When applied to CLIP features, our method demonstrates strong performance in open-vocabulary object localization tasks, highlighting the versatility of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2410.14462v4",
    "pdf_url": "http://arxiv.org/pdf/2410.14462v4",
    "published_date": "2024-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "lighting",
      "geometry",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set",
    "authors": [
      "Wenyuan Zhang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "It is vital to infer a signed distance function (SDF) in multi-view based surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel perspective for volume rendering, and shows advantages in rendering efficiency and quality. Although 3DGS provides a promising neural rendering option, it is still hard to infer SDFs for surface reconstruction with 3DGS due to the discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To resolve these issues, we propose a method that seamlessly merge 3DGS with the learning of neural SDFs. Our key idea is to more effectively constrain the SDF inference with the multi-view consistency. To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF using neural pulling, and then render the aligned 3D Gaussians through the differentiable rasterization. Meanwhile, we update the neural SDF by pulling neighboring space to the pulled 3D Gaussians, which progressively refine the signed distance field near the surface. With both differentiable pulling and splatting, we jointly optimize 3D Gaussians and the neural SDF with both RGB and geometry constraints, which recovers more accurate, smooth, and complete surfaces with more geometry details. Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2410.14189v1",
    "pdf_url": "http://arxiv.org/pdf/2410.14189v1",
    "published_date": "2024-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DaRePlane: Direction-aware Representations for Dynamic Scene Reconstruction",
    "authors": [
      "Ange Lou",
      "Benjamin Planche",
      "Zhongpai Gao",
      "Yamin Li",
      "Tianyu Luan",
      "Hao Ding",
      "Meng Zheng",
      "Terrence Chen",
      "Ziyan Wu",
      "Jack Noble"
    ],
    "abstract": "Numerous recent approaches to modeling and re-rendering dynamic scenes leverage plane-based explicit representations, addressing slow training times associated with models like neural radiance fields (NeRF) and Gaussian splatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D plane-based representations is insufficient for high-fidelity re-rendering of scenes with complex motions. In response, we present DaRePlane, a novel direction-aware representation approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. Within NeRF pipelines, DaRePlane computes features for each space-time point by fusing vectors from these recovered planes, then passed to a tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane computes the features of Gaussian points, followed by a tiny multi-head MLP for spatial-time deformation prediction. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. To demonstrate the generality and efficiency of DaRePlane, we test it on both regular and surgical dynamic scenes, for both NeRF and GS systems. Extensive experiments show that DaRePlane yields state-of-the-art performance in novel view synthesis for various complex dynamic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2410.14169v1",
    "pdf_url": "http://arxiv.org/pdf/2410.14169v1",
    "published_date": "2024-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "motion",
      "deformation",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
    "authors": [
      "Haofei Xu",
      "Songyou Peng",
      "Fangjinhua Wang",
      "Hermann Blum",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "Gaussian splatting and single-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale multi-view posed datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. In addition, DepthSplat enables feed-forward reconstruction from 12 input views (512x960 resolutions) in 0.6 seconds.",
    "arxiv_url": "http://arxiv.org/abs/2410.13862v3",
    "pdf_url": "http://arxiv.org/pdf/2410.13862v3",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Differentiable Robot Rendering",
    "authors": [
      "Ruoshi Liu",
      "Alper Canberk",
      "Shuran Song",
      "Carl Vondrick"
    ],
    "abstract": "Vision foundation models trained on massive amounts of visual data have shown unprecedented reasoning and planning skills in open-world settings. A key challenge in applying them to robotic tasks is the modality gap between visual data and action data. We introduce differentiable robot rendering, a method allowing the visual appearance of a robot body to be directly differentiable with respect to its control parameters. Our model integrates a kinematics-aware deformable model and Gaussians Splatting and is compatible with any robot form factors and degrees of freedom. We demonstrate its capability and usage in applications including reconstruction of robot poses from images and controlling robots through vision language models. Quantitative and qualitative results show that our differentiable rendering model provides effective gradients for robotic control directly from pixels, setting the foundation for the future applications of vision foundation models in robotics.",
    "arxiv_url": "http://arxiv.org/abs/2410.13851v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13851v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "robotics",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes",
    "authors": [
      "Xinjie Zhang",
      "Zhening Liu",
      "Yifan Zhang",
      "Xingtong Ge",
      "Dailan He",
      "Tongda Xu",
      "Yan Wang",
      "Zehong Lin",
      "Shuicheng Yan",
      "Jun Zhang"
    ],
    "abstract": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190$\\times$ and 125$\\times$ on the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field.",
    "arxiv_url": "http://arxiv.org/abs/2410.13613v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13613v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "deformation",
      "lightweight",
      "4d",
      "dynamic",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering",
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng",
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Xu Zhou"
    ],
    "abstract": "Dynamic scenes rendering is an intriguing yet challenging problem. Although current methods based on NeRF have achieved satisfactory performance, they still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS) has garnered researchers attention due to their outstanding rendering quality and real-time speed. Therefore, a new paradigm has been proposed: defining a canonical 3D gaussians and deforming it to individual frames in deformable fields. However, since the coordinates of canonical 3D gaussians are filled with noise, which can transfer noise into the deformable fields, and there is currently no method that adequately considers the aggregation of 4D information. Therefore, we propose Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering (DN-4DGS). Specifically, a Noise Suppression Strategy is introduced to change the distribution of the coordinates of the canonical 3D gaussians and suppress noise. Additionally, a Decoupled Temporal-Spatial Aggregation Module is designed to aggregate information from adjacent points and frames. Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level.",
    "arxiv_url": "http://arxiv.org/abs/2410.13607v2",
    "pdf_url": "http://arxiv.org/pdf/2410.13607v2",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
    "authors": [
      "Guosheng Zhao",
      "Chaojun Ni",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Xueyang Zhang",
      "Yida Wang",
      "Guan Huang",
      "Xinze Chen",
      "Boyuan Wang",
      "Youyi Zhang",
      "Wenjun Mei",
      "Xingang Wang"
    ],
    "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which are largely confined to forward-driving scenarios. Consequently, these methods face limitations when rendering complex maneuvers (e.g., lane change, acceleration, deceleration). Recent advancements in autonomous-driving world models have demonstrated the potential to generate diverse driving videos. However, these approaches remain constrained to 2D video generation, inherently lacking the spatiotemporal coherence required to capture intricacies of dynamic driving environments. In this paper, we introduce DriveDreamer4D, which enhances 4D driving scene representation leveraging world model priors. Specifically, we utilize the world model as a data machine to synthesize novel trajectory videos, where structured conditions are explicitly leveraged to control the spatial-temporal consistency of traffic elements. Besides, the cousin data training strategy is proposed to facilitate merging real and synthetic data for optimizing 4DGS. To our knowledge, DriveDreamer4D is the first to utilize video generation models for improving 4D reconstruction in driving scenarios. Experimental results reveal that DriveDreamer4D significantly enhances generation quality under novel trajectory views, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3% compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D markedly enhances the spatiotemporal coherence of driving agents, which is verified by a comprehensive user study and the relative increases of 22.6%, 43.5%, and 15.6% in the NTA-IoU metric.",
    "arxiv_url": "http://arxiv.org/abs/2410.13571v3",
    "pdf_url": "http://arxiv.org/pdf/2410.13571v3",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "face",
      "4d",
      "acceleration",
      "ar",
      "nerf",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L3DG: Latent 3D Gaussian Diffusion",
    "authors": [
      "Barbara Roessle",
      "Norman M√ºller",
      "Lorenzo Porzi",
      "Samuel Rota Bul√≤",
      "Peter Kontschieder",
      "Angela Dai",
      "Matthias Nie√üner"
    ],
    "abstract": "We propose L3DG, the first approach for generative 3D modeling of 3D Gaussians through a latent 3D Gaussian diffusion formulation. This enables effective generative 3D modeling, scaling to generation of entire room-scale scenes which can be very efficiently rendered. To enable effective synthesis of 3D Gaussians, we propose a latent diffusion formulation, operating in a compressed latent space of 3D Gaussians. This compressed latent space is learned by a vector-quantized variational autoencoder (VQ-VAE), for which we employ a sparse convolutional architecture to efficiently operate on room-scale scenes. This way, the complexity of the costly generation process via diffusion is substantially reduced, allowing higher detail on object-level generation, as well as scalability to large scenes. By leveraging the 3D Gaussian representation, the generated scenes can be rendered from arbitrary viewpoints in real-time. We demonstrate that our approach significantly improves visual quality over prior work on unconditional object-level radiance field synthesis and showcase its applicability to room-scale scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2410.13530v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13530v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting",
    "authors": [
      "Shuichang Lai",
      "Letian Huang",
      "Jie Guo",
      "Kai Cheng",
      "Bowen Pan",
      "Xiaoxiao Long",
      "Jiangjing Lyu",
      "Chengfei Lv",
      "Yanwen Guo"
    ],
    "abstract": "Reconstructing objects from posed images is a crucial and complex task in computer graphics and computer vision. While NeRF-based neural reconstruction methods have exhibited impressive reconstruction ability, they tend to be time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS) for inverse rendering, which have led to quick and effective outcomes. However, these techniques generally have difficulty in producing believable geometries and materials for glossy objects, a challenge that stems from the inherent ambiguities of inverse rendering. To address this, we introduce GlossyGS, an innovative 3D-GS-based inverse rendering framework that aims to precisely reconstruct the geometry and materials of glossy objects by integrating material priors. The key idea is the use of micro-facet geometry segmentation prior, which helps to reduce the intrinsic ambiguities and improve the decomposition of geometries and materials. Additionally, we introduce a normal map prefiltering strategy to more accurately simulate the normal distribution of reflective surfaces. These strategies are integrated into a hybrid geometry and material representation that employs both explicit and implicit methods to depict glossy objects. We demonstrate through quantitative analysis and qualitative visualization that the proposed method is effective to reconstruct high-fidelity geometries and materials of glossy objects, and performs favorably against state-of-the-arts.",
    "arxiv_url": "http://arxiv.org/abs/2410.13349v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13349v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "segmentation",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid bundle-adjusting 3D Gaussians for view consistent rendering with pose optimization",
    "authors": [
      "Yanan Guo",
      "Ying Xie",
      "Ying Chang",
      "Benkui Zhang",
      "Bo Jia",
      "Lin Cao"
    ],
    "abstract": "Novel view synthesis has made significant progress in the field of 3D computer vision. However, the rendering of view-consistent novel views from imperfect camera poses remains challenging. In this paper, we introduce a hybrid bundle-adjusting 3D Gaussians model that enables view-consistent rendering with pose optimization. This model jointly extract image-based and neural 3D representations to simultaneously generate view-consistent images and camera poses within forward-facing scenes. The effective of our model is demonstrated through extensive experiments conducted on both real and synthetic datasets. These experiments clearly illustrate that our model can effectively optimize neural scene representations while simultaneously resolving significant camera pose misalignments. The source code is available at https://github.com/Bistu3DV/hybridBA.",
    "arxiv_url": "http://arxiv.org/abs/2410.13280v1",
    "pdf_url": "http://arxiv.org/pdf/2410.13280v1",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Bistu3DV/hybridBA",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Modeling Unitary 3D Gaussians for Novel View Synthesis from Sparse-view Images",
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Yukai Shi",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ],
    "abstract": "In this work, we introduce UniGS, a novel 3D Gaussian reconstruction and novel view synthesis model that predicts a high-fidelity representation of 3D Gaussians from arbitrary number of posed sparse-view images. Previous methods often regress 3D Gaussians locally on a per-pixel basis for each view and then transfer them to world space and merge them through point concatenation. In contrast, Our approach involves modeling unitary 3D Gaussians in world space and updating them layer by layer. To leverage information from multi-view inputs for updating the unitary 3D Gaussians, we develop a DETR (DEtection TRansformer)-like framework, which treats 3D Gaussians as queries and updates their parameters by performing multi-view cross-attention (MVDFA) across multiple input images, which are treated as keys and values. This approach effectively avoids `ghosting' issue and allocates more 3D Gaussians to complex regions. Moreover, since the number of 3D Gaussians used as decoder queries is independent of the number of input views, our method allows arbitrary number of multi-view images as input without causing memory explosion or requiring retraining. Extensive experiments validate the advantages of our approach, showcasing superior performance over existing methods quantitatively (improving PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and qualitatively. The code will be released at https://github.com/jwubz123/UNIG.",
    "arxiv_url": "http://arxiv.org/abs/2410.13195v3",
    "pdf_url": "http://arxiv.org/pdf/2410.13195v3",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jwubz123/UNIG",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "high-fidelity",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
    "authors": [
      "Chen Ziwen",
      "Hao Tan",
      "Kai Zhang",
      "Sai Bi",
      "Fujun Luan",
      "Yicong Hong",
      "Li Fuxin",
      "Zexiang Xu"
    ],
    "abstract": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm",
    "arxiv_url": "http://arxiv.org/abs/2410.12781v1",
    "pdf_url": "http://arxiv.org/pdf/2410.12781v1",
    "published_date": "2024-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TV-3DG: Mastering Text-to-3D Customized Generation with Visual Prompt",
    "authors": [
      "Jiahui Yang",
      "Donglin Di",
      "Baorui Ma",
      "Xun Yang",
      "Yongjia Ma",
      "Wenzhang Sun",
      "Wei Chen",
      "Jianxun Cui",
      "Zhou Xue",
      "Meng Wang",
      "Yebin Liu"
    ],
    "abstract": "In recent years, advancements in generative models have significantly expanded the capabilities of text-to-3D generation. Many approaches rely on Score Distillation Sampling (SDS) technology. However, SDS struggles to accommodate multi-condition inputs, such as text and visual prompts, in customized generation tasks. To explore the core reasons, we decompose SDS into a difference term and a classifier-free guidance term. Our analysis identifies the core issue as arising from the difference term and the random noise addition during the optimization process, both contributing to deviations from the target mode during distillation. To address this, we propose a novel algorithm, Classifier Score Matching (CSM), which removes the difference term in SDS and uses a deterministic noise addition process to reduce noise during optimization, effectively overcoming the low-quality limitations of SDS in our customized generation framework. Based on CSM, we integrate visual prompt information with an attention fusion mechanism and sampling guidance techniques, forming the Visual Prompt CSM (VPCSM) algorithm. Furthermore, we introduce a Semantic-Geometry Calibration (SGC) module to enhance quality through improved textual information integration. We present our approach as TV-3DG, with extensive experiments demonstrating its capability to achieve stable, high-quality, customized 3D generation. Project page: \\url{https://yjhboy.github.io/TV-3DG}",
    "arxiv_url": "http://arxiv.org/abs/2410.21299v2",
    "pdf_url": "http://arxiv.org/pdf/2410.21299v2",
    "published_date": "2024-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting in Robotics: A Survey",
    "authors": [
      "Siting Zhu",
      "Guangming Wang",
      "Xin Kong",
      "Dezhi Kong",
      "Hesheng Wang"
    ],
    "abstract": "Dense 3D representations of the environment have been a long-term goal in the robotics field. While previous Neural Radiance Fields (NeRF) representation have been prevalent for its implicit, coordinate-based model, the recent emergence of 3D Gaussian Splatting (3DGS) has demonstrated remarkable potential in its explicit radiance field representation. By leveraging 3D Gaussian primitives for explicit scene representation and enabling differentiable rendering, 3DGS has shown significant advantages over other radiance fields in real-time rendering and photo-realistic performance, which is beneficial for robotic applications. In this survey, we provide a comprehensive understanding of 3DGS in the field of robotics. We divide our discussion of the related works into two main categories: the application of 3DGS and the advancements in 3DGS techniques. In the application section, we explore how 3DGS has been utilized in various robotics tasks from scene understanding and interaction perspectives. The advance of 3DGS section focuses on the improvements of 3DGS own properties in its adaptability and efficiency, aiming to enhance its performance in robotics. We then summarize the most commonly used datasets and evaluation metrics in robotics. Finally, we identify the challenges and limitations of current 3DGS methods and discuss the future development of 3DGS in robotics.",
    "arxiv_url": "http://arxiv.org/abs/2410.12262v2",
    "pdf_url": "http://arxiv.org/pdf/2410.12262v2",
    "published_date": "2024-10-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "survey",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection",
    "authors": [
      "Yizhe Liu",
      "Yan Song Hu",
      "Yuhao Chen",
      "John Zelek"
    ],
    "abstract": "Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has emerged in industrial quality control. This task seeks to find anomalies from query images of a tested object given a set of reference images of an anomaly-free object. The challenge is that the query views (a.k.a poses) are unknown and can be different from the reference views. Currently, new methods such as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing pseudo reference images at the query views for pixel-to-pixel comparison. However, none of these methods can infer in real-time, which is critical in industrial quality control for massive production. For this reason, we propose SplatPose+, which employs a hybrid representation consisting of a Structure from Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS) model for Novel View Synthesis. Although our proposed pipeline requires the computation of an additional SfM model, it offers real-time inference speeds and faster training compared to SplatPose. Quality-wise, we achieved a new SOTA on the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly Detection (MAD-SIM) dataset.",
    "arxiv_url": "http://arxiv.org/abs/2410.12080v1",
    "pdf_url": "http://arxiv.org/pdf/2410.12080v1",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoGS: Visual Localization via Gaussian Splatting with Fewer Training Images",
    "authors": [
      "Yuzhou Cheng",
      "Jianhao Jiao",
      "Yue Wang",
      "Dimitrios Kanoulas"
    ],
    "abstract": "Visual localization involves estimating a query image's 6-DoF (degrees of freedom) camera pose, which is a fundamental component in various computer vision and robotic tasks. This paper presents LoGS, a vision-based localization pipeline utilizing the 3D Gaussian Splatting (GS) technique as scene representation. This novel representation allows high-quality novel view synthesis. During the mapping phase, structure-from-motion (SfM) is applied first, followed by the generation of a GS map. During localization, the initial position is obtained through image retrieval, local feature matching coupled with a PnP solver, and then a high-precision pose is achieved through the analysis-by-synthesis manner on the GS map. Experimental results on four large-scale datasets demonstrate the proposed approach's SoTA accuracy in estimating camera poses and robustness under challenging few-shot conditions.",
    "arxiv_url": "http://arxiv.org/abs/2410.11505v1",
    "pdf_url": "http://arxiv.org/pdf/2410.11505v1",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "few-shot",
      "mapping",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS^3: Efficient Relighting with Triple Gaussian Splatting",
    "authors": [
      "Zoubin Bi",
      "Yixin Zeng",
      "Chong Zeng",
      "Fan Pei",
      "Xiang Feng",
      "Kun Zhou",
      "Hongzhi Wu"
    ],
    "abstract": "We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2410.11419v1",
    "pdf_url": "http://arxiv.org/pdf/2410.11419v1",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relighting",
      "lighting",
      "global illumination",
      "shadow",
      "geometry",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian Radiance Fields",
    "authors": [
      "Yuru Xiao",
      "Deming Zhai",
      "Wenbo Zhao",
      "Kui Jiang",
      "Junjun Jiang",
      "Xianming Liu"
    ],
    "abstract": "Radiance fields represented by 3D Gaussians excel at synthesizing novel views, offering both high training efficiency and fast rendering. However, with sparse input views, the lack of multi-view consistency constraints results in poorly initialized point clouds and unreliable heuristics for optimization and densification, leading to suboptimal performance. Existing methods often incorporate depth priors from dense estimation networks but overlook the inherent multi-view consistency in input images. Additionally, they rely on multi-view stereo (MVS)-based initialization, which limits the efficiency of scene representation. To overcome these challenges, we propose a view synthesis framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic scene reconstruction from sparse input views. The key innovations of MCGS in enhancing multi-view consistency are as follows: i) We introduce an initialization method by leveraging a sparse matcher combined with a random filling strategy, yielding a compact yet sufficient set of initial points. This approach enhances the initial geometry prior, promoting efficient scene representation. ii) We develop a multi-view consistency-guided progressive pruning strategy to refine the Gaussian field by strengthening consistency and eliminating low-contribution Gaussians. These modular, plug-and-play strategies enhance robustness to sparse input views, accelerate rendering, and reduce memory consumption, making MCGS a practical and efficient framework for 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2410.11394v1",
    "pdf_url": "http://arxiv.org/pdf/2410.11394v1",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSORB-SLAM: Gaussian Splatting SLAM benefits from ORB features and Transmittance information",
    "authors": [
      "Wancai Zheng",
      "Xinyi Yu",
      "Jintao Rong",
      "Linlin Ou",
      "Yan Wei",
      "Libo Zhou"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has recently ignited a renewed wave of research in dense visual SLAM. However, existing approaches encounter challenges, including sensitivity to artifacts and noise, suboptimal selection of training viewpoints, and the absence of global optimization. In this paper, we propose GSORB-SLAM, a dense SLAM framework that integrates 3DGS with ORB features through a tightly coupled optimization pipeline. To mitigate the effects of noise and artifacts, we propose a novel geometric representation and optimization method for tracking, which significantly enhances localization accuracy and robustness. For high-fidelity mapping, we develop an adaptive Gaussian expansion and regularization method that facilitates compact yet expressive scene modeling while suppressing redundant primitives. Furthermore, we design a hybrid graph-based viewpoint selection mechanism that effectively reduces overfitting and accelerates convergence. Extensive evaluations across various datasets demonstrate that our system achieves state-of-the-art performance in both tracking precision-improving RMSE by 16.2% compared to ORB-SLAM2 baselines-and reconstruction quality-improving PSNR by 3.93 dB compared to 3DGS-SLAM baselines. The project: https://aczheng-cai.github.io/gsorb-slam.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2410.11356v3",
    "pdf_url": "http://arxiv.org/pdf/2410.11356v3",
    "published_date": "2024-10-15",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "high-fidelity",
      "tracking",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery with 3D Gaussian Splatting",
    "authors": [
      "Yuanbo Chen",
      "Chengyu Zhang",
      "Jason Wang",
      "Xuefan Gao",
      "Avideh Zakhor"
    ],
    "abstract": "Scene reconstruction and novel-view synthesis for large, complex, multi-story, indoor scenes is a challenging and time-consuming task. Prior methods have utilized drones for data capture and radiance fields for scene reconstruction, both of which present certain challenges. First, in order to capture diverse viewpoints with the drone's front-facing camera, some approaches fly the drone in an unstable zig-zag fashion, which hinders drone-piloting and generates motion blur in the captured data. Secondly, most radiance field methods do not easily scale to arbitrarily large number of images. This paper proposes an efficient and scalable pipeline for indoor novel-view synthesis from drone-captured 360 videos using 3D Gaussian Splatting. 360 cameras capture a wide set of viewpoints, allowing for comprehensive scene capture under a simple straightforward drone trajectory. To scale our method to large scenes, we devise a divide-and-conquer strategy to automatically split the scene into smaller blocks that can be reconstructed individually and in parallel. We also propose a coarse-to-fine alignment strategy to seamlessly match these blocks together to compose the entire scene. Our experiments demonstrate marked improvement in both reconstruction quality, i.e. PSNR and SSIM, and computation time compared to prior approaches.",
    "arxiv_url": "http://arxiv.org/abs/2410.11285v1",
    "pdf_url": "http://arxiv.org/pdf/2410.11285v1",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting",
    "authors": [
      "Raja Kumar",
      "Vanshika Vats"
    ],
    "abstract": "3D Gaussian splatting has surpassed neural radiance field methods in novel view synthesis by achieving lower computational costs and real-time high-quality rendering. Although it produces a high-quality rendering with a lot of input views, its performance drops significantly when only a few views are available. In this work, we address this by proposing a depth-aware Gaussian splatting method for few-shot novel view synthesis. We use monocular depth prediction as a prior, along with a scale-invariant depth loss, to constrain the 3D shape under just a few input views. We also model color using lower-order spherical harmonics to avoid overfitting. Further, we observe that removing splats with lower opacity periodically, as performed in the original work, leads to a very sparse point cloud and, hence, a lower-quality rendering. To mitigate this, we retain all the splats, leading to a better reconstruction in a few view settings. Experimental results show that our method outperforms the traditional 3D Gaussian splatting methods by achieving improvements of 10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and 14.1% in perceptual similarity, thereby validating the effectiveness of our approach. The code will be made available at: https://github.com/raja-kumar/depth-aware-3DGS",
    "arxiv_url": "http://arxiv.org/abs/2410.11080v1",
    "pdf_url": "http://arxiv.org/pdf/2410.11080v1",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "https://github.com/raja-kumar/depth-aware-3DGS",
    "keywords": [
      "3d gaussian",
      "few-shot",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DArticCyclists: Generating Synthetic Articulated 8D Pose-Controllable Cyclist Data for Computer Vision Applications",
    "authors": [
      "Eduardo R. Corral-Soto",
      "Yang Liu",
      "Tongtong Cao",
      "Yuan Ren",
      "Liu Bingbing"
    ],
    "abstract": "In Autonomous Driving (AD) Perception, cyclists are considered safety-critical scene objects. Commonly used publicly-available AD datasets typically contain large amounts of car and vehicle object instances but a low number of cyclist instances, usually with limited appearance and pose diversity. This cyclist training data scarcity problem not only limits the generalization of deep-learning perception models for cyclist semantic segmentation, pose estimation, and cyclist crossing intention prediction, but also limits research on new cyclist-related tasks such as fine-grained cyclist pose estimation and spatio-temporal analysis under complex interactions between humans and articulated objects. To address this data scarcity problem, in this paper we propose a framework to generate synthetic dynamic 3D cyclist data assets that can be used to generate training data for different tasks. In our framework, we designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that we use to train a 3D Gaussian Splatting (3DGS)-based reconstruction and image rendering method. We then propose a parametric bicycle 3DGS composition model to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person, while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement. We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method.",
    "arxiv_url": "http://arxiv.org/abs/2410.10782v2",
    "pdf_url": "http://arxiv.org/pdf/2410.10782v2",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "segmentation",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4-LEGS: 4D Language Embedded Gaussian Splatting",
    "authors": [
      "Gal Fiebelman",
      "Tamir Cohen",
      "Ayellet Morgenstern",
      "Peter Hedman",
      "Hadar Averbuch-Elor"
    ],
    "abstract": "The emergence of neural representations has revolutionized our means for digitally viewing a wide range of 3D scenes, enabling the synthesis of photorealistic images rendered from novel views. Recently, several techniques have been proposed for connecting these low-level representations with the high-level semantics understanding embodied within the scene. These methods elevate the rich semantic understanding from 2D imagery to 3D representations, distilling high-dimensional spatial features onto 3D space. In our work, we are interested in connecting language with a dynamic modeling of the world. We show how to lift spatio-temporal features to a 4D representation based on 3D Gaussian Splatting. This enables an interactive interface where the user can spatiotemporally localize events in the video from text prompts. We demonstrate our system on public 3D video datasets of people and animals performing various actions.",
    "arxiv_url": "http://arxiv.org/abs/2410.10719v3",
    "pdf_url": "http://arxiv.org/pdf/2410.10719v3",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "understanding",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting",
    "authors": [
      "Wanlin Liang",
      "Hongbin Xu",
      "Weitao Chen",
      "Feng Xiao",
      "Wenxiong Kang"
    ],
    "abstract": "3D neural style transfer has gained significant attention for its potential to provide user-friendly stylization with spatial consistency. However, existing 3D style transfer methods often fall short in terms of inference efficiency, generalization ability, and struggle to handle dynamic scenes with temporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D style transfer framework designed to achieve real-time stylization of arbitrary style references while maintaining reasonable content affinity, multi-view consistency, and temporal coherence. Our approach leverages an embedded 4D Gaussian Splatting technique, which is trained using a reversible neural network for reducing content loss in the feature distillation process. Utilizing the 4D embedded Gaussians, we predict a 4D style transformation matrix that facilitates spatially and temporally consistent style transfer with Gaussian Splatting. Experiments demonstrate that our method can achieve high-quality and zero-shot stylization for 4D scenarios with enhanced efficiency and spatial-temporal consistency.",
    "arxiv_url": "http://arxiv.org/abs/2410.10412v1",
    "pdf_url": "http://arxiv.org/pdf/2410.10412v1",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GUISE: Graph GaUssIan Shading watErmark",
    "authors": [
      "Renyi Yang"
    ],
    "abstract": "In the expanding field of generative artificial intelligence, integrating robust watermarking technologies is essential to protect intellectual property and maintain content authenticity. Traditionally, watermarking techniques have been developed primarily for rich information media such as images and audio. However, these methods have not been adequately adapted for graph-based data, particularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an ascendant approach in the molecular graph generation field. This model effectively manages the complexities of molecular structures, preserving essential symmetries and topological features. We adapt the Gaussian Shading, a proven performance lossless watermarking technique, to the latent graph diffusion domain to protect this sophisticated new technology. Our adaptation simplifies the watermark diffusion process through duplication and padding, making it adaptable and suitable for various message types. We conduct several experiments using the LDM-3DG model on publicly available datasets QM9 and Drugs, to assess the robustness and effectiveness of our technique. Our results demonstrate that the watermarked molecules maintain statistical parity in 9 out of 10 performance metrics compared to the original. Moreover, they exhibit a 100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while also showing robustness against post-editing attacks.",
    "arxiv_url": "http://arxiv.org/abs/2410.10178v1",
    "pdf_url": "http://arxiv.org/pdf/2410.10178v1",
    "published_date": "2024-10-14",
    "categories": [
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Visual MPC for Granular Media Manipulation",
    "authors": [
      "Wei-Cheng Tseng",
      "Ellina Zhang",
      "Krishna Murthy Jatavallabhula",
      "Florian Shkurti"
    ],
    "abstract": "Recent advancements in learned 3D representations have enabled significant progress in solving complex robotic manipulation tasks, particularly for rigid-body objects. However, manipulating granular materials such as beans, nuts, and rice, remains challenging due to the intricate physics of particle interactions, high-dimensional and partially observable state, inability to visually track individual particles in a pile, and the computational demands of accurate dynamics prediction. Current deep latent dynamics models often struggle to generalize in granular material manipulation due to a lack of inductive biases. In this work, we propose a novel approach that learns a visual dynamics model over Gaussian splatting representations of scenes and leverages this model for manipulating granular media via Model-Predictive Control. Our method enables efficient optimization for complex manipulation tasks on piles of granular media. We evaluate our approach in both simulated and real-world settings, demonstrating its ability to solve unseen planning tasks and generalize to new environments in a zero-shot transfer. We also show significant prediction and manipulation performance improvements compared to existing granular media manipulation methods.",
    "arxiv_url": "http://arxiv.org/abs/2410.09740v3",
    "pdf_url": "http://arxiv.org/pdf/2410.09740v3",
    "published_date": "2024-10-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Single Image to 3D Generation using Gaussian Splatting and Hybrid Diffusion Priors",
    "authors": [
      "Hritam Basak",
      "Hadi Tabatabaee",
      "Shreekant Gayaka",
      "Ming-Feng Li",
      "Xin Yang",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Min Sun",
      "Zhaozheng Yin"
    ],
    "abstract": "3D object generation from a single image involves estimating the full 3D geometry and texture of unseen views from an unposed RGB image captured in the wild. Accurately reconstructing an object's complete 3D structure and texture has numerous applications in real-world scenarios, including robotic manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements in 3D object generation have introduced techniques that reconstruct an object's 3D shape and texture by optimizing the efficient representation of Gaussian Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable disparity exists between the training datasets of these models, leading to distinct differences in their outputs. While 2D models generate highly detailed visuals, they lack cross-view consistency in geometry and texture. In contrast, 3D models ensure consistency across different views but often result in overly smooth textures. We propose bridging the gap between 2D and 3D diffusion models to address this limitation by integrating a two-stage frequency-based distillation loss with Gaussian Splatting. Specifically, we leverage geometric priors in the low-frequency spectrum from a 3D diffusion model to maintain consistent geometry and use a 2D diffusion model to refine the fidelity and texture in the high-frequency spectrum of the generated 3D structure, resulting in more detailed and fine-grained outcomes. Our approach enhances geometric consistency and visual quality, outperforming the current SOTA. Additionally, we demonstrate the easy adaptability of our method for efficient object pose estimation and tracking.",
    "arxiv_url": "http://arxiv.org/abs/2410.09467v2",
    "pdf_url": "http://arxiv.org/pdf/2410.09467v2",
    "published_date": "2024-10-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "tracking",
      "understanding",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction",
    "authors": [
      "Jialei Chen",
      "Xin Zhang",
      "Mobarakol Islam",
      "Francisco Vasconcelos",
      "Danail Stoyanov",
      "Daniel S. Elson",
      "Baoru Huang"
    ],
    "abstract": "Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting methods have shown promise in achieving high-quality reconstructions with fast rendering speeds, their use of inverse depth loss functions compresses depth variations. This can lead to a loss of fine geometric details, limiting their ability to capture precise 3D geometry and effectiveness in intraoperative application. To address these challenges, we present SurgicalGS, a dynamic 3D Gaussian Splatting framework specifically designed for surgical scene reconstruction with improved geometric accuracy. Our approach first initialises a Gaussian point cloud using depth priors, employing binary motion masks to identify pixels with significant depth variations and fusing point clouds from depth maps across frames for initialisation. We use the Flexible Deformation Model to represent dynamic scene and introduce a normalised depth regularisation loss along with an unsupervised depth smoothness constraint to ensure more accurate geometric reconstruction. Extensive experiments on two real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art reconstruction quality, especially in terms of accurate geometry, advancing the usability of 3D Gaussian Splatting in robotic-assisted surgery.",
    "arxiv_url": "http://arxiv.org/abs/2410.09292v1",
    "pdf_url": "http://arxiv.org/pdf/2410.09292v1",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "deformation",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering",
    "authors": [
      "Jaehoon Choi",
      "Yonghan Lee",
      "Hyungtae Lee",
      "Heesung Kwon",
      "Dinesh Manocha"
    ],
    "abstract": "Recently, 3D Gaussian splatting has gained attention for its capability to generate high-fidelity rendering results. At the same time, most applications such as games, animation, and AR/VR use mesh-based representations to represent and render 3D scenes. We propose a novel approach that integrates mesh representation with 3D Gaussian splats to perform high-quality rendering of reconstructed real-world scenes. In particular, we introduce a distance-based Gaussian splatting technique to align the Gaussian splats with the mesh surface and remove redundant Gaussian splats that do not contribute to the rendering. We consider the distance between each Gaussian splat and the mesh surface to distinguish between tightly-bound and loosely-bound Gaussian splats. The tightly-bound splats are flattened and aligned well with the mesh geometry. The loosely-bound Gaussian splats are used to account for the artifacts in reconstructed 3D meshes in terms of rendering. We present a training strategy of binding Gaussian splats to the mesh geometry, and take into account both types of splats. In this context, we introduce several regularization techniques aimed at precisely aligning tightly-bound Gaussian splats with the mesh surface during the training process. We validate the effectiveness of our method on large and unbounded scene from mip-NeRF 360 and Deep Blending datasets. Our method surpasses recent mesh-based neural rendering techniques by achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset, demonstrating better rendering quality. We provide analyses for each type of Gaussian splat and achieve a reduction in the number of Gaussian splats by 30% compared to the original 3D Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2410.08941v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08941v1",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "high-fidelity",
      "vr",
      "outdoor",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars",
    "authors": [
      "Xuan Huang",
      "Hanhui Li",
      "Wanquan Liu",
      "Xiaodan Liang",
      "Yiqiang Yan",
      "Yuhao Cheng",
      "Chengqiang Gao"
    ],
    "abstract": "In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Project Page: \\url{https://github.com/XuanHuang0/GuassianHand}.",
    "arxiv_url": "http://arxiv.org/abs/2410.08840v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08840v1",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/XuanHuang0/GuassianHand",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization",
    "authors": [
      "Christian Schmidt",
      "Jens Piekenbrinck",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs .",
    "arxiv_url": "http://arxiv.org/abs/2410.08743v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08743v1",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Schmiddo/noposegs",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FusionSense: Bridging Common Sense, Vision, and Touch for Robust Sparse-View Reconstruction",
    "authors": [
      "Irving Fang",
      "Kairui Shi",
      "Xujin He",
      "Siqi Tan",
      "Yifan Wang",
      "Hanwen Zhao",
      "Hung-Jui Huang",
      "Wenzhen Yuan",
      "Chen Feng",
      "Jing Zhang"
    ],
    "abstract": "Humans effortlessly integrate common-sense knowledge with sensory input from vision and touch to understand their surroundings. Emulating this capability, we introduce FusionSense, a novel 3D reconstruction framework that enables robots to fuse priors from foundation models with highly sparse observations from vision and tactile sensors. FusionSense addresses three key challenges: (i) How can robots efficiently acquire robust global shape information about the surrounding scene and objects? (ii) How can robots strategically select touch points on the object using geometric and common-sense priors? (iii) How can partial observations such as tactile signals improve the overall representation of the object? Our framework employs 3D Gaussian Splatting as a core representation and incorporates a hierarchical optimization strategy involving global structure construction, object visual hull pruning and local geometric constraints. This advancement results in fast and robust perception in environments with traditionally challenging objects that are transparent, reflective, or dark, enabling more downstream manipulation or navigation tasks. Experiments on real-world data suggest that our framework outperforms previously state-of-the-art sparse-view methods. All code and data are open-sourced on the project website.",
    "arxiv_url": "http://arxiv.org/abs/2410.08282v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08282v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "I.4.5; I.4.8"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting",
    "authors": [
      "Jiahao Lu",
      "Yifan Zhang",
      "Qiuhong Shen",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "abstract": "3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems. Our code is available at https://github.com/jiahaolu97/poison-splat .",
    "arxiv_url": "http://arxiv.org/abs/2410.08190v2",
    "pdf_url": "http://arxiv.org/pdf/2410.08190v2",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "https://github.com/jiahaolu97/poison-splat",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DifFRelight: Diffusion-Based Facial Performance Relighting",
    "authors": [
      "Mingming He",
      "Pascal Clausen",
      "Ahmet Levent Ta≈üel",
      "Li Ma",
      "Oliver Pilarski",
      "Wenqi Xian",
      "Laszlo Rikker",
      "Xueming Yu",
      "Ryan Burgert",
      "Ning Yu",
      "Paul Debevec"
    ],
    "abstract": "We present a novel framework for free-viewpoint facial performance relighting using diffusion-based image-to-image translation. Leveraging a subject-specific dataset containing diverse facial expressions captured under various lighting conditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we train a diffusion model for precise lighting control, enabling high-fidelity relit facial images from flat-lit inputs. Our framework includes spatially-aligned conditioning of flat-lit captures and random noise, along with integrated lighting information for global control, utilizing prior knowledge from the pre-trained Stable Diffusion model. This model is then applied to dynamic facial performances captured in a consistent flat-lit environment and reconstructed for novel-view synthesis using a scalable dynamic 3D Gaussian Splatting method to maintain quality and consistency in the relit results. In addition, we introduce unified lighting control by integrating a novel area lighting representation with directional lighting, allowing for joint adjustments in light size and direction. We also enable high dynamic range imaging (HDRI) composition using multiple directional lights to produce dynamic sequences under complex lighting conditions. Our evaluations demonstrate the models efficiency in achieving precise lighting control and generalizing across various facial expressions while preserving detailed features such as skintexture andhair. The model accurately reproduces complex lighting effects like eye reflections, subsurface scattering, self-shadowing, and translucency, advancing photorealism within our framework.",
    "arxiv_url": "http://arxiv.org/abs/2410.08188v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08188v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "reflection",
      "relighting",
      "lighting",
      "shadow",
      "face",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS Generative Model from a Single Image",
    "authors": [
      "Xiaoxue Chen",
      "Jv Zheng",
      "Hao Huang",
      "Haoran Xu",
      "Weihao Gu",
      "Kangliang Chen",
      "He xiang",
      "Huan-ang Gao",
      "Hao Zhao",
      "Guyue Zhou",
      "Yaqin Zhang"
    ],
    "abstract": "The generation of high-quality 3D car assets is essential for various applications, including video games, autonomous driving, and virtual reality. Current 3D generation methods utilizing NeRF or 3D-GS as representations for 3D objects, generate a Lambertian object under fixed lighting and lack separated modelings for material and global illumination. As a result, the generated assets are unsuitable for relighting under varying lighting conditions, limiting their applicability in downstream tasks. To address this challenge, we propose a novel relightable 3D object generative framework that automates the creation of 3D car assets, enabling the swift and accurate reconstruction of a vehicle's geometry, texture, and material properties from a single input image. Our approach begins with introducing a large-scale synthetic car dataset comprising over 1,000 high-precision 3D vehicle models. We represent 3D objects using global illumination and relightable 3D Gaussian primitives integrating with BRDF parameters. Building on this representation, we introduce a feed-forward model that takes images as input and outputs both relightable 3D Gaussians and global illumination parameters. Experimental results demonstrate that our method produces photorealistic 3D car assets that can be seamlessly integrated into road scenes with different illuminations, which offers substantial practical benefits for industrial applications.",
    "arxiv_url": "http://arxiv.org/abs/2410.08181v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08181v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "high-fidelity",
      "relightable",
      "autonomous driving",
      "relighting",
      "lighting",
      "global illumination",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics",
    "authors": [
      "Junyi Cao",
      "Shanyan Guan",
      "Yanhao Ge",
      "Wei Li",
      "Xiaokang Yang",
      "Chao Ma"
    ],
    "abstract": "While humans effortlessly discern intrinsic dynamics and adapt to new scenarios, modern AI systems often struggle. Current methods for visual grounding of dynamics either use pure neural-network-based simulators (black box), which may violate physical laws, or traditional physical simulators (white box), which rely on expert-defined equations that may not fully capture actual dynamics. We propose the Neural Material Adaptor (NeuMA), which integrates existing physical laws with learned corrections, facilitating accurate learning of actual dynamics while maintaining the generalizability and interpretability of physical priors. Additionally, we propose Particle-GS, a particle-driven 3D Gaussian Splatting variant that bridges simulation and observed images, allowing back-propagate image gradients to optimize the simulator. Comprehensive experiments on various dynamics in terms of grounded particle accuracy, dynamic rendering quality, and generalization ability demonstrate that NeuMA can accurately capture intrinsic dynamics.",
    "arxiv_url": "http://arxiv.org/abs/2410.08257v1",
    "pdf_url": "http://arxiv.org/pdf/2410.08257v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency",
    "authors": [
      "Florian Hahlbohm",
      "Fabian Friederichs",
      "Tim Weyrich",
      "Linus Franke",
      "Moritz Kappel",
      "Susana Castillo",
      "Marc Stamminger",
      "Martin Eisemann",
      "Marcus Magnor"
    ],
    "abstract": "3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both for inverse rendering as well as real-time exploration of scenes. In these applications, coherence across camera frames and multiple views is crucial, be it for robust convergence of a scene reconstruction or for artifact-free fly-throughs. Recent work started mitigating artifacts that break multi-view coherence, including popping artifacts due to inconsistent transparency sorting and perspective-correct outlines of (2D) splats. At the same time, real-time requirements forced such implementations to accept compromises in how transparency of large assemblies of 3D Gaussians is resolved, in turn breaking coherence in other ways. In our work, we aim at achieving maximum coherence, by rendering fully perspective-correct 3D Gaussians while using a high-quality approximation of accurate blending, hybrid transparency, on a per-pixel level, in order to retain real-time frame rates. Our fast and perspectively accurate approach for evaluation of 3D Gaussians does not require matrix inversions, thereby ensuring numerical stability and eliminating the need for special handling of degenerate splats, and the hybrid transparency formulation for blending maintains similar quality as fully resolved per-pixel transparencies at a fraction of the rendering costs. We further show that each of these two components can be independently integrated into Gaussian splatting systems. In combination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster optimization, and equal or better image quality with fewer rendering artifacts compared to traditional 3DGS on common benchmarks.",
    "arxiv_url": "http://arxiv.org/abs/2410.08129v3",
    "pdf_url": "http://arxiv.org/pdf/2410.08129v3",
    "published_date": "2024-10-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera",
    "authors": [
      "Jian Huang",
      "Chengrui Dong",
      "Xuanhua Chen",
      "Peidong Liu"
    ],
    "abstract": "Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for novel view synthesis have achieved remarkable progress with frame-based camera (e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel type of bio-inspired visual sensor, i.e. event camera, has demonstrated advantages in high temporal resolution, high dynamic range, low power consumption and low latency. Due to its unique asynchronous and irregular data capturing process, limited work has been proposed to apply neural representation or 3D Gaussian splatting for an event camera. In this work, we present IncEventGS, an incremental 3D Gaussian Splatting reconstruction algorithm with a single event camera. To recover the 3D scene representation incrementally, we exploit the tracking and mapping paradigm of conventional SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker firstly estimates an initial camera motion based on prior reconstructed 3D-GS scene representation. The mapper then jointly refines both the 3D scene representation and camera motion based on the previously estimated motion trajectory from the tracker. The experimental results demonstrate that IncEventGS delivers superior performance compared to prior NeRF-based methods and other related baselines, even we do not have the ground-truth camera poses. Furthermore, our method can also deliver better performance compared to state-of-the-art event visual odometry methods in terms of camera motion estimation. Code is publicly available at: https://github.com/wu-cvgl/IncEventGS.",
    "arxiv_url": "http://arxiv.org/abs/2410.08107v4",
    "pdf_url": "http://arxiv.org/pdf/2410.08107v4",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/wu-cvgl/IncEventGS",
    "keywords": [
      "tracking",
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Feedforward 3D Gaussian Splatting Compression",
    "authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Mengyao Li",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity rendering for novel view synthesis, storage requirements pose challenges for their widespread adoption. Although various compression techniques have been proposed, previous art suffers from a common limitation: for any existing 3DGS, per-scene optimization is needed to achieve compression, making the compression sluggish and slow. To address this issue, we introduce Fast Compression of 3D Gaussian Splatting (FCGS), an optimization-free model that can compress 3DGS representations rapidly in a single feed-forward pass, which significantly reduces compression time from minutes to seconds. To enhance compression efficiency, we propose a multi-path entropy module that assigns Gaussian attributes to different entropy constraint paths for balance between size and fidelity. We also carefully design both inter- and intra-Gaussian context models to remove redundancies among the unstructured Gaussian blobs. Overall, FCGS achieves a compression ratio of over 20X while maintaining fidelity, surpassing most per-scene SOTA optimization-based methods. Our code is available at: https://github.com/YihangChen-ee/FCGS.",
    "arxiv_url": "http://arxiv.org/abs/2410.08017v3",
    "pdf_url": "http://arxiv.org/pdf/2410.08017v3",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YihangChen-ee/FCGS",
    "keywords": [
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generalizable and Animatable Gaussian Head Avatar",
    "authors": [
      "Xuangeng Chu",
      "Tatsuya Harada"
    ],
    "abstract": "In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture identity and facial details. Additionally, we leverage global image features and the 3D morphable model to construct 3D Gaussians for controlling expressions. After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds. Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy. We believe our method can establish new benchmarks for future research and advance applications of digital avatars. Code and demos are available https://github.com/xg-chu/GAGAvatar.",
    "arxiv_url": "http://arxiv.org/abs/2410.07971v1",
    "pdf_url": "http://arxiv.org/pdf/2410.07971v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/xg-chu/GAGAvatar",
    "keywords": [
      "head",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L-VITeX: Light-weight Visual Intuition for Terrain Exploration",
    "authors": [
      "Antar Mazumder",
      "Zarin Anjum Madhiha"
    ],
    "abstract": "This paper presents L-VITeX, a lightweight visual intuition system for terrain exploration designed for resource-constrained robots and swarms. L-VITeX aims to provide a hint of Regions of Interest (RoIs) without computationally expensive processing. By utilizing the Faster Objects, More Objects (FOMO) tinyML architecture, the system achieves high accuracy (>99%) in RoI detection while operating on minimal hardware resources (Peak RAM usage < 50 KB) with near real-time inference (<200 ms). The paper evaluates L-VITeX's performance across various terrains, including mountainous areas, underwater shipwreck debris regions, and Martian rocky surfaces. Additionally, it demonstrates the system's application in 3D mapping using a small mobile robot run by ESP32-Cam and Gaussian Splats (GS), showcasing its potential to enhance exploration efficiency and decision-making.",
    "arxiv_url": "http://arxiv.org/abs/2410.07872v1",
    "pdf_url": "http://arxiv.org/pdf/2410.07872v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "fast",
      "mapping",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
    "authors": [
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Hanzhi Chang",
      "Jiacheng Deng",
      "Jiahao Lu",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ],
    "abstract": "Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page",
    "arxiv_url": "http://arxiv.org/abs/2410.07707v1",
    "pdf_url": "http://arxiv.org/pdf/2410.07707v1",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Vision-Language Gaussian Splatting",
    "authors": [
      "Qucheng Peng",
      "Benjamin Planche",
      "Zhongpai Gao",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Chen Chen",
      "Ziyan Wu"
    ],
    "abstract": "Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2410.07577v2",
    "pdf_url": "http://arxiv.org/pdf/2410.07577v2",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "autonomous driving",
      "semantic",
      "understanding",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication",
    "authors": [
      "Erzhen Hu",
      "Mingyi Li",
      "Jungtaek Hong",
      "Xun Qian",
      "Alex Olwal",
      "David Kim",
      "Seongkook Heo",
      "Ruofei Du"
    ],
    "abstract": "During remote communication, participants often share both digital and physical content, such as product designs, digital assets, and environments, to enhance mutual understanding. Recent advances in augmented communication have facilitated users to swiftly create and share digital 2D copies of physical objects from video feeds into a shared space. However, conventional 2D representations of digital objects restricts users' ability to spatially reference items in a shared immersive environment. To address this, we propose Thing2Reality, an Extended Reality (XR) communication platform that enhances spontaneous discussions of both digital and physical items during remote sessions. With Thing2Reality, users can quickly materialize ideas or physical objects in immersive environments and share them as conditioned multiview renderings or 3D Gaussians. Thing2Reality enables users to interact with remote objects or discuss concepts in a collaborative manner. Our user study revealed that the ability to interact with and manipulate 3D representations of objects significantly enhances the efficiency of discussions, with the potential to augment discussion of 2D artifacts.",
    "arxiv_url": "http://arxiv.org/abs/2410.07119v1",
    "pdf_url": "http://arxiv.org/pdf/2410.07119v1",
    "published_date": "2024-10-09",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation",
    "authors": [
      "Zhiqi Li",
      "Yiming Chen",
      "Peidong Liu"
    ],
    "abstract": "Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm, which is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the deformation network are learned via reference view photometric loss, score distillation loss as well as other regularizers in a two-stage manner. Extensive experiments demonstrate superior performance of our method. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.",
    "arxiv_url": "http://arxiv.org/abs/2410.06756v1",
    "pdf_url": "http://arxiv.org/pdf/2410.06756v1",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "motion",
      "face",
      "deformation",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion",
    "authors": [
      "Lu Chen",
      "Yingfu Zeng",
      "Haoang Li",
      "Zhitao Deng",
      "Jiafu Yan",
      "Zhenjun Zhao"
    ],
    "abstract": "Accurate and affordable indoor 3D reconstruction is critical for effective robot navigation and interaction. Traditional LiDAR-based mapping provides high precision but is costly, heavy, and power-intensive, with limited ability for novel view rendering. Vision-based mapping, while cost-effective and capable of capturing visual data, often struggles with high-quality 3D reconstruction due to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a low-altitude camera and single-line LiDAR for high-quality 3D indoor reconstruction. Our system features Visual Error Construction (VEC) to enhance sparse point clouds by identifying and correcting areas with insufficient geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS initialization method guided by single-line LiDAR, overcoming the limitations of traditional multi-view setups and enabling effective reconstruction in resource-constrained environments. Extensive experimental results on our new Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian outperforms existing methods, particularly in challenging scenarios. The project page is available at https://chenlu-china.github.io/ES-Gaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2410.06613v2",
    "pdf_url": "http://arxiv.org/pdf/2410.06613v2",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d reconstruction",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Representation Methods: A Survey",
    "authors": [
      "Zhengren Wang"
    ],
    "abstract": "The field of 3D representation has experienced significant advancements, driven by the increasing demand for high-fidelity 3D models in various applications such as computer graphics, virtual reality, and autonomous systems. This review examines the development and current state of 3D representation methods, highlighting their research trajectories, innovations, strength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh, Signed Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian Splatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The review also introduces essential datasets that have been pivotal in advancing the field, highlighting their characteristics and impact on research progress. Finally, we explore potential research directions that hold promise for further expanding the capabilities and applications of 3D representation methods.",
    "arxiv_url": "http://arxiv.org/abs/2410.06475v1",
    "pdf_url": "http://arxiv.org/pdf/2410.06475v1",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "survey",
      "lighting",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting",
    "authors": [
      "Weixing Zhang",
      "Zongrui Li",
      "De Ma",
      "Huajin Tang",
      "Xudong Jiang",
      "Qian Zheng",
      "Gang Pan"
    ],
    "abstract": "3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes. Despite recent advances in improving surface reconstruction accuracy, the reconstructed results still exhibit bias and suffer from inefficiency in storage and training. This paper provides a different observation on the cause of the inefficiency and the reconstruction bias, which is attributed to the integration of the low-opacity parts (LOPs) of the generated Gaussians. We show that LOPs consist of Gaussians with overall low-opacity (LOGs) and the low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two types of LOPs by integrating spiking neurons into the Gaussian Splatting pipeline. Specifically, we introduce global and local full-precision integrate-and-fire spiking neurons to the opacity and representation function of flattened 3D Gaussians, respectively. Furthermore, we enhance the density control strategy with spiking neurons' thresholds and a new criterion on the scale of Gaussians. Our method can represent more accurate reconstructed surfaces at a lower cost. The supplementary material and code are available at https://github.com/zju-bmi-lab/SpikingGS.",
    "arxiv_url": "http://arxiv.org/abs/2410.07266v5",
    "pdf_url": "http://arxiv.org/pdf/2410.07266v5",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zju-bmi-lab/SpikingGS",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction",
    "authors": [
      "Shengji Tang",
      "Weicai Ye",
      "Peng Ye",
      "Weihao Lin",
      "Yang Zhou",
      "Tao Chen",
      "Wanli Ouyang"
    ],
    "abstract": "Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/",
    "arxiv_url": "http://arxiv.org/abs/2410.06245v1",
    "pdf_url": "http://arxiv.org/pdf/2410.06245v1",
    "published_date": "2024-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RelitLRM: Generative Relightable Radiance for Large Reconstruction Models",
    "authors": [
      "Tianyuan Zhang",
      "Zhengfei Kuang",
      "Haian Jin",
      "Zexiang Xu",
      "Sai Bi",
      "Hao Tan",
      "He Zhang",
      "Yiwei Hu",
      "Milos Hasan",
      "William T. Freeman",
      "Kai Zhang",
      "Fujun Luan"
    ],
    "abstract": "We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relit-lrm.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2410.06231v2",
    "pdf_url": "http://arxiv.org/pdf/2410.06231v2",
    "published_date": "2024-10-08",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "relightable",
      "relighting",
      "lighting",
      "fast",
      "shadow",
      "geometry",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSLoc: Visual Localization with 3D Gaussian Splatting",
    "authors": [
      "Kazii Botashev",
      "Vladislav Pyatov",
      "Gonzalo Ferrer",
      "Stamatios Lefkimmiatis"
    ],
    "abstract": "We present GSLoc: a new visual localization method that performs dense camera alignment using 3D Gaussian Splatting as a map representation of the scene. GSLoc backpropagates pose gradients over the rendering pipeline to align the rendered and target images, while it adopts a coarse-to-fine strategy by utilizing blurring kernels to mitigate the non-convexity of the problem and improve the convergence. The results show that our approach succeeds at visual localization in challenging conditions of relatively small overlap between initial and target frames inside textureless environments when state-of-the-art neural sparse methods provide inferior results. Using the byproduct of realistic rendering from the 3DGS map representation, we show how to enhance localization results by mixing a set of observed and virtual reference keyframes when solving the image retrieval problem. We evaluate our method both on synthetic and real-world data, discussing its advantages and application potential.",
    "arxiv_url": "http://arxiv.org/abs/2410.06165v1",
    "pdf_url": "http://arxiv.org/pdf/2410.06165v1",
    "published_date": "2024-10-08",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting",
    "authors": [
      "Xinyi Liu",
      "Tianyi Zhang",
      "Matthew Johnson-Roberson",
      "Weiming Zhi"
    ],
    "abstract": "Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.",
    "arxiv_url": "http://arxiv.org/abs/2410.06014v1",
    "pdf_url": "http://arxiv.org/pdf/2410.06014v1",
    "published_date": "2024-10-08",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters",
    "authors": [
      "Guoji Tian",
      "Chongcheng Chen",
      "Hongyu Huang"
    ],
    "abstract": "Accurate and efficient 3D reconstruction of trees is crucial for forest resource assessments and management. Close-Range Photogrammetry (CRP) is commonly used for reconstructing forest scenes but faces challenges like low efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have shown promise for 3D plant reconstruction with limited images. However, existing research mainly focuses on small plants in orchards or individual trees, leaving uncertainty regarding their application in larger, complex forest stands. In this study, we collected sequential images of forest plots with varying complexity and performed dense reconstruction using NeRF and 3DGS. The resulting point clouds were compared with those from photogrammetry and laser scanning. Results indicate that NVS methods significantly enhance reconstruction efficiency. Photogrammetry struggles with complex stands, leading to point clouds with excessive canopy noise and incorrectly reconstructed trees, such as duplicated trunks. NeRF, while better for canopy regions, may produce errors in ground areas with limited views. The 3DGS method generates sparser point clouds, particularly in trunk areas, affecting diameter at breast height (DBH) accuracy. All three methods can extract tree height information, with NeRF yielding the highest accuracy; however, photogrammetry remains superior for DBH accuracy. These findings suggest that NVS methods have significant potential for 3D reconstruction of forest stands, offering valuable support for complex forest resource inventory and visualization tasks.",
    "arxiv_url": "http://arxiv.org/abs/2410.05772v1",
    "pdf_url": "http://arxiv.org/pdf/2410.05772v1",
    "published_date": "2024-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PH-Dropout: Practical Epistemic Uncertainty Quantification for View Synthesis",
    "authors": [
      "Chuanhao Sun",
      "Thanos Triantafyllou",
      "Anthos Makris",
      "Maja Drmaƒç",
      "Kai Xu",
      "Luo Mai",
      "Mahesh K. Marina"
    ],
    "abstract": "View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) has demonstrated impressive fidelity in rendering real-world scenarios. However, practical methods for accurate and efficient epistemic Uncertainty Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF either introduce significant computational overhead (e.g., ``10x increase in training time\" or ``10x repeated training\") or are limited to specific uncertainty conditions or models. Notably, GS models lack any systematic approach for comprehensive epistemic UQ. This capability is crucial for improving the robustness and scalability of neural view synthesis, enabling active model updates, error estimation, and scalable ensemble modeling based on uncertainty. In this paper, we revisit NeRF and GS-based methods from a function approximation perspective, identifying key differences and connections in 3D representation learning. Building on these insights, we introduce PH-Dropout (Post hoc Dropout), the first real-time and accurate method for epistemic uncertainty estimation that operates directly on pre-trained NeRF and GS models. Extensive evaluations validate our theoretical findings and demonstrate the effectiveness of PH-Dropout.",
    "arxiv_url": "http://arxiv.org/abs/2410.05468v2",
    "pdf_url": "http://arxiv.org/pdf/2410.05468v2",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting",
    "authors": [
      "Yukang Cao",
      "Masoud Hadi",
      "Liang Pan",
      "Ziwei Liu"
    ],
    "abstract": "Diffusion-based 2D virtual try-on (VTON) techniques have recently demonstrated strong performance, while the development of 3D VTON has largely lagged behind. Despite recent advances in text-guided 3D scene editing, integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains challenging. The reasons are twofold. First, text prompts cannot provide sufficient details in describing clothing. Second, 2D VTON results generated from different viewpoints of the same 3D scene lack coherence and spatial relationships, hence frequently leading to appearance inconsistencies and geometric distortions. To resolve these problems, we introduce an image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained knowledge from 2D VTON models to 3D while improving cross-view consistency. (1) Specifically, we propose a personalized diffusion model that utilizes low-rank adaptation (LoRA) fine-tuning to incorporate personalized information into pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a reference-driven image editing approach that enables the simultaneous editing of multi-view images while ensuring consistency. (2) Furthermore, we propose a persona-aware 3DGS editing framework to facilitate effective editing while maintaining consistent cross-view appearance and high-quality 3D geometry. (3) Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which facilitates comprehensive qualitative and quantitative 3D VTON evaluations. Through extensive experiments and comparative analyses with existing methods, the proposed \\OM has demonstrated superior fidelity and advanced editing capabilities, affirming its effectiveness for 3D VTON.",
    "arxiv_url": "http://arxiv.org/abs/2410.05259v1",
    "pdf_url": "http://arxiv.org/pdf/2410.05259v1",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting",
    "authors": [
      "Qifeng Chen",
      "Sheng Yang",
      "Sicong Du",
      "Tao Tang",
      "Peng Chen",
      "Yuchi Huo"
    ],
    "abstract": "We present LiDAR-GS, a Gaussian Splatting (GS) method for real-time, high-fidelity re-simulation of LiDAR scans in public urban road scenes. Recent GS methods proposed for cameras have achieved significant advancements in real-time rendering beyond Neural Radiance Fields (NeRF). However, applying GS representation to LiDAR, an active 3D sensor type, poses several challenges that must be addressed to preserve high accuracy and unique characteristics. Specifically, LiDAR-GS designs a differentiable laser beam splatting, using range-view representation for precise surface splatting by projecting lasers onto micro cross-sections, effectively eliminating artifacts associated with local affine approximations. Furthermore, LiDAR-GS leverages Neural Gaussian Representation, which further integrate view-dependent clues, to represent key LiDAR properties that are influenced by the incident direction and external factors. Combining these practices with some essential adaptations, e.g., dynamic instances decomposition, LiDAR-GS succeeds in simultaneously re-simulating depth, intensity, and ray-drop channels, achieving state-of-the-art results in both rendering frame rate and quality on publically available large scene datasets when compared with the methods using explicit mesh or implicit NeRF. Our source code is publicly available at https://www.github.com/cqf7419/LiDAR-GS.",
    "arxiv_url": "http://arxiv.org/abs/2410.05111v2",
    "pdf_url": "http://arxiv.org/pdf/2410.05111v2",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/cqf7419/LiDAR-GS",
    "keywords": [
      "high-fidelity",
      "face",
      "real-time rendering",
      "ar",
      "large scene",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects",
    "authors": [
      "Nidhi Mathihalli",
      "Audrey Wei",
      "Giovanni Lavezzi",
      "Peng Mun Siew",
      "Victor Rodriguez-Fernandez",
      "Hodei Urrutxua",
      "Richard Linares"
    ],
    "abstract": "Novel view synthesis (NVS) enables to generate new images of a scene or convert a set of 2D images into a comprehensive 3D model. In the context of Space Domain Awareness, since space is becoming increasingly congested, NVS can accurately map space objects and debris, improving the safety and efficiency of space operations. Similarly, in Rendezvous and Proximity Operations missions, 3D models can provide details about a target object's shape, size, and orientation, allowing for better planning and prediction of the target's behavior. In this work, we explore the generalization abilities of these reconstruction techniques, aiming to avoid the necessity of retraining for each new scene, by presenting a novel approach to 3D spacecraft reconstruction from single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art single-view reconstruction model, on a high-quality dataset of 190 high-quality spacecraft models and integrating it into the DreamGaussian framework. We demonstrate consistent improvements in reconstruction quality across multiple metrics, including Contrastive Language-Image Pretraining (CLIP) score (+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS) (+0.16%) on a test set of 30 previously unseen spacecraft images. Our method addresses the lack of domain-specific 3D reconstruction tools in the space industry by leveraging state-of-the-art diffusion models and 3D Gaussian splatting techniques. This approach maintains the efficiency of the DreamGaussian framework while enhancing the accuracy and detail of spacecraft reconstructions. The code for this work can be accessed on GitHub (https://github.com/ARCLab-MIT/space-nvs).",
    "arxiv_url": "http://arxiv.org/abs/2410.05097v1",
    "pdf_url": "http://arxiv.org/pdf/2410.05097v1",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/ARCLab-MIT/space-nvs",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhotoReg: Photometrically Registering 3D Gaussian Splatting Models",
    "authors": [
      "Ziwen Yuan",
      "Tianyi Zhang",
      "Matthew Johnson-Roberson",
      "Weiming Zhi"
    ],
    "abstract": "Building accurate representations of the environment is critical for intelligent robots to make decisions during deployment. Advances in photorealistic environment models have enabled robots to develop hyper-realistic reconstructions, which can be used to generate images that are intuitive for human inspection. In particular, the recently introduced \\ac{3DGS}, which describes the scene with up to millions of primitive ellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained prominence. However, a critical unsolved problem persists: how can we fuse multiple \\ac{3DGS} into a single coherent model? Solving this problem will enable robot teams to jointly build \\ac{3DGS} models of their surroundings. A key insight of this work is to leverage the {duality} between photorealistic reconstructions, which render realistic 2D images from 3D structure, and \\emph{3D foundation models}, which predict 3D structure from image pairs. To this end, we develop PhotoReg, a framework to register multiple photorealistic \\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally built from monocular camera images, they have \\emph{arbitrary scale}. To resolve this, PhotoReg actively enforces scale consistency among the different \\ac{3DGS} models by considering depth estimates within these models. Then, the alignment is iteratively refined with fine-grained photometric losses to produce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on both standard benchmark datasets and our custom-collected datasets, including with two quadruped robots. The code is released at \\url{ziweny11.github.io/photoreg}.",
    "arxiv_url": "http://arxiv.org/abs/2410.05044v1",
    "pdf_url": "http://arxiv.org/pdf/2410.05044v1",
    "published_date": "2024-10-07",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based ray tracing with view-dependent effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D spatial-angular representation to better incorporate view-dependent effects, but the Gaussian representation and control scheme are sub-optimal. In this paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. Experiments demonstrate that 6DGS significantly outperforms 3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction of 66.5% Gaussian points compared to 3DGS. The project page is: https://gaozhongpai.github.io/6dgs/",
    "arxiv_url": "http://arxiv.org/abs/2410.04974v3",
    "pdf_url": "http://arxiv.org/pdf/2410.04974v3",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "high quality",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting",
    "authors": [
      "Matthew Strong",
      "Boshu Lei",
      "Aiden Swann",
      "Wen Jiang",
      "Kostas Daniilidis",
      "Monroe Kennedy III"
    ],
    "abstract": "We propose a framework for active next best view and touch selection for robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to represent scenes in a both photorealistic and geometrically accurate manner. However, in real-world, online robotic scenes where the number of views is limited given efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping and redundant. We address this issue by proposing an end-to-end online training and active view selection pipeline, which enhances the performance of 3DGS in few-view robotics settings. We first elevate the performance of few-shot 3DGS with a novel semantic depth alignment method using Segment Anything Model 2 (SAM2) that we supplement with Pearson depth and surface normal loss to improve color and depth reconstruction of real-world scenes. We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based on depth uncertainty. We perform online view selection on a real robot system during live 3DGS training. We motivate our improvements to few-shot GS scenes, and extend depth-based FisherRF to them, where we demonstrate both qualitative and quantitative improvements on challenging robot scenes. For more information, please see our project page at https://arm.stanford.edu/next-best-sense.",
    "arxiv_url": "http://arxiv.org/abs/2410.04680v4",
    "pdf_url": "http://arxiv.org/pdf/2410.04680v4",
    "published_date": "2024-10-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "few-shot",
      "face",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering",
    "authors": [
      "Yonghan Lee",
      "Jaehoon Choi",
      "Dongki Jung",
      "Jaeseong Yun",
      "Soohyun Ryu",
      "Dinesh Manocha",
      "Suyong Yeon"
    ],
    "abstract": "We present a novel-view rendering algorithm, Mode-GS, for ground-robot trajectory datasets. Our approach is based on using anchored Gaussian splats, which are designed to overcome the limitations of existing 3D Gaussian splatting algorithms. Prior neural rendering methods suffer from severe splat drift due to scene complexity and insufficient multi-view observation, and can fail to fix splats on the true geometry in ground-robot datasets. Our method integrates pixel-aligned anchors from monocular depths and generates Gaussian splats around these anchors using residual-form Gaussian decoders. To address the inherent scale ambiguity of monocular depth, we parameterize anchors with per-view depth-scales and employ scale-consistent depth loss for online scale calibration. Our method results in improved rendering performance, based on PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns, and achieves state-of-the-art rendering performance on the R3LIVE odometry dataset and the Tanks and Temples dataset.",
    "arxiv_url": "http://arxiv.org/abs/2410.04646v1",
    "pdf_url": "http://arxiv.org/pdf/2410.04646v1",
    "published_date": "2024-10-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreetSurfGS: Scalable Urban Street Surface Reconstruction with Planar-based Gaussian Splatting",
    "authors": [
      "Xiao Cui",
      "Weicai Ye",
      "Yifan Wang",
      "Guofeng Zhang",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "Reconstructing urban street scenes is crucial due to its vital role in applications such as autonomous driving and urban planning. These scenes are characterized by long and narrow camera trajectories, occlusion, complex object relationships, and data sparsity across multiple scales. Despite recent advancements, existing surface reconstruction methods, which are primarily designed for object-centric scenarios, struggle to adapt effectively to the unique characteristics of street scenes. To address this challenge, we introduce StreetSurfGS, the first method to employ Gaussian Splatting specifically tailored for scalable urban street scene surface reconstruction. StreetSurfGS utilizes a planar-based octree representation and segmented training to reduce memory costs, accommodate unique camera characteristics, and ensure scalability. Additionally, to mitigate depth inaccuracies caused by object overlap, we propose a guided smoothing strategy within regularization to eliminate inaccurate boundary points and outliers. Furthermore, to address sparse views and multi-scale challenges, we use a dual-step matching strategy that leverages adjacent and long-term information. Extensive experiments validate the efficacy of StreetSurfGS in both novel view synthesis and surface reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2410.04354v2",
    "pdf_url": "http://arxiv.org/pdf/2410.04354v2",
    "published_date": "2024-10-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "autonomous driving",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Variational Bayes Gaussian Splatting",
    "authors": [
      "Toon Van de Maele",
      "Ozan Catal",
      "Alexander Tschantz",
      "Christopher L. Buckley",
      "Tim Verbelen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has emerged as a promising approach for modeling 3D scenes using mixtures of Gaussians. The predominant optimization method for these models relies on backpropagating gradients through a differentiable rendering pipeline, which struggles with catastrophic forgetting when dealing with continuous streams of data. To address this limitation, we propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that frames training a Gaussian splat as variational inference over model parameters. By leveraging the conjugacy properties of multivariate Gaussians, we derive a closed-form variational update rule, allowing efficient updates from partial, sequential observations without the need for replay buffers. Our experiments show that VBGS not only matches state-of-the-art performance on static datasets, but also enables continual learning from sequentially streamed 2D and 3D data, drastically improving performance in this setting.",
    "arxiv_url": "http://arxiv.org/abs/2410.03592v1",
    "pdf_url": "http://arxiv.org/pdf/2410.03592v1",
    "published_date": "2024-10-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats",
    "authors": [
      "Mingyang Xie",
      "Haoming Cai",
      "Sachin Shah",
      "Yiran Xu",
      "Brandon Y. Feng",
      "Jia-Bin Huang",
      "Christopher A. Metzler"
    ],
    "abstract": "We introduce a simple yet effective approach for separating transmitted and reflected light. Our key insight is that the powerful novel view synthesis capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian splatting) allow one to perform flash/no-flash reflection separation using unpaired measurements -- this relaxation dramatically simplifies image acquisition over conventional paired flash/no-flash reflection separation methods. Through extensive real-world experiments, we demonstrate our method, Flash-Splat, accurately reconstructs both transmitted and reflected scenes in 3D. Our method outperforms existing 3D reflection separation methods, which do not leverage illumination control, by a large margin. Our project webpage is at https://flash-splat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2410.02764v1",
    "pdf_url": "http://arxiv.org/pdf/2410.02764v1",
    "published_date": "2024-10-03",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Hongze Chen",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "We present GI-GS, a novel inverse rendering framework that leverages 3D Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel view synthesis and relighting. In inverse rendering, accurately modeling the shading processes of objects is essential for achieving high-fidelity results. Therefore, it is critical to incorporate global illumination to account for indirect lighting that reaches an object after multiple bounces across the scene. Previous 3DGS-based methods have attempted to model indirect lighting by characterizing indirect illumination as learnable lighting volumes or additional attributes of each Gaussian, while using baked occlusion to represent shadow effects. These methods, however, fail to accurately model the complex physical interactions between light and objects, making it impossible to construct realistic indirect illumination during relighting. To address this limitation, we propose to calculate indirect lighting using efficient path tracing with deferred shading. In our framework, we first render a G-buffer to capture the detailed geometry and material properties of the scene. Then, we perform physically-based rendering (PBR) only for direct lighting. With the G-buffer and previous rendering results, the indirect lighting can be calculated through a lightweight path tracing. Our method effectively models indirect lighting under any given lighting conditions, thereby achieving better novel view synthesis and competitive relighting. Quantitative and qualitative results show that our GI-GS outperforms existing baselines in both rendering quality and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2410.02619v2",
    "pdf_url": "http://arxiv.org/pdf/2410.02619v2",
    "published_date": "2024-10-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "path tracing",
      "relighting",
      "global illumination",
      "lighting",
      "shadow",
      "lightweight",
      "geometry",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGS: Super-Resolution 3D Gaussian Splatting Enhanced by Variational Residual Features and Uncertainty-Augmented Learning",
    "authors": [
      "Shiyun Xie",
      "Zhiru Wang",
      "Xu Wang",
      "Yinghao Zhu",
      "Chengwei Pan",
      "Xiwang Dong"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis (NVS) with its real-time rendering capabilities and superior quality. However, it faces challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion of 3DGS designed with a two-stage coarse-to-fine training framework. In this framework, we use a latent feature field to represent the low-resolution scene, serving as both the initialization and foundational information for super-resolution optimization. Additionally, we introduce variational residual features to enhance high-resolution details, using their variance as uncertainty estimates to guide the densification process and loss computation. Furthermore, the introduction of a multi-view joint learning approach helps mitigate ambiguities caused by multi-view inconsistencies in the pseudo labels. Extensive experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods on both real-world and synthetic datasets using only low-resolution inputs. Code is available at https://github.com/SYXieee/SuperGS.",
    "arxiv_url": "http://arxiv.org/abs/2410.02571v3",
    "pdf_url": "http://arxiv.org/pdf/2410.02571v3",
    "published_date": "2024-10-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/SYXieee/SuperGS",
    "keywords": [
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
    "authors": [
      "Xiaobiao Du",
      "Yida Wang",
      "Xin Yu"
    ],
    "abstract": "Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2410.02103v1",
    "pdf_url": "http://arxiv.org/pdf/2410.02103v1",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
    "authors": [
      "Alexander Mai",
      "Peter Hedman",
      "George Kopanas",
      "Dor Verbin",
      "David Futschik",
      "Qiangeng Xu",
      "Falko Kuester",
      "Jonathan T. Barron",
      "Yinda Zhang"
    ],
    "abstract": "We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time differentiable emission-only volume rendering. Unlike recent rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive based representation allows for exact volume rendering, rather than alpha compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does not suffer from popping artifacts and view dependent density, but still achieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our approach is built upon ray tracing it enables effects such as defocus blur and camera distortion (e.g. such as from fisheye cameras), which are difficult to achieve by rasterization. We show that our method is more accurate with fewer blending issues than 3DGS and follow-up work on view-consistent rendering, especially on the challenging large-scale scenes from the Zip-NeRF dataset where it achieves sharpest results among real-time techniques.",
    "arxiv_url": "http://arxiv.org/abs/2410.01804v5",
    "pdf_url": "http://arxiv.org/pdf/2410.01804v5",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
    "authors": [
      "Yang Cao",
      "Yuanliang Jv",
      "Dan Xu"
    ],
    "abstract": "Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.",
    "arxiv_url": "http://arxiv.org/abs/2410.01647v1",
    "pdf_url": "http://arxiv.org/pdf/2410.01647v1",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual Camera Optimization",
    "authors": [
      "Zihan Wang",
      "Shuzhe Wang",
      "Matias Turkulainen",
      "Junyuan Fang",
      "Juho Kannala"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized novel view synthesis, facilitating real-time, high-quality image rendering. However, in scenarios involving reflective surfaces, particularly mirrors, 3D-GS often misinterprets reflections as virtual spaces, resulting in blurred and inconsistent multi-view rendering within mirrors. Our paper presents a novel method aimed at obtaining high-quality multi-view consistent reflection rendering by modelling reflections as physically-based virtual cameras. We estimate mirror planes with depth and normal estimates from 3D-GS and define virtual cameras that are placed symmetrically about the mirror plane. These virtual cameras are then used to explain mirror reflections in the scene. To address imperfections in mirror plane estimates, we propose a straightforward yet effective virtual camera optimization method to enhance reflection quality. We collect a new mirror dataset including three real-world scenarios for more diverse evaluation. Experimental validation on both Mirror-Nerf and our real-world dataset demonstrate the efficacy of our approach. We achieve comparable or superior results while significantly reducing training time compared to previous state-of-the-art.",
    "arxiv_url": "http://arxiv.org/abs/2410.01614v1",
    "pdf_url": "http://arxiv.org/pdf/2410.01614v1",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians",
    "authors": [
      "Shuyi Jiang",
      "Qihao Zhao",
      "Hossein Rahmani",
      "De Wen Soh",
      "Jun Liu",
      "Na Zhao"
    ],
    "abstract": "Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel part-aware compositional reconstruction method, called GaussianBlock, that enables semantically coherent and disentangled representations, allowing for precise and physical editing akin to building blocks, while simultaneously maintaining high fidelity. Our GaussianBlock introduces a hybrid representation that leverages the advantages of both primitives, known for their flexible actionability and editability, and 3D Gaussians, which excel in reconstruction quality. Specifically, we achieve semantically coherent primitives through a novel attention-guided centering loss derived from 2D semantic priors, complemented by a dynamic splitting and fusion strategy. Furthermore, we utilize 3D Gaussians that hybridize with primitives to refine structural details and enhance fidelity. Additionally, a binding inheritance strategy is employed to strengthen and maintain the connection between the two. Our reconstructed scenes are evidenced to be disentangled, compositional, and compact across diverse benchmarks, enabling seamless, direct and precise editing while maintaining high quality.",
    "arxiv_url": "http://arxiv.org/abs/2410.01535v3",
    "pdf_url": "http://arxiv.org/pdf/2410.01535v3",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "high quality",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MiraGe: Editable 2D Images using Gaussian Splatting",
    "authors": [
      "Joanna Waczy≈Ñska",
      "Tomasz Szczepanik",
      "Piotr Borycki",
      "S≈Çawomir Tadeja",
      "Thomas Bohn√©",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Implicit Neural Representations (INRs) approximate discrete data through continuous functions and are commonly used for encoding 2D images. Traditional image-based INRs employ neural networks to map pixel coordinates to RGB values, capturing shapes, colors, and textures within the network's weights. Recently, GaussianImage has been proposed as an alternative, using Gaussian functions instead of neural networks to achieve comparable quality and compression. Such a solution obtains a quality and compression ratio similar to classical INR models but does not allow image modification. In contrast, our work introduces a novel method, MiraGe, which uses mirror reflections to perceive 2D images in 3D space and employs flat-controlled Gaussians for precise 2D image editing. Our approach improves the rendering quality and allows realistic image modifications, including human-inspired perception of photos in the 3D world. Thanks to modeling images in 3D space, we obtain the illusion of 3D-based modification in 2D images. We also show that our Gaussian representation can be easily combined with a physics engine to produce physics-based modification of 2D images. Consequently, MiraGe allows for better quality than the standard approach and natural modification of 2D images",
    "arxiv_url": "http://arxiv.org/abs/2410.01521v2",
    "pdf_url": "http://arxiv.org/pdf/2410.01521v2",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "human",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction",
    "authors": [
      "Haoran Wang",
      "Nantheera Anantrasirichai",
      "Fan Zhang",
      "David Bull"
    ],
    "abstract": "3D Gaussian splatting (3DGS) offers the capability to achieve real-time high quality 3D scene rendering. However, 3DGS assumes that the scene is in a clear medium environment and struggles to generate satisfactory representations in underwater scenes, where light absorption and scattering are prevalent and moving objects are involved. To overcome these, we introduce a novel Gaussian Splatting-based method, UW-GS, designed specifically for underwater applications. It introduces a color appearance that models distance-dependent color variation, employs a new physics-based density control strategy to enhance clarity for distant objects, and uses a binary motion mask to handle dynamic content. Optimized with a well-designed loss function supporting for scattering media and strengthened by pseudo-depth maps, UW-GS outperforms existing methods with PSNR gains up to 1.26dB. To fully verify the effectiveness of the model, we also developed a new underwater dataset, S-UW, with dynamic object masks.",
    "arxiv_url": "http://arxiv.org/abs/2410.01517v2",
    "pdf_url": "http://arxiv.org/pdf/2410.01517v2",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "high quality",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis under Diverse Multi-view Camera Settings",
    "authors": [
      "Yingdong Hu",
      "Zhening Liu",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Feed-forward based 3D Gaussian Splatting methods have demonstrated exceptional capability in real-time novel view synthesis for human models. However, current approaches are confined to either dense viewpoint configurations or restricted image resolutions. These limitations hinder their flexibility in free-viewpoint rendering across a wide range of camera view angle discrepancies, and also restrict their ability to recover fine-grained human details in real time using commonly available GPUs. To address these challenges, we propose a novel pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse multi-view camera settings. Specifically, we first design an Efficient Cross-View Attention (EVA) module to effectively fuse cross-view information under high resolution inputs and sparse view settings, while minimizing temporal and computational overhead. Additionally, we introduce a feature refinement mechianism to predict the attributes of the 3D Gaussians and assign a feature value to each Gaussian, enabling the correction of artifacts caused by geometric inaccuracies in position estimation and enhancing overall visual fidelity. Experimental results on the THuman2.0 and THumansit datasets showcase the superiority of EVA-Gaussian in rendering quality across diverse camera settings. Project page: https://zhenliuzju.github.io/huyingdong/EVA-Gaussian.",
    "arxiv_url": "http://arxiv.org/abs/2410.01425v2",
    "pdf_url": "http://arxiv.org/pdf/2410.01425v2",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "head",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection",
    "authors": [
      "Hongru Yan",
      "Yu Zheng",
      "Yueqi Duan"
    ],
    "abstract": "Skins wrapping around our bodies, leathers covering over the sofa, sheet metal coating the car - it suggests that objects are enclosed by a series of continuous surfaces, which provides us with informative geometry prior for objectness deduction. In this paper, we propose Gaussian-Det which leverages Gaussian Splatting as surface representation for multi-view based 3D object detection. Unlike existing monocular or NeRF-based methods which depict the objects via discrete positional data, Gaussian-Det models the objects in a continuous manner by formulating the input Gaussians as feature descriptors on a mass of partial surfaces. Furthermore, to address the numerous outliers inherently introduced by Gaussian splatting, we accordingly devise a Closure Inferring Module (CIM) for the comprehensive surface-based objectness deduction. CIM firstly estimates the probabilistic feature residuals for partial surfaces given the underdetermined nature of Gaussian Splatting, which are then coalesced into a holistic representation on the overall surface closure of the object proposal. In this way, the surface information Gaussian-Det exploits serves as the prior on the quality and reliability of objectness and the information basis of proposal refinement. Experiments on both synthetic and real-world datasets demonstrate that Gaussian-Det outperforms various existing approaches, in terms of both average precision and recall.",
    "arxiv_url": "http://arxiv.org/abs/2410.01404v2",
    "pdf_url": "http://arxiv.org/pdf/2410.01404v2",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation",
    "authors": [
      "Junlin Han",
      "Jianyuan Wang",
      "Andrea Vedaldi",
      "Philip Torr",
      "Filippos Kokkinos"
    ],
    "abstract": "Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications. Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.",
    "arxiv_url": "http://arxiv.org/abs/2410.00890v3",
    "pdf_url": "http://arxiv.org/pdf/2410.00890v3",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM",
    "authors": [
      "Dapeng Feng",
      "Zhiqiang Chen",
      "Yizhen Yin",
      "Shipeng Zhong",
      "Yuhua Qi",
      "Hongbo Chen"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with photorealistic scene reconstruction emerging as a key challenge. To address this, we introduce Computational Alignment for Real-Time Gaussian Splatting SLAM (CaRtGS), a novel method enhancing the efficiency and quality of photorealistic scene reconstruction in real-time environments. Leveraging 3D Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and processing speed, which is crucial for scene photorealistic reconstruction. Our approach tackles computational misalignment in Gaussian Splatting SLAM (GS-SLAM) through an adaptive strategy that enhances optimization iterations, addresses long-tail optimization, and refines densification. Experiments on Replica, TUM-RGBD, and VECtor datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity rendering with fewer Gaussian primitives. This work propels SLAM towards real-time, photorealistic dense rendering, significantly advancing photorealistic scene representation. For the benefit of the research community, we release the code and accompanying videos on our project website: https://dapengfeng.github.io/cartgs.",
    "arxiv_url": "http://arxiv.org/abs/2410.00486v4",
    "pdf_url": "http://arxiv.org/pdf/2410.00486v4",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "high-fidelity",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGR-CAR: Coronary artery reconstruction from ultra-sparse 2D X-ray views with a 3D Gaussians representation",
    "authors": [
      "Xueming Fu",
      "Yingtai Li",
      "Fenghe Tang",
      "Jun Li",
      "Mingyue Zhao",
      "Gao-Jun Teng",
      "S. Kevin Zhou"
    ],
    "abstract": "Reconstructing 3D coronary arteries is important for coronary artery disease diagnosis, treatment planning and operation navigation. Traditional reconstruction techniques often require many projections, while reconstruction from sparse-view X-ray projections is a potential way of reducing radiation dose. However, the extreme sparsity of coronary arteries in a 3D volume and ultra-limited number of projections pose significant challenges for efficient and accurate 3D reconstruction. To this end, we propose 3DGR-CAR, a 3D Gaussian Representation for Coronary Artery Reconstruction from ultra-sparse X-ray projections. We leverage 3D Gaussian representation to avoid the inefficiency caused by the extreme sparsity of coronary artery data and propose a Gaussian center predictor to overcome the noisy Gaussian initialization from ultra-sparse view projections. The proposed scheme enables fast and accurate 3D coronary artery reconstruction with only 2 views. Experimental results on two datasets indicate that the proposed approach significantly outperforms other methods in terms of voxel accuracy and visual quality of coronary arteries. The code will be available in https://github.com/windrise/3DGR-CAR.",
    "arxiv_url": "http://arxiv.org/abs/2410.00404v1",
    "pdf_url": "http://arxiv.org/pdf/2410.00404v1",
    "published_date": "2024-10-01",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "https://github.com/windrise/3DGR-CAR",
    "keywords": [
      "sparse view",
      "sparse-view",
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seamless Augmented Reality Integration in Arthroscopy: A Pipeline for Articular Reconstruction and Guidance",
    "authors": [
      "Hongchao Shu",
      "Mingxu Liu",
      "Lalithkumar Seenivasan",
      "Suxi Gu",
      "Ping-Cheng Ku",
      "Jonathan Knopf",
      "Russell Taylor",
      "Mathias Unberath"
    ],
    "abstract": "Arthroscopy is a minimally invasive surgical procedure used to diagnose and treat joint problems. The clinical workflow of arthroscopy typically involves inserting an arthroscope into the joint through a small incision, during which surgeons navigate and operate largely by relying on their visual assessment through the arthroscope. However, the arthroscope's restricted field of view and lack of depth perception pose challenges in navigating complex articular structures and achieving surgical precision during procedures. Aiming at enhancing intraoperative awareness, we present a robust pipeline that incorporates simultaneous localization and mapping, depth estimation, and 3D Gaussian splatting to realistically reconstruct intra-articular structures solely based on monocular arthroscope video. Extending 3D reconstruction to Augmented Reality (AR) applications, our solution offers AR assistance for articular notch measurement and annotation anchoring in a human-in-the-loop manner. Compared to traditional Structure-from-Motion and Neural Radiance Field-based methods, our pipeline achieves dense 3D reconstruction and competitive rendering fidelity with explicit 3D representation in 7 minutes on average. When evaluated on four phantom datasets, our method achieves RMSE = 2.21mm reconstruction error, PSNR = 32.86 and SSIM = 0.89 on average. Because our pipeline enables AR reconstruction and guidance directly from monocular arthroscopy without any additional data and/or hardware, our solution may hold the potential for enhancing intraoperative awareness and facilitating surgical precision in arthroscopy. Our AR measurement tool achieves accuracy within 1.59 +/- 1.81mm and the AR annotation tool achieves a mIoU of 0.721.",
    "arxiv_url": "http://arxiv.org/abs/2410.00386v1",
    "pdf_url": "http://arxiv.org/pdf/2410.00386v1",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV",
      "cs.LG",
      "F.2.2; I.2.7"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "motion",
      "mapping",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Zhangshuo Qi",
      "Junyi Ma",
      "Jingyi Xu",
      "Zijie Zhou",
      "Luqi Cheng",
      "Guangming Xiong"
    ],
    "abstract": "Place recognition is a crucial component that enables autonomous vehicles to obtain localization results in GPS-denied environments. In recent years, multimodal place recognition methods have gained increasing attention. They overcome the weaknesses of unimodal sensor systems by leveraging complementary information from different modalities. However, most existing methods explore cross-modality correlations through feature-level or descriptor-level fusion, suffering from a lack of interpretability. Conversely, the recently proposed 3D Gaussian Splatting provides a new perspective on multimodal fusion by harmonizing different modalities into an explicit scene representation. In this paper, we propose a 3D Gaussian Splatting-based multimodal place recognition network dubbed GSPR. It explicitly combines multi-view RGB images and LiDAR point clouds into a spatio-temporally unified scene representation with the proposed Multimodal Gaussian Splatting. A network composed of 3D graph convolution and transformer is designed to extract spatio-temporal features and global descriptors from the Gaussian scenes for place recognition. Extensive evaluations on three datasets demonstrate that our method can effectively leverage complementary strengths of both multi-view cameras and LiDAR, achieving SOTA place recognition performance while maintaining solid generalization ability. Our open-source code will be released at https://github.com/QiZS-BIT/GSPR.",
    "arxiv_url": "http://arxiv.org/abs/2410.00299v2",
    "pdf_url": "http://arxiv.org/pdf/2410.00299v2",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/QiZS-BIT/GSPR",
    "keywords": [
      "localization",
      "recognition",
      "autonomous driving",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
    "authors": [
      "Jeff Tan",
      "Donglai Xiang",
      "Shubham Tulsiani",
      "Deva Ramanan",
      "Gengshan Yang"
    ],
    "abstract": "We present a method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for high-quality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated \"bag-of-bones\" deformation (fit to a single video via test-time optimization). We accomplish this by learning a neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage image-based priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or further optimized as explicit 3D Gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art. Project page: https://jefftan969.github.io/dressrecon/",
    "arxiv_url": "http://arxiv.org/abs/2409.20563v2",
    "pdf_url": "http://arxiv.org/pdf/2409.20563v2",
    "published_date": "2024-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "face",
      "deformation",
      "body",
      "geometry",
      "3d gaussian",
      "4d",
      "3d reconstruction",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
    "authors": [
      "Yuxuan Wu",
      "Lei Pan",
      "Wenhua Wu",
      "Guangming Wang",
      "Yanzi Miao",
      "Fan Xu",
      "Hesheng Wang"
    ],
    "abstract": "Sim-to-Real refers to the process of transferring policies learned in simulation to the real world, which is crucial for achieving practical robotics applications. However, recent Sim2real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks. In recent years, with the emergence of radiance field reconstruction methods, especially 3D Gaussian splatting, it has become possible to construct realistic real-world scenes. To this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim-to-real transfer for vision-based deep reinforcement learning. We introduce a mesh-based 3D GS method with soft binding constraints, enhancing the rendering quality of mesh models. Then utilizing a GS editing approach to synchronize the rendering with the physics simulator, RL-GSBridge could reflect the visual interactions of the physical robot accurately. Through a series of sim-to-real experiments, including grasping and pick-and-place tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in real-world task completion during sim-to-real transfer. Furthermore, a series of rendering metrics and visualization results indicate that our proposed mesh-based 3D GS reduces artifacts in unstructured objects, demonstrating more realistic rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2409.20291v2",
    "pdf_url": "http://arxiv.org/pdf/2409.20291v2",
    "published_date": "2024-09-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Gaussian Splatting SLAM by Leveraging Loop Closure",
    "authors": [
      "Zunjie Zhu",
      "Youxu Fang",
      "Xin Li",
      "Chengang Yan",
      "Feng Xu",
      "Chau Yuen",
      "Yanyan Li"
    ],
    "abstract": "3D Gaussian Splatting algorithms excel in novel view rendering applications and have been adapted to extend the capabilities of traditional SLAM systems. However, current Gaussian Splatting SLAM methods, designed mainly for hand-held RGB or RGB-D sensors, struggle with tracking drifts when used with rotating RGB-D camera setups. In this paper, we propose a robust Gaussian Splatting SLAM architecture that utilizes inputs from rotating multiple RGB-D cameras to achieve accurate localization and photorealistic rendering performance. The carefully designed Gaussian Splatting Loop Closure module effectively addresses the issue of accumulated tracking and mapping errors found in conventional Gaussian Splatting SLAM systems. First, each Gaussian is associated with an anchor frame and categorized as historical or novel based on its timestamp. By rendering different types of Gaussians at the same viewpoint, the proposed loop detection strategy considers both co-visibility relationships and distinct rendering outcomes. Furthermore, a loop closure optimization approach is proposed to remove camera pose drift and maintain the high quality of 3D Gaussian models. The approach uses a lightweight pose graph optimization algorithm to correct pose drift and updates Gaussians based on the optimized poses. Additionally, a bundle adjustment scheme further refines camera poses using photometric and geometric constraints, ultimately enhancing the global consistency of scenarios. Quantitative and qualitative evaluations on both synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art methods in camera pose estimation and novel view rendering tasks. The code will be open-sourced for the community.",
    "arxiv_url": "http://arxiv.org/abs/2409.20111v1",
    "pdf_url": "http://arxiv.org/pdf/2409.20111v1",
    "published_date": "2024-09-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "mapping",
      "high quality",
      "3d gaussian",
      "gaussian splatting",
      "slam",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RNG: Relightable Neural Gaussians",
    "authors": [
      "Jiahui Fan",
      "Fujun Luan",
      "Jian Yang",
      "Milo≈° Ha≈°an",
      "Beibei Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown impressive results for the novel view synthesis task, where lighting is assumed to be fixed. However, creating relightable 3D assets, especially for objects with ill-defined shapes (fur, fabric, etc.), remains a challenging task. The decomposition between light, geometry, and material is ambiguous, especially if either smooth surface assumptions or surfacebased analytical shading models do not apply. We propose Relightable Neural Gaussians (RNG), a novel 3DGS-based framework that enables the relighting of objects with both hard surfaces or soft boundaries, while avoiding assumptions on the shading model. We condition the radiance at each point on both view and light directions. We also introduce a shadow cue, as well as a depth refinement network to improve shadow accuracy. Finally, we propose a hybrid forward-deferred fitting strategy to balance geometry and appearance quality. Our method achieves significantly faster training (1.3 hours) and rendering (60 frames per second) compared to a prior method based on neural radiance fields and produces higher-quality shadows than a concurrent 3DGS-based method. Project page: https://www.whois-jiahui.fun/project_pages/RNG.",
    "arxiv_url": "http://arxiv.org/abs/2409.19702v5",
    "pdf_url": "http://arxiv.org/pdf/2409.19702v5",
    "published_date": "2024-09-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "fast",
      "shadow",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G3R: Gradient Guided Generalizable Reconstruction",
    "authors": [
      "Yun Chen",
      "Jingkang Wang",
      "Ze Yang",
      "Sivabalan Manivasagam",
      "Raquel Urtasun"
    ],
    "abstract": "Large scale 3D scene reconstruction is important for applications such as virtual reality and simulation. Existing neural rendering approaches (e.g., NeRF, 3DGS) have achieved realistic reconstructions on large scenes, but optimize per scene, which is expensive and slow, and exhibit noticeable artifacts under large view changes due to overfitting. Generalizable approaches or large reconstruction models are fast, but primarily work for small scenes/objects and often produce lower quality rendering results. In this work, we introduce G3R, a generalizable reconstruction approach that can efficiently predict high-quality 3D scene representations for large scenes. We propose to learn a reconstruction network that takes the gradient feedback signals from differentiable rendering to iteratively update a 3D scene representation, combining the benefits of high photorealism from per-scene optimization with data-driven priors from fast feed-forward prediction methods. Experiments on urban-driving and drone datasets show that G3R generalizes across diverse large scenes and accelerates the reconstruction process by at least 10x while achieving comparable or better realism compared to 3DGS, and also being more robust to large view changes.",
    "arxiv_url": "http://arxiv.org/abs/2409.19405v1",
    "pdf_url": "http://arxiv.org/pdf/2409.19405v1",
    "published_date": "2024-09-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "large scene",
      "nerf",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-EVT: Cross-Modal Event Camera Tracking based on Gaussian Splatting",
    "authors": [
      "Tao Liu",
      "Runze Yuan",
      "Yi'ang Ju",
      "Xun Xu",
      "Jiaqi Yang",
      "Xiangting Meng",
      "Xavier Lagorce",
      "Laurent Kneip"
    ],
    "abstract": "Reliable self-localization is a foundational skill for many intelligent mobile platforms. This paper explores the use of event cameras for motion tracking thereby providing a solution with inherent robustness under difficult dynamics and illumination. In order to circumvent the challenge of event camera-based mapping, the solution is framed in a cross-modal way. It tracks a map representation that comes directly from frame-based cameras. Specifically, the proposed method operates on top of gaussian splatting, a state-of-the-art representation that permits highly efficient and realistic novel view synthesis. The key of our approach consists of a novel pose parametrization that uses a reference pose plus first order dynamics for local differential image rendering. The latter is then compared against images of integrated events in a staggered coarse-to-fine optimization scheme. As demonstrated by our results, the realistic view rendering ability of gaussian splatting leads to stable and accurate tracking across a variety of both publicly available and newly recorded data sequences.",
    "arxiv_url": "http://arxiv.org/abs/2409.19228v1",
    "pdf_url": "http://arxiv.org/pdf/2409.19228v1",
    "published_date": "2024-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "motion",
      "mapping",
      "ar",
      "illumination",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "1st Place Solution to the 8th HANDS Workshop Challenge -- ARCTIC Track: 3DGS-based Bimanual Category-agnostic Interaction Reconstruction",
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Hyein Hwang",
      "Soohyun Hwang",
      "Junuk Cha",
      "Jaewook Han",
      "Seungryul Baek"
    ],
    "abstract": "This report describes our 1st place solution to the 8th HANDS workshop challenge (ARCTIC track) in conjunction with ECCV 2024. In this challenge, we address the task of bimanual category-agnostic hand-object interaction reconstruction, which aims to generate 3D reconstructions of both hands and the object from a monocular video, without relying on predefined templates. This task is particularly challenging due to the significant occlusion and dynamic contact between the hands and the object during bimanual manipulation. We worked to resolve these issues by introducing a mask loss and a 3D contact loss, respectively. Moreover, we applied 3D Gaussian Splatting (3DGS) to this task. As a result, our method achieved a value of 38.69 in the main metric, CD$_h$, on the ARCTIC test set.",
    "arxiv_url": "http://arxiv.org/abs/2409.19215v2",
    "pdf_url": "http://arxiv.org/pdf/2409.19215v2",
    "published_date": "2024-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction under Complex Dynamic Scenes",
    "authors": [
      "Shuo Wang",
      "Binbin Huang",
      "Ruoyu Wang",
      "Shenghua Gao"
    ],
    "abstract": "Previous surface reconstruction methods either suffer from low geometric accuracy or lengthy training times when dealing with real-world complex dynamic scenes involving multi-person activities, and human-object interactions. To tackle the dynamic contents and the occlusions in complex scenes, we present a space-time 2D Gaussian Splatting approach. Specifically, to improve geometric quality in dynamic scenes, we learn canonical 2D Gaussian splats and deform these 2D Gaussian splats while enforcing the disks of the Gaussian located on the surface of the objects by introducing depth and normal regularizers. Further, to tackle the occlusion issues in complex scenes, we introduce a compositional opacity deformation strategy, which further reduces the surface recovery of those occluded areas. Experiments on real-world sparse-view video datasets and monocular dynamic datasets demonstrate that our reconstructions outperform state-of-the-art methods, especially for the surface of the details. The project page and more visualizations can be found at: https://tb2-sy.github.io/st-2dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2409.18852v1",
    "pdf_url": "http://arxiv.org/pdf/2409.18852v1",
    "published_date": "2024-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "face",
      "deformation",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation",
    "authors": [
      "Mahtab Dahaghin",
      "Myrna Castillo",
      "Kourosh Riahidehkordi",
      "Matteo Toso",
      "Alessio Del Bue"
    ],
    "abstract": "The creation of digital replicas of physical objects has valuable applications for the preservation and dissemination of tangible cultural heritage. However, existing methods are often slow, expensive, and require expert knowledge. We propose a pipeline to generate a 3D replica of a scene using only RGB images (e.g. photos of a museum) and then extract a model for each item of interest (e.g. pieces in the exhibit). We do this by leveraging the advancements in novel view synthesis and Gaussian Splatting, modified to enable efficient 3D segmentation. This approach does not need manual annotation, and the visual inputs can be captured using a standard smartphone, making it both affordable and easy to deploy. We provide an overview of the method and baseline evaluation of the accuracy of object segmentation. The code is available at https://mahtaabdn.github.io/gaussian_heritage.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2409.19039v1",
    "pdf_url": "http://arxiv.org/pdf/2409.19039v1",
    "published_date": "2024-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RT-GuIDE: Real-Time Gaussian splatting for Information-Driven Exploration",
    "authors": [
      "Yuezhan Tao",
      "Dexter Ong",
      "Varun Murali",
      "Igor Spasojevic",
      "Pratik Chaudhari",
      "Vijay Kumar"
    ],
    "abstract": "We propose a framework for active mapping and exploration that leverages Gaussian splatting for constructing dense maps. Further, we develop a GPU-accelerated motion planning algorithm that can exploit the Gaussian map for real-time navigation. The Gaussian map constructed onboard the robot is optimized for both photometric and geometric quality while enabling real-time situational awareness for autonomy. We show through simulation experiments that our method yields comparable Peak Signal-to-Noise Ratio (PSNR) and similar reconstruction error to state-of-the-art approaches, while being orders of magnitude faster to compute. In real-world experiments, our algorithm achieves better map quality (at least 0.8dB higher PSNR and more than 16% higher geometric reconstruction accuracy) than maps constructed by a state-of-the-art method, enabling semantic segmentation using off-the-shelf open-set models. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/RT_GuIDE/",
    "arxiv_url": "http://arxiv.org/abs/2409.18122v2",
    "pdf_url": "http://arxiv.org/pdf/2409.18122v2",
    "published_date": "2024-09-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "semantic",
      "mapping",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot",
    "authors": [
      "Justin Yu",
      "Kush Hari",
      "Kishore Srinivas",
      "Karim El-Refai",
      "Adam Rashid",
      "Chung Min Kim",
      "Justin Kerr",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ashwin Balakrishna",
      "Thomas Kollar",
      "Ken Goldberg"
    ],
    "abstract": "Building semantic 3D maps is valuable for searching for objects of interest in offices, warehouses, stores, and homes. We present a mapping system that incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation that encodes both appearance and semantics in a unified representation. LEGS is trained online as a robot traverses its environment to enable localization of open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where we query for objects in the scene to assess how LEGS can capture semantic meaning. We compare LEGS to LERF and find that while both systems have comparable object query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that a multi-camera setup and incremental bundle adjustment can boost visual reconstruction quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary and long-tail object queries with up to 66% accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2409.18108v1",
    "pdf_url": "http://arxiv.org/pdf/2409.18108v1",
    "published_date": "2024-09-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "fast",
      "semantic",
      "mapping",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D Gaussians",
    "authors": [
      "Dmytro Kotovenko",
      "Olga Grebenkova",
      "Nikolaos Sarafianos",
      "Avinash Paliwal",
      "Pingchuan Ma",
      "Omid Poursaeed",
      "Sreyas Mohan",
      "Yuchen Fan",
      "Yilei Li",
      "Rakesh Ranjan",
      "Bj√∂rn Ommer"
    ],
    "abstract": "While style transfer techniques have been well-developed for 2D image stylization, the extension of these methods to 3D scenes remains relatively unexplored. Existing approaches demonstrate proficiency in transferring colors and textures but often struggle with replicating the geometry of the scenes. In our work, we leverage an explicit Gaussian Splatting (GS) representation and directly match the distributions of Gaussians between style and content scenes using the Earth Mover's Distance (EMD). By employing the entropy-regularized Wasserstein-2 distance, we ensure that the transformation maintains spatial smoothness. Additionally, we decompose the scene stylization problem into smaller chunks to enhance efficiency. This paradigm shift reframes stylization from a pure generative process driven by latent space losses to an explicit matching of distributions between two Gaussian representations. Our method achieves high-resolution 3D stylization by faithfully transferring details from 3D style scenes onto the content scene. Furthermore, WaSt-3D consistently delivers results across diverse content and style scenes without necessitating any training, as it relies solely on optimization-based techniques. See our project page for additional results and source code: $\\href{https://compvis.github.io/wast3d/}{https://compvis.github.io/wast3d/}$.",
    "arxiv_url": "http://arxiv.org/abs/2409.17917v1",
    "pdf_url": "http://arxiv.org/pdf/2409.17917v1",
    "published_date": "2024-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGS-Planner: Hierarchical Planning Framework for Active Scene Reconstruction Using 3D Gaussian Splatting",
    "authors": [
      "Zijun Xu",
      "Rui Jin",
      "Ke Wu",
      "Yi Zhao",
      "Zhiwei Zhang",
      "Jieru Zhao",
      "Fei Gao",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "In complex missions such as search and rescue,robots must make intelligent decisions in unknown environments, relying on their ability to perceive and understand their surroundings. High-quality and real-time reconstruction enhances situational awareness and is crucial for intelligent robotics. Traditional methods often struggle with poor scene representation or are too slow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting (3DGS), we propose a hierarchical planning framework for fast and high-fidelity active reconstruction. Our method evaluates completion and quality gain to adaptively guide reconstruction, integrating global and local planning for efficiency. Experiments in simulated and real-world environments show our approach outperforms existing real-time methods.",
    "arxiv_url": "http://arxiv.org/abs/2409.17624v2",
    "pdf_url": "http://arxiv.org/pdf/2409.17624v2",
    "published_date": "2024-09-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model",
    "authors": [
      "Daniel Yang",
      "John J. Leonard",
      "Yogesh Girdhar"
    ],
    "abstract": "We introduce SeaSplat, a method to enable real-time rendering of underwater scenes leveraging recent advances in 3D radiance fields. Underwater scenes are challenging visual environments, as rendering through a medium such as water introduces both range and color dependent effects on image capture. We constrain 3D Gaussian Splatting (3DGS), a recent advance in radiance fields enabling rapid training and real-time rendering of full 3D scenes, with a physically grounded underwater image formation model. Applying SeaSplat to the real-world scenes from SeaThru-NeRF dataset, a scene collected by an underwater vehicle in the US Virgin Islands, and simulation-degraded real-world scenes, not only do we see increased quantitative performance on rendering novel viewpoints from the scene with the medium present, but are also able to recover the underlying true color of the scene and restore renders to be without the presence of the intervening medium. We show that the underwater image formation helps learn scene structure, with better depth maps, as well as show that our improvements maintain the significant computational improvements afforded by leveraging a 3D Gaussian representation.",
    "arxiv_url": "http://arxiv.org/abs/2409.17345v2",
    "pdf_url": "http://arxiv.org/pdf/2409.17345v2",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
    "authors": [
      "Hui En Pang",
      "Shuai Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Tianwei Zhang",
      "Ziwei Liu"
    ],
    "abstract": "We present \\textbf{Disco4D}, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. \\textbf{1)} Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. \\textbf{2)} It adopts diffusion models to enhance the 3D generation process, \\textit{e.g.}, modeling occluded parts not visible in the input image. \\textbf{3)} It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in \\url{https://disco-4d.github.io/}.",
    "arxiv_url": "http://arxiv.org/abs/2409.17280v1",
    "pdf_url": "http://arxiv.org/pdf/2409.17280v1",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "4d",
      "human",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion",
    "authors": [
      "Yukun Huang",
      "Jianan Wang",
      "Ailing Zeng",
      "Zheng-Jun Zha",
      "Lei Zhang",
      "Xihui Liu"
    ],
    "abstract": "Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.",
    "arxiv_url": "http://arxiv.org/abs/2409.17145v1",
    "pdf_url": "http://arxiv.org/pdf/2409.17145v1",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "efficient",
      "face",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Go-SLAM: Grounded Object Segmentation and Localization with Gaussian Splatting SLAM",
    "authors": [
      "Phu Pham",
      "Dipam Patel",
      "Damon Conover",
      "Aniket Bera"
    ],
    "abstract": "We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting SLAM to reconstruct dynamic environments while embedding object-level information within the scene representations. This framework employs advanced object segmentation techniques, assigning a unique identifier to each Gaussian splat that corresponds to the object it represents. Consequently, our system facilitates open-vocabulary querying, allowing users to locate objects using natural language descriptions. Furthermore, the framework features an optimal path generation module that calculates efficient navigation paths for robots toward queried objects, considering obstacles and environmental uncertainties. Comprehensive evaluations in various scene settings demonstrate the effectiveness of our approach in delivering high-fidelity scene reconstructions, precise object segmentation, flexible object querying, and efficient robot path planning. This work represents an additional step forward in bridging the gap between 3D scene reconstruction, semantic object understanding, and real-time environment interactions.",
    "arxiv_url": "http://arxiv.org/abs/2409.16944v1",
    "pdf_url": "http://arxiv.org/pdf/2409.16944v1",
    "published_date": "2024-09-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "slam",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model",
    "authors": [
      "Hongliang Zhong",
      "Can Wang",
      "Jingbo Zhang",
      "Jing Liao"
    ],
    "abstract": "Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2409.16938v2",
    "pdf_url": "http://arxiv.org/pdf/2409.16938v2",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat",
    "authors": [
      "Jonathan Michaux",
      "Seth Isaacson",
      "Challen Enninful Adu",
      "Adam Li",
      "Rahul Kashyap Swayampakula",
      "Parker Ewen",
      "Sean Rice",
      "Katherine A. Skinner",
      "Ram Vasudevan"
    ],
    "abstract": "Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning about collisions in radiance models and the computational complexity associated with operating in dense models. This paper addresses these challenges by proposing SPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting model. This paper first derives a method to rigorously upper-bound the probability of collision between a robot and a radiance field. Then, this paper introduces a normalized reformulation of Gaussian Splatting that enables efficient computation of this collision bound. Finally, this paper presents a method to optimize trajectories that avoid collisions in a Gaussian Splat. Experiments show that SPLANNING outperforms state-of-the-art methods in generating collision-free trajectories in cluttered environments. The proposed system is also tested on a real-world robot manipulator. A project page is available at https://roahmlab.github.io/splanning.",
    "arxiv_url": "http://arxiv.org/abs/2409.16915v2",
    "pdf_url": "http://arxiv.org/pdf/2409.16915v2",
    "published_date": "2024-09-25",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Unified 3D Hair Reconstruction from Single-View Portraits",
    "authors": [
      "Yujian Zheng",
      "Yuda Qiu",
      "Leyang Jin",
      "Chongyang Ma",
      "Haibin Huang",
      "Di Zhang",
      "Pengfei Wan",
      "Xiaoguang Han"
    ],
    "abstract": "Single-view 3D hair reconstruction is challenging, due to the wide range of shape variations among diverse hairstyles. Current state-of-the-art methods are specialized in recovering un-braided 3D hairs and often take braided styles as their failure cases, because of the inherent difficulty to define priors for complex hairstyles, whether rule-based or data-based. We propose a novel strategy to enable single-view 3D reconstruction for a variety of hair types via a unified pipeline. To achieve this, we first collect a large-scale synthetic multi-view hair dataset SynMvHair with diverse 3D hair in both braided and un-braided styles, and learn two diffusion priors specialized on hair. Then we optimize 3D Gaussian-based hair from the priors with two specially designed modules, i.e. view-wise and pixel-wise Gaussian refinement. Our experiments demonstrate that reconstructing braided and un-braided 3D hair from single-view images via a unified approach is possible and our method achieves the state-of-the-art performance in recovering complex hairstyles. It is worth to mention that our method shows good generalization ability to real images, although it learns hair priors from synthetic data.",
    "arxiv_url": "http://arxiv.org/abs/2409.16863v1",
    "pdf_url": "http://arxiv.org/pdf/2409.16863v1",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for Improved Visual Localization",
    "authors": [
      "Gennady Sidorov",
      "Malik Mohrat",
      "Denis Gridusov",
      "Ruslan Rakhimov",
      "Sergey Kolyubin"
    ],
    "abstract": "Although various visual localization approaches exist, such as scene coordinate regression and camera pose regression, these methods often struggle with optimization complexity or limited accuracy. To address these challenges, we explore the use of novel view synthesis techniques, particularly 3D Gaussian Splatting (3DGS), which enables the compact encoding of both 3D geometry and scene appearance. We propose a two-stage procedure that integrates dense and robust keypoint descriptors from the lightweight XFeat feature extractor into 3DGS, enhancing performance in both indoor and outdoor environments. The coarse pose estimates are directly obtained via 2D-3D correspondences between the 3DGS representation and query image descriptors. In the second stage, the initial pose estimate is refined by minimizing the rendering-based photometric warp loss. Benchmarking on widely used indoor and outdoor datasets demonstrates improvements over recent neural rendering-based localization methods, such as NeRFMatch and PNeRFLoc.",
    "arxiv_url": "http://arxiv.org/abs/2409.16502v3",
    "pdf_url": "http://arxiv.org/pdf/2409.16502v3",
    "published_date": "2024-09-24",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "outdoor",
      "lightweight",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-based View Selection in Gaussian Splatting Reconstruction",
    "authors": [
      "Monica M. Q. Li",
      "Pierre-Yves Lajoie",
      "Giovanni Beltrame"
    ],
    "abstract": "Three-dimensional reconstruction is a fundamental problem in robotics perception. We examine the problem of active view selection to perform 3D Gaussian Splatting reconstructions with as few input images as possible. Although 3D Gaussian Splatting has made significant progress in image rendering and 3D reconstruction, the quality of the reconstruction is strongly impacted by the selection of 2D images and the estimation of camera poses through Structure-from-Motion (SfM) algorithms. Current methods to select views that rely on uncertainties from occlusions, depth ambiguities, or neural network predictions directly are insufficient to handle the issue and struggle to generalize to new scenes. By ranking the potential views in the frequency domain, we are able to effectively estimate the potential information gain of new viewpoints without ground truth data. By overcoming current constraints on model architecture and efficacy, our method achieves state-of-the-art results in view selection, demonstrating its potential for efficient image-based 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.16470v1",
    "pdf_url": "http://arxiv.org/pdf/2409.16470v1",
    "published_date": "2024-09-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model",
    "authors": [
      "Zhenghao Qi",
      "Shenghai Yuan",
      "Fen Liu",
      "Haozhi Cao",
      "Tianchen Deng",
      "Jianfei Yang",
      "Lihua Xie"
    ],
    "abstract": "Recent advancements in 3D reconstruction and neural rendering have enhanced the creation of high-quality digital assets, yet existing methods struggle to generalize across varying object shapes, textures, and occlusions. While Next Best View (NBV) planning and Learning-based approaches offer solutions, they are often limited by predefined criteria and fail to manage occlusions with human-like common sense. To address these problems, we present AIR-Embodied, a novel framework that integrates embodied AI agents with large-scale pretrained multi-modal language models to improve active 3DGS reconstruction. AIR-Embodied utilizes a three-stage process: understanding the current reconstruction state via multi-modal prompts, planning tasks with viewpoint selection and interactive actions, and employing closed-loop reasoning to ensure accurate execution. The agent dynamically refines its actions based on discrepancies between the planned and actual outcomes. Experimental evaluations across virtual and real-world environments demonstrate that AIR-Embodied significantly enhances reconstruction efficiency and quality, providing a robust solution to challenges in active 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.16019v1",
    "pdf_url": "http://arxiv.org/pdf/2409.16019v1",
    "published_date": "2024-09-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "human",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantics-Controlled Gaussian Splatting for Outdoor Scene Reconstruction and Rendering in Virtual Reality",
    "authors": [
      "Hannah Schieber",
      "Jacob Young",
      "Tobias Langlotz",
      "Stefanie Zollmann",
      "Daniel Roth"
    ],
    "abstract": "Advancements in 3D rendering like Gaussian Splatting (GS) allow novel view synthesis and real-time rendering in virtual reality (VR). However, GS-created 3D environments are often difficult to edit. For scene enhancement or to incorporate 3D assets, segmenting Gaussians by class is essential. Existing segmentation approaches are typically limited to certain types of scenes, e.g., ''circular'' scenes, to determine clear object boundaries. However, this method is ineffective when removing large objects in non-''circling'' scenes such as large outdoor scenes. We propose Semantics-Controlled GS (SCGS), a segmentation-driven GS approach, enabling the separation of large scene parts in uncontrolled, natural environments. SCGS allows scene editing and the extraction of scene parts for VR. Additionally, we introduce a challenging outdoor dataset, overcoming the ''circling'' setup. We outperform the state-of-the-art in visual quality on our dataset and in segmentation quality on the 3D-OVS dataset. We conducted an exploratory user study, comparing a 360-video, plain GS, and SCGS in VR with a fixed viewpoint. In our subsequent main study, users were allowed to move freely, evaluating plain GS and SCGS. Our main study results show that participants clearly prefer SCGS over plain GS. We overall present an innovative approach that surpasses the state-of-the-art both technically and in user experience.",
    "arxiv_url": "http://arxiv.org/abs/2409.15959v1",
    "pdf_url": "http://arxiv.org/pdf/2409.15959v1",
    "published_date": "2024-09-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "outdoor",
      "semantic",
      "segmentation",
      "real-time rendering",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB",
    "authors": [
      "Jae Yong Lee",
      "Yuqun Wu",
      "Chuhang Zou",
      "Derek Hoiem",
      "Shenlong Wang"
    ],
    "abstract": "The goal of this paper is to encode a 3D scene into an extremely compact representation from 2D images and to enable its transmittance, decoding and rendering in real-time across various platforms. Despite the progress in NeRFs and Gaussian Splats, their large model size and specialized renderers make it challenging to distribute free-viewpoint 3D content as easily as images. To address this, we have designed a novel 3D representation that encodes the plenoptic function into sinusoidal function indexed dense volumes. This approach facilitates feature sharing across different locations, improving compactness over traditional spatial voxels. The memory footprint of the dense 3D feature grid can be further reduced using spatial decomposition techniques. This design combines the strengths of spatial hashing functions and voxel decomposition, resulting in a model size as small as 150 KB for each 3D scene. Moreover, PPNG features a lightweight rendering pipeline with only 300 lines of code that decodes its representation into standard GL textures and fragment shaders. This enables real-time rendering using the traditional GL pipeline, ensuring universal compatibility and efficiency across various platforms without additional dependencies.",
    "arxiv_url": "http://arxiv.org/abs/2409.15689v1",
    "pdf_url": "http://arxiv.org/pdf/2409.15689v1",
    "published_date": "2024-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "ar",
      "nerf",
      "compact",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream",
    "authors": [
      "Jinze Yu",
      "Xin Peng",
      "Zhengda Lu",
      "Laurent Kneip",
      "Yiqun Wang"
    ],
    "abstract": "A spike camera is a specialized high-speed visual sensor that offers advantages such as high temporal resolution and high dynamic range compared to conventional frame cameras. These features provide the camera with significant advantages in many computer vision tasks. However, the tasks of novel view synthesis based on spike cameras remain underdeveloped. Although there are existing methods for learning neural radiance fields from spike stream, they either lack robustness in extremely noisy, low-quality lighting conditions or suffer from high computational complexity due to the deep fully connected neural networks and ray marching rendering strategies used in neural radiance fields, making it difficult to recover fine texture details. In contrast, the latest advancements in 3DGS have achieved high-quality real-time rendering by optimizing the point cloud representation into Gaussian ellipsoids. Building on this, we introduce SpikeGS, the method to learn 3D Gaussian fields solely from spike stream. We designed a differentiable spike stream rendering framework based on 3DGS, incorporating noise embedding and spiking neurons. By leveraging the multi-view consistency of 3DGS and the tile-based multi-threaded parallel rendering mechanism, we achieved high-quality real-time rendering results. Additionally, we introduced a spike rendering loss function that generalizes under varying illumination conditions. Our method can reconstruct view synthesis results with fine texture details from a continuous spike stream captured by a moving spike camera, while demonstrating high robustness in extremely noisy low-light scenarios. Experimental results on both real and synthetic datasets demonstrate that our method surpasses existing approaches in terms of rendering quality and speed. Our code will be available at https://github.com/520jz/SpikeGS.",
    "arxiv_url": "http://arxiv.org/abs/2409.15176v5",
    "pdf_url": "http://arxiv.org/pdf/2409.15176v5",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/520jz/SpikeGS",
    "keywords": [
      "ray marching",
      "lighting",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "illumination",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
    "authors": [
      "Luchuan Song",
      "Lele Chen",
      "Celong Liu",
      "Pinxin Liu",
      "Chenliang Xu"
    ],
    "abstract": "We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.",
    "arxiv_url": "http://arxiv.org/abs/2410.07160v1",
    "pdf_url": "http://arxiv.org/pdf/2410.07160v1",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human Hair Reconstruction with Strand-Aligned 3D Gaussians",
    "authors": [
      "Egor Zakharov",
      "Vanessa Sklyarova",
      "Michael Black",
      "Giljoo Nam",
      "Justus Thies",
      "Otmar Hilliges"
    ],
    "abstract": "We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.14778v1",
    "pdf_url": "http://arxiv.org/pdf/2409.14778v1",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the \"Gaussian Deja-vu\" framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.",
    "arxiv_url": "http://arxiv.org/abs/2409.16147v3",
    "pdf_url": "http://arxiv.org/pdf/2409.16147v3",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "3d gaussian",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
    "authors": [
      "Wangze Xu",
      "Huachen Gao",
      "Shihe Shen",
      "Rui Peng",
      "Jianbo Jiao",
      "Ronggang Wang"
    ],
    "abstract": "Recently, the Neural Radiance Field (NeRF) advancement has facilitated few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D vision applications. Despite numerous attempts to reduce the dense input requirement in NeRF, it still suffers from time-consumed training and rendering processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time high-quality rendering with an explicit point-based representation. However, similar to NeRF, it tends to overfit the train views for lack of constraints. In this paper, we propose \\textbf{MVPGS}, a few-shot NVS method that excavates the multi-view priors based on 3D Gaussian Splatting. We leverage the recent learning-based Multi-view Stereo (MVS) to enhance the quality of geometric initialization for 3DGS. To mitigate overfitting, we propose a forward-warping method for additional appearance constraints conforming to scenes based on the computed geometry. Furthermore, we introduce a view-consistent geometry constraint for Gaussian parameters to facilitate proper optimization convergence and utilize a monocular depth regularization as compensation. Experiments show that the proposed method achieves state-of-the-art performance with real-time rendering speed. Project page: https://zezeaaa.github.io/projects/MVPGS/",
    "arxiv_url": "http://arxiv.org/abs/2409.14316v1",
    "pdf_url": "http://arxiv.org/pdf/2409.14316v1",
    "published_date": "2024-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatLoc: 3D Gaussian Splatting-based Visual Localization for Augmented Reality",
    "authors": [
      "Hongjia Zhai",
      "Xiyu Zhang",
      "Boming Zhao",
      "Hai Li",
      "Yijia He",
      "Zhaopeng Cui",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "abstract": "Visual localization plays an important role in the applications of Augmented Reality (AR), which enable AR devices to obtain their 6-DoF pose in the pre-build map in order to render virtual content in real scenes. However, most existing approaches can not perform novel view rendering and require large storage capacities for maps. To overcome these limitations, we propose an efficient visual localization method capable of high-quality rendering with fewer parameters. Specifically, our approach leverages 3D Gaussian primitives as the scene representation. To ensure precise 2D-3D correspondences for pose estimation, we develop an unbiased 3D scene-specific descriptor decoder for Gaussian primitives, distilled from a constructed feature volume. Additionally, we introduce a salient 3D landmark selection algorithm that selects a suitable primitive subset based on the saliency score for localization. We further regularize key Gaussian primitives to prevent anisotropic effects, which also improves localization performance. Extensive experiments on two widely used datasets demonstrate that our method achieves superior or comparable rendering and localization performance to state-of-the-art implicit-based visual localization approaches. Project page: \\href{https://zju3dv.github.io/splatloc}{https://zju3dv.github.io/splatloc}.",
    "arxiv_url": "http://arxiv.org/abs/2409.14067v1",
    "pdf_url": "http://arxiv.org/pdf/2409.14067v1",
    "published_date": "2024-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians",
    "authors": [
      "Penghao Wang",
      "Zhirui Zhang",
      "Liao Wang",
      "Kaixin Yao",
      "Siyuan Xie",
      "Jingyi Yu",
      "Minye Wu",
      "Lan Xu"
    ],
    "abstract": "Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V^3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at https://authoritywang.github.io/v3/.",
    "arxiv_url": "http://arxiv.org/abs/2409.13648v2",
    "pdf_url": "http://arxiv.org/pdf/2409.13648v2",
    "published_date": "2024-09-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "face",
      "ar",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Portrait Video Editing Empowered by Multimodal Generative Priors",
    "authors": [
      "Xuan Gao",
      "Haiyao Xiao",
      "Chenglai Zhong",
      "Shimin Hu",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "abstract": "We introduce PortraitGen, a powerful portrait video editing method that achieves consistent and expressive stylization with multimodal prompts. Traditional portrait video editing methods often struggle with 3D and temporal consistency, and typically lack in rendering quality and efficiency. To address these issues, we lift the portrait video frames to a unified dynamic 3D Gaussian field, which ensures structural and temporal coherence across frames. Furthermore, we design a novel Neural Gaussian Texture mechanism that not only enables sophisticated style editing but also achieves rendering speed over 100FPS. Our approach incorporates multimodal inputs through knowledge distilled from large-scale 2D generative models. Our system also incorporates expression similarity guidance and a face-aware portrait editing module, effectively mitigating degradation issues associated with iterative dataset updates. Extensive experiments demonstrate the temporal consistency, editing efficiency, and superior rendering quality of our method. The broad applicability of the proposed approach is demonstrated through various applications, including text-driven editing, image-driven editing, and relighting, highlighting its great potential to advance the field of video editing. Demo videos and released code are provided in our project page: https://ustc3dv.github.io/PortraitGen/",
    "arxiv_url": "http://arxiv.org/abs/2409.13591v1",
    "pdf_url": "http://arxiv.org/pdf/2409.13591v1",
    "published_date": "2024-09-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Elite-EvGS: Learning Event-based 3D Gaussian Splatting by Distilling Event-to-Video Priors",
    "authors": [
      "Zixin Zhang",
      "Kanghao Chen",
      "Lin Wang"
    ],
    "abstract": "Event cameras are bio-inspired sensors that output asynchronous and sparse event streams, instead of fixed frames. Benefiting from their distinct advantages, such as high dynamic range and high temporal resolution, event cameras have been applied to address 3D reconstruction, important for robotic mapping. Recently, neural rendering techniques, such as 3D Gaussian splatting (3DGS), have been shown successful in 3D reconstruction. However, it still remains under-explored how to develop an effective event-based 3DGS pipeline. In particular, as 3DGS typically depends on high-quality initialization and dense multiview constraints, a potential problem appears for the 3DGS optimization with events given its inherent sparse property. To this end, we propose a novel event-based 3DGS framework, named Elite-EvGS. Our key idea is to distill the prior knowledge from the off-the-shelf event-to-video (E2V) models to effectively reconstruct 3D scenes from events in a coarse-to-fine optimization manner. Specifically, to address the complexity of 3DGS initialization from events, we introduce a novel warm-up initialization strategy that optimizes a coarse 3DGS from the frames generated by E2V models and then incorporates events to refine the details. Then, we propose a progressive event supervision strategy that employs the window-slicing operation to progressively reduce the number of events used for supervision. This subtly relives the temporal randomness of the event frames, benefiting the optimization of local textural and global structural details. Experiments on the benchmark datasets demonstrate that Elite-EvGS can reconstruct 3D scenes with better textural and structural details. Meanwhile, our method yields plausible performance on the captured real-world data, including diverse challenging conditions, such as fast motion and low light scenes.",
    "arxiv_url": "http://arxiv.org/abs/2409.13392v1",
    "pdf_url": "http://arxiv.org/pdf/2409.13392v1",
    "published_date": "2024-09-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-GSW: 3D Gaussian Splatting for Robust Watermarking",
    "authors": [
      "Youngdong Jang",
      "Hyunje Park",
      "Feng Yang",
      "Heeju Ko",
      "Euijin Choo",
      "Sangpil Kim"
    ],
    "abstract": "As 3D Gaussian Splatting (3D-GS) gains significant attention and its commercial usage increases, the need for watermarking technologies to prevent unauthorized use of the 3D-GS models and rendered images has become increasingly important. In this paper, we introduce a robust watermarking method for 3D-GS that secures copyright of both the model and its rendered images. Our proposed method remains robust against distortions in rendered images and model attacks while maintaining high rendering quality. To achieve these objectives, we present Frequency-Guided Densification (FGD), which removes 3D Gaussians based on their contribution to rendering quality, enhancing real-time rendering and the robustness of the message. FGD utilizes Discrete Fourier Transform to split 3D Gaussians in high-frequency areas, improving rendering quality. Furthermore, we employ a gradient mask for 3D Gaussians and design a wavelet-subband loss to enhance rendering quality. Our experiments show that our method embeds the message in the rendered images invisibly and robustly against various attacks, including model distortion. Our method achieves superior performance in both rendering quality and watermark robustness while improving real-time rendering efficiency. Project page: https://kuai-lab.github.io/cvpr20253dgsw/",
    "arxiv_url": "http://arxiv.org/abs/2409.13222v4",
    "pdf_url": "http://arxiv.org/pdf/2409.13222v4",
    "published_date": "2024-09-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian Splatting",
    "authors": [
      "Yan Song Hu",
      "Nicolas Abboud",
      "Muhammad Qasim Ali",
      "Adam Srebrnjak Yang",
      "Imad Elhajj",
      "Daniel Asmar",
      "Yuhao Chen",
      "John S. Zelek"
    ],
    "abstract": "Real-time SLAM with dense 3D mapping is computationally challenging, especially on resource-limited devices. The recent development of 3D Gaussian Splatting (3DGS) offers a promising approach for real-time dense 3D reconstruction. However, existing 3DGS-based SLAM systems struggle to balance hardware simplicity, speed, and map quality. Most systems excel in one or two of the aforementioned aspects but rarely achieve all. A key issue is the difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To address these challenges, we present Monocular GSO (MGSO), a novel real-time SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM provides dense structured point clouds for 3DGS initialization, accelerating optimization and producing more efficient maps with fewer Gaussians. As a result, experiments show that our system generates reconstructions with a balance of quality, memory efficiency, and speed that outperforms the state-of-the-art. Furthermore, our system achieves all results using RGB inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current live dense reconstruction systems. Not only do we surpass contemporary systems, but experiments also show that we maintain our performance on laptop hardware, making it a practical solution for robotics, A/R, and other real-time applications.",
    "arxiv_url": "http://arxiv.org/abs/2409.13055v2",
    "pdf_url": "http://arxiv.org/pdf/2409.13055v2",
    "published_date": "2024-09-19",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "mapping",
      "3d gaussian",
      "slam",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled Appearance and Geometry Modeling",
    "authors": [
      "Victor Rong",
      "Jingxiang Chen",
      "Sherwin Bahmani",
      "Kiriakos N. Kutulakos",
      "David B. Lindell"
    ],
    "abstract": "Gaussian splatting has demonstrated excellent performance for view synthesis and scene reconstruction. The representation achieves photorealistic quality by optimizing the position, scale, color, and opacity of thousands to millions of 2D or 3D Gaussian primitives within a scene. However, since each Gaussian primitive encodes both appearance and geometry, these attributes are strongly coupled--thus, high-fidelity appearance modeling requires a large number of Gaussian primitives, even when the scene geometry is simple (e.g., for a textured planar surface). We propose to texture each 2D Gaussian primitive so that even a single Gaussian can be used to capture appearance details. By employing per-primitive texturing, our appearance representation is agnostic to the topology and complexity of the scene's geometry. We show that our approach, GStex, yields improved visual quality over prior work in texturing Gaussian splats. Furthermore, we demonstrate that our decoupling enables improved novel view synthesis performance compared to 2D Gaussian splatting when reducing the number of Gaussian primitives, and that GStex can be used for scene appearance editing and re-texturing.",
    "arxiv_url": "http://arxiv.org/abs/2409.12954v3",
    "pdf_url": "http://arxiv.org/pdf/2409.12954v3",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LI-GS: Gaussian Splatting with LiDAR Incorporated for Accurate Large-Scale Reconstruction",
    "authors": [
      "Changjian Jiang",
      "Ruilan Gao",
      "Kele Shao",
      "Yue Wang",
      "Rong Xiong",
      "Yu Zhang"
    ],
    "abstract": "Large-scale 3D reconstruction is critical in the field of robotics, and the potential of 3D Gaussian Splatting (3DGS) for achieving accurate object-level reconstruction has been demonstrated. However, ensuring geometric accuracy in outdoor and unbounded scenes remains a significant challenge. This study introduces LI-GS, a reconstruction system that incorporates LiDAR and Gaussian Splatting to enhance geometric accuracy in large-scale scenes. 2D Gaussain surfels are employed as the map representation to enhance surface alignment. Additionally, a novel modeling method is proposed to convert LiDAR point clouds to plane-constrained multimodal Gaussian Mixture Models (GMMs). The GMMs are utilized during both initialization and optimization stages to ensure sufficient and continuous supervision over the entire scene while mitigating the risk of over-fitting. Furthermore, GMMs are employed in mesh extraction to eliminate artifacts and improve the overall geometric quality. Experiments demonstrate that our method outperforms state-of-the-art methods in large-scale 3D reconstruction, achieving higher accuracy compared to both LiDAR-based methods and Gaussian-based methods with improvements of 52.6% and 68.7%, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2409.12899v1",
    "pdf_url": "http://arxiv.org/pdf/2409.12899v1",
    "published_date": "2024-09-19",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "outdoor",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
    "authors": [
      "Lukas H√∂llein",
      "Alja≈æ Bo≈æiƒç",
      "Michael Zollh√∂fer",
      "Matthias Nie√üner"
    ],
    "abstract": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 30% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2409.12892v1",
    "pdf_url": "http://arxiv.org/pdf/2409.12892v1",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting",
    "authors": [
      "Kunal Chelani",
      "Assia Benbihi",
      "Torsten Sattler",
      "Fredrik Kahl"
    ],
    "abstract": "With their meaningful geometry and their omnipresence in the 3D world, edges are extremely useful primitives in computer vision. 3D edges comprise of lines and curves, and methods to reconstruct them use either multi-view images or point clouds as input. State-of-the-art image-based methods first learn a 3D edge point cloud then fit 3D edges to it. The edge point cloud is obtained by learning a 3D neural implicit edge field from which the 3D edge points are sampled on a specific level set (0 or 1). However, such methods present two important drawbacks: i) it is not realistic to sample points on exact level sets due to float imprecision and training inaccuracies. Instead, they are sampled within a range of levels so the points do not lie accurately on the 3D edges and require further processing. ii) Such implicit representations are computationally expensive and require long training times. In this paper, we address these two limitations and propose a 3D edge mapping that is simpler, more efficient, and preserves accuracy. Our method learns explicitly the 3D edge points and their edge direction hence bypassing the need for point sampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge direction as the principal axis of the Gaussian. Such a representation has the advantage of being not only geometrically meaningful but also compatible with the efficient training optimization defined in Gaussian Splatting. Results show that the proposed method produces edges as accurate and complete as the state-of-the-art while being an order of magnitude faster. Code is released at https://github.com/kunalchelani/EdgeGaussians.",
    "arxiv_url": "http://arxiv.org/abs/2409.12886v2",
    "pdf_url": "http://arxiv.org/pdf/2409.12886v2",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/kunalchelani/EdgeGaussians",
    "keywords": [
      "efficient",
      "fast",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction",
    "authors": [
      "Hanyue Zhang",
      "Zhiliu Yang",
      "Xinhe Zuo",
      "Yuxin Tong",
      "Ying Long",
      "Chen Liu"
    ],
    "abstract": "This paper proposes a novel framework for large-scale scene reconstruction based on 3D Gaussian splatting (3DGS) and aims to address the scalability and accuracy challenges faced by existing methods. For tackling the scalability issue, we split the large scene into multiple cells, and the candidate point-cloud and camera views of each cell are correlated through a visibility-based camera selection and a progressive point-cloud extension. To reinforce the rendering quality, three highlighted improvements are made in comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian intersection and the novel Gaussians density control for learning efficiency, an appearance decoupling module based on ConvKAN network to solve uneven lighting conditions in large-scale scenes, and a refined final loss with the color loss, the depth distortion loss, and the normal consistency loss. Finally, the seamless stitching procedure is executed to merge the individual Gaussian radiance field for novel view synthesis across different cells. Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method consistently generates more high-fidelity rendering results than state-of-the-art methods of large-scale scene reconstruction. We further validate the generalizability of the proposed approach by rendering on self-collected video clips recorded by a commercial drone.",
    "arxiv_url": "http://arxiv.org/abs/2409.12774v3",
    "pdf_url": "http://arxiv.org/pdf/2409.12774v3",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy",
    "authors": [
      "Letian Huang",
      "Jie Guo",
      "Jialin Dan",
      "Ruoyu Fu",
      "Shujie Wang",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has achieved impressive results in novel view synthesis, demonstrating high fidelity and efficiency. However, it easily exhibits needle-like artifacts, especially when increasing the sampling rate. Mip-Splatting tries to remove these artifacts with a 3D smoothing filter for frequency constraints and a 2D Mip filter for approximated supersampling. Unfortunately, it tends to produce over-blurred results, and sometimes needle-like Gaussians still persist. Our spectral analysis of the covariance matrix during optimization and densification reveals that current 3D-GS lacks shape awareness, relying instead on spectral radius and view positional gradients to determine splitting. As a result, needle-like Gaussians with small positional gradients and low spectral entropy fail to split and overfit high-frequency details. Furthermore, both the filters used in 3D-GS and Mip-Splatting reduce the spectral entropy and increase the condition number during zooming in to synthesize novel view, causing view inconsistencies and more pronounced artifacts. Our Spectral-GS, based on spectral analysis, introduces 3D shape-aware splitting and 2D view-consistent filtering strategies, effectively addressing these issues, enhancing 3D-GS's capability to represent high-frequency details without noticeable artifacts, and achieving high-quality photorealistic rendering.",
    "arxiv_url": "http://arxiv.org/abs/2409.12771v2",
    "pdf_url": "http://arxiv.org/pdf/2409.12771v2",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose DrivingForward, a feed-forward Gaussian Splatting model that reconstructs driving scenes from flexible surround-view input. Driving scene images from vehicle-mounted cameras are typically sparse, with limited overlap, and the movement of the vehicle further complicates the acquisition of camera extrinsics. To tackle these challenges and achieve real-time reconstruction, we jointly train a pose network, a depth network, and a Gaussian network to predict the Gaussian primitives that represent the driving scenes. The pose network and depth network determine the position of the Gaussian primitives in a self-supervised manner, without using depth ground truth and camera extrinsics during training. The Gaussian network independently predicts primitive parameters from each input image, including covariance, opacity, and spherical harmonics coefficients. At the inference stage, our model can achieve feed-forward reconstruction from flexible multi-frame surround-view input. Experiments on the nuScenes dataset show that our model outperforms existing state-of-the-art feed-forward and scene-optimized reconstruction methods in terms of reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.12753v2",
    "pdf_url": "http://arxiv.org/pdf/2409.12753v2",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrossRT: A cross platform programming technology for hardware-accelerated ray tracing in CG and CV applications",
    "authors": [
      "Vladimir Frolov",
      "Vadim Sanzharov",
      "Garifullin Albert",
      "Maxim Raenchuk",
      "Alexei Voloboy"
    ],
    "abstract": "We propose a programming technology that bridges cross-platform compatibility and hardware acceleration in ray tracing applications. Our methodology enables developers to define algorithms while our translator manages implementation specifics for different hardware or APIs. Features include: generating hardware-accelerated code from hardware-agnostic, object-oriented C++ algorithm descriptions; enabling users to define software fallbacks for non-hardware-accelerated CPUs and GPUs; producing GPU programming API-based algorithm implementations resembling manually ported C++ versions. The generated code is editable and readable, allowing for additional hardware acceleration. Our translator supports single megakernel and multiple kernel path tracing implementations without altering the programming model or input source code. Wavefront mode is crucial for NeRF and SDF, ensuring efficient evaluation with multiple kernels. Validation on tasks such as BVH tree build/traversal, ray-surface intersection for SDF, ray-volume intersection for 3D Gaussian Splatting, and complex Path Tracing models showed comparable performance levels to expert-written implementations for GPUs. Our technology outperformed existing Path Tracing implementations.",
    "arxiv_url": "http://arxiv.org/abs/2409.12617v1",
    "pdf_url": "http://arxiv.org/pdf/2409.12617v1",
    "published_date": "2024-09-19",
    "categories": [
      "cs.GR",
      "I.3"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "path tracing",
      "face",
      "3d gaussian",
      "acceleration",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting",
    "authors": [
      "Boying Li",
      "Zhixi Cai",
      "Yuan-Fang Li",
      "Ian Reid",
      "Hamid Rezatofighi"
    ],
    "abstract": "We propose Hier-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a novel hierarchical categorical representation, which enables accurate global 3D semantic mapping, scaling-up capability, and explicit semantic label prediction in the 3D world. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making it particularly challenging and costly for scene understanding. To address this problem, we introduce a novel hierarchical representation that encodes semantic information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs). We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization. Furthermore, we enhance the whole SLAM system, resulting in improved tracking and mapping performance. Our \\MethodName{} outperforms existing dense SLAM methods in both mapping and tracking accuracy, while achieving a 2x operation speed-up. Additionally, it achieves on-par semantic rendering performance compared to existing methods while significantly reducing storage and training time requirements. Rendering FPS impressively reaches 2,000 with semantic information and 3,000 without it. Most notably, it showcases the capability of handling the complex real-world scene with more than 500 semantic classes, highlighting its valuable scaling-up capability. The open-source code is available at https://github.com/LeeBY68/Hier-SLAM",
    "arxiv_url": "http://arxiv.org/abs/2409.12518v4",
    "pdf_url": "http://arxiv.org/pdf/2409.12518v4",
    "published_date": "2024-09-19",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "https://github.com/LeeBY68/Hier-SLAM",
    "keywords": [
      "tracking",
      "lighting",
      "semantic",
      "mapping",
      "understanding",
      "3d gaussian",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus",
    "authors": [
      "Jinchang Zhang",
      "Ningning Xu",
      "Hao Zhang",
      "Guoyu Lu"
    ],
    "abstract": "Depth estimation is a fundamental task in 3D geometry. While stereo depth estimation can be achieved through triangulation methods, it is not as straightforward for monocular methods, which require the integration of global and local information. The Depth from Defocus (DFD) method utilizes camera lens models and parameters to recover depth information from blurred images and has been proven to perform well. However, these methods rely on All-In-Focus (AIF) images for depth estimation, which is nearly impossible to obtain in real-world applications. To address this issue, we propose a self-supervised framework based on 3D Gaussian splatting and Siamese networks. By learning the blur levels at different focal distances of the same scene in the focal stack, the framework predicts the defocus map and Circle of Confusion (CoC) from a single defocused image, using the defocus map as input to DepthNet for monocular depth estimation. The 3D Gaussian splatting model renders defocused images using the predicted CoC, and the differences between these and the real defocused images provide additional supervision signals for the Siamese Defocus self-supervised network. This framework has been validated on both artificially synthesized and real blurred datasets. Subsequent quantitative and visualization experiments demonstrate that our proposed framework is highly effective as a DFD method.",
    "arxiv_url": "http://arxiv.org/abs/2409.12323v2",
    "pdf_url": "http://arxiv.org/pdf/2409.12323v2",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Michael Bi Mi",
      "Xinchao Wang"
    ],
    "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D.",
    "arxiv_url": "http://arxiv.org/abs/2409.12193v1",
    "pdf_url": "http://arxiv.org/pdf/2409.12193v1",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GT",
      "cs.MM"
    ],
    "github_url": "https://github.com/florinshen/Vista3D",
    "keywords": [
      "geometry",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars from Coarse-to-fine Representations",
    "authors": [
      "Kartik Teotia",
      "Hyeongwoo Kim",
      "Pablo Garrido",
      "Marc Habermann",
      "Mohamed Elgharib",
      "Christian Theobalt"
    ],
    "abstract": "Real-time rendering of human head avatars is a cornerstone of many computer graphics applications, such as augmented reality, video games, and films, to name a few. Recent approaches address this challenge with computationally efficient geometry primitives in a carefully calibrated multi-view setup. Albeit producing photorealistic head renderings, it often fails to represent complex motion changes such as the mouth interior and strongly varying head poses. We propose a new method to generate highly dynamic and deformable human head avatars from multi-view imagery in real-time. At the core of our method is a hierarchical representation of head models that allows to capture the complex dynamics of facial expressions and head movements. First, with rich facial features extracted from raw input frames, we learn to deform the coarse facial geometry of the template mesh. We then initialize 3D Gaussians on the deformed surface and refine their positions in a fine step. We train this coarse-to-fine facial avatar model along with the head pose as a learnable parameter in an end-to-end framework. This enables not only controllable facial animation via video inputs, but also high-fidelity novel view synthesis of challenging facial expressions, such as tongue deformations and fine-grained teeth structure under large motion changes. Moreover, it encourages the learned head avatar to generalize towards new facial expressions and head poses at inference time. We demonstrate the performance of our method with comparisons against the related methods on different datasets, spanning challenging facial expression sequences across multiple identities. We also show the potential application of our approach by demonstrating a cross-identity facial performance transfer application.",
    "arxiv_url": "http://arxiv.org/abs/2409.11951v1",
    "pdf_url": "http://arxiv.org/pdf/2409.11951v1",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "high-fidelity",
      "head",
      "motion",
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SRIF: Semantic Shape Registration Empowered by Diffusion-based Image Morphing and Flow Estimation",
    "authors": [
      "Mingze Sun",
      "Chen Guo",
      "Puhua Jiang",
      "Shiwei Mao",
      "Yurun Chen",
      "Ruqi Huang"
    ],
    "abstract": "In this paper, we propose SRIF, a novel Semantic shape Registration framework based on diffusion-based Image morphing and Flow estimation. More concretely, given a pair of extrinsically aligned shapes, we first render them from multi-views, and then utilize an image interpolation framework based on diffusion models to generate sequences of intermediate images between them. The images are later fed into a dynamic 3D Gaussian splatting framework, with which we reconstruct and post-process for intermediate point clouds respecting the image morphing processing. In the end, tailored for the above, we propose a novel registration module to estimate continuous normalizing flow, which deforms source shape consistently towards the target, with intermediate point clouds as weak guidance. Our key insight is to leverage large vision models (LVMs) to associate shapes and therefore obtain much richer semantic information on the relationship between shapes than the ad-hoc feature extraction and alignment. As a consequence, SRIF achieves high-quality dense correspondences on challenging shape pairs, but also delivers smooth, semantically meaningful interpolation in between. Empirical evidence justifies the effectiveness and superiority of our method as well as specific design choices. The code is released at https://github.com/rqhuang88/SRIF.",
    "arxiv_url": "http://arxiv.org/abs/2409.11682v2",
    "pdf_url": "http://arxiv.org/pdf/2409.11682v2",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/rqhuang88/SRIF",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks",
    "authors": [
      "Joji Joseph",
      "Bharadwaj Amrutur",
      "Shalabh Bhatnagar"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful 3D scene representation technique, capturing fine details with high efficiency. In this paper, we introduce a novel voting-based method that extends 2D segmentation models to 3D Gaussian splats. Our approach leverages masked gradients, where gradients are filtered by input 2D masks, and these gradients are used as votes to achieve accurate segmentation. As a byproduct, we discovered that inference-time gradients can also be used to prune Gaussians, resulting in up to 21% compression. Additionally, we explore few-shot affordance transfer, allowing annotations from 2D images to be effectively transferred onto 3D Gaussian splats. The robust yet straightforward mathematical formulation underlying this approach makes it a highly effective tool for numerous downstream applications, such as augmented reality (AR), object editing, and robotics. The project code and additional resources are available at https://jojijoseph.github.io/3dgs-segmentation.",
    "arxiv_url": "http://arxiv.org/abs/2409.11681v1",
    "pdf_url": "http://arxiv.org/pdf/2409.11681v1",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "few-shot",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "compression",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RenderWorld: World Model with Self-Supervised 3D Label",
    "authors": [
      "Ziyang Yan",
      "Wenzhen Dong",
      "Yihua Shao",
      "Yuhang Lu",
      "Liu Haiyang",
      "Jingwen Liu",
      "Haozhe Wang",
      "Zhe Wang",
      "Yan Wang",
      "Fabio Remondino",
      "Yuexin Ma"
    ],
    "abstract": "End-to-end autonomous driving with vision-only is not only more cost-effective compared to LiDAR-vision fusion but also more reliable than traditional methods. To achieve a economical and robust purely visual autonomous driving system, we propose RenderWorld, a vision-only end-to-end autonomous driving framework, which generates 3D occupancy labels using a self-supervised gaussian-based Img2Occ Module, then encodes the labels by AM-VAE, and uses world model for forecasting and planning. RenderWorld employs Gaussian Splatting to represent 3D scenes and render 2D images greatly improves segmentation accuracy and reduces GPU memory consumption compared with NeRF-based methods. By applying AM-VAE to encode air and non-air separately, RenderWorld achieves more fine-grained scene element representation, leading to state-of-the-art performance in both 4D occupancy forecasting and motion planning from autoregressive world model.",
    "arxiv_url": "http://arxiv.org/abs/2409.11356v2",
    "pdf_url": "http://arxiv.org/pdf/2409.11356v2",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "motion",
      "4d",
      "gaussian splatting",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module",
    "authors": [
      "Yichen Zhang",
      "Zihan Wang",
      "Jiali Han",
      "Peilin Li",
      "Jiaxun Zhang",
      "Jianqiang Wang",
      "Lei He",
      "Keqiang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based representations and volumetric rendering techniques, enabling real-time, high-quality rendering. However, 3DGS models typically overfit to single-scene training and are highly sensitive to the initialization of Gaussian ellipsoids, heuristically derived from Structure from Motion (SfM) point clouds, which limits both generalization and practicality. To address these limitations, we propose GS-Net, a generalizable, plug-and-play 3DGS module that densifies Gaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure representation. To the best of our knowledge, GS-Net is the first plug-and-play 3DGS module with cross-scene generalization capabilities. Additionally, we introduce the CARLA-NVS dataset, which incorporates additional camera viewpoints to thoroughly evaluate reconstruction and rendering quality. Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR improvement of 2.08 dB for conventional viewpoints and 1.86 dB for novel viewpoints, confirming the method's effectiveness and robustness.",
    "arxiv_url": "http://arxiv.org/abs/2409.11307v1",
    "pdf_url": "http://arxiv.org/pdf/2409.11307v1",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction",
    "authors": [
      "Marko Mihajlovic",
      "Sergey Prokudin",
      "Siyu Tang",
      "Robert Maier",
      "Federica Bogo",
      "Tony Tung",
      "Edmond Boyer"
    ],
    "abstract": "Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.",
    "arxiv_url": "http://arxiv.org/abs/2409.11211v1",
    "pdf_url": "http://arxiv.org/pdf/2409.11211v1",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure",
    "authors": [
      "Ziheng Xu",
      "Qingfeng Li",
      "Chen Chen",
      "Xuefeng Liu",
      "Jianwei Niu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its application in dense Simultaneous Localization and Mapping (SLAM), enabling real-time rendering and high-fidelity mapping. However, existing 3DGS-based SLAM methods often suffer from accumulated tracking errors and map drift, particularly in large-scale environments. To address these issues, we introduce GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization of camera poses and scene models. Our approach employs frame-to-model tracking and triggers hierarchical loop closure using a global-to-local strategy to minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we facilitate efficient map updates following loop corrections in large scenes. Additionally, our uncertainty-minimized keyframe selection strategy prioritizes keyframes observing more valuable 3D Gaussians to enhance submap optimization. Experimental results on various datasets demonstrate that GLC-SLAM achieves superior or competitive tracking and mapping performance compared to state-of-the-art dense RGB-D SLAM systems.",
    "arxiv_url": "http://arxiv.org/abs/2409.10982v1",
    "pdf_url": "http://arxiv.org/pdf/2409.10982v1",
    "published_date": "2024-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGSLoc: 3DGS-based Heuristic Camera Pose Refinement",
    "authors": [
      "Zhongyan Niu",
      "Zhen Tan",
      "Jinpu Zhang",
      "Xueliang Yang",
      "Dewen Hu"
    ],
    "abstract": "Visual localization refers to the process of determining camera poses and orientation within a known scene representation. This task is often complicated by factors such as illumination changes and variations in viewing angles. In this paper, we propose HGSLoc, a novel lightweight, plug and-play pose optimization framework, which integrates 3D reconstruction with a heuristic refinement strategy to achieve higher pose estimation accuracy. Specifically, we introduce an explicit geometric map for 3D representation and high-fidelity rendering, allowing the generation of high-quality synthesized views to support accurate visual localization. Our method demonstrates a faster rendering speed and higher localization accuracy compared to NeRF-based neural rendering localization approaches. We introduce a heuristic refinement strategy, its efficient optimization capability can quickly locate the target node, while we set the step-level optimization step to enhance the pose accuracy in the scenarios with small errors. With carefully designed heuristic functions, it offers efficient optimization capabilities, enabling rapid error reduction in rough localization estimations. Our method mitigates the dependence on complex neural network models while demonstrating improved robustness against noise and higher localization accuracy in challenging environments, as compared to neural network joint optimization strategies. The optimization framework proposed in this paper introduces novel approaches to visual localization by integrating the advantages of 3D reconstruction and heuristic refinement strategy, which demonstrates strong performance across multiple benchmark datasets, including 7Scenes and DB dataset.",
    "arxiv_url": "http://arxiv.org/abs/2409.10925v2",
    "pdf_url": "http://arxiv.org/pdf/2409.10925v2",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "localization",
      "efficient",
      "high-fidelity",
      "fast",
      "3d reconstruction",
      "ar",
      "nerf",
      "neural rendering",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Euntae Choi",
      "Sungjoo Yoo"
    ],
    "abstract": "We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering. We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods. In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh. Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering.",
    "arxiv_url": "http://arxiv.org/abs/2409.10335v1",
    "pdf_url": "http://arxiv.org/pdf/2409.10335v1",
    "published_date": "2024-09-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting",
    "authors": [
      "Wugang Meng",
      "Tianfu Wu",
      "Huan Yin",
      "Fumin Zhang"
    ],
    "abstract": "Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot's sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2409.10216v1",
    "pdf_url": "http://arxiv.org/pdf/2409.10216v1",
    "published_date": "2024-09-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting",
    "authors": [
      "Mohammad Nomaan Qureshi",
      "Sparsh Garg",
      "Francisco Yandun",
      "David Held",
      "George Kantor",
      "Abhisesh Silwal"
    ],
    "abstract": "Sim2Real transfer, particularly for manipulation policies relying on RGB images, remains a critical challenge in robotics due to the significant domain shift between synthetic and real-world visual data. In this paper, we propose SplatSim, a novel framework that leverages Gaussian Splatting as the primary rendering primitive to reduce the Sim2Real gap for RGB-based manipulation policies. By replacing traditional mesh representations with Gaussian Splats in simulators, SplatSim produces highly photorealistic synthetic data while maintaining the scalability and cost-efficiency of simulation. We demonstrate the effectiveness of our framework by training manipulation policies within SplatSim and deploying them in the real world in a zero-shot manner, achieving an average success rate of 86.25%, compared to 97.5% for policies trained on real-world data. Videos can be found on our project page: https://splatsim.github.io",
    "arxiv_url": "http://arxiv.org/abs/2409.10161v3",
    "pdf_url": "http://arxiv.org/pdf/2409.10161v3",
    "published_date": "2024-09-16",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Segmentation-Based Initialization for Steered Mixture of Experts Image Regression",
    "authors": [
      "Yi-Hsin Li",
      "Sebastian Knorr",
      "M√•rten Sj√∂str√∂m",
      "Thomas Sikora"
    ],
    "abstract": "Kernel image regression methods have shown to provide excellent efficiency in many image processing task, such as image and light-field compression, Gaussian Splatting, denoising and super-resolution. The estimation of parameters for these methods frequently employ gradient descent iterative optimization, which poses significant computational burden for many applications. In this paper, we introduce a novel adaptive segmentation-based initialization method targeted for optimizing Steered-Mixture-of Experts (SMoE) gating networks and Radial-Basis-Function (RBF) networks with steering kernels. The novel initialization method allocates kernels into pre-calculated image segments. The optimal number of kernels, kernel positions, and steering parameters are derived per segment in an iterative optimization and kernel sparsification procedure. The kernel information from \"local\" segments is then transferred into a \"global\" initialization, ready for use in iterative optimization of SMoE, RBF, and related kernel image regression methods. Results show that drastic objective and subjective quality improvements are achievable compared to widely used regular grid initialization, \"state-of-the-art\" K-Means initialization and previously introduced segmentation-based initialization methods, while also drastically improving the sparsity of the regression models. For same quality, the novel initialization results in models with around 50% reduction of kernels. In addition, a significant reduction of convergence time is achieved, with overall run-time savings of up to 50%. The segmentation-based initialization strategy itself admits heavy parallel computation; in theory, it may be divided into as many tasks as there are segments in the images. By accessing only four parallel GPUs, run-time savings of already 50% for initialization are achievable.",
    "arxiv_url": "http://arxiv.org/abs/2409.10101v1",
    "pdf_url": "http://arxiv.org/pdf/2409.10101v1",
    "published_date": "2024-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban Environments",
    "authors": [
      "Mahmud A. Mohamad",
      "Gamal Elghazaly",
      "Arthur Hubert",
      "Raphael Frank"
    ],
    "abstract": "This paper presents DENSER, an efficient and effective approach leveraging 3D Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments. While several methods for photorealistic scene representations, both implicitly using neural radiance fields (NeRF) and explicitly using 3DGS have shown promising results in scene reconstruction of relatively complex dynamic scenes, modeling the dynamic appearance of foreground objects tend to be challenging, limiting the applicability of these methods to capture subtleties and details of the scenes, especially far dynamic objects. To this end, we propose DENSER, a framework that significantly enhances the representation of dynamic objects and accurately models the appearance of dynamic objects in the driving scene. Instead of directly using Spherical Harmonics (SH) to model the appearance of dynamic objects, we introduce and integrate a new method aiming at dynamically estimating SH bases using wavelets, resulting in better representation of dynamic objects appearance in both space and time. Besides object appearance, DENSER enhances object shape representation through densification of its point cloud across multiple scene frames, resulting in faster convergence of model training. Extensive evaluations on KITTI dataset show that the proposed approach significantly outperforms state-of-the-art methods by a wide margin. Source codes and models will be uploaded to this repository https://github.com/sntubix/denser",
    "arxiv_url": "http://arxiv.org/abs/2409.10041v1",
    "pdf_url": "http://arxiv.org/pdf/2409.10041v1",
    "published_date": "2024-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/sntubix/denser",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAFER-Splat: A Control Barrier Function for Safe Navigation with Online Gaussian Splatting Maps",
    "authors": [
      "Timothy Chen",
      "Aiden Swann",
      "Javier Yu",
      "Ola Shorinwa",
      "Riku Murai",
      "Monroe Kennedy III",
      "Mac Schwager"
    ],
    "abstract": "SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is a real-time, scalable, and minimally invasive action filter, based on control barrier functions, for safe robotic navigation in a detailed map constructed at runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier Function (CBF) that not only induces safety with respect to all Gaussian primitives in the scene, but when synthesized into a controller, is capable of processing hundreds of thousands of Gaussians while maintaining a minimal memory footprint and operating at 15 Hz during online Splat training. Of the total compute time, a small fraction of it consumes GPU resources, enabling uninterrupted training. The safety layer is minimally invasive, correcting robot actions only when they are unsafe. To showcase the safety filter, we also introduce SplatBridge, an open-source software package built with ROS for real-time GSplat mapping for robots. We demonstrate the safety and robustness of our pipeline first in simulation, where our method is 20-50x faster, safer, and less conservative than competing methods based on neural radiance fields. Further, we demonstrate simultaneous GSplat mapping and safety filtering on a drone hardware platform using only on-board perception. We verify that under teleoperation a human pilot cannot invoke a collision. Our videos and codebase can be found at https://chengine.github.io/safer-splat.",
    "arxiv_url": "http://arxiv.org/abs/2409.09868v2",
    "pdf_url": "http://arxiv.org/pdf/2409.09868v2",
    "published_date": "2024-09-15",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "mapping",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation",
    "authors": [
      "Shuzhao Xie",
      "Weixiang Zhang",
      "Chen Tang",
      "Yunpeng Bai",
      "Rongwei Lu",
      "Shijia Ge",
      "Zhi Wang"
    ],
    "abstract": "3D Gaussian Splatting demonstrates excellent quality and speed in novel view synthesis. Nevertheless, the huge file size of the 3D Gaussians presents challenges for transmission and storage. Current works design compact models to replace the substantial volume and attributes of 3D Gaussians, along with intensive training to distill information. These endeavors demand considerable training time, presenting formidable hurdles for practical deployment. To this end, we propose MesonGS, a codec for post-training compression of 3D Gaussians. Initially, we introduce a measurement criterion that considers both view-dependent and view-independent factors to assess the impact of each Gaussian point on the rendering output, enabling the removal of insignificant points. Subsequently, we decrease the entropy of attributes through two transformations that complement subsequent entropy coding techniques to enhance the file compression rate. More specifically, we first replace rotation quaternions with Euler angles; then, we apply region adaptive hierarchical transform to key attributes to reduce entropy. Lastly, we adopt finer-grained quantization to avoid excessive information loss. Moreover, a well-crafted finetune scheme is devised to restore quality. Extensive experiments demonstrate that MesonGS significantly reduces the size of 3D Gaussians while preserving competitive quality.",
    "arxiv_url": "http://arxiv.org/abs/2409.09756v1",
    "pdf_url": "http://arxiv.org/pdf/2409.09756v1",
    "published_date": "2024-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians",
    "authors": [
      "Dasong Gao",
      "Peter Zhi Xuan Li",
      "Vivienne Sze",
      "Sertac Karaman"
    ],
    "abstract": "Constructing a high-fidelity representation of the 3D scene using a monocular camera can enable a wide range of applications on mobile devices, such as micro-robots, smartphones, and AR/VR headsets. On these devices, memory is often limited in capacity and its access often dominates the consumption of compute energy. Although Gaussian Splatting (GS) allows for high-fidelity reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a large number of past images is stored to retrain Gaussians for reducing catastrophic forgetting. These images often require two-orders-of-magnitude higher memory than the map itself and thus dominate the total memory usage. In this work, we present GEVO, a GS-based monocular SLAM framework that achieves comparable fidelity as prior methods by rendering (instead of storing) them from the existing map. Novel Gaussian initialization and optimization techniques are proposed to remove artifacts from the map and delay the degradation of the rendered images over time. Across a variety of environments, GEVO achieves comparable map fidelity while reducing the memory overhead to around 58 MBs, which is up to 94x lower than prior works.",
    "arxiv_url": "http://arxiv.org/abs/2409.09295v2",
    "pdf_url": "http://arxiv.org/pdf/2409.09295v2",
    "published_date": "2024-09-14",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "vr",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis",
    "authors": [
      "Yohan Poirier-Ginter",
      "Alban Gauthier",
      "Julien Philip",
      "Jean-Francois Lalonde",
      "George Drettakis"
    ],
    "abstract": "Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/",
    "arxiv_url": "http://arxiv.org/abs/2409.08947v2",
    "pdf_url": "http://arxiv.org/pdf/2409.08947v2",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "3d gaussian",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius",
    "authors": [
      "Xinzhe Wang",
      "Ran Yi",
      "Lizhuang Ma"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a recent explicit 3D representation that has achieved high-quality reconstruction and real-time rendering of complex scenes. However, the rasterization pipeline still suffers from unnecessary overhead resulting from avoidable serial Gaussian culling, and uneven load due to the distinct number of Gaussian to be rendered across pixels, which hinders wider promotion and application of 3DGS. In order to accelerate Gaussian splatting, we propose AdR-Gaussian, which moves part of serial culling in Render stage into the earlier Preprocess stage to enable parallel culling, employing adaptive radius to narrow the rendering pixel range for each Gaussian, and introduces a load balancing method to minimize thread waiting time during the pixel-parallel rendering. Our contributions are threefold, achieving a rendering speed of 310% while maintaining equivalent or even better quality than the state-of-the-art. Firstly, we propose to early cull Gaussian-Tile pairs of low splatting opacity based on an adaptive radius in the Gaussian-parallel Preprocess stage, which reduces the number of affected tile through the Gaussian bounding circle, thus reducing unnecessary overhead and achieving faster rendering speed. Secondly, we further propose early culling based on axis-aligned bounding box for Gaussian splatting, which achieves a more significant reduction in ineffective expenses by accurately calculating the Gaussian size in the 2D directions. Thirdly, we propose a balancing algorithm for pixel thread load, which compresses the information of heavy-load pixels to reduce thread waiting time, and enhance information of light-load pixels to hedge against rendering quality loss. Experiments on three datasets demonstrate that our algorithm can significantly improve the Gaussian Splatting rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2409.08669v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08669v1",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints",
    "authors": [
      "Shan Chen",
      "Jiale Zhou",
      "Lei Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in scene synthesis and novel view synthesis tasks. Typically, the initialization of 3D Gaussian primitives relies on point clouds derived from Structure-from-Motion (SfM) methods. However, in scenarios requiring scene reconstruction from sparse viewpoints, the effectiveness of 3DGS is significantly constrained by the quality of these initial point clouds and the limited number of input images. In this study, we present Dust-GS, a novel framework specifically designed to overcome the limitations of 3DGS in sparse viewpoint conditions. Instead of relying solely on SfM, Dust-GS introduces an innovative point cloud initialization technique that remains effective even with sparse input data. Our approach leverages a hybrid strategy that integrates an adaptive depth-based masking technique, thereby enhancing the accuracy and detail of reconstructed scenes. Extensive experiments conducted on several benchmark datasets demonstrate that Dust-GS surpasses traditional 3DGS methods in scenarios with sparse viewpoints, achieving superior scene reconstruction quality with a reduced number of input images.",
    "arxiv_url": "http://arxiv.org/abs/2409.08613v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08613v1",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting",
    "authors": [
      "Runze Chen",
      "Mingyu Xiao",
      "Haiyong Luo",
      "Fang Zhao",
      "Fan Wu",
      "Hao Xiong",
      "Qi Liu",
      "Meng Song"
    ],
    "abstract": "We introduce Crowd-Sourced Splatting (CSS), a novel 3D Gaussian Splatting (3DGS) pipeline designed to overcome the challenges of pose-free scene reconstruction using crowd-sourced imagery. The dream of reconstructing historically significant but inaccessible scenes from collections of photographs has long captivated researchers. However, traditional 3D techniques struggle with missing camera poses, limited viewpoints, and inconsistent lighting. CSS addresses these challenges through robust geometric priors and advanced illumination modeling, enabling high-quality novel view synthesis under complex, real-world conditions. Our method demonstrates clear improvements over existing approaches, paving the way for more accurate and flexible applications in AR, VR, and large-scale 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.08562v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08562v1",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "lighting",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos",
    "authors": [
      "Yuheng Jiang",
      "Zhehao Shen",
      "Yu Hong",
      "Chengcheng Guo",
      "Yize Wu",
      "Yingliang Zhang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \\textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.",
    "arxiv_url": "http://arxiv.org/abs/2409.08353v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08353v1",
    "published_date": "2024-09-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "tracking",
      "vr",
      "motion",
      "human",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50$\\times$ faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.",
    "arxiv_url": "http://arxiv.org/abs/2409.08270v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08270v1",
    "published_date": "2024-09-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "https://github.com/florinshen/FlashSplat",
    "keywords": [
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis",
    "authors": [
      "Qian Chen",
      "Shihao Shu",
      "Xiangzhi Bai"
    ],
    "abstract": "Novel-view synthesis based on visible light has been extensively studied. In comparison to visible light imaging, thermal infrared imaging offers the advantage of all-weather imaging and strong penetration, providing increased possibilities for reconstruction in nighttime and adverse weather scenarios. However, thermal infrared imaging is influenced by physical characteristics such as atmospheric transmission effects and thermal conduction, hindering the precise reconstruction of intricate details in thermal infrared scenes, manifesting as issues of floaters and indistinct edge features in synthesized images. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, a temperature consistency constraint is incorporated into the optimization objective to enhance the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset for this field named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video scenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios, totaling 6,664 frames of thermal infrared image data. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.03 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features present in the baseline method. Our dataset and codebase will be released in \\href{https://github.com/mzzcdf/Thermal3DGS}{\\textcolor{red}{Thermal3DGS}}.",
    "arxiv_url": "http://arxiv.org/abs/2409.08042v1",
    "pdf_url": "http://arxiv.org/pdf/2409.08042v1",
    "published_date": "2024-09-12",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.3.3; I.4.5"
    ],
    "github_url": "https://github.com/mzzcdf/Thermal3DGS",
    "keywords": [
      "outdoor",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming with Arbitrary Length",
    "authors": [
      "Bangya Liu",
      "Suman Banerjee"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant attention in computer vision and computer graphics due to its high rendering speed and remarkable quality. While extant research has endeavored to extend the application of 3DGS from static to dynamic scenes, such efforts have been consistently impeded by excessive model sizes, constraints on video duration, and content deviation. These limitations significantly compromise the streamability of dynamic 3D Gaussian models, thereby restricting their utility in downstream applications, including volumetric video, autonomous vehicle, and immersive technologies such as virtual, augmented, and mixed reality.   This paper introduces SwinGS, a novel framework for training, delivering, and rendering volumetric video in a real-time streaming fashion. To address the aforementioned challenges and enhance streamability, SwinGS integrates spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to fit various 3D scenes across frames, in the meantime employing a sliding window captures Gaussian snapshots for each frame in an accumulative way. We implement a prototype of SwinGS and demonstrate its streamability across various datasets and scenes. Additionally, we develop an interactive WebGL viewer enabling real-time volumetric video playback on most devices with modern browsers, including smartphones and tablets. Experimental results show that SwinGS reduces transmission costs by 83.6% compared to previous work and could be easily scaled to volumetric videos with arbitrary length with no increasing of required GPU resources.",
    "arxiv_url": "http://arxiv.org/abs/2409.07759v3",
    "pdf_url": "http://arxiv.org/pdf/2409.07759v3",
    "published_date": "2024-09-12",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs",
    "authors": [
      "Sadra Safadoust",
      "Fabio Tosi",
      "Fatma G√ºney",
      "Matteo Poggi"
    ],
    "abstract": "3D Gaussian Splatting (GS) significantly struggles to accurately represent the underlying 3D scene geometry, resulting in inaccuracies and floating artifacts when rendering depth maps. In this paper, we address this limitation, undertaking a comprehensive analysis of the integration of depth priors throughout the optimization process of Gaussian primitives, and present a novel strategy for this purpose. This latter dynamically exploits depth cues from a readily available stereo network, processing virtual stereo pairs rendered by the GS model itself during training and achieving consistent self-improvement of the scene representation. Experimental results on three popular datasets, breaking ground as the first to assess depth accuracy for these models, validate our findings.",
    "arxiv_url": "http://arxiv.org/abs/2409.07456v1",
    "pdf_url": "http://arxiv.org/pdf/2409.07456v1",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models",
    "authors": [
      "Haibo Yang",
      "Yang Chen",
      "Yingwei Pan",
      "Ting Yao",
      "Zhineng Chen",
      "Chong-Wah Ngo",
      "Tao Mei"
    ],
    "abstract": "Despite having tremendous progress in image-to-3D generation, existing methods still struggle to produce multi-view consistent images with high-resolution textures in detail, especially in the paradigm of 2D diffusion that lacks 3D awareness. In this work, we present High-resolution Image-to-3D model (Hi3D), a new video diffusion based paradigm that redefines a single image to multi-view images as 3D-aware sequential image generation (i.e., orbital video generation). This methodology delves into the underlying temporal consistency knowledge in video diffusion model that generalizes well to geometry consistency across multiple views in 3D generation. Technically, Hi3D first empowers the pre-trained video diffusion model with 3D-aware prior (camera pose condition), yielding multi-view images with low-resolution texture details. A 3D-aware video-to-video refiner is learnt to further scale up the multi-view images with high-resolution texture details. Such high-resolution multi-view images are further augmented with novel views through 3D Gaussian Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D reconstruction. Extensive experiments on both novel view synthesis and single view reconstruction demonstrate that our Hi3D manages to produce superior multi-view consistency images with highly-detailed textures. Source code and data are available at \\url{https://github.com/yanghb22-fdu/Hi3D-Official}.",
    "arxiv_url": "http://arxiv.org/abs/2409.07452v1",
    "pdf_url": "http://arxiv.org/pdf/2409.07452v1",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/yanghb22-fdu/Hi3D-Official",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Facial Gaussians Translator for Relightable and Interactable Facial Rendering",
    "authors": [
      "Dafei Qin",
      "Hongyang Lin",
      "Qixuan Zhang",
      "Kaichun Qiao",
      "Longwen Zhang",
      "Zijun Zhao",
      "Jun Saito",
      "Jingyi Yu",
      "Lan Xu",
      "Taku Komura"
    ],
    "abstract": "We propose GauFace, a novel Gaussian Splatting representation, tailored for efficient animation and rendering of physically-based facial assets. Leveraging strong geometric priors and constrained optimization, GauFace ensures a neat and structured Gaussian representation, delivering high fidelity and real-time facial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.   Then, we introduce TransGS, a diffusion transformer that instantly translates physically-based facial assets into the corresponding GauFace representations. Specifically, we adopt a patch-based pipeline to handle the vast number of Gaussians effectively. We also introduce a novel pixel-aligned sampling scheme with UV positional encoding to ensure the throughput and rendering quality of GauFace assets generated by our TransGS. Once trained, TransGS can instantly translate facial assets with lighting conditions to GauFace representation, With the rich conditioning modalities, it also enables editing and animation capabilities reminiscent of traditional CG pipelines.   We conduct extensive evaluations and user studies, compared to traditional offline and online renderers, as well as recent neural rendering methods, which demonstrate the superior performance of our approach for facial asset rendering. We also showcase diverse immersive applications of facial assets using our TransGS approach and GauFace representation, across various platforms like PCs, phones and even VR headsets.",
    "arxiv_url": "http://arxiv.org/abs/2409.07441v2",
    "pdf_url": "http://arxiv.org/pdf/2409.07441v2",
    "published_date": "2024-09-11",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "relightable",
      "vr",
      "lighting",
      "face",
      "ar",
      "animation",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks",
    "authors": [
      "Ruihan Xu",
      "Anthony Opipari",
      "Joshua Mah",
      "Stanley Lewis",
      "Haoran Zhang",
      "Hanzhe Guo",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as an approach for SO(2)-Equivariant 3D object reconstruction from single-view image observations.   GSNs take a single observation as input to generate a Gaussian splat representation describing the observed object's geometry and texture. By using a shared feature extractor before decoding Gaussian colors, covariances, positions, and opacities, GSNs achieve extremely high throughput (>150FPS). Experiments demonstrate that GSNs can be trained efficiently using a multi-view rendering loss and are competitive, in quality, with expensive diffusion-based reconstruction algorithms. The GSN model is validated on multiple benchmark experiments. Moreover, we demonstrate the potential for GSNs to be used within a robotic manipulation pipeline for object-centric grasping.",
    "arxiv_url": "http://arxiv.org/abs/2409.07245v1",
    "pdf_url": "http://arxiv.org/pdf/2409.07245v1",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "efficient",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ThermalGaussian: Thermal 3D Gaussian Splatting",
    "authors": [
      "Rongfeng Lu",
      "Hangyu Chen",
      "Zunjie Zhu",
      "Yuhang Qin",
      "Ming Lu",
      "Le Zhang",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS) prevails due to its rapid training and real-time rendering. In this work, we propose ThermalGaussian, the first thermal 3DGS approach capable of rendering high-quality images in RGB and thermal modalities. We first calibrate the RGB camera and the thermal camera to ensure that both modalities are accurately aligned. Subsequently, we use the registered images to learn the multimodal 3D Gaussians. To prevent the overfitting of any single modality, we introduce several multimodal regularization constraints. We also develop smoothing constraints tailored to the physical characteristics of the thermal modality. Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a hand-hold thermal-infrared camera, facilitating future research on thermal scene reconstruction. We conduct comprehensive experiments to show that ThermalGaussian achieves photorealistic rendering of thermal images and improves the rendering quality of RGB images. With the proposed multimodal regularization constraints, we also reduced the model's storage cost by 90%. Our project page is at https://thermalgaussian.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2409.07200v2",
    "pdf_url": "http://arxiv.org/pdf/2409.07200v2",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "gsplat: An Open-Source Library for Gaussian Splatting",
    "authors": [
      "Vickie Ye",
      "Ruilong Li",
      "Justin Kerr",
      "Matias Turkulainen",
      "Brent Yi",
      "Zhuoyang Pan",
      "Otto Seiskari",
      "Jianbo Ye",
      "Jeffrey Hu",
      "Matthew Tancik",
      "Angjoo Kanazawa"
    ],
    "abstract": "gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.",
    "arxiv_url": "http://arxiv.org/abs/2409.06765v1",
    "pdf_url": "http://arxiv.org/pdf/2409.06765v1",
    "published_date": "2024-09-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/nerfstudio-project/gsplat",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction",
    "authors": [
      "Junyi Chen",
      "Weicai Ye",
      "Yifan Wang",
      "Danpeng Chen",
      "Di Huang",
      "Wanli Ouyang",
      "Guofeng Zhang",
      "Yu Qiao",
      "Tong He"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown promising performance in novel view synthesis. Previous methods adapt it to obtaining surfaces of either individual 3D objects or within limited scenes. In this paper, we make the first attempt to tackle the challenging task of large-scale scene surface reconstruction. This task is particularly difficult due to the high GPU memory consumption, different levels of details for geometric representation, and noticeable inconsistencies in appearance. To this end, we propose GigaGS, the first work for high-quality surface reconstruction for large-scale scenes using 3DGS. GigaGS first applies a partitioning strategy based on the mutual visibility of spatial regions, which effectively grouping cameras for parallel processing. To enhance the quality of the surface, we also propose novel multi-view photometric and geometric consistency constraints based on Level-of-Detail representation. In doing so, our method can reconstruct detailed surface structures. Comprehensive experiments are conducted on various datasets. The consistent improvement demonstrates the superiority of GigaGS.",
    "arxiv_url": "http://arxiv.org/abs/2409.06685v1",
    "pdf_url": "http://arxiv.org/pdf/2409.06685v1",
    "published_date": "2024-09-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "large scene",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification",
    "authors": [
      "Phu Pham",
      "Aradhya N. Mathur",
      "Ojaswa Sharma",
      "Aniket Bera"
    ],
    "abstract": "The field of text-to-3D content generation has made significant progress in generating realistic 3D objects, with existing methodologies like Score Distillation Sampling (SDS) offering promising guidance. However, these methods often encounter the \"Janus\" problem-multi-face ambiguities due to imprecise guidance. Additionally, while recent advancements in 3D gaussian splitting have shown its efficacy in representing 3D volumes, optimization of this representation remains largely unexplored. This paper introduces a unified framework for text-to-3D content generation that addresses these critical gaps. Our approach utilizes multi-view guidance to iteratively form the structure of the 3D model, progressively enhancing detail and accuracy. We also introduce a novel densification algorithm that aligns gaussians close to the surface, optimizing the structural integrity and fidelity of the generated models. Extensive experiments validate our approach, demonstrating that it produces high-quality visual outputs with minimal time cost. Notably, our method achieves high-quality results within half an hour of training, offering a substantial efficiency gain over most existing methods, which require hours of training time to achieve comparable results.",
    "arxiv_url": "http://arxiv.org/abs/2409.06620v1",
    "pdf_url": "http://arxiv.org/pdf/2409.06620v1",
    "published_date": "2024-09-10",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "high-fidelity",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sources of Uncertainty in 3D Scene Reconstruction",
    "authors": [
      "Marcus Klasson",
      "Riccardo Mereu",
      "Juho Kannala",
      "Arno Solin"
    ],
    "abstract": "The process of 3D scene reconstruction can be affected by numerous uncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack built-in mechanisms to directly address or quantify uncertainties arising from the presence of noise, occlusions, confounding outliers, and imprecise camera pose inputs. In this paper, we introduce a taxonomy that categorizes different sources of uncertainty inherent in these methods. Moreover, we extend NeRF- and GS-based methods with uncertainty estimation techniques, including learning uncertainty outputs and ensembles, and perform an empirical study to assess their ability to capture the sensitivity of the reconstruction. Our study highlights the need for addressing various uncertainty aspects when designing NeRF/GS-based methods for uncertainty-aware 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2409.06407v1",
    "pdf_url": "http://arxiv.org/pdf/2409.06407v1",
    "published_date": "2024-09-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Online 3D reconstruction and dense tracking in endoscopic videos",
    "authors": [
      "Michel Hayoz",
      "Christopher Hahne",
      "Thomas Kurmann",
      "Max Allan",
      "Guido Beldi",
      "Daniel Candinas",
      "ablo M√°rquez-Neila",
      "Raphael Sznitman"
    ],
    "abstract": "3D scene reconstruction from stereo endoscopic video data is crucial for advancing surgical interventions. In this work, we present an online framework for online, dense 3D scene reconstruction and tracking, aimed at enhancing surgical scene understanding and assisting interventions. Our method dynamically extends a canonical scene representation using Gaussian splatting, while modeling tissue deformations through a sparse set of control points. We introduce an efficient online fitting algorithm that optimizes the scene parameters, enabling consistent tracking and accurate reconstruction. Through experiments on the StereoMIS dataset, we demonstrate the effectiveness of our approach, outperforming state-of-the-art tracking methods and achieving comparable performance to offline reconstruction techniques. Our work enables various downstream applications thus contributing to advancing the capabilities of surgical assistance systems.",
    "arxiv_url": "http://arxiv.org/abs/2409.06037v1",
    "pdf_url": "http://arxiv.org/pdf/2409.06037v1",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "deformation",
      "understanding",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GASP: Gaussian Splatting for Physic-Based Simulations",
    "authors": [
      "Piotr Borycki",
      "Weronika Smolak",
      "Joanna Waczy≈Ñska",
      "Marcin Mazur",
      "S≈Çawomir Tadeja",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Physics simulation is paramount for modeling and utilization of 3D scenes in various real-world applications. However, its integration with state-of-the-art 3D scene rendering techniques such as Gaussian Splatting (GS) remains challenging. Existing models use additional meshing mechanisms, including triangle or tetrahedron meshing, marching cubes, or cage meshes. As an alternative, we can modify the physics grounded Newtonian dynamics to align with 3D Gaussian components. Current models take the first-order approximation of a deformation map, which locally approximates the dynamics by linear transformations. In contrast, our Gaussian Splatting for Physics-Based Simulations (GASP) model uses such a map (without any modifications) and flat Gaussian distributions, which are parameterized by three points (mesh faces). Subsequently, each 3D point (mesh face node) is treated as a discrete entity within a 3D space. Consequently, the problem of modeling Gaussian components is reduced to working with 3D points. Additionally, the information on mesh faces can be used to incorporate further properties into the physics model, facilitating the use of triangles. Resulting solution can be integrated into any physics engine that can be treated as a black box. As demonstrated in our studies, the proposed model exhibits superior performance on a diverse range of benchmark datasets designed for 3D object rendering.",
    "arxiv_url": "http://arxiv.org/abs/2409.05819v2",
    "pdf_url": "http://arxiv.org/pdf/2409.05819v2",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiDAR-3DGS: LiDAR Reinforced 3D Gaussian Splatting for Multimodal Radiance Field Rendering",
    "authors": [
      "Hansol Lim",
      "Hanbeom Chang",
      "Jongseong Brad Choi",
      "Chul Min Yeum"
    ],
    "abstract": "In this paper, we explore the capabilities of multimodal inputs to 3D Gaussian Splatting (3DGS) based Radiance Field Rendering. We present LiDAR-3DGS, a novel method of reinforcing 3DGS inputs with LiDAR generated point clouds to significantly improve the accuracy and detail of 3D models. We demonstrate a systematic approach of LiDAR reinforcement to 3DGS to enable capturing of important features such as bolts, apertures, and other details that are often missed by image-based features alone. These details are crucial for engineering applications such as remote monitoring and maintenance. Without modifying the underlying 3DGS algorithm, we demonstrate that even a modest addition of LiDAR generated point cloud significantly enhances the perceptual quality of the models. At 30k iterations, the model generated by our method resulted in an increase of 7.064% in PSNR and 0.565% in SSIM, respectively. Since the LiDAR used in this research was a commonly used commercial-grade device, the improvements observed were modest and can be further enhanced with higher-grade LiDAR systems. Additionally, these improvements can be supplementary to other derivative works of Radiance Field Rendering and also provide a new insight for future LiDAR and computer vision integrated modeling.",
    "arxiv_url": "http://arxiv.org/abs/2409.16296v1",
    "pdf_url": "http://arxiv.org/pdf/2409.16296v1",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lagrangian Hashing for Compressed Neural Field Representations",
    "authors": [
      "Shrisudhan Govindarajan",
      "Zeno Sambugaro",
      "Akhmedkhan",
      "Shabanov",
      "Towaki Takikawa",
      "Daniel Rebain",
      "Weiwei Sun",
      "Nicola Conci",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "We present Lagrangian Hashing, a representation for neural fields combining the characteristics of fast training NeRF methods that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We achieve this by incorporating a point-based representation into the high-resolution layers of the hierarchical hash tables of an InstantNGP representation. As our points are equipped with a field of influence, our representation can be interpreted as a mixture of Gaussians stored within the hash table. We propose a loss that encourages the movement of our Gaussians towards regions that require more representation budget to be sufficiently well represented. Our main finding is that our representation allows the reconstruction of signals using a more compact representation without compromising quality.",
    "arxiv_url": "http://arxiv.org/abs/2409.05334v1",
    "pdf_url": "http://arxiv.org/pdf/2409.05334v1",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping",
    "authors": [
      "Zeyu Cai",
      "Duotun Wang",
      "Yixun Liang",
      "Zhijing Shao",
      "Ying-Cong Chen",
      "Xiaohang Zhan",
      "Zeyu Wang"
    ],
    "abstract": "Score Distillation Sampling (SDS) has emerged as a prevalent technique for text-to-3D generation, enabling 3D content creation by distilling view-dependent information from text-to-2D guidance. However, they frequently exhibit shortcomings such as over-saturated color and excess smoothness. In this paper, we conduct a thorough analysis of SDS and refine its formulation, finding that the core design is to model the distribution of rendered images. Following this insight, we introduce a novel strategy called Variational Distribution Mapping (VDM), which expedites the distribution modeling process by regarding the rendered images as instances of degradation from diffusion-based generation. This special design enables the efficient training of variational distribution by skipping the calculations of the Jacobians in the diffusion U-Net. We also introduce timestep-dependent Distribution Coefficient Annealing (DCA) to further improve distilling precision. Leveraging VDM and DCA, we use Gaussian Splatting as the 3D representation and build a text-to-3D generation framework. Extensive experiments and evaluations demonstrate the capability of VDM and DCA to generate high-fidelity and realistic assets with optimization efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2409.05099v4",
    "pdf_url": "http://arxiv.org/pdf/2409.05099v4",
    "published_date": "2024-09-08",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.4.9; I.3.6"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "mapping",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-PT: Exploiting 3D Gaussian Splatting for Comprehensive Point Cloud Understanding via Self-supervised Learning",
    "authors": [
      "Keyi Liu",
      "Yeqi Luo",
      "Weidong Yang",
      "Jingyi Xu",
      "Zhijun Li",
      "Wen-Ming Chen",
      "Ben Fei"
    ],
    "abstract": "Self-supervised learning of point cloud aims to leverage unlabeled 3D data to learn meaningful representations without reliance on manual annotations. However, current approaches face challenges such as limited data diversity and inadequate augmentation for effective feature learning. To address these challenges, we propose GS-PT, which integrates 3D Gaussian Splatting (3DGS) into point cloud self-supervised learning for the first time. Our pipeline utilizes transformers as the backbone for self-supervised pre-training and introduces novel contrastive learning tasks through 3DGS. Specifically, the transformers aim to reconstruct the masked point cloud. 3DGS utilizes multi-view rendered images as input to generate enhanced point cloud distributions and novel view images, facilitating data augmentation and cross-modal contrastive learning. Additionally, we incorporate features from depth maps. By optimizing these tasks collectively, our method enriches the tri-modal self-supervised learning process, enabling the model to leverage the correlation across 3D point clouds and 2D images from various modalities. We freeze the encoder after pre-training and test the model's performance on multiple downstream tasks. Experimental results indicate that GS-PT outperforms the off-the-shelf self-supervised learning methods on various downstream tasks including 3D object classification, real-world classifications, and few-shot learning and segmentation.",
    "arxiv_url": "http://arxiv.org/abs/2409.04963v1",
    "pdf_url": "http://arxiv.org/pdf/2409.04963v1",
    "published_date": "2024-09-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "face",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras",
    "authors": [
      "Zimu Liao",
      "Siyan Chen",
      "Rong Fu",
      "Yi Wang",
      "Zhongling Su",
      "Hao Luo",
      "Li Ma",
      "Linning Xu",
      "Bo Dai",
      "Hengjie Li",
      "Zhilin Pei",
      "Xingcheng Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2409.04751v2",
    "pdf_url": "http://arxiv.org/pdf/2409.04751v2",
    "published_date": "2024-09-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers",
    "authors": [
      "Lorenza Prospero",
      "Abdullah Hamdi",
      "Joao F. Henriques",
      "Christian Rupprecht"
    ],
    "abstract": "Reconstructing posed 3D human models from monocular images has important applications in the sports industry, including performance tracking, injury prevention and virtual training. In this work, we combine 3D human pose and shape estimation with 3D Gaussian Splatting (3DGS), a representation of the scene composed of a mixture of Gaussians. This allows training or fine-tuning a human model predictor on multi-view images alone, without 3D ground truth. Predicting such mixtures for a human from a single input image is challenging due to self-occlusions and dependence on articulations, while also needing to retain enough flexibility to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate spatial density and approximate initial position for the Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other 3DGS attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve near real-time inference of 3D human models from a single image without expensive diffusion models or 3D points supervision, thus making it ideal for the sport industry at any level. More importantly, rendering is an effective auxiliary objective to refine 3D pose estimation by accounting for clothes and other geometric variations. The code is available at https://github.com/prosperolo/GST.",
    "arxiv_url": "http://arxiv.org/abs/2409.04196v2",
    "pdf_url": "http://arxiv.org/pdf/2409.04196v2",
    "published_date": "2024-09-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/prosperolo/GST",
    "keywords": [
      "tracking",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian Geometric Priors",
    "authors": [
      "Yujun Huang",
      "Bin Chen",
      "Niu Lian",
      "Baoyi An",
      "Shu-Tao Xia"
    ],
    "abstract": "Existing multi-view image compression methods often rely on 2D projection-based similarities between views to estimate disparities. While effective for small disparities, such as those in stereo images, these methods struggle with the more complex disparities encountered in wide-baseline multi-camera systems, commonly found in virtual reality and autonomous driving applications. To address this limitation, we propose 3D-LMVIC, a novel learning-based multi-view image compression framework that leverages 3D Gaussian Splatting to derive geometric priors for accurate disparity estimation. Furthermore, we introduce a depth map compression model to minimize geometric redundancy across views, along with a multi-view sequence ordering strategy based on a defined distance measure between views to enhance correlations between adjacent views. Experimental results demonstrate that 3D-LMVIC achieves superior performance compared to both traditional and learning-based methods. Additionally, it significantly improves disparity estimation accuracy over existing two-view approaches.",
    "arxiv_url": "http://arxiv.org/abs/2409.04013v2",
    "pdf_url": "http://arxiv.org/pdf/2409.04013v2",
    "published_date": "2024-09-06",
    "categories": [
      "cs.CV",
      "cs.IT",
      "cs.MM",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model Priors",
    "authors": [
      "Hanyang Yu",
      "Xiaoxiao Long",
      "Ping Tan"
    ],
    "abstract": "We aim to address sparse-view reconstruction of a 3D scene by leveraging priors from large-scale vision models. While recent advancements such as 3D Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D reconstruction, these methods typically necessitate hundreds of input images that densely capture the underlying scene, making them time-consuming and impractical for real-world applications. However, sparse-view reconstruction is inherently ill-posed and under-constrained, often resulting in inferior and incomplete outcomes. This is due to issues such as failed initialization, overfitting on input images, and a lack of details. To mitigate these challenges, we introduce LM-Gaussian, a method capable of generating high-quality reconstructions from a limited number of images. Specifically, we propose a robust initialization module that leverages stereo priors to aid in the recovery of camera poses and the reliable point clouds. Additionally, a diffusion-based refinement is iteratively applied to incorporate image diffusion priors into the Gaussian optimization process to preserve intricate scene details. Finally, we utilize video diffusion priors to further enhance the rendered images for realistic visual effects. Overall, our approach significantly reduces the data acquisition requirements compared to previous 3DGS methods. We validate the effectiveness of our framework through experiments on various public datasets, demonstrating its potential for high-quality 360-degree scene reconstruction. Visual results are on our website.",
    "arxiv_url": "http://arxiv.org/abs/2409.03456v2",
    "pdf_url": "http://arxiv.org/pdf/2409.03456v2",
    "published_date": "2024-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction",
    "authors": [
      "Shen Chen",
      "Jiale Zhou",
      "Lei Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for 3D scene representation, offering a reduction in computational overhead compared to Neural Radiance Fields (NeRF). However, 3DGS is susceptible to high-frequency artifacts and demonstrates suboptimal performance under sparse viewpoint conditions, thereby limiting its applicability in robotics and computer vision. To address these limitations, we introduce SVS-GS, a novel framework for Sparse Viewpoint Scene reconstruction that integrates a 3D Gaussian smoothing filter to suppress artifacts. Furthermore, our approach incorporates a Depth Gradient Profile Prior (DGPP) loss with a dynamic depth mask to sharpen edges and 2D diffusion with Score Distillation Sampling (SDS) loss to enhance geometric consistency in novel view synthesis. Experimental evaluations on the MipNeRF-360 and SeaThru-NeRF datasets demonstrate that SVS-GS markedly improves 3D reconstruction from sparse viewpoints, offering a robust and efficient solution for scene understanding in robotics and computer vision applications.",
    "arxiv_url": "http://arxiv.org/abs/2409.03213v1",
    "pdf_url": "http://arxiv.org/pdf/2409.03213v1",
    "published_date": "2024-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "robotics",
      "efficient",
      "head",
      "understanding",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models",
    "authors": [
      "Zhibin Liu",
      "Haoye Dong",
      "Aviral Chharia",
      "Hefeng Wu"
    ],
    "abstract": "Generating lifelike 3D humans from a single RGB image remains a challenging task in computer vision, as it requires accurate modeling of geometry, high-quality texture, and plausible unseen parts. Existing methods typically use multi-view diffusion models for 3D generation, but they often face inconsistent view issues, which hinder high-quality 3D human generation. To address this, we propose Human-VDM, a novel method for generating 3D human from a single RGB image using Video Diffusion Models. Human-VDM provides temporally consistent views for 3D human generation using Gaussian Splatting. It consists of three modules: a view-consistent human video diffusion module, a video augmentation module, and a Gaussian Splatting module. First, a single image is fed into a human video diffusion module to generate a coherent human video. Next, the video augmentation module applies super-resolution and video interpolation to enhance the textures and geometric smoothness of the generated video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans under the guidance of these high-resolution and view-consistent images. Experiments demonstrate that Human-VDM achieves high-quality 3D human from a single image, outperforming state-of-the-art methods in both generation quality and quantity. Project page: https://human-vdm.github.io/Human-VDM/",
    "arxiv_url": "http://arxiv.org/abs/2409.02851v1",
    "pdf_url": "http://arxiv.org/pdf/2409.02851v1",
    "published_date": "2024-09-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object Gaussian for Monocular 6D Pose Estimation from Sparse Views",
    "authors": [
      "Luqing Luo",
      "Shichu Sun",
      "Jiangang Yang",
      "Linfang Zheng",
      "Jinwei Du",
      "Jian Liu"
    ],
    "abstract": "Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2409.02581v1",
    "pdf_url": "http://arxiv.org/pdf/2409.02581v1",
    "published_date": "2024-09-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "robotics",
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving",
    "authors": [
      "Huasong Han",
      "Kaixuan Zhou",
      "Xiaoxiao Long",
      "Yusen Wang",
      "Chunxia Xiao"
    ],
    "abstract": "We propose GGS, a Generalizable Gaussian Splatting method for Autonomous Driving which can achieve realistic rendering under large viewpoint changes. Previous generalizable 3D gaussian splatting methods are limited to rendering novel views that are very close to the original pair of images, which cannot handle large differences in viewpoint. Especially in autonomous driving scenarios, images are typically collected from a single lane. The limited training perspective makes rendering images of a different lane very challenging. To further improve the rendering capability of GGS under large viewpoint changes, we introduces a novel virtual lane generation module into GSS method to enables high-quality lane switching even without a multi-lane dataset. Besides, we design a diffusion loss to supervise the generation of virtual lane image to further address the problem of lack of data in the virtual lanes. Finally, we also propose a depth refinement module to optimize depth estimation in the GSS model. Extensive validation of our method, compared to existing approaches, demonstrates state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2409.02382v1",
    "pdf_url": "http://arxiv.org/pdf/2409.02382v1",
    "published_date": "2024-09-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction",
    "authors": [
      "Jenny Seidenschwarz",
      "Qunjie Zhou",
      "Bardienus Duisterhof",
      "Deva Ramanan",
      "Laura Leal-Taix√©"
    ],
    "abstract": "Reconstructing scenes and tracking motion are two sides of the same coin. Tracking points allow for geometric reconstruction [14], while geometric reconstruction of (dynamic) scenes allows for 3D tracking of points over time [24, 39]. The latter was recently also exploited for 2D point tracking to overcome occlusion ambiguities by lifting tracking directly into 3D [38]. However, above approaches either require offline processing or multi-view camera setups both unrealistic for real-world applications like robot navigation or mixed reality. We target the challenge of online 2D and 3D point tracking from unposed monocular camera input introducing Dynamic Online Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to reconstruct dynamic scenes in an online fashion. Our approach extends 3D Gaussians to capture new content and object motions while estimating camera movements from a single RGB frame. DynOMo stands out by enabling emergence of point trajectories through robust image feature reconstruction and a novel similarity-enhanced regularization term, without requiring any correspondence-level supervision. It sets the first baseline for online point tracking with monocular unposed cameras, achieving performance on par with existing methods. We aim to inspire the community to advance online point tracking and reconstruction, expanding the applicability to diverse real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2409.02104v2",
    "pdf_url": "http://arxiv.org/pdf/2409.02104v2",
    "published_date": "2024-09-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRoGS: Progressive Rendering of Gaussian Splats",
    "authors": [
      "Brent Zoomers",
      "Maarten Wijnants",
      "Ivan Molenaers",
      "Joni Vanherck",
      "Jeroen Put",
      "Lode Jorissen",
      "Nick Michiels"
    ],
    "abstract": "Over the past year, 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However, it can require a substantial amount of storage since each splat's individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint, they still necessitate retrieving the entire scene before any part of it can be rendered. In this work, we introduce a novel approach for progressively rendering such scenes, aiming to display visible content that closely approximates the final scene as early as possible without loading the entire scene into memory. This approach benefits both on-device rendering applications limited by memory constraints and streaming applications where minimal bandwidth usage is preferred. To achieve this, we approximate the contribution of each Gaussian to the final scene and construct an order of prioritization on their inclusion in the rendering process. Additionally, we demonstrate that our approach can be combined with existing compression methods to progressively render (and stream) 3DGS scenes, optimizing bandwidth usage by focusing on the most important splats within a scene. Overall, our work establishes a foundation for making remotely hosted 3DGS content more quickly accessible to end-users in over-the-top consumption scenarios, with our results showing significant improvements in quality across all metrics compared to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2409.01761v1",
    "pdf_url": "http://arxiv.org/pdf/2409.01761v1",
    "published_date": "2024-09-03",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting",
    "authors": [
      "Zixuan Guo",
      "Yifan Xie",
      "Weijing Xie",
      "Peng Huang",
      "Fei Ma",
      "Fei Richard Yu"
    ],
    "abstract": "Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2409.01581v1",
    "pdf_url": "http://arxiv.org/pdf/2409.01581v1",
    "published_date": "2024-09-03",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "understanding",
      "3d gaussian",
      "human",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Free-DyGS: Camera-Pose-Free Scene Reconstruction for Dynamic Surgical Videos with Gaussian Splatting",
    "authors": [
      "Qian Li",
      "Shuojue Yang",
      "Daiyun Shen",
      "Jimmy Bok Yan So",
      "Jing Qin",
      "Yueming Jin"
    ],
    "abstract": "High-fidelity reconstruction of surgical scene is a fundamentally crucial task to support many applications, such as intra-operative navigation and surgical education. However, most existing methods assume the ideal surgical scenarios - either focus on dynamic reconstruction with deforming tissue yet assuming a given fixed camera pose, or allow endoscope movement yet reconstructing the static scenes. In this paper, we target at a more realistic yet challenging setup - free-pose reconstruction with a moving camera for highly dynamic surgical scenes. Meanwhile, we take the first step to introduce Gaussian Splitting (GS) technique to tackle this challenging setting and propose a novel GS-based framework for fast reconstruction, termed \\textit{Free-DyGS}. Concretely, our model embraces a novel scene initialization in which a pre-trained Sparse Gaussian Regressor (SGR) can efficiently parameterize the initial attributes. For each subsequent frame, we propose to jointly optimize the deformation model and 6D camera poses in a frame-by-frame manner, easing training given the limited deformation differences between consecutive frames. A Scene Expansion scheme is followed to expand the GS model for the unseen regions introduced by the moving camera. Moreover, the framework is equipped with a novel Retrospective Deformation Recapitulation (RDR) strategy to preserve the entire-clip deformations throughout the frame-by-frame training scheme. The efficacy of the proposed Free-DyGS is substantiated through extensive experiments on two datasets: StereoMIS and Hamlyn datasets. The experimental outcomes underscore that Free-DyGS surpasses other advanced methods in both rendering accuracy and efficiency. Code will be available.",
    "arxiv_url": "http://arxiv.org/abs/2409.01003v3",
    "pdf_url": "http://arxiv.org/pdf/2409.01003v3",
    "published_date": "2024-09-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Large-scale Surface Reconstruction from Aerial Images",
    "authors": [
      "YuanZheng Wu",
      "Jin Liu",
      "Shunping Ji"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent ability in small-scale 3D surface reconstruction. However, extending 3DGS to large-scale scenes remains a significant challenge. To address this gap, we propose a novel 3DGS-based method for large-scale surface reconstruction using aerial multi-view stereo (MVS) images, named Aerial Gaussian Splatting (AGS). First, we introduce a data chunking method tailored for large-scale aerial images, making 3DGS feasible for surface reconstruction over extensive scenes. Second, we integrate the Ray-Gaussian Intersection method into 3DGS to obtain depth and normal information. Finally, we implement multi-view geometric consistency constraints to enhance the geometric consistency across different views. Our experiments on multiple datasets demonstrate, for the first time, the 3DGS-based method can match conventional aerial MVS methods on geometric accuracy in aerial large-scale surface reconstruction, and our method also beats state-of-the-art GS-based methods both on geometry and rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2409.00381v3",
    "pdf_url": "http://arxiv.org/pdf/2409.00381v3",
    "published_date": "2024-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM",
    "authors": [
      "Mostafa Mansour",
      "Ahmed Abdelsalam",
      "Ari Happonen",
      "Jari Porras",
      "Esa Rahtu"
    ],
    "abstract": "Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM. This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.",
    "arxiv_url": "http://arxiv.org/abs/2409.00362v2",
    "pdf_url": "http://arxiv.org/pdf/2409.00362v2",
    "published_date": "2024-08-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense Mapping",
    "authors": [
      "Meng Wang",
      "Junyi Wang",
      "Changqun Xia",
      "Chen Wang",
      "Yue Qi"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has recently demonstrated promising advancements in RGB-D online dense mapping. Nevertheless, existing methods excessively rely on per-pixel depth cues to perform map densification, which leads to significant redundancy and increased sensitivity to depth noise. Additionally, explicitly storing 3D Gaussian parameters of room-scale scene poses a significant storage challenge. In this paper, we introduce OG-Mapping, which leverages the robust scene structural representation capability of sparse octrees, combined with structured 3D Gaussian representations, to achieve efficient and robust online dense mapping. Moreover, OG-Mapping employs an anchor-based progressive map refinement strategy to recover the scene structures at multiple levels of detail. Instead of maintaining a small number of active keyframes with a fixed keyframe window as previous approaches do, a dynamic keyframe window is employed to allow OG-Mapping to better tackle false local minima and forgetting issues. Experimental results demonstrate that OG-Mapping delivers more robust and superior realism mapping results than existing Gaussian-based RGB-D online mapping methods with a compact model, and no additional post-processing is required.",
    "arxiv_url": "http://arxiv.org/abs/2408.17223v1",
    "pdf_url": "http://arxiv.org/pdf/2408.17223v1",
    "published_date": "2024-08-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "mapping",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and Better Geometry Reconstruction",
    "authors": [
      "Ruihan Yu",
      "Tianyu Huang",
      "Jingwang Ling",
      "Feng Xu"
    ],
    "abstract": "2D Gaussian Splatting has recently emerged as a significant method in 3D reconstruction, enabling novel view synthesis and geometry reconstruction simultaneously. While the well-known Gaussian kernel is broadly used, its lack of anisotropy and deformation ability leads to dim and vague edges at object silhouettes, limiting the reconstruction quality of current Gaussian splatting methods. To enhance the representation power, we draw inspiration from quantum physics and propose to use the Gaussian-Hermite kernel as the new primitive in Gaussian splatting. The new kernel takes a unified mathematical form and extends the Gaussian function, which serves as the zero-rank term in the updated formulation. Our experiments demonstrate the extraordinary performance of Gaussian-Hermite kernel in both geometry reconstruction and novel-view synthesis tasks. The proposed kernel outperforms traditional Gaussian Splatting kernels, showcasing its potential for high-quality 3D reconstruction and rendering.",
    "arxiv_url": "http://arxiv.org/abs/2408.16982v1",
    "pdf_url": "http://arxiv.org/pdf/2408.16982v1",
    "published_date": "2024-08-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "geometry",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
    "authors": [
      "Fangfu Liu",
      "Wenqiang Sun",
      "Hanyang Wang",
      "Yikai Wang",
      "Haowen Sun",
      "Junliang Ye",
      "Jun Zhang",
      "Yueqi Duan"
    ],
    "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.",
    "arxiv_url": "http://arxiv.org/abs/2408.16767v2",
    "pdf_url": "http://arxiv.org/pdf/2408.16767v2",
    "published_date": "2024-08-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniRe: Omni Urban Scene Reconstruction",
    "authors": [
      "Ziyu Chen",
      "Jiawei Yang",
      "Jiahui Huang",
      "Riccardo de Lutio",
      "Janick Martinez Esturo",
      "Boris Ivanovic",
      "Or Litany",
      "Zan Gojcic",
      "Sanja Fidler",
      "Marco Pavone",
      "Li Song",
      "Yue Wang"
    ],
    "abstract": "We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes.",
    "arxiv_url": "http://arxiv.org/abs/2408.16760v2",
    "pdf_url": "http://arxiv.org/pdf/2408.16760v2",
    "published_date": "2024-08-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "human",
      "ar",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generic Objects as Pose Probes for Few-shot View Synthesis",
    "authors": [
      "Zhirui Gao",
      "Renjiao Yi",
      "Chenyang Zhu",
      "Ke Zhuang",
      "Wei Chen",
      "Kai Xu"
    ],
    "abstract": "Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as \"pose probes\". The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this https URL}",
    "arxiv_url": "http://arxiv.org/abs/2408.16690v4",
    "pdf_url": "http://arxiv.org/pdf/2408.16690v4",
    "published_date": "2024-08-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "few-shot",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching",
    "authors": [
      "Xinyu Gao",
      "Ziyi Yang",
      "Bingchen Gong",
      "Xiaoguang Han",
      "Sipeng Yang",
      "Xiaogang Jin"
    ],
    "abstract": "Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradient-based strategy and grid-based representation. To this end, we present an example-based modeling method that combines multiple Gaussian fields in a point-based representation using sample-guided synthesis. Specifically, as for composition, we create a GUI to segment and transform multiple fields in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workflow consists of three steps: 1) real-time segmentation and transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and gradient constraints. Extensive experimental results validate that our approach significantly outperforms previous works in terms of realistic synthesis, demonstrating its practicality. More demos are available at https://ingra14m.github.io/gs_stitching_website.",
    "arxiv_url": "http://arxiv.org/abs/2408.15708v1",
    "pdf_url": "http://arxiv.org/pdf/2408.15708v1",
    "published_date": "2024-08-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G-Style: Stylized Gaussian Splatting",
    "authors": [
      "√Åron Samuel Kov√°cs",
      "Pedro Hermosilla",
      "Renata G. Raidou"
    ],
    "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting is a powerful 3D representation for novel view synthesis, as -- compared to other approaches based on Neural Radiance Fields -- it provides fast scene renderings and user control over the scene. Recent pre-prints have demonstrated that the style of Gaussian Splatting scenes can be modified using an image exemplar. However, since the scene geometry remains fixed during the stylization process, current solutions fall short of producing satisfactory results. Our algorithm aims to address these limitations by following a three-step process: In a pre-processing step, we remove undesirable Gaussians with large projection areas or highly elongated shapes. Subsequently, we combine several losses carefully designed to preserve different scales of the style in the image, while maintaining as much as possible the integrity of the original scene content. During the stylization process and following the original design of Gaussian Splatting, we split Gaussians where additional detail is necessary within our scene by tracking the gradient of the stylized color. Our experiments demonstrate that G-Style generates high-quality stylizations within just a few minutes, outperforming existing methods both qualitatively and quantitatively.",
    "arxiv_url": "http://arxiv.org/abs/2408.15695v2",
    "pdf_url": "http://arxiv.org/pdf/2408.15695v2",
    "published_date": "2024-08-28",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty",
    "authors": [
      "Saining Zhang",
      "Baijun Ye",
      "Xiaoxue Chen",
      "Yuantao Chen",
      "Zongzheng Zhang",
      "Cheng Peng",
      "Yongliang Shi",
      "Hao Zhao"
    ],
    "abstract": "Robust and realistic rendering for large-scale road scenes is essential in autonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made groundbreaking progress in neural rendering, but the general fidelity of large-scale road scene renderings is often limited by the input imagery, which usually has a narrow field of view and focuses mainly on the street-level local area. Intuitively, the data from the drone's perspective can provide a complementary viewpoint for the data from the ground vehicle's perspective, enhancing the completeness of scene reconstruction and rendering. However, training naively with aerial and ground images, which exhibit large view disparity, poses a significant convergence challenge for 3D-GS, and does not demonstrate remarkable improvements in performance on road views. In order to enhance the novel view synthesis of road views and to effectively use the aerial information, we design an uncertainty-aware training method that allows aerial images to assist in the synthesis of areas where ground images have poor learning outcomes instead of weighting all pixels equally in 3D-GS training like prior work did. We are the first to introduce the cross-view uncertainty to 3D-GS by matching the car-view ensemble-based rendering uncertainty to aerial images, weighting the contribution of each pixel to the training process. Additionally, to systematically quantify evaluation metrics, we assemble a high-quality synthesized dataset comprising both aerial and ground images for road scenes.",
    "arxiv_url": "http://arxiv.org/abs/2408.15242v1",
    "pdf_url": "http://arxiv.org/pdf/2408.15242v1",
    "published_date": "2024-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning-based Multi-View Stereo: A Survey",
    "authors": [
      "Fangjinhua Wang",
      "Qingtian Zhu",
      "Di Chang",
      "Quankai Gao",
      "Junlin Han",
      "Tong Zhang",
      "Richard Hartley",
      "Marc Pollefeys"
    ],
    "abstract": "3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented/Virtual Reality (AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area.",
    "arxiv_url": "http://arxiv.org/abs/2408.15235v2",
    "pdf_url": "http://arxiv.org/pdf/2408.15235v2",
    "published_date": "2024-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "vr",
      "autonomous driving",
      "survey",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation",
    "authors": [
      "Haozhe Lou",
      "Yurong Liu",
      "Yike Pan",
      "Yiran Geng",
      "Jianteng Chen",
      "Wenlong Ma",
      "Chenglong Li",
      "Lin Wang",
      "Hengzhen Feng",
      "Lu Shi",
      "Liyi Luo",
      "Yongliang Shi"
    ],
    "abstract": "Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes.   We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms.   This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods.   The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com",
    "arxiv_url": "http://arxiv.org/abs/2408.14873v2",
    "pdf_url": "http://arxiv.org/pdf/2408.14873v2",
    "published_date": "2024-08-27",
    "categories": [
      "cs.RO",
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming",
    "authors": [
      "Yuang Shi",
      "G√©raldine Morin",
      "Simone Gasparini",
      "Wei Tsang Ooi"
    ],
    "abstract": "The rise of Extended Reality (XR) requires efficient streaming of 3D online worlds, challenging current 3DGS representations to adapt to bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS that supports adaptive streaming and progressive rendering. Our method constructs a layered structure for cumulative representation, incorporates dynamic opacity optimization to maintain visual fidelity, and utilizes occupancy maps to efficiently manage Gaussian splats. This proposed model offers a progressive representation supporting a continuous rendering quality adapted for bandwidth-aware streaming. Extensive experiments validate the effectiveness of our approach in balancing visual fidelity with the compactness of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in LPIPS with 23% of the original model size, and shows its potential for bandwidth-adapted 3D streaming and rendering applications.",
    "arxiv_url": "http://arxiv.org/abs/2408.14823v2",
    "pdf_url": "http://arxiv.org/pdf/2408.14823v2",
    "published_date": "2024-08-27",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Avatar Concept Slider: Controllable Editing of Concepts in 3D Human Avatars",
    "authors": [
      "Lin Geng Foo",
      "Yixuan He",
      "Ajmal Saeed Mian",
      "Hossein Rahmani",
      "Jun Liu",
      "Christian Theobalt"
    ],
    "abstract": "Text-based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise editing of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs: Firstly, a Concept Sliding Loss based on linear discriminant analysis to pinpoint the concept-specific axes for precise editing. Secondly, an Attribute Preserving Loss based on principal component analysis for improved preservation of avatar identity during editing. We further propose a 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables controllable 3D avatar editing, without compromising the avatar quality or its identifying attributes.",
    "arxiv_url": "http://arxiv.org/abs/2408.13995v3",
    "pdf_url": "http://arxiv.org/pdf/2408.13995v3",
    "published_date": "2024-08-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian Splatting",
    "authors": [
      "Weiwei Cai",
      "Weicai Ye",
      "Peng Ye",
      "Tong He",
      "Tao Chen"
    ],
    "abstract": "Dynamic scene reconstruction has garnered significant attention in recent years due to its capabilities in high-quality and real-time rendering. Among various methodologies, constructing a 4D spatial-temporal representation, such as 4D-GS, has gained popularity for its high-quality rendered images. However, these methods often produce suboptimal surfaces, as the discrete 3D Gaussian point clouds fail to align with the object's surface precisely. To address this problem, we propose DynaSurfGS to achieve both photorealistic rendering and high-fidelity surface reconstruction of dynamic scenarios. Specifically, the DynaSurfGS framework first incorporates Gaussian features from 4D neural voxels with the planar-based Gaussian Splatting to facilitate precise surface reconstruction. It leverages normal regularization to enforce the smoothness of the surface of dynamic objects. It also incorporates the as-rigid-as-possible (ARAP) constraint to maintain the approximate rigidity of local neighborhoods of 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain closely aligned throughout. Extensive experiments demonstrate that DynaSurfGS surpasses state-of-the-art methods in both high-fidelity surface reconstruction and photorealistic rendering.",
    "arxiv_url": "http://arxiv.org/abs/2408.13972v1",
    "pdf_url": "http://arxiv.org/pdf/2408.13972v1",
    "published_date": "2024-08-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "real-time rendering",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs",
    "authors": [
      "Brandon Smart",
      "Chuanxia Zheng",
      "Iro Laina",
      "Victor Adrian Prisacariu"
    ],
    "abstract": "In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.",
    "arxiv_url": "http://arxiv.org/abs/2408.13912v2",
    "pdf_url": "http://arxiv.org/pdf/2408.13912v2",
    "published_date": "2024-08-25",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DESI Peculiar Velocity Survey -- Fundamental Plane",
    "authors": [
      "Khaled Said",
      "Cullan Howlett",
      "Tamara Davis",
      "John Lucey",
      "Christoph Saulder",
      "Kelly Douglass",
      "Alex G. Kim",
      "Anthony Kremin",
      "Caitlin Ross",
      "Greg Aldering",
      "Jessica Nicole Aguilar",
      "Steven Ahlen",
      "Segev BenZvi",
      "Davide Bianchi",
      "David Brooks",
      "Todd Claybaugh",
      "Kyle Dawson",
      "Axel de la Macorra",
      "Biprateep Dey",
      "Peter Doel",
      "Kevin Fanning",
      "Simone Ferraro",
      "Andreu Font-Ribera",
      "Jaime E. Forero-Romero",
      "Enrique Gazta√±aga",
      "Satya Gontcho A Gontcho",
      "Julien Guy",
      "Klaus Honscheid",
      "Robert Kehoe",
      "Theodore Kisner",
      "Andrew Lambert",
      "Martin Landriau",
      "Laurent Le Guillou",
      "Marc Manera",
      "Aaron Meisner",
      "Ramon Miquel",
      "John Moustakas",
      "Andrea Mu√±oz-Guti√©rrez",
      "Adam Myers",
      "Jundan Nie",
      "Nathalie Palanque-Delabrouille",
      "Will Percival",
      "Francisco Prada",
      "Graziano Rossi",
      "Eusebio Sanchez",
      "David Schlegel",
      "Michael Schubnell",
      "Joseph Harry Silber",
      "David Sprayberry",
      "Gregory Tarl√©",
      "Mariana Vargas Magana",
      "Benjamin Alan Weaver",
      "Risa Wechsler",
      "Zhimin Zhou",
      "Hu Zou"
    ],
    "abstract": "The Dark Energy Spectroscopic Instrument (DESI) Peculiar Velocity Survey aims to measure the peculiar velocities of early and late type galaxies within the DESI footprint using both the Fundamental Plane and optical Tully-Fisher relations. Direct measurements of peculiar velocities can significantly improve constraints on the growth rate of structure, reducing uncertainty by a factor of approximately 2.5 at redshift 0.1 compared to the DESI Bright Galaxy Survey's redshift space distortion measurements alone. We assess the quality of stellar velocity dispersion measurements from DESI spectroscopic data. These measurements, along with photometric data from the Legacy Survey, establish the Fundamental Plane relation and determine distances and peculiar velocities of early-type galaxies. During Survey Validation, we obtain spectra for 6698 unique early-type galaxies, up to a photometric redshift of 0.15. 64\\% of observed galaxies (4267) have relative velocity dispersion errors below 10\\%. This percentage increases to 75\\% if we restrict our sample to galaxies with spectroscopic redshifts below 0.1. We use the measured central velocity dispersion, along with photometry from the DESI Legacy Imaging Surveys, to fit the Fundamental Plane parameters using a 3D Gaussian maximum likelihood algorithm that accounts for measurement uncertainties and selection cuts. In addition, we conduct zero-point calibration using the absolute distance measurements to the Coma cluster, leading to a value of the Hubble constant, $H_0 = 76.05 \\pm 0.35$(statistical) $\\pm 0.49$(systematic FP) $\\pm 4.86$(statistical due to calibration) $\\mathrm{km \\ s^{-1} Mpc^{-1}}$. This $H_0$ value is within $2\\sigma$ of Planck Cosmic Microwave Background results and within $1\\sigma$, of other low redshift distance indicator-based measurements.",
    "arxiv_url": "http://arxiv.org/abs/2408.13842v2",
    "pdf_url": "http://arxiv.org/pdf/2408.13842v2",
    "published_date": "2024-08-25",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers",
    "authors": [
      "Chuanrui Zhang",
      "Yingshuang Zou",
      "Zhuoling Li",
      "Minmin Yi",
      "Haoqian Wang"
    ],
    "abstract": "Compared with previous 3D reconstruction methods like Nerf, recent Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even in the sparse-view setting. However, the promising reconstruction performance of existing G-3DGS methods relies heavily on accurate multi-view feature matching, which is quite challenging. Especially for the scenes that have many non-overlapping areas between various views and contain numerous similar regions, the matching performance of existing methods is poor and the reconstruction precision is limited. To address this problem, we develop a strategy that utilizes a predicted depth confidence map to guide accurate local feature matching. In addition, we propose to utilize the knowledge of existing monocular depth estimation models as prior to boost the depth estimation precision in non-overlapping areas between views. Combining the proposed strategies, we present a novel G-3DGS method named TranSplat, which obtains the best performance on both the RealEstate10K and ACID benchmarks while maintaining competitive speed and presenting strong cross-dataset generalization ability. Our code, and demos will be available at: https://xingyoujun.github.io/transplat.",
    "arxiv_url": "http://arxiv.org/abs/2408.13770v1",
    "pdf_url": "http://arxiv.org/pdf/2408.13770v1",
    "published_date": "2024-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with Panoramic Gaussian Splatting",
    "authors": [
      "Wenrui Li",
      "Fucheng Cai",
      "Yapeng Mi",
      "Zhe Yang",
      "Wangmeng Zuo",
      "Xingtao Wang",
      "Xiaopeng Fan"
    ],
    "abstract": "Text-driven 3D scene generation has seen significant advancements recently. However, most existing methods generate single-view images using generative models and then stitch them together in 3D space. This independent generation for each view often results in spatial inconsistency and implausibility in the 3D scenes. To address this challenge, we proposed a novel text-driven 3D-consistent scene generation model: SceneDreamer360. Our proposed method leverages a text-driven panoramic image generation model as a prior for 3D scene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency across multi-view panoramic images. Specifically, SceneDreamer360 enhances the fine-tuned Panfusion generator with a three-stage panoramic enhancement, enabling the generation of high-resolution, detail-rich panoramic images. During the 3D scene construction, a novel point cloud fusion initialization method is used, producing higher quality and spatially consistent point clouds. Our extensive experiments demonstrate that compared to other methods, SceneDreamer360 with its panoramic image generation and 3DGS can produce higher quality, spatially consistent, and visually appealing 3D scenes from any text prompt. Our codes are available at \\url{https://github.com/liwrui/SceneDreamer360}.",
    "arxiv_url": "http://arxiv.org/abs/2408.13711v2",
    "pdf_url": "http://arxiv.org/pdf/2408.13711v2",
    "published_date": "2024-08-25",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/liwrui/SceneDreamer360",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian Splatting",
    "authors": [
      "Zhenyuan Liu",
      "Yu Guo",
      "Xinyuan Li",
      "Bernd Bickel",
      "Ran Zhang"
    ],
    "abstract": "We present Bidirectional Gaussian Primitives, an image-based novel view synthesis technique designed to represent and render 3D objects with surface and volumetric materials under dynamic illumination. Our approach integrates light intrinsic decomposition into the Gaussian splatting framework, enabling real-time relighting of 3D objects. To unify surface and volumetric material within a cohesive appearance model, we adopt a light- and view-dependent scattering representation via bidirectional spherical harmonics. Our model does not use a specific surface normal-related reflectance function, making it more compatible with volumetric representations like Gaussian splatting, where the normals are undefined. We demonstrate our method by reconstructing and rendering objects with complex materials. Using One-Light-At-a-Time (OLAT) data as input, we can reproduce photorealistic appearances under novel lighting conditions in real time.",
    "arxiv_url": "http://arxiv.org/abs/2408.13370v1",
    "pdf_url": "http://arxiv.org/pdf/2408.13370v1",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "illumination",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation",
    "authors": [
      "Shuai Yang",
      "Jing Tan",
      "Mengchen Zhang",
      "Tong Wu",
      "Yixuan Li",
      "Gordon Wetzstein",
      "Ziwei Liu",
      "Dahua Lin"
    ],
    "abstract": "3D immersive scene generation is a challenging yet critical task in computer vision and graphics. A desired virtual 3D scene should 1) exhibit omnidirectional view consistency, and 2) allow for free exploration in complex scene hierarchies. Existing methods either rely on successive scene expansion via inpainting or employ panorama representation to represent large FOV scene environments. However, the generated scene suffers from semantic drift during expansion and is unable to handle occlusion among scene hierarchies. To tackle these challenges, we introduce Layerpano3D, a novel framework for full-view, explorable panoramic 3D scene generation from a single text prompt. Our key insight is to decompose a reference 2D panorama into multiple layers at different depth levels, where each layer reveals the unseen space from the reference views via diffusion prior. Layerpano3D comprises multiple dedicated designs: 1) We introduce a new panorama dataset Upright360, comprising 9k high-quality and upright panorama images, and finetune the advanced Flux model on Upright360 for high-quality, upright and consistent panorama generation. 2) We pioneer the Layered 3D Panorama as underlying representation to manage complex scene hierarchies and lift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes with unconstrained viewing paths. Extensive experiments demonstrate that our framework generates state-of-the-art 3D panoramic scene in both full view consistency and immersive exploratory experience. We believe that Layerpano3D holds promise for advancing 3D panoramic scene creation with numerous applications.",
    "arxiv_url": "http://arxiv.org/abs/2408.13252v2",
    "pdf_url": "http://arxiv.org/pdf/2408.13252v2",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpecGaussian with Latent Features: A High-quality Modeling of the View-dependent Appearance for 3D Gaussian Splatting",
    "authors": [
      "Zhiru Wang",
      "Shiyun Xie",
      "Chengwei Pan",
      "Guoping Wang"
    ],
    "abstract": "Recently, the 3D Gaussian Splatting (3D-GS) method has achieved great success in novel view synthesis, providing real-time rendering while ensuring high-quality rendering results. However, this method faces challenges in modeling specular reflections and handling anisotropic appearance components, especially in dealing with view-dependent color under complex lighting conditions. Additionally, 3D-GS uses spherical harmonic to learn the color representation, which has limited ability to represent complex scenes. To overcome these challenges, we introduce Lantent-SpecGS, an approach that utilizes a universal latent neural descriptor within each 3D Gaussian. This enables a more effective representation of 3D feature fields, including appearance and geometry. Moreover, two parallel CNNs are designed to decoder the splatting feature maps into diffuse color and specular color separately. A mask that depends on the viewpoint is learned to merge these two colors, resulting in the final rendered image. Experimental results demonstrate that our method obtains competitive performance in novel view synthesis and extends the ability of 3D-GS to handle intricate scenarios with specular reflections.",
    "arxiv_url": "http://arxiv.org/abs/2409.05868v1",
    "pdf_url": "http://arxiv.org/pdf/2409.05868v1",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Atlas Gaussians Diffusion for 3D Generation",
    "authors": [
      "Haitao Yang",
      "Yuan Dong",
      "Hanwen Jiang",
      "Dejia Xu",
      "Georgios Pavlakos",
      "Qixing Huang"
    ],
    "abstract": "Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation. Project page: https://yanghtr.github.io/projects/atlas_gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2408.13055v3",
    "pdf_url": "http://arxiv.org/pdf/2408.13055v3",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "high-fidelity",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points",
    "authors": [
      "Bing He",
      "Yunuo Chen",
      "Guo Lu",
      "Qi Wang",
      "Qunshan Gu",
      "Rong Xie",
      "Li Song",
      "Wenjun Zhang"
    ],
    "abstract": "Dynamic scene reconstruction using Gaussians has recently attracted increased interest. Mainstream approaches typically employ a global deformation field to warp a 3D scene in canonical space. However, the inherent low-frequency nature of implicit neural fields often leads to ineffective representations of complex motions. Moreover, their structural rigidity can hinder adaptation to scenes with varying resolutions and durations. To address these challenges, we introduce a novel approach for streaming 4D real-world reconstruction utilizing discrete 3D control points. This method physically models local rays and establishes a motion-decoupling coordinate system. By effectively merging traditional graphics with learnable pipelines, it provides a robust and efficient local 6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have developed a generalized framework that integrates our control points with Gaussians. Starting from an initial 3D reconstruction, our workflow decomposes the streaming 4D reconstruction into four independent submodules: 3D segmentation, 3D control point generation, object-wise motion manipulation, and residual compensation. Experimental results demonstrate that our method outperforms existing state-of-the-art 4D Gaussian splatting techniques on both the Neu3DV and CMU-Panoptic datasets. Notably, the optimization of our 3D control points is achievable in 100 iterations and within just 2 seconds per frame on a single NVIDIA 4070 GPU.",
    "arxiv_url": "http://arxiv.org/abs/2408.13036v2",
    "pdf_url": "http://arxiv.org/pdf/2408.13036v2",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "segmentation",
      "4d",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering",
    "authors": [
      "Yunji Seo",
      "Young Sun Choi",
      "Hyun Seung Son",
      "Youngjung Uh"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) and its subsequent works are restricted to specific hardware setups, either on only low-cost or on only high-end configurations. Approaches aimed at reducing 3DGS memory usage enable rendering on low-cost GPU but compromise rendering quality, which fails to leverage the hardware capabilities in the case of higher-end GPU. Conversely, methods that enhance rendering quality require high-end GPU with large VRAM, making such methods impractical for lower-end devices with limited memory capacity. Consequently, 3DGS-based works generally assume a single hardware setup and lack the flexibility to adapt to varying hardware constraints.   To overcome this limitation, we propose Flexible Level of Detail (FLoD) for 3DGS. FLoD constructs a multi-level 3DGS representation through level-specific 3D scale constraints, where each level independently reconstructs the entire scene with varying detail and GPU memory usage. A level-by-level training strategy is introduced to ensure structural consistency across levels. Furthermore, the multi-level structure of FLoD allows selective rendering of image regions at different detail levels, providing additional memory-efficient rendering options. To our knowledge, among prior works which incorporate the concept of Level of Detail (LoD) with 3DGS, FLoD is the first to follow the core principle of LoD by offering adjustable options for a broad range of GPU settings.   Experiments demonstrate that FLoD provides various rendering options with trade-offs between quality and memory usage, enabling real-time rendering under diverse memory constraints. Furthermore, we show that FLoD generalizes to different 3DGS frameworks, indicating its potential for integration into future state-of-the-art developments.",
    "arxiv_url": "http://arxiv.org/abs/2408.12894v2",
    "pdf_url": "http://arxiv.org/pdf/2408.12894v2",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV",
      "68U05 (Primary) 68T45 (Secondary)",
      "I.3.3; I.3.7; I.3.5"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "efficient rendering",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF Fusion",
    "authors": [
      "Jiaxin Wei",
      "Stefan Leutenegger"
    ],
    "abstract": "Traditional volumetric fusion algorithms preserve the spatial structure of 3D scenes, which is beneficial for many tasks in computer vision and robotics. However, they often lack realism in terms of visualization. Emerging 3D Gaussian splatting bridges this gap, but existing Gaussian-based reconstruction methods often suffer from artifacts and inconsistencies with the underlying 3D structure, and struggle with real-time optimization, unable to provide users with immediate feedback in high quality. One of the bottlenecks arises from the massive amount of Gaussian parameters that need to be updated during optimization. Instead of using 3D Gaussian as a standalone map representation, we incorporate it into a volumetric mapping system to take advantage of geometric information and propose to use a quadtree data structure on images to drastically reduce the number of splats initialized. In this way, we simultaneously generate a compact 3D Gaussian map with fewer artifacts and a volumetric map on the fly. Our method, GSFusion, significantly enhances computational efficiency without sacrificing rendering quality, as demonstrated on both synthetic and real datasets. Code will be available at https://github.com/goldoak/GSFusion.",
    "arxiv_url": "http://arxiv.org/abs/2408.12677v3",
    "pdf_url": "http://arxiv.org/pdf/2408.12677v3",
    "published_date": "2024-08-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/goldoak/GSFusion",
    "keywords": [
      "robotics",
      "mapping",
      "high quality",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Subsurface Scattering for 3D Gaussian Splatting",
    "authors": [
      "Jan-Niklas Dihlmann",
      "Arjun Majumdar",
      "Andreas Engelhardt",
      "Raphael Braun",
      "Hendrik P. A. Lensch"
    ],
    "abstract": "3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting and novel view synthesis at interactive rates. We show successful application on synthetic data and introduce a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes. Project page https://sss.jdihlmann.com/",
    "arxiv_url": "http://arxiv.org/abs/2408.12282v2",
    "pdf_url": "http://arxiv.org/pdf/2408.12282v2",
    "published_date": "2024-08-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relighting",
      "lighting",
      "shadow",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "light transport",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of Distractors",
    "authors": [
      "Paul Ungermann",
      "Armin Ettenhofer",
      "Matthias Nie√üner",
      "Barbara Roessle"
    ],
    "abstract": "3D Gaussian Splatting has shown impressive novel view synthesis results; nonetheless, it is vulnerable to dynamic objects polluting the input data of an otherwise static scene, so called distractors. Distractors have severe impact on the rendering quality as they get represented as view-dependent effects or result in floating artifacts. Our goal is to identify and ignore such distractors during the 3D Gaussian optimization to obtain a clean reconstruction. To this end, we take a self-supervised approach that looks at the image residuals during the optimization to determine areas that have likely been falsified by a distractor. In addition, we leverage a pretrained segmentation network to provide object awareness, enabling more accurate exclusion of distractors. This way, we obtain segmentation masks of distractors to effectively ignore them in the loss formulation. We demonstrate that our approach is robust to various distractors and strongly improves rendering quality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2408.11697v1",
    "pdf_url": "http://arxiv.org/pdf/2408.11697v1",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy Environments",
    "authors": [
      "Shuhong Liu",
      "Xiang Chen",
      "Hongming Chen",
      "Quanfeng Xu",
      "Mingrui Li"
    ],
    "abstract": "Reconstruction under adverse rainy conditions poses significant challenges due to reduced visibility and the distortion of visual perception. These conditions can severely impair the quality of geometric maps, which is essential for applications ranging from autonomous planning to environmental monitoring. In response to these challenges, this study introduces the novel task of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed to address the complexities of reconstructing 3D scenes under rainy conditions. To benchmark this task, we construct the HydroViews dataset that comprises a diverse collection of both synthesized and real-world scene images characterized by various intensities of rain streaks and raindrops. Furthermore, we propose DeRainGS, the first 3DGS method tailored for reconstruction in adverse rainy environments. Extensive experiments across a wide range of rain scenarios demonstrate that our method delivers state-of-the-art performance, remarkably outperforming existing occlusion-free methods.",
    "arxiv_url": "http://arxiv.org/abs/2408.11540v4",
    "pdf_url": "http://arxiv.org/pdf/2408.11540v4",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting",
    "authors": [
      "Wanshui Gan",
      "Fang Liu",
      "Hongbin Xu",
      "Ningkai Mo",
      "Naoto Yokoya"
    ],
    "abstract": "We introduce GaussianOcc, a systematic method that investigates the two usages of Gaussian splatting for fully self-supervised and efficient 3D occupancy estimation in surround views. First, traditional methods for self-supervised 3D occupancy estimation still require ground truth 6D poses from sensors during training. To address this limitation, we propose Gaussian Splatting for Projection (GSP) module to provide accurate scale information for fully self-supervised training from adjacent view projection. Additionally, existing methods rely on volume rendering for final 3D voxel representation learning using 2D signals (depth maps, semantic maps), which is both time-consuming and less effective. We propose Gaussian Splatting from Voxel space (GSV) to leverage the fast rendering properties of Gaussian splatting. As a result, the proposed GaussianOcc method enables fully self-supervised (no ground truth pose) 3D occupancy estimation in competitive performance with low computational cost (2.7 times faster in training and 5 times faster in rendering). The relevant code is available in https://github.com/GANWANSHUI/GaussianOcc.git.",
    "arxiv_url": "http://arxiv.org/abs/2408.11447v3",
    "pdf_url": "http://arxiv.org/pdf/2408.11447v3",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/GANWANSHUI/GaussianOcc.git",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pano2Room: Novel View Synthesis from a Single Indoor Panorama",
    "authors": [
      "Guo Pu",
      "Yiming Zhao",
      "Zhouhui Lian"
    ],
    "abstract": "Recent single-view 3D generative methods have made significant advancements by leveraging knowledge distilled from extensive 3D object datasets. However, challenges persist in the synthesis of 3D scenes from a single view, primarily due to the complexity of real-world environments and the limited availability of high-quality prior resources. In this paper, we introduce a novel approach called Pano2Room, designed to automatically reconstruct high-quality 3D indoor scenes from a single panoramic image. These panoramic images can be easily generated using a panoramic RGBD inpainter from captures at a single location with any camera. The key idea is to initially construct a preliminary mesh from the input panorama, and iteratively refine this mesh using a panoramic RGBD inpainter while collecting photo-realistic 3D-consistent pseudo novel views. Finally, the refined mesh is converted into a 3D Gaussian Splatting field and trained with the collected pseudo novel views. This pipeline enables the reconstruction of real-world 3D scenes, even in the presence of large occlusions, and facilitates the synthesis of photo-realistic novel views with detailed geometry. Extensive qualitative and quantitative experiments have been conducted to validate the superiority of our method in single-panorama indoor novel synthesis compared to the state-of-the-art. Our code and data are available at \\url{https://github.com/TrickyGo/Pano2Room}.",
    "arxiv_url": "http://arxiv.org/abs/2408.11413v2",
    "pdf_url": "http://arxiv.org/pdf/2408.11413v2",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/TrickyGo/Pano2Room",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting",
    "authors": [
      "Changkun Liu",
      "Shuai Chen",
      "Yash Bhalgat",
      "Siyan Hu",
      "Ming Cheng",
      "Zirui Wang",
      "Victor Adrian Prisacariu",
      "Tristan Braud"
    ],
    "abstract": "We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model renders high-quality synthetic images and depth maps to facilitate the establishment of 2D-3D correspondences. GS-CPR obviates the need for training feature extractors or descriptors by operating directly on RGB images, utilizing the 3D foundation model, MASt3R, for precise 2D matching. To improve the robustness of our model in challenging outdoor environments, we incorporate an exposure-adaptive module within the 3DGS framework. Consequently, GS-CPR enables efficient one-shot pose refinement given a single RGB query and a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based optimization methods in both accuracy and runtime across indoor and outdoor visual localization benchmarks, achieving new state-of-the-art accuracy on two indoor datasets. The project page is available at https://xrim-lab.github.io/GS-CPR/.",
    "arxiv_url": "http://arxiv.org/abs/2408.11085v4",
    "pdf_url": "http://arxiv.org/pdf/2408.11085v4",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "outdoor",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Point-to-Gaussian Model for Image-to-3D Generation",
    "authors": [
      "Longfei Lu",
      "Huachen Gao",
      "Tao Dai",
      "Yaohua Zha",
      "Zhi Hou",
      "Junta Wu",
      "Shu-Tao Xia"
    ],
    "abstract": "Recently, image-to-3D approaches have significantly advanced the generation quality and speed of 3D assets based on large reconstruction models, particularly 3D Gaussian reconstruction models. Existing large 3D Gaussian models directly map 2D image to 3D Gaussian parameters, while regressing 2D image to 3D Gaussian representations is challenging without 3D priors. In this paper, we propose a large Point-to-Gaussian model, that inputs the initial point cloud produced from large 3D diffusion model conditional on 2D image to generate the Gaussian parameters, for image-to-3D generation. The point cloud provides initial 3D geometry prior for Gaussian generation, thus significantly facilitating image-to-3D Generation. Moreover, we present the \\textbf{A}ttention mechanism, \\textbf{P}rojection mechanism, and \\textbf{P}oint feature extractor, dubbed as \\textbf{APP} block, for fusing the image features with point cloud features. The qualitative and quantitative experiments extensively demonstrate the effectiveness of the proposed approach on GSO and Objaverse datasets, and show the proposed method achieves state-of-the-art performance.",
    "arxiv_url": "http://arxiv.org/abs/2408.10935v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10935v1",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining",
    "authors": [
      "Qi Ma",
      "Yue Li",
      "Bin Ren",
      "Nicu Sebe",
      "Ender Konukoglu",
      "Theo Gevers",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build a large-scale dataset of 3DGS using the commonly used ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects from 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 2 GPU years on a TITAN XP GPU.   We utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce \\textbf{\\textit{Gaussian-MAE}}, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks.",
    "arxiv_url": "http://arxiv.org/abs/2408.10906v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10906v1",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics",
    "authors": [
      "Zhirui Gao",
      "Renjiao Yi",
      "Yuhang Huang",
      "Wei Chen",
      "Chenyang Zhu",
      "Kai Xu"
    ],
    "abstract": "Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D Gaussians, are commonly used to represent 3D objects or scenes. However, human perception typically understands 3D objects at a higher level as a composition of parts or structures rather than points or voxels. Representing 3D objects or scenes as semantic parts can benefit further understanding and applications. In this paper, we introduce $\\textbf{PartGS}$, $\\textbf{part}$-aware 3D reconstruction by a hybrid representation of 2D $\\textbf{G}$aussians and $\\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts, digging 3D structural clues from multi-view image inputs. Accurate structured geometry reconstruction and high-quality rendering are achieved at the same time. Our method simultaneously optimizes superquadric meshes and Gaussians by coupling their parameters within our hybrid representation. On one hand, this hybrid representation inherits the advantage of superquadrics to represent different shape primitives, supporting flexible part decomposition of scenes. On the other hand, 2D Gaussians capture complex texture and geometry details, ensuring high-quality appearance and geometry reconstruction. Our method is fully unsupervised and outperforms existing state-of-the-art approaches in extensive experiments on DTU, ShapeNet, and real-life datasets.",
    "arxiv_url": "http://arxiv.org/abs/2408.10789v2",
    "pdf_url": "http://arxiv.org/pdf/2408.10789v2",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGAS: Detailed Expressions on Full-Body Gaussian Avatars",
    "authors": [
      "Zhijing Shao",
      "Duotun Wang",
      "Qing-Yao Tian",
      "Yao-Dong Yang",
      "Hengyu Meng",
      "Zeyu Cai",
      "Bo Dong",
      "Yu Zhang",
      "Kang Zhang",
      "Zeyu Wang"
    ],
    "abstract": "Although neural rendering has made significant advances in creating lifelike, animatable full-body and head avatars, incorporating detailed expressions into full-body avatars remains largely unexplored. We present DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for full-body avatars with rich facial expressions. Trained on multiview videos of a given subject, our method learns a conditional variational autoencoder that takes both the body motion and facial expression as driving signals to generate Gaussian maps in the UV layout. To drive the facial expressions, instead of the commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression latent space trained solely on 2D portrait images, bridging the gap between 2D talking faces and 3D avatars. Leveraging the rendering capability of 3DGS and the rich expressiveness of the expression latent space, the learned avatars can be reenacted to reproduce photorealistic rendering images with subtle and accurate facial expressions. Experiments on an existing dataset and our newly proposed dataset of full-body talking avatars demonstrate the efficacy of our method. We also propose an audio-driven extension of our method with the help of 2D talking faces, opening new possibilities for interactive AI agents.",
    "arxiv_url": "http://arxiv.org/abs/2408.10588v2",
    "pdf_url": "http://arxiv.org/pdf/2408.10588v2",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "face",
      "body",
      "3d gaussian",
      "ar",
      "avatar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoopSplat: Loop Closure by Registering 3D Gaussian Splats",
    "authors": [
      "Liyuan Zhu",
      "Yue Li",
      "Erik Sandstr√∂m",
      "Shengyu Huang",
      "Konrad Schindler",
      "Iro Armeni"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code is available at loopsplat.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2408.10154v2",
    "pdf_url": "http://arxiv.org/pdf/2408.10154v2",
    "published_date": "2024-08-19",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "mapping",
      "3d gaussian",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane Representation",
    "authors": [
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Recent advancements in photo-realistic novel view synthesis have been significantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit nature of 3DGS data entails considerable storage requirements, highlighting a pressing need for more efficient data representations. To address this, we present Implicit Gaussian Splatting (IGS), an innovative hybrid model that integrates explicit point clouds with implicit feature embeddings through a multi-level tri-plane architecture. This architecture features 2D feature grids at various resolutions across different levels, facilitating continuous spatial domain representation and enhancing spatial correlations among Gaussian primitives. Building upon this foundation, we introduce a level-based progressive training scheme, which incorporates explicit spatial regularization. This method capitalizes on spatial correlations to enhance both the rendering quality and the compactness of the IGS representation. Furthermore, we propose a novel compression pipeline tailored for both point clouds and 2D feature grids, considering the entropy variations across different levels. Extensive experimental evaluations demonstrate that our algorithm can deliver high-quality rendering using only a few MBs, effectively balancing storage efficiency and rendering fidelity, and yielding results that are competitive with the state-of-the-art.",
    "arxiv_url": "http://arxiv.org/abs/2408.10041v2",
    "pdf_url": "http://arxiv.org/pdf/2408.10041v2",
    "published_date": "2024-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Topology-aware Human Avatars with Semantically-guided Gaussian Splatting",
    "authors": [
      "Haoyu Zhao",
      "Chen Yang",
      "Hao Wang",
      "Xingyue Zhao",
      "Wei Shen"
    ],
    "abstract": "Reconstructing photo-realistic and topology-aware animatable human avatars from monocular videos remains challenging in computer vision and graphics. Recently, methods using 3D Gaussians to represent the human body have emerged, offering faster optimization and real-time rendering. However, due to ignoring the crucial role of human body semantic information which represents the explicit topological and intrinsic structure within human body, they fail to achieve fine-detail reconstruction of human avatars. To address this issue, we propose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid deformation, and non-rigid cloth dynamics deformation to create photo-realistic human avatars. We then design a Semantic Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient body part semantic labeling. The generated labels are used to guide the optimization of semantic attributes of Gaussian. To capture the explicit topological structure of the human body, we employ a 3D network that integrates both topological and geometric associations for human avatar deformation. We further implement three key strategies to enhance the semantic accuracy of 3D Gaussians and rendering quality: semantic projection with 2D regularization, semantic-guided density regularization and semantic-aware regularization with neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves state-of-the-art geometry and appearance reconstruction performance.",
    "arxiv_url": "http://arxiv.org/abs/2408.09665v2",
    "pdf_url": "http://arxiv.org/pdf/2408.09665v2",
    "published_date": "2024-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "deformation",
      "body",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning",
    "authors": [
      "Haoyu Zhao",
      "Hao Wang",
      "Chen Yang",
      "Wei Shen"
    ],
    "abstract": "Existing approaches for human avatar generation--both NeRF-based and 3D Gaussian Splatting (3DGS) based--struggle with maintaining 3D consistency and exhibit degraded detail reconstruction, particularly when training with sparse inputs. To address this challenge, we propose CHASE, a novel framework that achieves dense-input-level performance using only sparse inputs through two key innovations: cross-pose intrinsic 3D consistency supervision and 3D geometry contrastive learning. Building upon prior skeleton-driven approaches that combine rigid deformation with non-rigid cloth dynamics, we first establish baseline avatars with fundamental 3D consistency. To enhance 3D consistency under sparse inputs, we introduce a Dynamic Avatar Adjustment (DAA) module, which refines deformed Gaussians by leveraging similar poses from the training set. By minimizing the rendering discrepancy between adjusted Gaussians and reference poses, DAA provides additional supervision for avatar reconstruction. We further maintain global 3D consistency through a novel geometry-aware contrastive learning strategy. While designed for sparse inputs, CHASE surpasses state-of-the-art methods across both full and sparse settings on ZJU-MoCap and H36M datasets, demonstrating that our enhanced 3D consistency leads to superior rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2408.09663v3",
    "pdf_url": "http://arxiv.org/pdf/2408.09663v3",
    "published_date": "2024-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "deformation",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting",
    "authors": [
      "Sheng Ye",
      "Zhen-Hui Dong",
      "Yubin Hu",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.",
    "arxiv_url": "http://arxiv.org/abs/2408.09130v2",
    "pdf_url": "http://arxiv.org/pdf/2408.09130v2",
    "published_date": "2024-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "shadow",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS",
    "authors": [
      "Wei Sun",
      "Xiaosong Zhang",
      "Fang Wan",
      "Yanzhao Zhou",
      "Yuan Li",
      "Qixiang Ye",
      "Jianbin Jiao"
    ],
    "abstract": "Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed camera poses--referred to as SfM-free methods--is crucial for promoting rapid response capabilities and enhancing robustness against variable operating conditions. Recent SfM-free methods have integrated pose optimization, designing end-to-end frameworks for joint camera pose estimation and NVS. However, most existing works rely on per-pixel image loss functions, such as L2 loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue, which, under the constraints of per-pixel image loss functions, results in excessive gradients, causing unstable optimization and poor convergence for NVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian splatting for NVS. We use correspondences between the target and the rendered result to achieve better pixel alignment, facilitating the optimization of relative poses between frames. We then apply the learned poses to optimize the entire scene. Each 2D screen-space pixel is associated with its corresponding 3D Gaussians through approximated surface rendering to facilitate gradient back propagation. Experimental results underline the superior performance and time efficiency of the proposed approach compared to the state-of-the-art baselines.",
    "arxiv_url": "http://arxiv.org/abs/2408.08723v1",
    "pdf_url": "http://arxiv.org/pdf/2408.08723v1",
    "published_date": "2024-08-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization",
    "authors": [
      "Kang Du",
      "Zhihao Liang",
      "Zeyu Wang"
    ],
    "abstract": "We present GS-ID, a novel framework for illumination decomposition on Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive light editing. Illumination decomposition is an ill-posed problem facing three main challenges: 1) priors for geometry and material are often lacking; 2) complex illumination conditions involve multiple unknown light sources; and 3) calculating surface shading with numerous light sources is computationally expensive. To address these challenges, we first introduce intrinsic diffusion priors to estimate the attributes for physically based rendering. Then we divide the illumination into environmental and direct components for joint optimization. Last, we employ deferred rendering to reduce the computational load. Our framework uses a learnable environment map and Spherical Gaussians (SGs) to represent light sources parametrically, therefore enabling controllable and photorealistic relighting on Gaussian Splatting. Extensive experiments and applications demonstrate that GS-ID produces state-of-the-art illumination decomposition results while achieving better geometry reconstruction and rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2408.08524v1",
    "pdf_url": "http://arxiv.org/pdf/2408.08524v1",
    "published_date": "2024-08-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "face",
      "geometry",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting",
    "authors": [
      "Huapeng Li",
      "Wenxuan Song",
      "Tianao Xu",
      "Alexandre Elsig",
      "Jonas Kulhanek"
    ],
    "abstract": "The underwater 3D scene reconstruction is a challenging, yet interesting problem with applications ranging from naval robots to VR experiences. The problem was successfully tackled by fully volumetric NeRF-based methods which can model both the geometry and the medium (water). Unfortunately, these methods are slow to train and do not offer real-time rendering. More recently, 3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs. However, because it is an explicit method that renders only the geometry, it cannot render the medium and is therefore unsuited for underwater reconstruction. Therefore, we propose a novel approach that fuses volumetric rendering with 3DGS to handle underwater data effectively. Our method employs 3DGS for explicit geometry representation and a separate volumetric field (queried once per pixel) for capturing the scattering medium. This dual representation further allows the restoration of the scenes by removing the scattering medium. Our method outperforms state-of-the-art NeRF-based methods in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it does so while offering real-time rendering performance, addressing the efficiency limitations of existing methods. Web: https://water-splatting.github.io",
    "arxiv_url": "http://arxiv.org/abs/2408.08206v2",
    "pdf_url": "http://arxiv.org/pdf/2408.08206v2",
    "published_date": "2024-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering",
    "authors": [
      "Guofeng Feng",
      "Siyan Chen",
      "Rong Fu",
      "Zimu Liao",
      "Yi Wang",
      "Tao Liu",
      "Zhilin Pei",
      "Hengjie Li",
      "Xingcheng Zhang",
      "Bo Dai"
    ],
    "abstract": "This work introduces FlashGS, an open-source CUDA Python library, designed to facilitate the efficient differentiable rasterization of 3D Gaussian Splatting through algorithmic and kernel-level optimizations. FlashGS is developed based on the observations from a comprehensive analysis of the rendering process to enhance computational efficiency and bring the technique to wide adoption. The paper includes a suite of optimization strategies, encompassing redundancy elimination, efficient pipelining, refined control and scheduling mechanisms, and memory access optimizations, all of which are meticulously integrated to amplify the performance of the rasterization process. An extensive evaluation of FlashGS' performance has been conducted across a diverse spectrum of synthetic and real-world large-scale scenes, encompassing a variety of image resolutions. The empirical findings demonstrate that FlashGS consistently achieves an average 4x acceleration over mobile consumer GPUs, coupled with reduced memory consumption. These results underscore the superior performance and resource optimization capabilities of FlashGS, positioning it as a formidable tool in the domain of 3D rendering.",
    "arxiv_url": "http://arxiv.org/abs/2408.07967v2",
    "pdf_url": "http://arxiv.org/pdf/2408.07967v2",
    "published_date": "2024-08-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Progressive Radiance Distillation for Inverse Rendering with Gaussian Splatting",
    "authors": [
      "Keyang Ye",
      "Qiming Hou",
      "Kun Zhou"
    ],
    "abstract": "We propose progressive radiance distillation, an inverse rendering method that combines physically-based rendering with Gaussian-based radiance field rendering using a distillation progress map. Taking multi-view images as input, our method starts from a pre-trained radiance field guidance, and distills physically-based light and material parameters from the radiance field using an image-fitting process. The distillation progress map is initialized to a small value, which favors radiance field rendering. During early iterations when fitted light and material parameters are far from convergence, the radiance field fallback ensures the sanity of image loss gradients and avoids local minima that attracts under-fit states. As fitted parameters converge, the physical model gradually takes over and the distillation progress increases correspondingly. In presence of light paths unmodeled by the physical model, the distillation progress never finishes on affected pixels and the learned radiance field stays in the final rendering. With this designed tolerance for physical model limitations, we prevent unmodeled color components from leaking into light and material parameters, alleviating relighting artifacts. Meanwhile, the remaining radiance field compensates for the limitations of the physical model, guaranteeing high-quality novel views synthesis. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques quality-wise in both novel view synthesis and relighting. The idea of progressive radiance distillation is not limited to Gaussian splatting. We show that it also has positive effects for prominently specular scenes when adapted to a mesh-based inverse rendering method.",
    "arxiv_url": "http://arxiv.org/abs/2408.07595v1",
    "pdf_url": "http://arxiv.org/pdf/2408.07595v1",
    "published_date": "2024-08-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "lighting",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Editing with A Single Image",
    "authors": [
      "Guan Luo",
      "Tian-Xing Xu",
      "Ying-Tian Liu",
      "Xiao-Xiong Fan",
      "Fang-Lue Zhang",
      "Song-Hai Zhang"
    ],
    "abstract": "The modeling and manipulation of 3D scenes captured from the real world are pivotal in various applications, attracting growing research interest. While previous works on editing have achieved interesting results through manipulating 3D meshes, they often require accurately reconstructed meshes to perform editing, which limits their application in 3D content generation. To address this gap, we introduce a novel single-image-driven 3D scene editing approach based on 3D Gaussian Splatting, enabling intuitive manipulation via directly editing the content on a 2D image plane. Our method learns to optimize the 3D Gaussians to align with an edited version of the image rendered from a user-specified viewpoint of the original scene. To capture long-range object deformation, we introduce positional loss into the optimization process of 3D Gaussian Splatting and enable gradient propagation through reparameterization. To handle occluded 3D Gaussians when rendering from the specified viewpoint, we build an anchor-based structure and employ a coarse-to-fine optimization strategy capable of handling long-range deformation while maintaining structural stability. Furthermore, we design a novel masking strategy to adaptively identify non-rigid deformation regions for fine-scale modeling. Extensive experiments show the effectiveness of our method in handling geometric details, long-range, and non-rigid deformation, demonstrating superior editing flexibility and quality compared to previous approaches.",
    "arxiv_url": "http://arxiv.org/abs/2408.07540v1",
    "pdf_url": "http://arxiv.org/pdf/2408.07540v1",
    "published_date": "2024-08-14",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space",
    "authors": [
      "Hyunjee Lee",
      "Youngsik Yun",
      "Jeongmin Bae",
      "Seoha Kim",
      "Youngjung Uh"
    ],
    "abstract": "Understanding the 3D semantics of a scene is a fundamental problem for various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view synthesis, previous methods for understanding their semantics have been limited to incomplete 3D understanding: their segmentation results are rendered as 2D masks that do not represent the entire 3D space. To address this limitation, we redefine the problem to segment the 3D volume and propose the following methods for better 3D understanding. We directly supervise the 3D points to train the language embedding field, unlike previous methods that anchor supervision at 2D pixels. We transfer the learned language field to 3DGS, achieving the first real-time rendering speed without sacrificing training time or accuracy. Lastly, we introduce a 3D querying and evaluation protocol for assessing the reconstructed geometry and semantics together. Code, checkpoints, and annotations are available at the project page.",
    "arxiv_url": "http://arxiv.org/abs/2408.07416v3",
    "pdf_url": "http://arxiv.org/pdf/2408.07416v3",
    "published_date": "2024-08-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "real-time rendering",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene representation, visualization and analysis",
    "authors": [
      "Saptarshi Neil Sinha",
      "Holger Graf",
      "Michael Weinmann"
    ],
    "abstract": "We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and semantically meaningful splats from registered multi-view spectrum and segmentation maps. This extension enhances the representation of scenes with multiple spectra, providing insights into the underlying materials and segmentation. We introduce an improved physically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accuracy and realism. In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our approach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as well as other non-spectral state-of-the-art learning-based approaches. Our work also demonstrates the potential of spectral scene understanding for precise scene editing techniques like style transfer, inpainting, and removal. Thereby, our contributions address challenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications.",
    "arxiv_url": "http://arxiv.org/abs/2408.06975v1",
    "pdf_url": "http://arxiv.org/pdf/2408.06975v1",
    "published_date": "2024-08-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "I.2.10; I.3.7; I.4.8; I.4.1"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDRGS: High Dynamic Range Gaussian Splatting",
    "authors": [
      "Jiahao Wu",
      "Lu Xiao",
      "Rui Peng",
      "Kaiqiang Xiong",
      "Ronggang Wang"
    ],
    "abstract": "Recent years have witnessed substantial advancements in the field of 3D reconstruction from 2D images, particularly following the introduction of the neural radiance field (NeRF) technique. However, reconstructing a 3D high dynamic range (HDR) radiance field, which aligns more closely with real-world conditions, from 2D multi-exposure low dynamic range (LDR) images continues to pose significant challenges. Approaches to this issue fall into two categories: grid-based and implicit-based. Implicit methods, using multi-layer perceptrons (MLP), face inefficiencies, limited solvability, and overfitting risks. Conversely, grid-based methods require significant memory and struggle with image quality and long training times. In this paper, we introduce Gaussian Splatting-a recent, high-quality, real-time 3D reconstruction technique-into this domain. We further develop the High Dynamic Range Gaussian Splatting (HDR-GS) method, designed to address the aforementioned challenges. This method enhances color dimensionality by including luminance and uses an asymmetric grid for tone-mapping, swiftly and precisely converting pixel irradiance to color. Our approach improves HDR scene recovery accuracy and integrates a novel coarse-to-fine strategy to speed up model convergence, enhancing robustness against sparse viewpoints and exposure extremes, and preventing local optima. Extensive testing confirms that our method surpasses current state-of-the-art techniques in both synthetic and real-world scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2408.06543v3",
    "pdf_url": "http://arxiv.org/pdf/2408.06543v3",
    "published_date": "2024-08-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "face",
      "mapping",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering",
    "authors": [
      "Jiameng Li",
      "Yue Shi",
      "Jiezhang Cao",
      "Bingbing Ni",
      "Wenjun Zhang",
      "Kai Zhang",
      "Luc Van Gool"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted great attention in novel view synthesis because of its superior rendering efficiency and high fidelity. However, the trained Gaussians suffer from severe zooming degradation due to non-adjustable representation derived from single-scale training. Though some methods attempt to tackle this problem via post-processing techniques such as selective rendering or filtering techniques towards primitives, the scale-specific information is not involved in Gaussians. In this paper, we propose a unified optimization method to make Gaussians adaptive for arbitrary scales by self-adjusting the primitive properties (e.g., color, shape and size) and distribution (e.g., position). Inspired by the mipmap technique, we design pseudo ground-truth for the target scale and propose a scale-consistency guidance loss to inject scale information into 3D Gaussians. Our method is a plug-in module, applicable for any 3DGS models to solve the zoom-in and zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.",
    "arxiv_url": "http://arxiv.org/abs/2408.06286v1",
    "pdf_url": "http://arxiv.org/pdf/2408.06286v1",
    "published_date": "2024-08-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Developing Smart MAVs for Autonomous Inspection in GPS-denied Constructions",
    "authors": [
      "Paoqiang Pan",
      "Kewei Hu",
      "Xiao Huang",
      "Wei Ying",
      "Xiaoxuan Xie",
      "Yue Ma",
      "Naizhong Zhang",
      "Hanwen Kang"
    ],
    "abstract": "Smart Micro Aerial Vehicles (MAVs) have transformed infrastructure inspection by enabling efficient, high-resolution monitoring at various stages of construction, including hard-to-reach areas. Traditional manual operation of drones in GPS-denied environments, such as industrial facilities and infrastructure, is labour-intensive, tedious and prone to error. This study presents an innovative framework for smart MAV inspections in such complex and GPS-denied indoor environments. The framework features a hierarchical perception and planning system that identifies regions of interest and optimises task paths. It also presents an advanced MAV system with enhanced localisation and motion planning capabilities, integrated with Neural Reconstruction technology for comprehensive 3D reconstruction of building structures. The effectiveness of the framework was empirically validated in a 4,000 square meters indoor infrastructure facility with an interior length of 80 metres, a width of 50 metres and a height of 7 metres. The main structure consists of columns and walls. Experimental results show that our MAV system performs exceptionally well in autonomous inspection tasks, achieving a 100\\% success rate in generating and executing scan paths. Extensive experiments validate the manoeuvrability of our developed MAV, achieving a 100\\% success rate in motion planning with a tracking error of less than 0.1 metres. In addition, the enhanced reconstruction method using 3D Gaussian Splatting technology enables the generation of high-fidelity rendering models from the acquired data. Overall, our novel method represents a significant advancement in the use of robotics for infrastructure inspection.",
    "arxiv_url": "http://arxiv.org/abs/2408.06030v1",
    "pdf_url": "http://arxiv.org/pdf/2408.06030v1",
    "published_date": "2024-08-12",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "high-fidelity",
      "tracking",
      "vr",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors",
    "authors": [
      "Xiaozheng Zheng",
      "Chao Wen",
      "Zhaohu Li",
      "Weiyi Zhang",
      "Zhuo Su",
      "Xu Chang",
      "Yang Zhao",
      "Zheng Lv",
      "Xiaoyuan Zhang",
      "Yongjie Zhang",
      "Guidong Wang",
      "Lan Xu"
    ],
    "abstract": "In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.",
    "arxiv_url": "http://arxiv.org/abs/2408.06019v2",
    "pdf_url": "http://arxiv.org/pdf/2408.06019v2",
    "published_date": "2024-08-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "head",
      "high-fidelity",
      "few-shot",
      "fast",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel View Synthesis",
    "authors": [
      "Zhongche Qu",
      "Zhi Zhang",
      "Cong Liu",
      "Jianhua Yin"
    ],
    "abstract": "Conventional geometry-based SLAM systems lack dense 3D reconstruction capabilities since their data association usually relies on feature correspondences. Additionally, learning-based SLAM systems often fall short in terms of real-time performance and accuracy. Balancing real-time performance with dense 3D reconstruction capabilities is a challenging problem. In this paper, we propose a real-time RGB-D SLAM system that incorporates a novel view synthesis technique, 3D Gaussian Splatting, for 3D scene representation and pose estimation. This technique leverages the real-time rendering performance of 3D Gaussian Splatting with rasterization and allows for differentiable optimization in real time through CUDA implementation. We also enable mesh reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To estimate accurate camera poses, we utilize a rotation-translation decoupled strategy with inverse optimization. This involves iteratively updating both in several iterations through gradient-based optimization. This process includes differentiably rendering RGB, depth, and silhouette maps and updating the camera parameters to minimize a combined loss of photometric loss, depth geometry loss, and visibility loss, given the existing 3D Gaussian map. However, 3D Gaussian Splatting (3DGS) struggles to accurately represent surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to reduced accuracy in both camera pose estimation and scene reconstruction. To address this, we utilize depth priors as additional regularization to enforce geometric constraints, thereby improving the accuracy of both pose estimation and 3D reconstruction. We also provide extensive experimental results on public benchmark datasets to demonstrate the effectiveness of our proposed methods in terms of pose accuracy, geometric accuracy, and rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2408.05635v2",
    "pdf_url": "http://arxiv.org/pdf/2408.05635v2",
    "published_date": "2024-08-10",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRTGaussian: Efficient Relighting Using 3D Gaussians with Precomputed Radiance Transfer",
    "authors": [
      "Libo Zhang",
      "Yuxuan Han",
      "Wenbin Lin",
      "Jingwang Ling",
      "Feng Xu"
    ],
    "abstract": "We present PRTGaussian, a realtime relightable novel-view synthesis method made possible by combining 3D Gaussians and Precomputed Radiance Transfer (PRT). By fitting relightable Gaussians to multi-view OLAT data, our method enables real-time, free-viewpoint relighting. By estimating the radiance transfer based on high-order spherical harmonics, we achieve a balance between capturing detailed relighting effects and maintaining computational efficiency. We utilize a two-stage process: in the first stage, we reconstruct a coarse geometry of the object from multi-view images. In the second stage, we initialize 3D Gaussians with the obtained point cloud, then simultaneously refine the coarse geometry and learn the light transport for each Gaussian. Extensive experiments on synthetic datasets show that our approach can achieve fast and high-quality relighting for general objects. Code and data are available at https://github.com/zhanglbthu/PRTGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2408.05631v1",
    "pdf_url": "http://arxiv.org/pdf/2408.05631v1",
    "published_date": "2024-08-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/zhanglbthu/PRTGaussian",
    "keywords": [
      "efficient",
      "relightable",
      "relighting",
      "lighting",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "light transport"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlowDreamer: Exploring High Fidelity Text-to-3D Generation via Rectified Flow",
    "authors": [
      "Hangyu Li",
      "Xiangxiang Chu",
      "Dingyuan Shi",
      "Wang Lin"
    ],
    "abstract": "Recent advances in text-to-3D generation have made significant progress. In particular, with the pretrained diffusion models, existing methods predominantly use Score Distillation Sampling (SDS) to train 3D models such as Neural RaRecent advances in text-to-3D generation have made significant progress. In particular, with the pretrained diffusion models, existing methods predominantly use Score Distillation Sampling (SDS) to train 3D models such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a hurdle is that they often encounter difficulties with over-smoothing textures and over-saturating colors. The rectified flow model -- which utilizes a simple ordinary differential equation (ODE) to represent a straight trajectory -- shows promise as an alternative prior to text-to-3D generation. It learns a time-independent vector field, thereby reducing the ambiguity in 3D model update gradients that are calculated using time-dependent scores in the SDS framework. In light of this, we first develop a mathematical analysis to seamlessly integrate SDS with rectified flow model, paving the way for our initial framework known as Vector Field Distillation Sampling (VFDS). However, empirical findings indicate that VFDS still results in over-smoothing outcomes. Therefore, we analyze the grounding reasons for such a failure from the perspective of ODE trajectories. On top, we propose a novel framework, named FlowDreamer, which yields high fidelity results with richer textual details and faster convergence. The key insight is to leverage the coupling and reversible properties of the rectified flow model to search for the corresponding noise, rather than using randomly sampled noise as in VFDS. Accordingly, we introduce a novel Unique Couple Matching (UCM) loss, which guides the 3D model to optimize along the same trajectory.",
    "arxiv_url": "http://arxiv.org/abs/2408.05008v3",
    "pdf_url": "http://arxiv.org/pdf/2408.05008v3",
    "published_date": "2024-08-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D Reconstruction",
    "authors": [
      "Bi'an Du",
      "Lingbei Meng",
      "Wei Hu"
    ],
    "abstract": "Sparse-view 3D reconstruction is a major challenge in computer vision, aiming to create complete three-dimensional models from limited viewing angles. Key obstacles include: 1) a small number of input images with inconsistent information; 2) dependence on input image quality; and 3) large model parameter sizes. To tackle these issues, we propose a self-augmented two-stage Gaussian splatting framework enhanced with structural masks for sparse-view 3D reconstruction. Initially, our method generates a basic 3D Gaussian representation from sparse inputs and renders multi-view images. We then fine-tune a pre-trained 2D diffusion model to enhance these images, using them as augmented data to further optimize the 3D Gaussians. Additionally, a structural masking strategy during training enhances the model's robustness to sparse inputs and noise. Experiments on benchmarks like MipNeRF360, OmniObject3D, and OpenIllumination demonstrate that our approach achieves state-of-the-art performance in perceptual quality and multi-view consistency with sparse inputs.",
    "arxiv_url": "http://arxiv.org/abs/2408.04831v4",
    "pdf_url": "http://arxiv.org/pdf/2408.04831v4",
    "published_date": "2024-08-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery",
    "authors": [
      "Mengya Xu",
      "Ziqi Guo",
      "An Wang",
      "Long Bai",
      "Hongliang Ren"
    ],
    "abstract": "As a crucial and intricate task in robotic minimally invasive surgery, reconstructing surgical scenes using stereo or monocular endoscopic video holds immense potential for clinical applications. NeRF-based techniques have recently garnered attention for the ability to reconstruct scenes implicitly. On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly using 3D Gaussians and projects them onto a 2D plane as a replacement for the complex volume rendering in NeRF. However, these methods face challenges regarding surgical scene reconstruction, such as slow inference, dynamic scenes, and surgical tool occlusion. This work explores and reviews state-of-the-art (SOTA) approaches, discussing their innovations and implementation principles. Furthermore, we replicate the models and conduct testing and evaluation on two datasets. The test results demonstrate that with advancements in these techniques, achieving real-time, high-quality reconstructions becomes feasible.",
    "arxiv_url": "http://arxiv.org/abs/2408.04426v1",
    "pdf_url": "http://arxiv.org/pdf/2408.04426v1",
    "published_date": "2024-08-08",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian Splatting",
    "authors": [
      "Xin-Yi Yu",
      "Jun-Xin Yu",
      "Li-Bo Zhou",
      "Yan Wei",
      "Lin-Lin Ou"
    ],
    "abstract": "We present InstantStyleGaussian, an innovative 3D style transfer method based on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target-style image, it quickly generates new 3D GS scenes. Our method operates on pre-reconstructed GS scenes, combining diffusion models with an improved iterative dataset update strategy. It utilizes diffusion models to generate target style images, adds these new images to the training dataset, and uses this dataset to iteratively update and optimize the GS scenes, significantly accelerating the style editing process while ensuring the quality of the generated scenes. Extensive experimental results demonstrate that our method ensures high-quality stylized scenes while offering significant advantages in style transfer speed and consistency.",
    "arxiv_url": "http://arxiv.org/abs/2408.04249v2",
    "pdf_url": "http://arxiv.org/pdf/2408.04249v2",
    "published_date": "2024-08-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Real-Time Gaussian Splatting: Accelerating 3DGS through Photometric SLAM",
    "authors": [
      "Yan Song Hu",
      "Dayou Mao",
      "Yuhao Chen",
      "John Zelek"
    ],
    "abstract": "Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous Localization and Mapping (VSLAM) demonstrate the generation of high-quality volumetric reconstructions from monocular video streams. However, despite these promising advancements, current 3DGS integrations have reduced tracking performance and lower operating speeds compared to traditional VSLAM. To address these issues, we propose integrating 3DGS with Direct Sparse Odometry, a monocular photometric SLAM system. We have done preliminary experiments showing that using Direct Sparse Odometry point cloud outputs, as opposed to standard structure-from-motion methods, significantly shortens the training time needed to achieve high-quality renders. Reducing 3DGS training time enables the development of 3DGS-integrated SLAM systems that operate in real-time on mobile hardware. These promising initial findings suggest further exploration is warranted in combining traditional VSLAM systems with 3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2408.03825v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03825v1",
    "published_date": "2024-08-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields",
    "authors": [
      "Joo Chan Lee",
      "Daniel Rho",
      "Xiangyu Sun",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussian-based representation and introduces an approximated volumetric rendering, achieving very fast rendering speed and promising image quality. Furthermore, subsequent studies have successfully extended 3DGS to dynamic 3D scenes, demonstrating its wide range of applications. However, a significant drawback arises as 3DGS and its following methods entail a substantial number of Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while preserving high performance. In addition, we propose a compact but effective representation of view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics. Finally, we learn codebooks to compactly represent the geometric and temporal attributes by residual vector quantization. With model compression techniques such as quantization and entropy coding, we consistently show over 25x reduced storage and enhanced rendering speed compared to 3DGS for static scenes, while maintaining the quality of the scene representation. For dynamic scenes, our approach achieves more than 12x storage efficiency and retains a high-quality reconstruction compared to the existing state-of-the-art methods. Our work provides a comprehensive framework for 3D scene representation, achieving high performance, fast training, compactness, and real-time rendering. Our project page is available at https://maincold2.github.io/c3dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2408.03822v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03822v1",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting",
    "authors": [
      "Zhe Jun Tang",
      "Tat-Jen Cham"
    ],
    "abstract": "The use of 3D Gaussians as representation of radiance fields has enabled high quality novel view synthesis at real-time rendering speed. However, the choice of optimising the outgoing radiance of each Gaussian independently as spherical harmonics results in unsatisfactory view dependent effects. In response to these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering quality. Instead of optimising a single outgoing radiance parameter, 3iGS enhances 3DGS view-dependent effects by expressing the outgoing radiance as a function of a local illumination field and Bidirectional Reflectance Distribution Function (BRDF) features. We optimise a continuous incident illumination field through a Tensorial Factorisation representation, while separately fine-tuning the BRDF features of each 3D Gaussian relative to this illumination field. Our methodology significantly enhances the rendering quality of specular view-dependent effects of 3DGS, while maintaining rapid training and rendering speeds.",
    "arxiv_url": "http://arxiv.org/abs/2408.03753v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03753v1",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting",
    "authors": [
      "Yijia Guo",
      "Yuanxi Bai",
      "Liwen Hu",
      "Ziyi Guo",
      "Mianzhi Liu",
      "Yu Cai",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a real-time high-quality relighting method for Gaussian splats in low-frequency lighting environments that captures soft shadows and interreflections by precomputing 3D Gaussian splats' radiance transfer. Existing studies have demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields' efficiency for dynamic lighting scenarios. However, the current relighting method based on 3DGS still struggles to compute high-quality shadow and indirect illumination in real time for dynamic light, leading to unrealistic rendering results. We solve this problem by precomputing the expensive transport simulations required for complex transfer functions like shadowing, the resulting transfer functions are represented as dense sets of vectors or matrices for every Gaussian splat. We introduce distinct precomputing methods tailored for training and rendering stages, along with unique ray tracing and indirect lighting precomputation techniques for 3D Gaussian splats to accelerate training speed and compute accurate indirect lighting related to environment light. Experimental analyses demonstrate that our approach achieves state-of-the-art visual quality while maintaining competitive training times and allows high-quality real-time (30+ fps) relighting for dynamic light and relatively complex scenes at 1080p resolution.",
    "arxiv_url": "http://arxiv.org/abs/2408.03538v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03538v1",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "reflection",
      "relighting",
      "lighting",
      "shadow",
      "3d gaussian",
      "ar",
      "illumination",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language Embedded 3D Gaussian",
    "authors": [
      "Amirhosein Chahe",
      "Lifeng Zhou"
    ],
    "abstract": "This paper introduces a novel method for open-vocabulary 3D scene querying in autonomous driving by combining Language Embedded 3D Gaussians with Large Language Models (LLMs). We propose utilizing LLMs to generate both contextually canonical phrases and helping positive words for enhanced segmentation and scene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to create a high-quality text dataset, which we then use to fine-tune smaller, more efficient LLMs for on-device deployment. Our comprehensive evaluation on the WayveScenes101 dataset demonstrates that LLM-guided segmentation significantly outperforms traditional approaches based on predefined canonical phrases. Notably, our fine-tuned smaller models achieve performance comparable to larger expert models while maintaining faster inference times. Through ablation studies, we discover that the effectiveness of helping positive words correlates with model scale, with larger models better equipped to leverage additional semantic information. This work represents a significant advancement towards more efficient, context-aware autonomous driving systems, effectively bridging 3D scene representation with high-level semantic querying while maintaining practical deployment considerations.",
    "arxiv_url": "http://arxiv.org/abs/2408.03516v3",
    "pdf_url": "http://arxiv.org/pdf/2408.03516v3",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "fast",
      "semantic",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LumiGauss: Relightable Gaussian Splatting in the Wild",
    "authors": [
      "Joanna Kaleta",
      "Kacper Kania",
      "Tomasz Trzcinski",
      "Marek Kowalski"
    ],
    "abstract": "Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue, often at the expense of output fidelity, which questions the practicality of such methods. We introduce LumiGauss - a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows, common in outdoor scenes, by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer. We validate our method on the NeRF-OSR dataset, demonstrating superior performance over baseline methods. Moreover, LumiGauss can synthesize realistic images for unseen environment maps. Our code: https://github.com/joaxkal/lumigauss.",
    "arxiv_url": "http://arxiv.org/abs/2408.04474v2",
    "pdf_url": "http://arxiv.org/pdf/2408.04474v2",
    "published_date": "2024-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/joaxkal/lumigauss",
    "keywords": [
      "relightable",
      "outdoor",
      "lighting",
      "fast",
      "shadow",
      "geometry",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness",
    "authors": [
      "Lutao Jiang",
      "Hangyu Li",
      "Lin Wang"
    ],
    "abstract": "Text-to-3D content creation has recently received much attention, especially with the prevalence of 3D Gaussians Splatting. In general, GS-based methods comprise two key stages: initialization and rendering optimization. To achieve initialization, existing works directly apply random sphere initialization or 3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such strategies suffer from two critical yet challenging problems: 1) the final shapes are still similar to the initial ones even after training; 2) shapes can be produced only from simple texts, e.g., \"a dog\", not for lexically richer texts, e.g., \"a dog is sitting on the top of the airplane\". To address these problems, this paper proposes a novel general framework to boost the 3D GS Initialization for text-to-3D generation upon the lexical richness. Our key idea is to aggregate 3D Gaussians into spatially uniform voxels to represent complex shapes while enabling the spatial interaction among the 3D Gaussians and semantic interaction between Gaussians and texts. Specifically, we first construct a voxelized representation, where each voxel holds a 3D Gaussian with its position, scale, and rotation fixed while setting opacity as the sole factor to determine a position's occupancy. We then design an initialization network mainly consisting of two novel components: 1) Global Information Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design enables each 3D Gaussian to assimilate the spatial information from other areas and semantic information from texts. Extensive experiments show the superiority of our framework of high-quality 3D GS initialization against the existing methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts. Also, our framework can be seamlessly plugged into SoTA training frameworks, e.g., LucidDreamer, for semantically consistent text-to-3D generation.",
    "arxiv_url": "http://arxiv.org/abs/2408.01269v1",
    "pdf_url": "http://arxiv.org/pdf/2408.01269v1",
    "published_date": "2024-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation with Volumetric Visual Data Fusion",
    "authors": [
      "Ke Li",
      "Reinhard Bacher",
      "Susanne Schmidt",
      "Wim Leemans",
      "Frank Steinicke"
    ],
    "abstract": "We introduce Reality Fusion, a novel robot teleoperation system that localizes, streams, projects, and merges a typical onboard depth sensor with a photorealistic, high resolution, high framerate, and wide field of view (FoV) rendering of the complex remote environment represented as 3D Gaussian splats (3DGS). Our framework enables robust egocentric and exocentric robot teleoperation in immersive VR, with the 3DGS effectively extending spatial information of a depth sensor with limited FoV and balancing the trade-off between data streaming costs and data visual quality. We evaluated our framework through a user study with 24 participants, which revealed that Reality Fusion leads to significantly better user performance, situation awareness, and user preferences. To support further research and development, we provide an open-source implementation with an easy-to-replicate custom-made telepresence robot, a high-performance virtual reality 3DGS renderer, and an immersive robot control package. (Source code: https://github.com/uhhhci/RealityFusion)",
    "arxiv_url": "http://arxiv.org/abs/2408.01225v1",
    "pdf_url": "http://arxiv.org/pdf/2408.01225v1",
    "published_date": "2024-08-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "https://github.com/uhhhci/RealityFusion",
    "keywords": [
      "3d gaussian",
      "vr",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IG-SLAM: Instant Gaussian SLAM",
    "authors": [
      "F. Aykut Sarikamis",
      "A. Aydin Alatan"
    ],
    "abstract": "3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC datasets. The system achieves photo-realistic 3D reconstruction in large-scale sequences, particularly in the EuRoC dataset.",
    "arxiv_url": "http://arxiv.org/abs/2408.01126v2",
    "pdf_url": "http://arxiv.org/pdf/2408.01126v2",
    "published_date": "2024-08-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head",
    "authors": [
      "Qianyun He",
      "Xinya Ji",
      "Yicheng Gong",
      "Yuanxun Lu",
      "Zhengyu Diao",
      "Linjia Huang",
      "Yao Yao",
      "Siyu Zhu",
      "Zhan Ma",
      "Songcen Xu",
      "Xiaofei Wu",
      "Zixiao Zhang",
      "Xun Cao",
      "Hao Zhu"
    ],
    "abstract": "We present a novel approach for synthesizing 3D talking heads with controllable emotion, featuring enhanced lip synchronization and rendering quality. Despite significant progress in the field, prior methods still suffer from multi-view consistency and a lack of emotional expressiveness. To address these issues, we collect EmoTalk3D dataset with calibrated multi-view videos, emotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D dataset, we propose a \\textit{`Speech-to-Geometry-to-Appearance'} mapping framework that first predicts faithful 3D geometry sequence from the audio features, then the appearance of a 3D talking head represented by 4D Gaussians is synthesized from the predicted geometry. The appearance is further disentangled into canonical and dynamic Gaussians, learned from multi-view videos, and fused to render free-view talking head animation. Moreover, our model enables controllable emotion in the generated talking heads and can be rendered in wide-range views. Our method exhibits improved rendering quality and stability in lip motion generation while capturing dynamic facial details such as wrinkles and subtle expressions. Experiments demonstrate the effectiveness of our approach in generating high-fidelity and emotion-controllable 3D talking heads. The code and EmoTalk3D dataset are released at https://nju-3dv.github.io/projects/EmoTalk3D.",
    "arxiv_url": "http://arxiv.org/abs/2408.00297v1",
    "pdf_url": "http://arxiv.org/pdf/2408.00297v1",
    "published_date": "2024-08-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "motion",
      "mapping",
      "geometry",
      "4d",
      "ar",
      "animation",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting",
    "authors": [
      "Zhenyu Bao",
      "Guibiao Liao",
      "Kaichen Zhou",
      "Kanglin Liu",
      "Qing Li",
      "Guoping Qiu"
    ],
    "abstract": "Despite the photorealistic novel view synthesis (NVS) performance achieved by the original 3D Gaussian splatting (3DGS), its rendering quality significantly degrades with sparse input views. This performance drop is mainly caused by the limited number of initial points generated from the sparse input, insufficient supervision during the training process, and inadequate regularization of the oversized Gaussian ellipsoids. To handle these issues, we propose the LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis task. In specific, we propose a loop-based Progressive Gaussian Initialization (PGI) strategy that could iteratively densify the initialized point cloud using the rendered pseudo images during the training process. Then, the sparse and reliable depth from the Structure from Motion, and the window-based dense monocular depth are leveraged to provide precise geometric supervision via the proposed Depth-alignment Regularization (DAR). Additionally, we introduce a novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian ellipsoids leading to large pixel errors. Comprehensive experiments on four datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art methods for sparse-input novel view synthesis, across indoor, outdoor, and object-level scenes with various image resolutions.",
    "arxiv_url": "http://arxiv.org/abs/2408.00254v1",
    "pdf_url": "http://arxiv.org/pdf/2408.00254v1",
    "published_date": "2024-08-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "outdoor",
      "motion",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Localized Gaussian Splatting Editing with Contextual Awareness",
    "authors": [
      "Hanyuan Xiao",
      "Yingshu Chen",
      "Huajian Huang",
      "Haolin Xiong",
      "Jing Yang",
      "Pratusha Prasad",
      "Yajie Zhao"
    ],
    "abstract": "Recent text-guided generation of individual 3D object has achieved great success using diffusion priors. However, these methods are not suitable for object insertion and replacement tasks as they do not consider the background, leading to illumination mismatches within the environment. To bridge the gap, we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian Splatting (3DGS) representation. Our key observation is that inpainting by the state-of-the-art conditional 2D diffusion model is consistent with background in lighting. To leverage the prior knowledge from the well-trained diffusion models for 3D object generation, our approach employs a coarse-to-fine objection optimization pipeline with inpainted views. In the first coarse step, we achieve image-to-3D lifting given an ideal inpainted view. The process employs 3D-aware diffusion prior from a view-conditioned diffusion model, which preserves illumination present in the conditioning image. To acquire an ideal inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a single view that best represents the scene illumination in target region. In the second Texture Enhancement step, we introduce a novel Depth-guided Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and texture details with the inpainting diffusion prior, beyond the scope of the 3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only provides fine-grained texture enhancement, but also urges optimization to respect scene lighting. Our approach efficiently achieves local editing with global illumination consistency without explicitly modeling light transport. We demonstrate robustness of our method by evaluating editing in real scenes containing explicit highlight and shadows, and compare against the state-of-the-art text-to-3D editing methods.",
    "arxiv_url": "http://arxiv.org/abs/2408.00083v1",
    "pdf_url": "http://arxiv.org/pdf/2408.00083v1",
    "published_date": "2024-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "efficient",
      "lighting",
      "global illumination",
      "shadow",
      "geometry",
      "3d gaussian",
      "ar",
      "light transport",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Expressive Whole-Body 3D Gaussian Avatar",
    "authors": [
      "Gyeongsik Moon",
      "Takaaki Shiratori",
      "Shunsuke Saito"
    ],
    "abstract": "Facial expression and hand motions are necessary to express our emotions and interact with the world. Nevertheless, most of the 3D human avatars modeled from a casually captured video only support body motions without facial expressions and hand motions.In this work, we present ExAvatar, an expressive whole-body 3D human avatar learned from a short monocular video. We design ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and 3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of facial expressions and poses in the video and 2) the absence of 3D observations, such as 3D scans and RGBD images. The limited diversity in the video makes animations with novel facial expressions and poses non-trivial. In addition, the absence of 3D observations could cause significant ambiguity in human parts that are not observed in the video, which can result in noticeable artifacts under novel motions. To address them, we introduce our hybrid representation of the mesh and 3D Gaussians. Our hybrid representation treats each 3D Gaussian as a vertex on the surface with pre-defined connectivity information (i.e., triangle faces) between them following the mesh topology of SMPL-X. It makes our ExAvatar animatable with novel facial expressions by driven by the facial expression space of SMPL-X. In addition, by using connectivity-based regularizers, we significantly reduce artifacts in novel facial expressions and poses.",
    "arxiv_url": "http://arxiv.org/abs/2407.21686v1",
    "pdf_url": "http://arxiv.org/pdf/2407.21686v1",
    "published_date": "2024-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneTeller: Language-to-3D Scene Generation",
    "authors": [
      "Ba≈üak Melis √ñcal",
      "Maxim Tatarchenko",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "abstract": "Designing high-quality indoor 3D scenes is important in many practical applications, such as room planning or game development. Conventionally, this has been a time-consuming process which requires both artistic skill and familiarity with professional software, making it hardly accessible for layman users. However, recent advances in generative AI have established solid foundation for democratizing 3D design. In this paper, we propose a pioneering approach for text-based 3D room design. Given a prompt in natural language describing the object placement in the room, our method produces a high-quality 3D scene corresponding to it. With an additional text prompt the users can change the appearance of the entire scene or of individual objects in it. Built using in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based stylization, our turnkey pipeline produces state-of-the-art 3D scenes, while being easy to use even for novices. Our project page is available at https://sceneteller.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2407.20727v1",
    "pdf_url": "http://arxiv.org/pdf/2407.20727v1",
    "published_date": "2024-07-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
    "authors": [
      "Yuanwen Yue",
      "Anurag Das",
      "Francis Engelmann",
      "Siyu Tang",
      "Jan Eric Lenssen"
    ],
    "abstract": "Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: https://ywyue.github.io/FiT3D.",
    "arxiv_url": "http://arxiv.org/abs/2407.20229v1",
    "pdf_url": "http://arxiv.org/pdf/2407.20229v1",
    "published_date": "2024-07-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Registering Neural 4D Gaussians for Endoscopic Surgery",
    "authors": [
      "Yiming Huang",
      "Beilei Cui",
      "Ikemura Kei",
      "Jiekai Zhang",
      "Long Bai",
      "Hongliang Ren"
    ],
    "abstract": "The recent advance in neural rendering has enabled the ability to reconstruct high-quality 4D scenes using neural networks. Although 4D neural reconstruction is popular, registration for such representations remains a challenging task, especially for dynamic scene registration in surgical planning and simulation. In this paper, we propose a novel strategy for dynamic surgical neural scene registration. We first utilize 4D Gaussian Splatting to represent the surgical scene and capture both static and dynamic scenes effectively. Then, a spatial aware feature aggregation method, Spatially Weight Cluttering (SWC) is proposed to accurately align the feature between surgical scenes, enabling precise and realistic surgical simulations. Lastly, we present a novel strategy of deformable scene registration to register two dynamic scenes. By incorporating both spatial and temporal information for correspondence matching, our approach achieves superior performance compared to existing registration methods for implicit neural representation. The proposed method has the potential to improve surgical planning and training, ultimately leading to better patient outcomes.",
    "arxiv_url": "http://arxiv.org/abs/2407.20213v1",
    "pdf_url": "http://arxiv.org/pdf/2407.20213v1",
    "published_date": "2024-07-29",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiance Fields for Robotic Teleoperation",
    "authors": [
      "Maximum Wilder-Smith",
      "Vaishakh Patil",
      "Marco Hutter"
    ],
    "abstract": "Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. For videos and code, check out https://rffr.leggedrobotics.com/works/teleoperation/.",
    "arxiv_url": "http://arxiv.org/abs/2407.20194v2",
    "pdf_url": "http://arxiv.org/pdf/2407.20194v2",
    "published_date": "2024-07-29",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "head",
      "high-fidelity",
      "vr",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian Splatting",
    "authors": [
      "Shen Chen",
      "Jiale Zhou",
      "Zhongyu Jiang",
      "Tianfang Zhang",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "abstract": "The creation of high-quality 3D assets is paramount for applications in digital heritage preservation, entertainment, and robotics. Traditionally, this process necessitates skilled professionals and specialized software for the modeling, texturing, and rendering of 3D objects. However, the rising demand for 3D assets in gaming and virtual reality (VR) has led to the creation of accessible image-to-3D technologies, allowing non-professionals to produce 3D content and decreasing dependence on expert input. Existing methods for 3D content generation struggle to simultaneously achieve detailed textures and strong geometric consistency. We introduce a novel 3D content creation framework, ScalingGaussian, which combines 3D and 2D diffusion models to achieve detailed textures and geometric consistency in generated 3D assets. Initially, a 3D diffusion model generates point clouds, which are then densified through a process of selecting local regions, introducing Gaussian noise, followed by using local density-weighted selection. To refine the 3D gaussians, we utilize a 2D diffusion model with Score Distillation Sampling (SDS) loss, guiding the 3D Gaussians to clone and split. Finally, the 3D Gaussians are converted into meshes, and the surface textures are optimized using Mean Square Error(MSE) and Gradient Profile Prior(GPP) losses. Our method addresses the common issue of sparse point clouds in 3D diffusion, resulting in improved geometric structure and detailed textures. Experiments on image-to-3D tasks demonstrate that our approach efficiently generates high-quality 3D assets.",
    "arxiv_url": "http://arxiv.org/abs/2407.19035v1",
    "pdf_url": "http://arxiv.org/pdf/2407.19035v1",
    "published_date": "2024-07-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "vr",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution",
    "authors": [
      "Jintong Hu",
      "Bin Xia",
      "Bin Chen",
      "Wenming Yang",
      "Lei Zhang"
    ],
    "abstract": "Implicit neural representations (INRs) have significantly advanced the field of arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based ASSR networks first extract features from the given low-resolution image using an encoder, and then render the super-resolved result via a multi-layer perceptron decoder. Although these approaches have shown promising results, their performance is constrained by the limited representation ability of discrete latent codes in the encoded features. In this paper, we propose a novel ASSR method named GaussianSR that overcomes this limitation through 2D Gaussian Splatting (2DGS). Unlike traditional methods that treat pixels as discrete points, GaussianSR represents each pixel as a continuous Gaussian field. The encoded features are simultaneously refined and upsampled by rendering the mutually stacked Gaussian fields. As a result, long-range dependencies are established to enhance representation ability. In addition, a classifier is developed to dynamically assign Gaussian kernels to all pixels to further improve flexibility. All components of GaussianSR (i.e., encoder, classifier, Gaussian kernels, and decoder) are jointly learned end-to-end. Experiments demonstrate that GaussianSR achieves superior ASSR performance with fewer parameters than existing methods while enjoying interpretable and content-aware feature aggregations.",
    "arxiv_url": "http://arxiv.org/abs/2407.18046v1",
    "pdf_url": "http://arxiv.org/pdf/2407.18046v1",
    "published_date": "2024-07-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities",
    "authors": [
      "Yanqi Bao",
      "Tianyu Ding",
      "Jing Huo",
      "Yaoli Liu",
      "Yuxin Li",
      "Wenbin Li",
      "Yang Gao",
      "Jiebo Luo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the potential to become a mainstream method for 3D representations. It can effectively transform multi-view images into explicit 3D Gaussian through efficient training, and achieve real-time rendering of novel views. This survey aims to analyze existing 3DGS-related works from multiple intersecting perspectives, including related tasks, technologies, challenges, and opportunities. The primary objective is to provide newcomers with a rapid understanding of the field and to assist researchers in methodically organizing existing technologies and challenges. Specifically, we delve into the optimization, application, and extension of 3DGS, categorizing them based on their focuses or motivations. Additionally, we summarize and classify nine types of technical modules and corresponding improvements identified in existing works. Based on these analyses, we further examine the common challenges and technologies across various tasks, proposing potential research opportunities.",
    "arxiv_url": "http://arxiv.org/abs/2407.17418v2",
    "pdf_url": "http://arxiv.org/pdf/2407.17418v2",
    "published_date": "2024-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene",
    "authors": [
      "Xi Shi",
      "Lingli Chen",
      "Peng Wei",
      "Xi Wu",
      "Tian Jiang",
      "Yonggang Luo",
      "Lecheng Xie"
    ],
    "abstract": "Existing Gaussian splatting methods often fall short in achieving satisfactory novel view synthesis in driving scenes, primarily due to the absence of crafty designs and geometric constraints for the involved elements. This paper introduces a novel neural rendering method termed Decoupled Hybrid Gaussian Splatting (DHGS), targeting at promoting the rendering quality of novel view synthesis for static driving scenes. The novelty of this work lies in the decoupled and hybrid pixel-level blender for road and non-road layers, without the conventional unified differentiable rendering logic for the entire scene. Still, consistency and continuity in superimposition are preserved through the proposed depth-ordered hybrid rendering strategy. Additionally, an implicit road representation comprised of a Signed Distance Function (SDF) is trained to supervise the road surface with subtle geometric attributes. Accompanied by the use of auxiliary transmittance loss and consistency loss, novel images with imperceptible boundary and elevated fidelity are ultimately obtained. Substantial experiments on the Waymo dataset prove that DHGS outperforms the state-of-the-art methods. The project page where more video evidences are given is: https://ironbrotherstyle.github.io/dhgs_web.",
    "arxiv_url": "http://arxiv.org/abs/2407.16600v3",
    "pdf_url": "http://arxiv.org/pdf/2407.16600v3",
    "published_date": "2024-07-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene Reconstruction from Raw Images",
    "authors": [
      "Shreyas Singh",
      "Aryan Garg",
      "Kaushik Mitra"
    ],
    "abstract": "The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D scene reconstruction space enabling high-fidelity novel view synthesis in real-time. However, with the exception of RawNeRF, all prior 3DGS and NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for scene reconstruction. Such methods struggle to achieve accurate reconstructions in scenes that require a higher dynamic range. Examples include scenes captured in nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as well as daylight scenes with shadow regions exhibiting extreme contrast. Our proposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw images in near darkness which preserves the scenes' full dynamic range and content. Our key contributions are two-fold: Firstly, we propose a linear HDR space-suited loss that effectively extracts scene information from noisy dark regions and nearly saturated bright regions simultaneously, while also handling view-dependent colors without increasing the degree of spherical harmonics. Secondly, through careful rasterization tuning, we implicitly overcome the heavy reliance and sensitivity of 3DGS on point cloud initialization. This is critical for accurate reconstruction in regions of low texture, high depth of field, and low illumination. HDRSplat is the fastest method to date that does 14-bit (HDR) 3D scene reconstruction in $\\le$15 minutes/scene ($\\sim$30x faster than prior state-of-the-art RawNeRF). It also boasts the fastest inference speed at $\\ge$120fps. We further demonstrate the applicability of our HDR scene reconstruction by showcasing various applications like synthetic defocus, dense depth map extraction, and post-capture control of exposure, tone-mapping and view-point.",
    "arxiv_url": "http://arxiv.org/abs/2407.16503v1",
    "pdf_url": "http://arxiv.org/pdf/2407.16503v1",
    "published_date": "2024-07-23",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "high-fidelity",
      "shadow",
      "fast",
      "mapping",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with SAM Mask Guidance",
    "authors": [
      "Jiyeop Kim",
      "Jongwoo Lim"
    ],
    "abstract": "We present a novel approach for 3D indoor scene reconstruction that combines 3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the room layout of the indoor scene, such as walls, ceilings, and floors, while employing 3D Gaussians for other objects. This hybrid approach leverages the strengths of both representations, offering enhanced flexibility and ease of editing. However, joint training of meshes and 3D Gaussians is challenging because it is not clear which primitive should affect which part of the rendered image. Objects close to the room layout often struggle during training, particularly when the room layout is textureless, which can lead to incorrect optimizations and unnecessary 3D Gaussians. To overcome these challenges, we employ Segment Anything Model (SAM) to guide the selection of primitives. The SAM mask loss enforces each instance to be represented by either Gaussians or meshes, ensuring clear separation and stable training. Furthermore, we introduce an additional densification stage without resetting the opacity after the standard densification. This stage mitigates the degradation of image quality caused by a limited number of 3D Gaussians after the standard densification.",
    "arxiv_url": "http://arxiv.org/abs/2407.16173v1",
    "pdf_url": "http://arxiv.org/pdf/2407.16173v1",
    "published_date": "2024-07-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting Model",
    "authors": [
      "Matteo Bortolon",
      "Theodore Tsesmelis",
      "Stuart James",
      "Fabio Poiesi",
      "Alessio Del Bue"
    ],
    "abstract": "We propose 6DGS to estimate the camera pose of a target RGB image given a 3D Gaussian Splatting (3DGS) model representing the scene. 6DGS avoids the iterative process typical of analysis-by-synthesis methods (e.g. iNeRF) that also require an initialization of the camera pose in order to converge. Instead, our method estimates a 6DoF pose by inverting the 3DGS rendering process. Starting from the object surface, we define a radiant Ellicell that uniformly generates rays departing from each ellipsoid that parameterize the 3DGS model. Each Ellicell ray is associated with the rendering parameters of each ellipsoid, which in turn is used to obtain the best bindings between the target image pixels and the cast rays. These pixel-ray bindings are then ranked to select the best scoring bundle of rays, which their intersection provides the camera center and, in turn, the camera rotation. The proposed solution obviates the necessity of an \"a priori\" pose for initialization, and it solves 6DoF pose estimation in closed form, without the need for iterations. Moreover, compared to the existing Novel View Synthesis (NVS) baselines for pose estimation, 6DGS can improve the overall average rotational accuracy by 12% and translation accuracy by 22% on real scenes, despite not requiring any initialization pose. At the same time, our method operates near real-time, reaching 15fps on consumer hardware.",
    "arxiv_url": "http://arxiv.org/abs/2407.15484v1",
    "pdf_url": "http://arxiv.org/pdf/2407.15484v1",
    "published_date": "2024-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancement of 3D Gaussian Splatting using Raw Mesh for Photorealistic Recreation of Architectures",
    "authors": [
      "Ruizhe Wang",
      "Chunliang Hua",
      "Tomakayev Shingys",
      "Mengyuan Niu",
      "Qingxin Yang",
      "Lizhong Gao",
      "Yi Zheng",
      "Junyan Yang",
      "Qiao Wang"
    ],
    "abstract": "The photorealistic reconstruction and rendering of architectural scenes have extensive applications in industries such as film, games, and transportation. It also plays an important role in urban planning, architectural design, and the city's promotion, especially in protecting historical and cultural relics. The 3D Gaussian Splatting, due to better performance over NeRF, has become a mainstream technology in 3D reconstruction. Its only input is a set of images but it relies heavily on geometric parameters computed by the SfM process. At the same time, there is an existing abundance of raw 3D models, that could inform the structural perception of certain buildings but cannot be applied. In this paper, we propose a straightforward method to harness these raw 3D models to guide 3D Gaussians in capturing the basic shape of the building and improve the visual quality of textures and details when photos are captured non-systematically. This exploration opens up new possibilities for improving the effectiveness of 3D reconstruction techniques in the field of architectural design.",
    "arxiv_url": "http://arxiv.org/abs/2407.15435v2",
    "pdf_url": "http://arxiv.org/pdf/2407.15435v2",
    "published_date": "2024-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions",
    "authors": [
      "Haiyang Zhou",
      "Xinhua Cheng",
      "Wangbo Yu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "3D scene generation is in high demand across various domains, including virtual reality, gaming, and the film industry. Owing to the powerful generative capabilities of text-to-image diffusion models that provide reliable priors, the creation of 3D scenes using only text prompts has become viable, thereby significantly advancing researches in text-driven 3D scene generation. In order to obtain multiple-view supervision from 2D diffusion models, prevailing methods typically employ the diffusion model to generate an initial local image, followed by iteratively outpainting the local image using diffusion models to gradually generate scenes. Nevertheless, these outpainting-based approaches prone to produce global inconsistent scene generation results without high degree of completeness, restricting their broader applications. To tackle these problems, we introduce HoloDreamer, a framework that first generates high-definition panorama as a holistic initialization of the full 3D scene, then leverage 3D Gaussian Splatting (3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation of view-consistent and fully enclosed 3D scenes. Specifically, we propose Stylized Equirectangular Panorama Generation, a pipeline that combines multiple diffusion models to enable stylized and detailed equirectangular panorama generation from complex text prompts. Subsequently, Enhanced Two-Stage Panorama Reconstruction is introduced, conducting a two-stage optimization of 3D-GS to inpaint the missing region and enhance the integrity of the scene. Comprehensive experiments demonstrated that our method outperforms prior works in terms of overall visual consistency and harmony as well as reconstruction quality and rendering robustness when generating fully enclosed scenes.",
    "arxiv_url": "http://arxiv.org/abs/2407.15187v1",
    "pdf_url": "http://arxiv.org/pdf/2407.15187v1",
    "published_date": "2024-07-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GPHM: Gaussian Parametric Head Model for Monocular Head Avatar Reconstruction",
    "authors": [
      "Yuelang Xu",
      "Zhaoqi Su",
      "Qingyao Wu",
      "Yebin Liu"
    ],
    "abstract": "Creating high-fidelity 3D human head avatars is crucial for applications in VR/AR, digital human, and film production. Recent advances have leveraged morphable face models to generate animated head avatars from easily accessible data, representing varying identities and expressions within a low-dimensional parametric space. However, existing methods often struggle with modeling complex appearance details, e.g., hairstyles, and suffer from low rendering quality and efficiency. In this paper we introduce a novel approach, 3D Gaussian Parametric Head Model, which employs 3D Gaussians to accurately represent the complexities of the human head, allowing precise control over both identity and expression. The Gaussian model can handle intricate details, enabling realistic representations of varying appearances and complex expressions. Furthermore, we presents a well-designed training framework to ensure smooth convergence, providing a robust guarantee for learning the rich content. Our method achieves high-quality, photo-realistic rendering with real-time efficiency, making it a valuable contribution to the field of parametric head models. Finally, we apply the 3D Gaussian Parametric Head Model to monocular video or few-shot head avatar reconstruction tasks, which enables instant reconstruction of high-quality 3D head avatars even when input data is extremely limited, surpassing previous methods in terms of reconstruction quality and training speed.",
    "arxiv_url": "http://arxiv.org/abs/2407.15070v2",
    "pdf_url": "http://arxiv.org/pdf/2407.15070v2",
    "published_date": "2024-07-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "vr",
      "few-shot",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Realistic Surgical Image Dataset Generation Based On 3D Gaussian Splatting",
    "authors": [
      "Tianle Zeng",
      "Gerardo Loza Galindo",
      "Junlei Hu",
      "Pietro Valdastri",
      "Dominic Jones"
    ],
    "abstract": "Computer vision technologies markedly enhance the automation capabilities of robotic-assisted minimally invasive surgery (RAMIS) through advanced tool tracking, detection, and localization. However, the limited availability of comprehensive surgical datasets for training represents a significant challenge in this field. This research introduces a novel method that employs 3D Gaussian Splatting to generate synthetic surgical datasets. We propose a method for extracting and combining 3D Gaussian representations of surgical instruments and background operating environments, transforming and combining them to generate high-fidelity synthetic surgical scenarios. We developed a data recording system capable of acquiring images alongside tool and camera poses in a surgical scene. Using this pose data, we synthetically replicate the scene, thereby enabling direct comparisons of the synthetic image quality (29.592 PSNR). As a further validation, we compared two YOLOv5 models trained on the synthetic and real data, respectively, and assessed their performance in an unseen real-world test dataset. Comparing the performances, we observe an improvement in neural network performance, with the synthetic-trained model outperforming the real-world trained model by 12%, testing both on real-world data.",
    "arxiv_url": "http://arxiv.org/abs/2407.14846v1",
    "pdf_url": "http://arxiv.org/pdf/2407.14846v1",
    "published_date": "2024-07-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "high-fidelity",
      "tracking",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Benchmark for Gaussian Splatting Compression and Quality Assessment Study",
    "authors": [
      "Qi Yang",
      "Kaifa Yang",
      "Yuke Xing",
      "Yiling Xu",
      "Zhu Li"
    ],
    "abstract": "To fill the gap of traditional GS compression method, in this paper, we first propose a simple and effective GS data compression anchor called Graph-based GS Compression (GGSC). GGSC is inspired by graph signal processing theory and uses two branches to compress the primitive center and attributes. We split the whole GS sample via KDTree and clip the high-frequency components after the graph Fourier transform. Followed by quantization, G-PCC and adaptive arithmetic coding are used to compress the primitive center and attribute residual matrix to generate the bitrate file. GGSS is the first work to explore traditional GS compression, with advantages that can reveal the GS distortion characteristics corresponding to typical compression operation, such as high-frequency clipping and quantization. Second, based on GGSC, we create a GS Quality Assessment dataset (GSQA) with 120 samples. A subjective experiment is conducted in a laboratory environment to collect subjective scores after rendering GS into Processed Video Sequences (PVS). We analyze the characteristics of different GS distortions based on Mean Opinion Scores (MOS), demonstrating the sensitivity of different attributes distortion to visual quality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are made publicly available at https://github.com/Qi-Yangsjtu/GGSC.",
    "arxiv_url": "http://arxiv.org/abs/2407.14197v1",
    "pdf_url": "http://arxiv.org/pdf/2407.14197v1",
    "published_date": "2024-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Qi-Yangsjtu/GGSC",
    "keywords": [
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV Segmentation",
    "authors": [
      "Florian Chabot",
      "Nicolas Granger",
      "Guillaume Lapouge"
    ],
    "abstract": "The Bird's-eye View (BeV) representation is widely used for 3D perception from multi-view camera images. It allows to merge features from different cameras into a common space, providing a unified representation of the 3D scene. The key component is the view transformer, which transforms image views into the BeV. However, actual view transformer methods based on geometry or cross-attention do not provide a sufficiently detailed representation of the scene, as they use a sub-sampling of the 3D space that is non-optimal for modeling the fine structures of the environment. In this paper, we propose GaussianBeV, a novel method for transforming image features to BeV by finely representing the scene using a set of 3D gaussians located and oriented in 3D space. This representation is then splattered to produce the BeV feature map by adapting recent advances in 3D representation rendering based on gaussian splatting. GaussianBeV is the first approach to use this 3D gaussian modeling and 3D scene rendering process online, i.e. without optimizing it on a specific scene and directly integrated into a single stage model for BeV scene understanding. Experiments show that the proposed representation is highly effective and place GaussianBeV as the new state-of-the-art on the BeV semantic segmentation task on the nuScenes dataset.",
    "arxiv_url": "http://arxiv.org/abs/2407.14108v2",
    "pdf_url": "http://arxiv.org/pdf/2407.14108v2",
    "published_date": "2024-07-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DirectL: Efficient Radiance Fields Rendering for 3D Light Field Displays",
    "authors": [
      "Zongyuan Yang",
      "Baolin Liu",
      "Yingde Song",
      "Yongping Xiong",
      "Lan Yi",
      "Zhaohe Zhang",
      "Xunbo Yu"
    ],
    "abstract": "Autostereoscopic display, despite decades of development, has not achieved extensive application, primarily due to the daunting challenge of 3D content creation for non-specialists. The emergence of Radiance Field as an innovative 3D representation has markedly revolutionized the domains of 3D reconstruction and generation. This technology greatly simplifies 3D content creation for common users, broadening the applicability of Light Field Displays (LFDs). However, the combination of these two fields remains largely unexplored. The standard paradigm to create optimal content for parallax-based light field displays demands rendering at least 45 slightly shifted views preferably at high resolution per frame, a substantial hurdle for real-time rendering. We introduce DirectL, a novel rendering paradigm for Radiance Fields on 3D displays. We thoroughly analyze the interweaved mapping of spatial rays to screen subpixels, precisely determine the light rays entering the human eye, and propose subpixel repurposing to significantly reduce the pixel count required for rendering. Tailored for the two predominant radiance fields--Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we propose corresponding optimized rendering pipelines that directly render the light field images instead of multi-view images. Extensive experiments across various displays and user study demonstrate that DirectL accelerates rendering by up to 40 times compared to the standard paradigm without sacrificing visual quality. Its rendering process-only modification allows seamless integration into subsequent radiance field tasks. Finally, we integrate DirectL into diverse applications, showcasing the stunning visual experiences and the synergy between LFDs and Radiance Fields, which unveils tremendous potential for commercialization applications. \\href{direct-l.github.io}{\\textbf{Project Homepage}",
    "arxiv_url": "http://arxiv.org/abs/2407.14053v1",
    "pdf_url": "http://arxiv.org/pdf/2407.14053v1",
    "published_date": "2024-07-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PlacidDreamer: Advancing Harmony in Text-to-3D Generation",
    "authors": [
      "Shuo Huang",
      "Shikun Sun",
      "Zixuan Wang",
      "Xiaoyu Qin",
      "Yanmin Xiong",
      "Yuan Zhang",
      "Pengfei Wan",
      "Di Zhang",
      "Jia Jia"
    ],
    "abstract": "Recently, text-to-3D generation has attracted significant attention, resulting in notable performance enhancements. Previous methods utilize end-to-end 3D generation models to initialize 3D Gaussians, multi-view diffusion models to enforce multi-view consistency, and text-to-image diffusion models to refine details with score distillation algorithms. However, these methods exhibit two limitations. Firstly, they encounter conflicts in generation directions since different models aim to produce diverse 3D assets. Secondly, the issue of over-saturation in score distillation has not been thoroughly investigated and solved. To address these limitations, we propose PlacidDreamer, a text-to-3D framework that harmonizes initialization, multi-view generation, and text-conditioned generation with a single multi-view diffusion model, while simultaneously employing a novel score distillation algorithm to achieve balanced saturation. To unify the generation direction, we introduce the Latent-Plane module, a training-friendly plug-in extension that enables multi-view diffusion models to provide fast geometry reconstruction for initialization and enhanced multi-view images to personalize the text-to-image diffusion model. To address the over-saturation problem, we propose to view score distillation as a multi-objective optimization problem and introduce the Balanced Score Distillation algorithm, which offers a Pareto Optimal solution that achieves both rich details and balanced saturation. Extensive experiments validate the outstanding capabilities of our PlacidDreamer. The code is available at \\url{https://github.com/HansenHuang0823/PlacidDreamer}.",
    "arxiv_url": "http://arxiv.org/abs/2407.13976v1",
    "pdf_url": "http://arxiv.org/pdf/2407.13976v1",
    "published_date": "2024-07-19",
    "categories": [
      "cs.CV",
      "I.4.0"
    ],
    "github_url": "https://github.com/HansenHuang0823/PlacidDreamer",
    "keywords": [
      "geometry",
      "3d gaussian",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Connecting Consistency Distillation to Score Distillation for Text-to-3D Generation",
    "authors": [
      "Zongrui Li",
      "Minghui Hu",
      "Qian Zheng",
      "Xudong Jiang"
    ],
    "abstract": "Although recent advancements in text-to-3D generation have significantly improved generation quality, issues like limited level of detail and low fidelity still persist, which requires further improvement. To understand the essence of those issues, we thoroughly analyze current score distillation methods by connecting theories of consistency distillation to score distillation. Based on the insights acquired through analysis, we propose an optimization framework, Guided Consistency Sampling (GCS), integrated with 3D Gaussian Splatting (3DGS) to alleviate those issues. Additionally, we have observed the persistent oversaturation in the rendered views of generated 3D assets. From experiments, we find that it is caused by unwanted accumulated brightness in 3DGS during optimization. To mitigate this issue, we introduce a Brightness-Equalized Generation (BEG) scheme in 3DGS rendering. Experimental results demonstrate that our approach generates 3D assets with more details and higher fidelity than state-of-the-art methods. The codes are released at https://github.com/LMozart/ECCV2024-GCS-BEG.",
    "arxiv_url": "http://arxiv.org/abs/2407.13584v2",
    "pdf_url": "http://arxiv.org/pdf/2407.13584v2",
    "published_date": "2024-07-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/LMozart/ECCV2024-GCS-BEG",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting",
    "authors": [
      "Yuchen Weng",
      "Zhengwen Shen",
      "Ruofan Chen",
      "Qi Wang",
      "Jun Wang"
    ],
    "abstract": "3D deblurring reconstruction techniques have recently seen significant advancements with the development of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these techniques can recover relatively clear 3D reconstructions from blurry image inputs, they still face limitations in handling severe blurring and complex camera motion. To address these issues, we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which integrates event camera data to enhance the robustness of 3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and using novel loss functions, EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating performance comparable to state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2407.13520v3",
    "pdf_url": "http://arxiv.org/pdf/2407.13520v3",
    "published_date": "2024-07-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generalizable Human Gaussians for Sparse View Synthesis",
    "authors": [
      "Youngjoong Kwon",
      "Baole Fang",
      "Yixing Lu",
      "Haoye Dong",
      "Cheng Zhang",
      "Francisco Vicente Carrasco",
      "Albert Mosella-Montoro",
      "Jianjin Xu",
      "Shingo Takagi",
      "Daeil Kim",
      "Aayush Prakash",
      "Fernando De la Torre"
    ],
    "abstract": "Recent progress in neural rendering has brought forth pioneering methods, such as NeRF and Gaussian Splatting, which revolutionize view rendering across various domains like AR/VR, gaming, and content creation. While these methods excel at interpolating {\\em within the training data}, the challenge of generalizing to new scenes and objects from very sparse views persists. Specifically, modeling 3D humans from sparse views presents formidable hurdles due to the inherent complexity of human geometry, resulting in inaccurate reconstructions of geometry and textures. To tackle this challenge, this paper leverages recent advancements in Gaussian Splatting and introduces a new method to learn generalizable human Gaussians that allows photorealistic and accurate view-rendering of a new human subject from a limited set of sparse views in a feed-forward manner. A pivotal innovation of our approach involves reformulating the learning of 3D Gaussian parameters into a regression process defined on the 2D UV space of a human template, which allows leveraging the strong geometry prior and the advantages of 2D convolutions. In addition, a multi-scaffold is proposed to effectively represent the offset details. Our method outperforms recent methods on both within-dataset generalization as well as cross-dataset generalization settings.",
    "arxiv_url": "http://arxiv.org/abs/2407.12777v1",
    "pdf_url": "http://arxiv.org/pdf/2407.12777v1",
    "published_date": "2024-07-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "vr",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for Unconstrained Photo Collections",
    "authors": [
      "Congrong Xu",
      "Justin Kerr",
      "Angjoo Kanazawa"
    ],
    "abstract": "Novel view synthesis from unconstrained in-the-wild image collections remains a significant yet challenging task due to photometric variations and transient occluders that complicate accurate scene reconstruction. Previous methods have approached these issues by integrating per-image appearance features embeddings in Neural Radiance Fields (NeRFs). Although 3D Gaussian Splatting (3DGS) offers faster training and real-time rendering, adapting it for unconstrained image collections is non-trivial due to the substantially different architecture. In this paper, we introduce Splatfacto-W, an approach that integrates per-Gaussian neural color features and per-image appearance embeddings into the rasterization process, along with a spherical harmonics-based background model to represent varying photometric appearances and better depict backgrounds. Our key contributions include latent appearance modeling, efficient transient object handling, and precise background modeling. Splatfacto-W delivers high-quality, real-time novel view synthesis with improved scene consistency in in-the-wild scenarios. Our method improves the Peak Signal-to-Noise Ratio (PSNR) by an average of 5.3 dB compared to 3DGS, enhances training speed by 150 times compared to NeRF-based methods, and achieves a similar rendering speed to 3DGS. Additional video results and code integrated into Nerfstudio are available at https://kevinxu02.github.io/splatfactow/.",
    "arxiv_url": "http://arxiv.org/abs/2407.12306v2",
    "pdf_url": "http://arxiv.org/pdf/2407.12306v2",
    "published_date": "2024-07-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive Quantile-Based Geometric Consistency Densification",
    "authors": [
      "Zhuoxiao Li",
      "Shanliang Yao",
      "Yijie Chu",
      "Angel F. Garcia-Fernandez",
      "Yong Yue",
      "Eng Gee Lim",
      "Xiaohui Zhu"
    ],
    "abstract": "In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting (3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements. Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to effectively enhance mesh extraction quality, this compression can potentially lead to a decrease in rendering quality. Additionally, unreliable densification processes and the calculation of depth through the accumulation of opacity can compromise the detail of mesh extraction. To address this issue, we introduce MVG-Splatting, a solution guided by Multi-View considerations. Specifically, we integrate an optimized method for calculating normals, which, combined with image gradients, helps rectify inconsistencies in the original depth computations. Additionally, utilizing projection strategies akin to those in Multi-View Stereo (MVS), we propose an adaptive quantile-based method that dynamically determines the level of additional densification guided by depth maps, from coarse to fine detail. Experimental evidence demonstrates that our method not only resolves the issues of rendering quality degradation caused by depth discrepancies but also facilitates direct mesh extraction from dense Gaussian point clouds using the Marching Cubes algorithm. This approach significantly enhances the overall fidelity and accuracy of the 3D reconstruction process, ensuring that both the geometric details and visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2407.11840v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11840v1",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
    "authors": [
      "Seokhun Choi",
      "Hyeonseop Song",
      "Jaechul Kim",
      "Taehyeong Kim",
      "Hoseok Do"
    ],
    "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy. Our project page is available at https://seokhunchoi.github.io/Click-Gaussian",
    "arxiv_url": "http://arxiv.org/abs/2407.11793v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11793v1",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation",
    "authors": [
      "Jiwook Kim",
      "Seonho Lee",
      "Jaeyo Shin",
      "Jiho Choi",
      "Hyunjung Shim"
    ],
    "abstract": "Score distillation sampling (SDS) has emerged as an effective framework in text-driven 3D editing tasks, leveraging diffusion models for 3D-consistent editing. However, existing SDS-based 3D editing methods suffer from long training times and produce low-quality results. We identify that the root cause of this performance degradation is \\textit{their conflict with the sampling dynamics of diffusion models}. Addressing this conflict allows us to treat SDS as a diffusion reverse process for 3D editing via sampling from data space. In contrast, existing methods naively distill the score function using diffusion models. From these insights, we propose DreamCatalyst, a novel framework that considers these sampling dynamics in the SDS framework. Specifically, we devise the optimization process of our DreamCatalyst to approximate the diffusion reverse process in editing tasks, thereby aligning with diffusion sampling dynamics. As a result, DreamCatalyst successfully reduces training time and improves editing quality. Our method offers two modes: (1) a fast mode that edits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than current state-of-the-art NeRF editing methods, and (2) a high-quality mode that produces superior results about 8 times faster than these methods. Notably, our high-quality mode outperforms current state-of-the-art NeRF editing methods in terms of both speed and quality. DreamCatalyst also surpasses the state-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing itself as an effective and model-agnostic 3D editing solution. See more extensive results on our project page: https://dream-catalyst.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2407.11394v3",
    "pdf_url": "http://arxiv.org/pdf/2407.11394v3",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "I$^2$-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM",
    "authors": [
      "Gwangtak Bae",
      "Changwoon Choi",
      "Hyeongjun Heo",
      "Sang Min Kim",
      "Young Min Kim"
    ],
    "abstract": "We present an inverse image-formation module that can enhance the robustness of existing visual SLAM pipelines for casually captured scenarios. Casual video captures often suffer from motion blur and varying appearances, which degrade the final quality of coherent 3D visual representation. We propose integrating the physical imaging into the SLAM system, which employs linear HDR radiance maps to collect measurements. Specifically, individual frames aggregate images of multiple poses along the camera trajectory to explain prevalent motion blur in hand-held videos. Additionally, we accommodate per-frame appearance variation by dedicating explicit variables for image formation steps, namely white balance, exposure time, and camera response function. Through joint optimization of additional variables, the SLAM pipeline produces high-quality images with more accurate trajectories. Extensive experiments demonstrate that our approach can be incorporated into recent visual SLAM pipelines using various scene representations, such as neural radiance fields or Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2407.11347v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11347v1",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "slam",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ev-GS: Event-based Gaussian splatting for Efficient and Accurate Radiance Field Rendering",
    "authors": [
      "Jingqian Wu",
      "Shuo Zhu",
      "Chutian Wang",
      "Edmund Y. Lam"
    ],
    "abstract": "Computational neuromorphic imaging (CNI) with event cameras offers advantages such as minimal motion blur and enhanced dynamic range, compared to conventional frame-based methods. Existing event-based radiance field rendering methods are built on neural radiance field, which is computationally heavy and slow in reconstruction speed. Motivated by the two aspects, we introduce Ev-GS, the first CNI-informed scheme to infer 3D Gaussian splatting from a monocular event camera, enabling efficient novel view synthesis. Leveraging 3D Gaussians with pure event-based supervision, Ev-GS overcomes challenges such as the detection of fast-moving objects and insufficient lighting. Experimental results show that Ev-GS outperforms the method that takes frame-based signals as input by rendering realistic views with reduced blurring and improved visual quality. Moreover, it demonstrates competitive reconstruction quality and reduced computing occupancy compared to existing methods, which paves the way to a highly efficient CNI approach for signal processing.",
    "arxiv_url": "http://arxiv.org/abs/2407.11343v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11343v1",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "fast",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Lucas-Kanade",
    "authors": [
      "Liuyue Xie",
      "Joel Julin",
      "Koichiro Niinuma",
      "Laszlo A. Jeni"
    ],
    "abstract": "Gaussian Splatting and its dynamic extensions are effective for reconstructing 3D scenes from 2D images when there is significant camera movement to facilitate motion parallax and when scene objects remain relatively static. However, in many real-world scenarios, these conditions are not met. As a consequence, data-driven semantic and geometric priors have been favored as regularizers, despite their bias toward training data and their neglect of broader movement dynamics.   Departing from this practice, we propose a novel analytical approach that adapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By leveraging the intrinsic properties of the forward warp field network, we derive an analytical velocity field that, through time integration, facilitates accurate scene flow computation. This enables the precise enforcement of motion constraints on warp fields, thus constraining both 2D motion and 3D positions of the Gaussians. Our method excels in reconstructing highly dynamic scenes with minimal camera movement, as demonstrated through experiments on both synthetic and real-world scenes.",
    "arxiv_url": "http://arxiv.org/abs/2407.11309v2",
    "pdf_url": "http://arxiv.org/pdf/2407.11309v2",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
    "authors": [
      "Pramish Paudel",
      "Anubhav Khanal",
      "Ajad Chhatkuli",
      "Danda Pani Paudel",
      "Jyoti Tandukar"
    ],
    "abstract": "Personalized 3D avatars require an animatable representation of digital humans. Doing so instantly from monocular videos offers scalability to broad class of users and wide-scale applications. In this paper, we present a fast, simple, yet effective method for creating animatable 3D digital humans from monocular videos. Our method utilizes the efficiency of Gaussian splatting to model both 3D geometry and appearance. However, we observed that naively optimizing Gaussian splats results in inaccurate geometry, thereby leading to poor animations. This work achieves and illustrates the need of accurate 3D mesh-type modelling of the human body for animatable digitization through Gaussian splats. This is achieved by developing a novel pipeline that benefits from three key aspects: (a) implicit modelling of surface's displacements and the color's spherical harmonics; (b) binding of 3D Gaussians to the respective triangular faces of the body template; (c) a novel technique to render normals followed by their auxiliary supervision. Our exhaustive experiments on three different benchmark datasets demonstrates the state-of-the-art results of our method, in limited time settings. In fact, our method is faster by an order of magnitude (in terms of training time) than its closest competitor. At the same time, we achieve superior rendering and 3D reconstruction performance under the change of poses.",
    "arxiv_url": "http://arxiv.org/abs/2407.11174v1",
    "pdf_url": "http://arxiv.org/pdf/2407.11174v1",
    "published_date": "2024-07-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs",
    "authors": [
      "W. J. Meijer",
      "A. C. Kemmeren",
      "E. H. J. Riemens",
      "J. E. Fransman",
      "M. van Bekkum",
      "G. J. Burghouts",
      "J. D. van Mil"
    ],
    "abstract": "This paper addresses the challenge of scaling Large Multimodal Models (LMMs) to expansive 3D environments. Solving this open problem is especially relevant for robot deployment in many first-responder scenarios, such as search-and-rescue missions that cover vast spaces. The use of LMMs in these settings is currently hampered by the strict context windows that limit the LMM's input size. We therefore introduce a novel approach that utilizes a datagraph structure, which allows the LMM to iteratively query smaller sections of a large environment. Using the datagraph in conjunction with graph traversal algorithms, we can prioritize the most relevant locations to the query, thereby improving the scalability of 3D scene language tasks. We illustrate the datagraph using 3D scenes, but these can be easily substituted by other dense modalities that represent the environment, such as pointclouds or Gaussian splats. We demonstrate the potential to use the datagraph for two 3D scene language task use cases, in a search-and-rescue mission example.",
    "arxiv_url": "http://arxiv.org/abs/2407.10743v1",
    "pdf_url": "http://arxiv.org/pdf/2407.10743v1",
    "published_date": "2024-07-15",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interactive Rendering of Relightable and Animatable Gaussian Avatars",
    "authors": [
      "Youyi Zhan",
      "Tianjia Shao",
      "He Wang",
      "Yin Yang",
      "Kun Zhou"
    ],
    "abstract": "Creating relightable and animatable avatars from multi-view or monocular videos is a challenging task for digital human creation and virtual reality applications. Previous methods rely on neural radiance fields or ray tracing, resulting in slow training and rendering processes. By utilizing Gaussian Splatting, we propose a simple and efficient method to decouple body materials and lighting from sparse-view or monocular avatar videos, so that the avatar can be rendered simultaneously under novel viewpoints, poses, and lightings at interactive frame rates (6.9 fps). Specifically, we first obtain the canonical body mesh using a signed distance function and assign attributes to each mesh vertex. The Gaussians in the canonical space then interpolate from nearby body mesh vertices to obtain the attributes. We subsequently deform the Gaussians to the posed space using forward skinning, and combine the learnable environment light with the Gaussian attributes for shading computation. To achieve fast shadow modeling, we rasterize the posed body mesh from dense viewpoints to obtain the visibility. Our approach is not only simple but also fast enough to allow interactive rendering of avatar animation under environmental light changes. Experiments demonstrate that, compared to previous works, our method can render higher quality results at a faster speed on both synthetic and real datasets.",
    "arxiv_url": "http://arxiv.org/abs/2407.10707v2",
    "pdf_url": "http://arxiv.org/pdf/2407.10707v2",
    "published_date": "2024-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "relightable",
      "ray tracing",
      "lighting",
      "fast",
      "shadow",
      "body",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pathformer3D: A 3D Scanpath Transformer for 360¬∞ Images",
    "authors": [
      "Rong Quan",
      "Yantao Lai",
      "Mengyu Qiu",
      "Dong Liang"
    ],
    "abstract": "Scanpath prediction in 360{\\deg} images can help realize rapid rendering and better user interaction in Virtual/Augmented Reality applications. However, existing scanpath prediction models for 360{\\deg} images execute scanpath prediction on 2D equirectangular projection plane, which always result in big computation error owing to the 2D plane's distortion and coordinate discontinuity. In this work, we perform scanpath prediction for 360{\\deg} images in 3D spherical coordinate system and proposed a novel 3D scanpath Transformer named Pathformer3D. Specifically, a 3D Transformer encoder is first used to extract 3D contextual feature representation for the 360{\\deg} image. Then, the contextual feature representation and historical fixation information are input into a Transformer decoder to output current time step's fixation embedding, where the self-attention module is used to imitate the visual working memory mechanism of human visual system and directly model the time dependencies among the fixations. Finally, a 3D Gaussian distribution is learned from each fixation embedding, from which the fixation position can be sampled. Evaluation on four panoramic eye-tracking datasets demonstrates that Pathformer3D outperforms the current state-of-the-art methods. Code is available at https://github.com/lsztzp/Pathformer3D .",
    "arxiv_url": "http://arxiv.org/abs/2407.10563v1",
    "pdf_url": "http://arxiv.org/pdf/2407.10563v1",
    "published_date": "2024-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/lsztzp/Pathformer3D",
    "keywords": [
      "3d gaussian",
      "human",
      "tracking",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RecGS: Removing Water Caustic with Recurrent Gaussian Splatting",
    "authors": [
      "Tianyi Zhang",
      "Weiming Zhi",
      "Kaining Huang",
      "Joshua Mangelson",
      "Corina Barbalata",
      "Matthew Johnson-Roberson"
    ],
    "abstract": "Water caustics are commonly observed in seafloor imaging data from shallow-water areas. Traditional methods that remove caustic patterns from images often rely on 2D filtering or pre-training on an annotated dataset, hindering the performance when generalizing to real-world seafloor data with 3D structures. In this paper, we present a novel method Recurrent Gaussian Splatting (RecGS), which takes advantage of today's photorealistic 3D reconstruction technology, 3DGS, to separate caustics from seafloor imagery. With a sequence of images taken by an underwater robot, we build 3DGS recurrently and decompose the caustic with low-pass filtering in each iteration. In the experiments, we analyze and compare with different methods, including joint optimization, 2D filtering, and deep learning approaches. The results show that our method can effectively separate the caustic from the seafloor, improving the visual appearance, and can be potentially applied on more problems with inconsistent illumination.",
    "arxiv_url": "http://arxiv.org/abs/2407.10318v2",
    "pdf_url": "http://arxiv.org/pdf/2407.10318v2",
    "published_date": "2024-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DEgo: 3D Editing on the Go!",
    "authors": [
      "Umar Khalid",
      "Hasan Iqbal",
      "Azib Farooq",
      "Jing Hua",
      "Chen Chen"
    ],
    "abstract": "We introduce 3DEgo to address a novel problem of directly synthesizing photorealistic 3D scenes from monocular videos guided by textual prompts. Conventional methods construct a text-conditioned 3D scene through a three-stage process, involving pose estimation using Structure-from-Motion (SfM) libraries like COLMAP, initializing the 3D model with unedited images, and iteratively updating the dataset with edited images to achieve a 3D scene with text fidelity. Our framework streamlines the conventional multi-stage 3D editing process into a single-stage workflow by overcoming the reliance on COLMAP and eliminating the cost of model initialization. We apply a diffusion model to edit video frames prior to 3D scene creation by incorporating our designed noise blender module for enhancing multi-view editing consistency, a step that does not require additional training or fine-tuning of T2I diffusion models. 3DEgo utilizes 3D Gaussian Splatting to create 3D scenes from the multi-view consistent edited frames, capitalizing on the inherent temporal continuity and explicit point cloud data. 3DEgo demonstrates remarkable editing precision, speed, and adaptability across a variety of video sources, as validated by extensive evaluations on six datasets, including our own prepared GS25 dataset. Project Page: https://3dego.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2407.10102v1",
    "pdf_url": "http://arxiv.org/pdf/2407.10102v1",
    "published_date": "2024-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion",
    "authors": [
      "Jiyuan Zhang",
      "Kang Chen",
      "Shiyan Chen",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ],
    "abstract": "Novel View Synthesis plays a crucial role by generating new 2D renderings from multi-view images of 3D scenes. However, capturing high-speed scenes with conventional cameras often leads to motion blur, hindering the effectiveness of 3D reconstruction. To address this challenge, high-frame-rate dense 3D reconstruction emerges as a vital technique, enabling detailed and accurate modeling of real-world objects or scenes in various fields, including Virtual Reality or embodied AI. Spike cameras, a novel type of neuromorphic sensor, continuously record scenes with an ultra-high temporal resolution, showing potential for accurate 3D reconstruction. Despite their promise, existing approaches, such as applying Neural Radiance Fields (NeRF) to spike cameras, encounter challenges due to the time-consuming rendering process. To address this issue, we make the first attempt to introduce the 3D Gaussian Splatting (3DGS) into spike cameras in high-speed capture, providing 3DGS as dense and continuous clues of views, then constructing SpikeGS. Specifically, to train SpikeGS, we establish computational equations between the rendering process of 3DGS and the processes of instantaneous imaging and exposing-like imaging of the continuous spike stream. Besides, we build a very lightweight but effective mapping process from spikes to instant images to support training. Furthermore, we introduced a new spike-based 3D rendering dataset for validation. Extensive experiments have demonstrated our method possesses the high quality of novel view rendering, proving the tremendous potential of spike cameras in modeling 3D scenes.",
    "arxiv_url": "http://arxiv.org/abs/2407.10062v1",
    "pdf_url": "http://arxiv.org/pdf/2407.10062v1",
    "published_date": "2024-07-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "mapping",
      "lightweight",
      "high quality",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity",
    "authors": [
      "Zhentao Huang",
      "Minglun Gong"
    ],
    "abstract": "In this paper, we introduce Textured-GS, an innovative method for rendering Gaussian splatting that incorporates spatially defined color and opacity variations using Spherical Harmonics (SH). This approach enables each Gaussian to exhibit a richer representation by accommodating varying colors and opacities across its surface, significantly enhancing rendering quality compared to traditional methods. To demonstrate the merits of our approach, we have adapted the Mini-Splatting architecture to integrate textured Gaussians without increasing the number of Gaussians. Our experiments across multiple real-world datasets show that Textured-GS consistently outperforms both the baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The results highlight the potential of Textured-GS to advance Gaussian-based rendering technologies, promising more efficient and high-quality scene reconstructions. Our implementation is available at https://github.com/ZhentaoHuang/Textured-GS.",
    "arxiv_url": "http://arxiv.org/abs/2407.09733v3",
    "pdf_url": "http://arxiv.org/pdf/2407.09733v3",
    "published_date": "2024-07-13",
    "categories": [
      "cs.CV",
      "I.4.0"
    ],
    "github_url": "https://github.com/ZhentaoHuang/Textured-GS",
    "keywords": [
      "efficient",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting",
    "authors": [
      "Sahil Jain",
      "Avik Kuthiala",
      "Prabhdeep Singh Sethi",
      "Prakanshul Saxena"
    ],
    "abstract": "Recent advancements in radiance fields have opened new avenues for creating high-quality 3D assets and scenes. Style transfer can enhance these 3D assets with diverse artistic styles, transforming creative expression. However, existing techniques are often slow or unable to localize style transfer to specific objects. We introduce StyleSplat, a lightweight method for stylizing 3D objects in scenes represented by 3D Gaussians from reference style images. Our approach first learns a photorealistic representation of the scene using 3D Gaussian splatting while jointly segmenting individual 3D objects. We then use a nearest-neighbor feature matching loss to finetune the Gaussians of the selected objects, aligning their spherical harmonic coefficients with the style image to ensure consistency and visual appeal. StyleSplat allows for quick, customizable style transfer and localized stylization of multiple objects within a scene, each with a different style. We demonstrate its effectiveness across various 3D scenes and styles, showcasing enhanced control and customization in 3D creation.",
    "arxiv_url": "http://arxiv.org/abs/2407.09473v1",
    "pdf_url": "http://arxiv.org/pdf/2407.09473v1",
    "published_date": "2024-07-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WildGaussians: 3D Gaussian Splatting in the Wild",
    "authors": [
      "Jonas Kulhanek",
      "Songyou Peng",
      "Zuzana Kukelova",
      "Marc Pollefeys",
      "Torsten Sattler"
    ],
    "abstract": "While the field of 3D scene reconstruction is dominated by NeRFs due to their photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged, offering similar quality with real-time rendering speeds. However, both methods primarily excel with well-controlled 3D scenes, while in-the-wild data - characterized by occlusions, dynamic objects, and varying illumination - remains challenging. NeRFs can adapt to such conditions easily through per-image embedding vectors, but 3DGS struggles due to its explicit representation and lack of shared parameters. To address this, we introduce WildGaussians, a novel approach to handle occlusions and appearance changes with 3DGS. By leveraging robust DINO features and integrating an appearance modeling module within 3DGS, our method achieves state-of-the-art results. We demonstrate that WildGaussians matches the real-time rendering speed of 3DGS while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all within a simple architectural framework.",
    "arxiv_url": "http://arxiv.org/abs/2407.08447v2",
    "pdf_url": "http://arxiv.org/pdf/2407.08447v2",
    "published_date": "2024-07-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Survey on Fundamental Deep Learning 3D Reconstruction Techniques",
    "authors": [
      "Yonge Bai",
      "LikHang Wong",
      "TszYin Twan"
    ],
    "abstract": "This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction techniques that produce photo-realistic 3D models and scenes, highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their strengths and tradeoffs, and project future research trajectories in this rapidly evolving field. We provide a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction, offering insights into their potential applications and limitations.",
    "arxiv_url": "http://arxiv.org/abs/2407.08137v1",
    "pdf_url": "http://arxiv.org/pdf/2407.08137v1",
    "published_date": "2024-07-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "lighting",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition",
    "authors": [
      "Aggelina Chatziagapi",
      "Grigorios G. Chrysos",
      "Dimitris Samaras"
    ],
    "abstract": "We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that learns a single neural representation for multiple identities, using only monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human avatars require per-identity optimization. However, learning a multi-identity representation presents advantages in robustly animating humans under arbitrary poses. We propose to construct a high-order tensor that combines all the learnable 3DGS parameters for all the training identities. By assuming a low-rank structure and factorizing the tensor, we model the complex rigid and non-rigid deformations of multiple subjects in a unified network, significantly reducing the total number of parameters. Our proposed approach leverages information from all the training identities and enables robust animation under challenging unseen poses, outperforming existing approaches. It can also be extended to learn unseen identities.",
    "arxiv_url": "http://arxiv.org/abs/2407.07284v2",
    "pdf_url": "http://arxiv.org/pdf/2407.07284v2",
    "published_date": "2024-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reference-based Controllable Scene Stylization with Gaussian Splatting",
    "authors": [
      "Yiqun Mei",
      "Jiacong Xu",
      "Vishal M. Patel"
    ],
    "abstract": "Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses discrete Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGs can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation.",
    "arxiv_url": "http://arxiv.org/abs/2407.07220v1",
    "pdf_url": "http://arxiv.org/pdf/2407.07220v1",
    "published_date": "2024-07-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes",
    "authors": [
      "Nicolas Moenne-Loccoz",
      "Ashkan Mirzaei",
      "Or Perel",
      "Riccardo de Lutio",
      "Janick Martinez Esturo",
      "Gavriel State",
      "Sanja Fidler",
      "Nicholas Sharp",
      "Zan Gojcic"
    ],
    "abstract": "Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.",
    "arxiv_url": "http://arxiv.org/abs/2407.07090v3",
    "pdf_url": "http://arxiv.org/pdf/2407.07090v3",
    "published_date": "2024-07-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "ray tracing",
      "reflection",
      "lighting",
      "fast",
      "shadow",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PICA: Physics-Integrated Clothed Avatar",
    "authors": [
      "Bo Peng",
      "Yunfan Tao",
      "Haoyu Zhan",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "abstract": "We introduce PICA, a novel representation for high-fidelity animatable clothed human avatars with physics-accurate dynamics, even for loose clothing. Previous neural rendering-based representations of animatable clothed humans typically employ a single model to represent both the clothing and the underlying body. While efficient, these approaches often fail to accurately represent complex garment dynamics, leading to incorrect deformations and noticeable rendering artifacts, especially for sliding or loose garments. Furthermore, previous works represent garment dynamics as pose-dependent deformations and facilitate novel pose animations in a data-driven manner. This often results in outcomes that do not faithfully represent the mechanics of motion and are prone to generating artifacts in out-of-distribution poses. To address these issues, we adopt two individual 3D Gaussian Splatting (3DGS) models with different deformation characteristics, modeling the human body and clothing separately. This distinction allows for better handling of their respective motion characteristics. With this representation, we integrate a graph neural network (GNN)-based clothed body physics simulation module to ensure an accurate representation of clothing dynamics. Our method, through its carefully designed features, achieves high-fidelity rendering of clothed human bodies in complex and novel driving poses, significantly outperforming previous methods under the same settings.",
    "arxiv_url": "http://arxiv.org/abs/2407.05324v1",
    "pdf_url": "http://arxiv.org/pdf/2407.05324v1",
    "published_date": "2024-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "high-fidelity",
      "motion",
      "deformation",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussReg: Fast 3D Registration with Gaussian Splatting",
    "authors": [
      "Jiahao Chang",
      "Yinglin Xu",
      "Yihao Li",
      "Yuantao Chen",
      "Xiaoguang Han"
    ],
    "abstract": "Point cloud registration is a fundamental problem for large-scale 3D scene scanning and reconstruction. With the help of deep learning, registration methods have evolved significantly, reaching a nearly-mature stage. As the introduction of Neural Radiance Fields (NeRF), it has become the most popular 3D scene representation as its powerful view synthesis capabilities. Regarding NeRF representation, its registration is also required for large-scale scene reconstruction. However, this topic extremly lacks exploration. This is due to the inherent challenge to model the geometric relationship among two scenes with implicit representations. The existing methods usually convert the implicit representation to explicit representation for further registration. Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D Gaussian. This method significantly enhances rendering speed while maintaining high rendering quality. Given two scenes with explicit GS representations, in this work, we explore the 3D registration task between them. To this end, we propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The coarse stage follows existing point cloud registration methods and estimates a rough alignment for point clouds from GS. We further newly present an image-guided fine registration approach, which renders images from GS to provide more detailed geometric information for precise alignment. To support comprehensive evaluation, we carefully build a scene-level dataset called ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an in-the-wild dataset called GSReg. Experimental results demonstrate our method achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44 times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as the matcher) with comparable accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2407.05254v1",
    "pdf_url": "http://arxiv.org/pdf/2407.05254v1",
    "published_date": "2024-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Panopticon: a telescope for our times",
    "authors": [
      "Will Saunders",
      "Timothy Chin",
      "Michael Goodwin"
    ],
    "abstract": "We present a design for a wide-field spectroscopic telescope. The only large powered mirror is spherical, the resulting spherical aberration is corrected for each target separately, giving exceptional image quality. The telescope is a transit design, but still allows all-sky coverage. Three simultaneous modes are proposed: (a) natural seeing multi-object spectroscopy with 12m aperture over 3dg FoV with ~25,000 targets; (b) multi-object AO with 12m aperture over 3dg FoV with ~100 AO-corrected Integral Field Units each with 4 arcsec FoV; (c) ground layer AO-corrected integral field spectroscopy with 15m aperture and 13 arcmin FoV. Such a telescope would be uniquely powerful for large-area follow-up of imaging surveys; in each mode, the AOmega and survey speed exceed all existing facilities combined. The expected cost of this design is relatively modest, much closer to $500M than $1000M.",
    "arxiv_url": "http://arxiv.org/abs/2407.05103v2",
    "pdf_url": "http://arxiv.org/pdf/2407.05103v2",
    "published_date": "2024-07-06",
    "categories": [
      "astro-ph.IM"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction",
    "authors": [
      "Weixing Xie",
      "Junfeng Yao",
      "Xianpeng Cao",
      "Qiqin Lin",
      "Zerui Tang",
      "Xiao Dong",
      "Xiaohu Guo"
    ],
    "abstract": "Dynamic reconstruction of deformable tissues in endoscopic video is a key technology for robot-assisted surgery. Recent reconstruction methods based on neural radiance fields (NeRFs) have achieved remarkable results in the reconstruction of surgical scenes. However, based on implicit representation, NeRFs struggle to capture the intricate details of objects in the scene and cannot achieve real-time rendering. In addition, restricted single view perception and occluded instruments also propose special challenges in surgical scene reconstruction. To address these issues, we develop SurgicalGaussian, a deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our approach models the spatio-temporal features of soft tissues at each time stamp via a forward-mapping deformation MLP and regularization to constrain local 3D Gaussians to comply with consistent movement. With the depth initialization strategy and tool mask-guided training, our method can remove surgical instruments and reconstruct high-fidelity surgical scenes. Through experiments on various surgical videos, our network outperforms existing method on many aspects, including rendering quality, rendering speed and GPU usage. The project page can be found at https://surgicalgaussian.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2407.05023v1",
    "pdf_url": "http://arxiv.org/pdf/2407.05023v1",
    "published_date": "2024-07-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Eigen Models for Human Heads",
    "authors": [
      "Wojciech Zielonka",
      "Timo Bolkart",
      "Thabo Beeler",
      "Justus Thies"
    ],
    "abstract": "Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEM's self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM's higher visual quality and better generalization to new expressions.",
    "arxiv_url": "http://arxiv.org/abs/2407.04545v4",
    "pdf_url": "http://arxiv.org/pdf/2407.04545v4",
    "published_date": "2024-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "face",
      "3d gaussian",
      "gaussian splatting",
      "human",
      "ar",
      "avatar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment Any 4D Gaussians",
    "authors": [
      "Shengxiang Ji",
      "Guanjun Wu",
      "Jiemin Fang",
      "Jiazhong Cen",
      "Taoran Yi",
      "Wenyu Liu",
      "Qi Tian",
      "Xinggang Wang"
    ],
    "abstract": "Modeling, understanding, and reconstructing the real world are crucial in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable success in modeling and understanding 3D scenes. Similarly, various 4D representations have demonstrated the ability to capture the dynamics of the 4D world. However, there is a dearth of research focusing on segmentation within 4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an efficient temporal identity feature field is introduced to handle Gaussian drifting, with the potential to learn precise identity features from noisy and sparse input. Additionally, a 4D segmentation refinement process is proposed to remove artifacts. Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians and shows the ability to remove, recolor, compose, and render high-quality anything masks. More demos are available at: https://jsxzs.github.io/sa4d/.",
    "arxiv_url": "http://arxiv.org/abs/2407.04504v2",
    "pdf_url": "http://arxiv.org/pdf/2407.04504v2",
    "published_date": "2024-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "understanding",
      "segmentation",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction",
    "authors": [
      "Yuxuan Mu",
      "Xinxin Zuo",
      "Chuan Guo",
      "Yilin Wang",
      "Juwei Lu",
      "Xiaofeng Wu",
      "Songcen Xu",
      "Peng Dai",
      "Youliang Yan",
      "Li Cheng"
    ],
    "abstract": "We present GSD, a diffusion model approach based on Gaussian Splatting (GS) representation for 3D object reconstruction from a single view. Prior works suffer from inconsistent 3D geometry or mediocre rendering quality due to improper representations. We take a step towards resolving these shortcomings by utilizing the recent state-of-the-art 3D explicit representation, Gaussian Splatting, and an unconditional diffusion model. This model learns to generate 3D objects represented by sets of GS ellipsoids. With these strong generative 3D priors, though learning unconditionally, the diffusion model is ready for view-guided reconstruction without further model fine-tuning. This is achieved by propagating fine-grained 2D features through the efficient yet flexible splatting function and the guided denoising sampling process. In addition, a 2D diffusion model is further employed to enhance rendering fidelity, and improve reconstructed GS quality by polishing and re-using the rendered images. The final reconstructed objects explicitly come with high-quality 3D structure and texture, and can be efficiently rendered in arbitrary views. Experiments on the challenging real-world CO3D dataset demonstrate the superiority of our approach. Project page: https://yxmu.foo/GSD/",
    "arxiv_url": "http://arxiv.org/abs/2407.04237v4",
    "pdf_url": "http://arxiv.org/pdf/2407.04237v4",
    "published_date": "2024-07-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion-Blurred Images",
    "authors": [
      "Jungho Lee",
      "Donghyeong Kim",
      "Dogyoon Lee",
      "Suhwan Cho",
      "Minhyeok Lee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for their high-quality novel view rendering, motivating research to address real-world challenges. A critical issue is the camera motion blur caused by movement during exposure, which hinders accurate 3D scene reconstruction. In this study, we propose CRiM-GS, a \\textbf{C}ontinuous \\textbf{Ri}gid \\textbf{M}otion-aware \\textbf{G}aussian \\textbf{S}platting that reconstructs precise 3D scenes from motion-blurred images while maintaining real-time rendering speed. Considering the complex motion patterns inherent in real-world camera movements, we predict continuous camera trajectories using neural ordinary differential equations (ODE). To ensure accurate modeling, we employ rigid body transformations with proper regularization, preserving object shape and size. Additionally, we introduce an adaptive distortion-aware transformation to compensate for potential nonlinear distortions, such as rolling shutter effects, and unpredictable camera movements. By revisiting fundamental camera theory and leveraging advanced neural training techniques, we achieve precise modeling of continuous camera trajectories. Extensive experiments demonstrate state-of-the-art performance both quantitatively and qualitatively on benchmark datasets.",
    "arxiv_url": "http://arxiv.org/abs/2407.03923v2",
    "pdf_url": "http://arxiv.org/pdf/2407.03923v2",
    "published_date": "2024-07-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "body",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PFGS: High Fidelity Point Cloud Rendering via Feature Splatting",
    "authors": [
      "Jiaxu Wang",
      "Ziyi Zhang",
      "Junhao He",
      "Renjing Xu"
    ],
    "abstract": "Rendering high-fidelity images from sparse point clouds is still challenging. Existing learning-based approaches suffer from either hole artifacts, missing details, or expensive computations. In this paper, we propose a novel framework to render high-quality images from sparse points. This method first attempts to bridge the 3D Gaussian Splatting and point cloud rendering, which includes several cascaded modules. We first use a regressor to estimate Gaussian properties in a point-wise manner, the estimated properties are used to rasterize neural feature descriptors into 2D planes which are extracted from a multiscale extractor. The projected feature volume is gradually decoded toward the final prediction via a multiscale and progressive decoder. The whole pipeline experiences a two-stage training and is driven by our well-designed progressive and multiscale reconstruction loss. Experiments on different benchmarks show the superiority of our method in terms of rendering qualities and the necessities of our main components.",
    "arxiv_url": "http://arxiv.org/abs/2407.03857v1",
    "pdf_url": "http://arxiv.org/pdf/2407.03857v1",
    "published_date": "2024-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors",
    "authors": [
      "Yijia Guo",
      "Liwen Hu",
      "Yuanxi Bai",
      "Jiawei Yao",
      "Lei Ma",
      "Tiejun Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images. Fulfilling this requirement can be challenging in real-world scenarios especially when the camera moves fast, which severely limits the application of 3DGS. To address these challenges, we proposed Spike Gausian Splatting (SpikeGS), the first framework that integrates the spike streams into 3DGS pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With accumulation rasterization, interval supervision, and a specially designed pipeline, SpikeGS extracts detailed geometry and texture from high temporal resolution but texture lacking spike stream, reconstructs 3D scenes captured in 1 second. Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of SpikeGS compared with existing spike-based and deblur 3D scene reconstruction methods. Codes and data will be released soon.",
    "arxiv_url": "http://arxiv.org/abs/2407.03771v4",
    "pdf_url": "http://arxiv.org/pdf/2407.03771v4",
    "published_date": "2024-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Expressive Gaussian Human Avatars from Monocular RGB Video",
    "authors": [
      "Hezhen Hu",
      "Zhiwen Fan",
      "Tianhao Wu",
      "Yihan Xi",
      "Seoyoung Lee",
      "Georgios Pavlakos",
      "Zhangyang Wang"
    ],
    "abstract": "Nuanced expressiveness, particularly through fine-grained hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations. In this work, we focus on investigating the expressiveness of human avatars when learned from monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details. To this end, we introduce EVA, a drivable human model that meticulously sculpts fine details based on 3D Gaussians and SMPL-X, an expressive parametric human model. Focused on enhancing expressiveness, our work makes three key contributions. First, we highlight the critical importance of aligning the SMPL-X model with RGB frames for effective avatar learning. Recognizing the limitations of current SMPL-X prediction methods for in-the-wild videos, we introduce a plug-and-play module that significantly ameliorates misalignment issues. Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts. Last but not least, we develop a feedback mechanism that predicts per-pixel confidence to better guide the learning of 3D Gaussians. Extensive experiments on two benchmarks demonstrate the superiority of our framework both quantitatively and qualitatively, especially on the fine-grained hand and facial details. See the project website at \\url{https://evahuman.github.io}",
    "arxiv_url": "http://arxiv.org/abs/2407.03204v1",
    "pdf_url": "http://arxiv.org/pdf/2407.03204v1",
    "published_date": "2024-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors",
    "authors": [
      "Sungwon Hwang",
      "Min-Jung Kim",
      "Taewoong Kang",
      "Jayeon Kang",
      "Jaegul Choo"
    ],
    "abstract": "Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction. Link to our project page: https://vegs3d.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2407.02945v3",
    "pdf_url": "http://arxiv.org/pdf/2407.02945v3",
    "published_date": "2024-07-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "neural rendering",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene Reconstruction",
    "authors": [
      "Jiaxin Guo",
      "Jiangliu Wang",
      "Di Kang",
      "Wenzhen Dong",
      "Wenting Wang",
      "Yun-hui Liu"
    ],
    "abstract": "Real-time 3D reconstruction of surgical scenes plays a vital role in computer-assisted surgery, holding a promise to enhance surgeons' visibility. Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential for real-time novel view synthesis of general scenes, which relies on accurate poses and point clouds generated by Structure-from-Motion (SfM) for initialization. However, 3DGS with SfM fails to recover accurate camera poses and geometry in surgical scenes due to the challenges of minimal textures and photometric inconsistencies. To tackle this problem, in this paper, we propose the first SfM-free 3DGS-based method for surgical scene reconstruction by jointly optimizing the camera poses and scene representation. Based on the video continuity, the key of our method is to exploit the immediate optical flow priors to guide the projection flow derived from 3D Gaussians. Unlike most previous methods relying on photometric loss only, we formulate the pose estimation problem as minimizing the flow loss between the projection flow and optical flow. A consistency check is further introduced to filter the flow outliers by detecting the rigid and reliable points that satisfy the epipolar geometry. During 3D Gaussian optimization, we randomly sample frames to optimize the scene representations to grow the 3D Gaussian progressively. Experiments on the SCARED dataset demonstrate our superior performance over existing methods in novel view synthesis and pose estimation with high efficiency. Code is available at https://github.com/wrld/Free-SurGS.",
    "arxiv_url": "http://arxiv.org/abs/2407.02918v1",
    "pdf_url": "http://arxiv.org/pdf/2407.02918v1",
    "published_date": "2024-07-03",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "https://github.com/wrld/Free-SurGS",
    "keywords": [
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spatially Coherent 3D Distributions of HI and CO in the Milky Way",
    "authors": [
      "Laurin S√∂ding",
      "Gordian Edenhofer",
      "Torsten A. En√ülin",
      "Philipp Frank",
      "Ralf Kissmann",
      "Vo Hong Minh Phan",
      "Andr√©s Ram√≠rez",
      "Hanieh Zhandinejad",
      "Philipp Mertsch"
    ],
    "abstract": "The spatial distribution of the gaseous components of the Milky Way is of great importance for a number of different fields, e.g. Galactic structure, star formation and cosmic rays. However, obtaining distance information to gaseous clouds in the interstellar medium from Doppler-shifted line emission is notoriously difficult given our unique vantage point in the Galaxy. It requires precise knowledge of gas velocities and generally suffers from distance ambiguities.   Previous works often assumed the optically thin limit (no absorption), a fixed velocity field, and lack resolution overall. We aim to overcome these issues and improve previous reconstructions of the gaseous constituents of the interstellar medium of the Galaxy.   We use 3D Gaussian processes to model correlations in the interstellar medium, including correlations between different lines of sight, and enforce a spatially coherent structure in the prior. For modelling the transport of radiation from the emitting gas to us as observers, we take absorption effects into account. A special numerical grid ensures high resolution nearby. We infer the spatial distributions of HI, CO, their emission line-widths, and the Galactic velocity field in a joint Bayesian inference. We further constrain these fields with complementary data from Galactic masers and young stellar object clusters.   Our main result consists of a set of samples that implicitly contain statistical uncertainties. The resulting maps are spatially coherent and reproduce the data with high fidelity. We confirm previous findings regarding the warping and flaring of the Galactic disc. A comparison with 3D dust maps reveals a good agreement on scales larger than approximately 400 pc. While our results are not free of artefacts, they present a big step forward in obtaining high quality 3D maps of the interstellar medium.",
    "arxiv_url": "http://arxiv.org/abs/2407.02859v1",
    "pdf_url": "http://arxiv.org/pdf/2407.02859v1",
    "published_date": "2024-07-03",
    "categories": [
      "astro-ph.GA"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction",
    "authors": [
      "Mustafa Khan",
      "Hamidreza Fazlali",
      "Dhruv Sharma",
      "Tongtong Cao",
      "Dongfeng Bai",
      "Yuan Ren",
      "Bingbing Liu"
    ],
    "abstract": "Realistic scene reconstruction and view synthesis are essential for advancing autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian Splatting excels in real-time rendering and static scene reconstructions but struggles with modeling driving scenarios due to complex backgrounds, dynamic objects, and sparse views. We propose AutoSplat, a framework employing Gaussian splatting to achieve highly realistic reconstructions of autonomous driving scenes. By imposing geometric constraints on Gaussians representing the road and sky regions, our method enables multi-view consistent simulation of challenging scenarios including lane changes. Leveraging 3D templates, we introduce a reflected Gaussian consistency constraint to supervise both the visible and unseen side of foreground objects. Moreover, to model the dynamic appearance of foreground objects, we estimate residual spherical harmonics for each foreground Gaussian. Extensive experiments on Pandaset and KITTI demonstrate that AutoSplat outperforms state-of-the-art methods in scene reconstruction and novel view synthesis across diverse driving scenarios. Visit our project page at https://autosplat.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2407.02598v2",
    "pdf_url": "http://arxiv.org/pdf/2407.02598v2",
    "published_date": "2024-07-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "autonomous driving",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D Gaussian Splatting Manipulation",
    "authors": [
      "Chaofan Luo",
      "Donglin Di",
      "Xun Yang",
      "Yongjia Ma",
      "Zhou Xue",
      "Chen Wei",
      "Yebin Liu"
    ],
    "abstract": "Despite significant strides in the field of 3D scene editing, current methods encounter substantial challenge, particularly in preserving 3D consistency in multi-view editing process. To tackle this challenge, we propose a progressive 3D editing strategy that ensures multi-view consistency via a Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism. Specifically, TAS facilitates a tightly coupled iterative process between 2D view editing and 3D updating, preventing error accumulation yielded from text-to-image process. Additionally, we explore the relationship between optimization-based methods and reconstruction-based methods, offering a unified perspective for selecting superior design choice, supporting the rationale behind the designed TAS. We further present a tuning-free View-Consistent Attention Control (VCAC) module that leverages cross-view semantic and geometric reference from the source branch to yield aligned views from the target branch during the editing of 2D views. To validate the effectiveness of our method, we analyze 2D examples to demonstrate the improved consistency with the VCAC module. Further extensive quantitative and qualitative results in text-guided 3D scene editing indicate that our method achieves superior editing quality compared to state-of-the-art methods. We will make the complete codebase publicly available following the conclusion of the review process.",
    "arxiv_url": "http://arxiv.org/abs/2407.02034v2",
    "pdf_url": "http://arxiv.org/pdf/2407.02034v2",
    "published_date": "2024-07-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DRAGON: Drone and Ground Gaussian Splatting for 3D Building Reconstruction",
    "authors": [
      "Yujin Ham",
      "Mateusz Michalkiewicz",
      "Guha Balakrishnan"
    ],
    "abstract": "3D building reconstruction from imaging data is an important task for many applications ranging from urban planning to reconnaissance. Modern Novel View synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful techniques for developing 3D models from natural 2D imagery in an unsupervised fashion. These algorithms generally require input training views surrounding the scene of interest, which, in the case of large buildings, is typically not available across all camera elevations. In particular, the most readily available camera viewpoints at scale across most buildings are at near-ground (e.g., with mobile phones) and aerial (drones) elevations. However, due to the significant difference in viewpoint between drone and ground image sets, camera registration - a necessary step for NVS algorithms - fails. In this work we propose a method, DRAGON, that can take drone and ground building imagery as input and produce a 3D NVS model. The key insight of DRAGON is that intermediate elevation imagery may be extrapolated by an NVS algorithm itself in an iterative procedure with perceptual regularization, thereby bridging the visual feature gap between the two elevations and enabling registration. We compiled a semi-synthetic dataset of 9 large building scenes using Google Earth Studio, and quantitatively and qualitatively demonstrate that DRAGON can generate compelling renderings on this dataset compared to baseline strategies.",
    "arxiv_url": "http://arxiv.org/abs/2407.01761v1",
    "pdf_url": "http://arxiv.org/pdf/2407.01761v1",
    "published_date": "2024-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianStego: A Generalizable Stenography Pipeline for Generative 3D Gaussians Splatting",
    "authors": [
      "Chenxin Li",
      "Hengyu Liu",
      "Zhiwen Fan",
      "Wuyang Li",
      "Yifan Liu",
      "Panwang Pan",
      "Yixuan Yuan"
    ],
    "abstract": "Recent advancements in large generative models and real-time neural rendering using point-based techniques pave the way for a future of widespread visual data distribution through sharing synthesized 3D assets. However, while standardized methods for embedding proprietary or copyright information, either overtly or subtly, exist for conventional visual content such as images and videos, this issue remains unexplored for emerging generative 3D formats like Gaussian Splatting. We present GaussianStego, a method for embedding steganographic information in the rendering of generated 3D assets. Our approach employs an optimization framework that enables the accurate extraction of hidden information from images rendered using Gaussian assets derived from large models, while maintaining their original visual quality. We conduct preliminary evaluations of our method across several potential deployment scenarios and discuss issues identified through analysis. GaussianStego represents an initial exploration into the novel challenge of embedding customizable, imperceptible, and recoverable information within the renders produced by current 3D generative models, while ensuring minimal impact on the rendered content's quality.",
    "arxiv_url": "http://arxiv.org/abs/2407.01301v1",
    "pdf_url": "http://arxiv.org/pdf/2407.01301v1",
    "published_date": "2024-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation",
    "authors": [
      "Zihan Gao",
      "Lingling Li",
      "Licheng Jiao",
      "Fang Liu",
      "Xu Liu",
      "Wenping Ma",
      "Yuwei Guo",
      "Shuyuan Yang"
    ],
    "abstract": "Understanding 3D scenes is a crucial challenge in computer vision research with applications spanning multiple domains. Recent advancements in distilling 2D vision-language foundation models into neural fields, like NeRF and 3DGS, enable open-vocabulary segmentation of 3D scenes from 2D multi-view images without the need for precise 3D annotations. However, while effective, these methods typically rely on the per-pixel distillation of high-dimensional CLIP features, introducing ambiguity and necessitating complex regularization strategies, which adds inefficiency during training. This paper presents MaskField, which enables efficient 3D open-vocabulary segmentation with neural fields from a novel perspective. Unlike previous methods, MaskField decomposes the distillation of mask and semantic features from foundation models by formulating a mask feature field and queries. MaskField overcomes ambiguous object boundaries by naturally introducing SAM segmented object shapes without extra regularization during training. By circumventing the direct handling of dense high-dimensional CLIP features during training, MaskField is particularly compatible with explicit scene representations like 3DGS. Our extensive experiments show that MaskField not only surpasses prior state-of-the-art methods but also achieves remarkably fast convergence. We hope that MaskField will inspire further exploration into how neural fields can be trained to comprehend 3D scenes from 2D models.",
    "arxiv_url": "http://arxiv.org/abs/2407.01220v3",
    "pdf_url": "http://arxiv.org/pdf/2407.01220v3",
    "published_date": "2024-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "understanding",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning 3D Gaussians for Extremely Sparse-View Cone-Beam CT Reconstruction",
    "authors": [
      "Yiqun Lin",
      "Hualiang Wang",
      "Jixiang Chen",
      "Xiaomeng Li"
    ],
    "abstract": "Cone-Beam Computed Tomography (CBCT) is an indispensable technique in medical imaging, yet the associated radiation exposure raises concerns in clinical practice. To mitigate these risks, sparse-view reconstruction has emerged as an essential research direction, aiming to reduce the radiation dose by utilizing fewer projections for CT reconstruction. Although implicit neural representations have been introduced for sparse-view CBCT reconstruction, existing methods primarily focus on local 2D features queried from sparse projections, which is insufficient to process the more complicated anatomical structures, such as the chest. To this end, we propose a novel reconstruction framework, namely DIF-Gaussian, which leverages 3D Gaussians to represent the feature distribution in the 3D space, offering additional 3D spatial information to facilitate the estimation of attenuation coefficients. Furthermore, we incorporate test-time optimization during inference to further improve the generalization capability of the model. We evaluate DIF-Gaussian on two public datasets, showing significantly superior reconstruction performance than previous state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2407.01090v2",
    "pdf_url": "http://arxiv.org/pdf/2407.01090v2",
    "published_date": "2024-07-01",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "medical",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using Gaussian Splatting",
    "authors": [
      "Chenxin Li",
      "Brandon Y. Feng",
      "Yifan Liu",
      "Hengyu Liu",
      "Cheng Wang",
      "Weihao Yu",
      "Yixuan Yuan"
    ],
    "abstract": "3D reconstruction of biological tissues from a collection of endoscopic images is a key to unlock various important downstream surgical applications with 3D capabilities. Existing methods employ various advanced neural rendering techniques for photorealistic view synthesis, but they often struggle to recover accurate 3D representations when only sparse observations are available, which is usually the case in real-world clinical scenarios. To tackle this {sparsity} challenge, we propose a framework leveraging the prior knowledge from multiple foundation models during the reconstruction process, dubbed as \\textit{EndoSparse}. Experimental results indicate that our proposed strategy significantly improves the geometric and appearance quality under challenging sparse-view conditions, including using only three views. In rigorous benchmarking experiments against state-of-the-art methods, \\textit{EndoSparse} achieves superior results in terms of accurate geometry, realistic appearance, and rendering efficiency, confirming the robustness to sparse-view limitations in endoscopic reconstruction. \\textit{EndoSparse} signifies a steady step towards the practical deployment of neural 3D reconstruction in real-world clinical scenarios. Project page: https://endo-sparse.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2407.01029v1",
    "pdf_url": "http://arxiv.org/pdf/2407.01029v1",
    "published_date": "2024-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "geometry",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OccFusion: Rendering Occluded Humans with Generative Diffusion Priors",
    "authors": [
      "Adam Sun",
      "Tiange Xiang",
      "Scott Delp",
      "Li Fei-Fei",
      "Ehsan Adeli"
    ],
    "abstract": "Most existing human rendering methods require every part of the human to be fully visible throughout the input video. However, this assumption does not hold in real-life settings where obstructions are common, resulting in only partial visibility of the human. Considering this, we present OccFusion, an approach that utilizes efficient 3D Gaussian splatting supervised by pretrained 2D diffusion models for efficient and high-fidelity human rendering. We propose a pipeline consisting of three stages. In the Initialization stage, complete human masks are generated from partial visibility masks. In the Optimization stage, 3D human Gaussians are optimized with additional supervision by Score-Distillation Sampling (SDS) to create a complete geometry of the human. Finally, in the Refinement stage, in-context inpainting is designed to further improve rendering quality on the less observed human body parts. We evaluate OccFusion on ZJU-MoCap and challenging OcMotion sequences and find that it achieves state-of-the-art performance in the rendering of occluded humans.",
    "arxiv_url": "http://arxiv.org/abs/2407.00316v1",
    "pdf_url": "http://arxiv.org/pdf/2407.00316v1",
    "published_date": "2024-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting",
    "authors": [
      "Sara Sabour",
      "Lily Goli",
      "George Kopanas",
      "Mark Matthews",
      "Dmitry Lagun",
      "Leonidas Guibas",
      "Alec Jacobson",
      "David J. Fleet",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction, offering efficient training and rendering speeds, making it suitable for real-time applications.However, current methods require highly controlled environments (no moving people or wind-blown elements, and consistent lighting) to meet the inter-view consistency assumption of 3DGS. This makes reconstruction of real-world captures problematic. We present SpotLessSplats, an approach that leverages pre-trained and general-purpose features coupled with robust optimization to effectively ignore transient distractors. Our method achieves state-of-the-art reconstruction quality both visually and quantitatively, on casual captures. Additional results available at: https://spotlesssplats.github.io",
    "arxiv_url": "http://arxiv.org/abs/2406.20055v2",
    "pdf_url": "http://arxiv.org/pdf/2406.20055v2",
    "published_date": "2024-06-28",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting",
    "authors": [
      "Daiwei Zhang",
      "Gengyan Li",
      "Jiajie Li",
      "Micka√´l Bressieux",
      "Otmar Hilliges",
      "Marc Pollefeys",
      "Luc Van Gool",
      "Xi Wang"
    ],
    "abstract": "Human activities are inherently complex, often involving numerous object interactions. To better understand these activities, it is crucial to model their interactions with the environment captured through dynamic changes. The recent availability of affordable head-mounted cameras and egocentric data offers a more accessible and efficient means to understand human-object interactions in 3D environments. However, most existing methods for human activity modeling neglect the dynamic interactions with objects, resulting in only static representations. The few existing solutions often require inputs from multiple sources, including multi-camera setups, depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the first method capable of simultaneously reconstructing 3D scenes and dynamically tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely discrete nature of Gaussian Splatting and segment dynamic interactions from the background, with both having explicit representations. Our approach employs a clip-level online learning pipeline that leverages the dynamic nature of human activities, allowing us to reconstruct the temporal evolution of the scene in chronological order and track rigid object motion. EgoGaussian shows significant improvements in terms of both dynamic object and background reconstruction quality compared to the state-of-the-art. We also qualitatively demonstrate the high quality of the reconstructed models.",
    "arxiv_url": "http://arxiv.org/abs/2406.19811v2",
    "pdf_url": "http://arxiv.org/pdf/2406.19811v2",
    "published_date": "2024-06-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "tracking",
      "motion",
      "understanding",
      "high quality",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightweight Predictive 3D Gaussian Splats",
    "authors": [
      "Junli Cao",
      "Vidit Goel",
      "Chaoyang Wang",
      "Anil Kag",
      "Ju Hu",
      "Sergei Korolev",
      "Chenfanfu Jiang",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "Recent approaches representing 3D objects and scenes using Gaussian splats show increased rendering speed across a variety of platforms and devices. While rendering such representations is indeed extremely efficient, storing and transmitting them is often prohibitively expensive. To represent large-scale scenes, one often needs to store millions of 3D Gaussians, occupying gigabytes of disk space. This poses a very practical limitation, prohibiting widespread adoption.Several solutions have been proposed to strike a balance between disk size and rendering quality, noticeably reducing the visual quality. In this work, we propose a new representation that dramatically reduces the hard drive footprint while featuring similar or improved quality when compared to the standard 3D Gaussian splats. When compared to other compact solutions, ours offers higher quality renderings with significantly reduced storage, being able to efficiently run on a mobile device in real-time. Our key observation is that nearby points in the scene can share similar representations. Hence, only a small ratio of 3D points needs to be stored. We introduce an approach to identify such points which are called parent points. The discarded points called children points along with attributes can be efficiently predicted by tiny MLPs.",
    "arxiv_url": "http://arxiv.org/abs/2406.19434v1",
    "pdf_url": "http://arxiv.org/pdf/2406.19434v1",
    "published_date": "2024-06-27",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compact",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FAGhead: Fully Animate Gaussian Head from Monocular Videos",
    "authors": [
      "Yixin Xuan",
      "Xinyang Li",
      "Gongxin Yao",
      "Shiwei Zhou",
      "Donghui Sun",
      "Xiaoxin Chen",
      "Yu Pan"
    ],
    "abstract": "High-fidelity reconstruction of 3D human avatars has a wild application in visual reality. In this paper, we introduce FAGhead, a method that enables fully controllable human portraits from monocular videos. We explicit the traditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to reconstruct with complex expressions. Furthermore, we employ a novel Point-based Learnable Representation Field (PLRF) with learnable Gaussian point positions to enhance reconstruction performance. Meanwhile, to effectively manage the edges of avatars, we introduced the alpha rendering to supervise the alpha value of each pixel. Extensive experimental results on the open-source datasets and our capturing datasets demonstrate that our approach is able to generate high-fidelity 3D head avatars and fully control the expression and pose of the virtual avatars, which is outperforming than existing works.",
    "arxiv_url": "http://arxiv.org/abs/2406.19070v2",
    "pdf_url": "http://arxiv.org/pdf/2406.19070v2",
    "published_date": "2024-06-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos",
    "authors": [
      "Colton Stearns",
      "Adam Harley",
      "Mikaela Uy",
      "Florian Dubost",
      "Federico Tombari",
      "Gordon Wetzstein",
      "Leonidas Guibas"
    ],
    "abstract": "Gaussian splatting has become a popular representation for novel-view synthesis, exhibiting clear strengths in efficiency, photometric quality, and compositional edibility. Following its success, many works have extended Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while also tracking scene geometry far better than alternative representations. Yet, these methods assume dense multi-view videos as supervision. In this work, we are interested in extending the capability of Gaussian scene representations to casually captured monocular videos. We show that existing 4D Gaussian methods dramatically fail in this setup because the monocular setting is underconstrained. Building off this finding, we propose a method we call Dynamic Gaussian Marbles, which consist of three core modifications that target the difficulties of the monocular setting. First, we use isotropic Gaussian \"marbles'', reducing the degrees of freedom of each Gaussian. Second, we employ a hierarchical divide and-conquer learning strategy to efficiently guide the optimization towards solutions with globally coherent motion. Finally, we add image-level and geometry-level priors into the optimization, including a tracking loss that takes advantage of recent progress in point tracking. By constraining the optimization, Dynamic Gaussian Marbles learns Gaussian trajectories that enable novel-view rendering and accurately capture the 3D motion of the scene elements. We evaluate on the Nvidia Dynamic Scenes dataset and the DyCheck iPhone dataset, and show that Gaussian Marbles significantly outperforms other Gaussian baselines in quality, and is on-par with non-Gaussian representations, all while maintaining the efficiency, compositionality, editability, and tracking benefits of Gaussians. Our project page can be found here https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2406.18717v2",
    "pdf_url": "http://arxiv.org/pdf/2406.18717v2",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "motion",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On Scaling Up 3D Gaussian Splatting Training",
    "authors": [
      "Hexu Zhao",
      "Haoyang Weng",
      "Daohan Lu",
      "Ang Li",
      "Jinyang Li",
      "Aurojit Panda",
      "Saining Xie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS",
    "arxiv_url": "http://arxiv.org/abs/2406.18533v1",
    "pdf_url": "http://arxiv.org/pdf/2406.18533v1",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV",
      "I.4.5"
    ],
    "github_url": "https://github.com/nyu-systems/Grendel-GS",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly Enhanced Quality",
    "authors": [
      "Taoran Yi",
      "Jiemin Fang",
      "Zanwei Zhou",
      "Junjie Wang",
      "Guanjun Wu",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wenyu Liu",
      "Xinggang Wang",
      "Qi Tian"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3D-GS) has achieved great success in reconstructing and rendering real-world scenes. To transfer the high rendering quality to generation tasks, a series of research works attempt to generate 3D-Gaussian assets from text. However, the generated assets have not achieved the same quality as those in reconstruction tasks. We observe that Gaussians tend to grow without control as the generation process may cause indeterminacy. Aiming at highly enhancing the generation quality, we propose a novel framework named GaussianDreamerPro. The main idea is to bind Gaussians to reasonable geometry, which evolves over the whole generation process. Along different stages of our framework, both the geometry and appearance can be enriched progressively. The final output asset is constructed with 3D Gaussians bound to mesh, which shows significantly enhanced details and quality compared with previous methods. Notably, the generated asset can also be seamlessly integrated into downstream manipulation pipelines, e.g. animation, composition, and simulation etc., greatly promoting its potential in wide applications. Demos are available at https://taoranyi.com/gaussiandreamerpro/.",
    "arxiv_url": "http://arxiv.org/abs/2406.18462v1",
    "pdf_url": "http://arxiv.org/pdf/2406.18462v1",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning",
    "authors": [
      "Muhammad Salman Ali",
      "Maryam Qamar",
      "Sung-Ho Bae",
      "Enzo Tartaglione"
    ],
    "abstract": "In recent times, the utilization of 3D models has gained traction, owing to the capacity for end-to-end training initially offered by Neural Radiance Fields and more recently by 3D Gaussian Splatting (3DGS) models. The latter holds a significant advantage by inherently easing rapid convergence during training and offering extensive editability. However, despite rapid advancements, the literature still lives in its infancy regarding the scalability of these models. In this study, we take some initial steps in addressing this gap, showing an approach that enables both the memory and computational scalability of such models. Specifically, we propose \"Trimming the fat\", a post-hoc gradient-informed iterative pruning technique to eliminate redundant information encoded in the model. Our experimental findings on widely acknowledged benchmarks attest to the effectiveness of our approach, revealing that up to 75% of the Gaussians can be removed while maintaining or even improving upon baseline performance. Our approach achieves around 50$\\times$ compression while preserving performance similar to the baseline model, and is able to speed-up computation up to 600 FPS.",
    "arxiv_url": "http://arxiv.org/abs/2406.18214v2",
    "pdf_url": "http://arxiv.org/pdf/2406.18214v2",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting",
    "authors": [
      "Jiaze Li",
      "Zhengyu Wen",
      "Luo Zhang",
      "Jiangbei Hu",
      "Fei Hou",
      "Zhebin Zhang",
      "Ying He"
    ],
    "abstract": "The 3D Gaussian Splatting technique has significantly advanced the construction of radiance fields from multi-view images, enabling real-time rendering. While point-based rasterization effectively reduces computational demands for rendering, it often struggles to accurately reconstruct the geometry of the target object, especially under strong lighting. To address this challenge, we introduce a novel approach that combines octree-based implicit surface representations with Gaussian splatting. Our method consists of four stages. Initially, it reconstructs a signed distance field (SDF) and a radiance field through volume rendering, encoding them in a low-resolution octree. The initial SDF represents the coarse geometry of the target object. Subsequently, it introduces 3D Gaussians as additional degrees of freedom, which are guided by the SDF. In the third stage, the optimized Gaussians further improve the accuracy of the SDF, allowing it to recover finer geometric details compared to the initial SDF obtained in the first stage. Finally, it adopts the refined SDF to further optimize the 3D Gaussians via splatting, eliminating those that contribute little to visual appearance. Experimental results show that our method, which leverages the distribution of 3D Gaussians with SDFs, reconstructs more accurate geometry, particularly in images with specular highlights caused by strong lighting.",
    "arxiv_url": "http://arxiv.org/abs/2406.18199v1",
    "pdf_url": "http://arxiv.org/pdf/2406.18199v1",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDG: Vision-Only Dynamic Gaussian for Driving Simulation",
    "authors": [
      "Hao Li",
      "Jingfeng Li",
      "Dingwen Zhang",
      "Chenming Wu",
      "Jieqi Shi",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Junwei Han"
    ],
    "abstract": "Dynamic Gaussian splatting has led to impressive scene reconstruction and image synthesis advances in novel views. Existing methods, however, heavily rely on pre-computed poses and Gaussian initialization by Structure from Motion (SfM) algorithms or expensive sensors. For the first time, this paper addresses this issue by integrating self-supervised VO into our pose-free dynamic Gaussian method (VDG) to boost pose and depth initialization and static-dynamic decomposition. Moreover, VDG can work with only RGB image input and construct dynamic scenes at a faster speed and larger scenes compared with the pose-free dynamic view-synthesis method. We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods. Additional video and source code will be posted on our project page at https://3d-aigc.github.io/VDG.",
    "arxiv_url": "http://arxiv.org/abs/2406.18198v1",
    "pdf_url": "http://arxiv.org/pdf/2406.18198v1",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text",
    "authors": [
      "Xinyang Li",
      "Zhangyu Lai",
      "Linning Xu",
      "Yansong Qu",
      "Liujuan Cao",
      "Shengchuan Zhang",
      "Bo Dai",
      "Rongrong Ji"
    ],
    "abstract": "Recent advancements in 3D generation have leveraged synthetic datasets with ground truth 3D assets and predefined cameras. However, the potential of adopting real-world datasets, which can produce significantly more realistic 3D scenes, remains largely unexplored. In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions. (2) Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the Decorator, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. (3) Lastly, the 3D Gaussians are refined by a novel SDS++ loss as the Detailer, which incorporates the prior of the 2D diffusion model. Extensive experiments demonstrate that Director3D outperforms existing methods, offering superior performance in real-world 3D generation.",
    "arxiv_url": "http://arxiv.org/abs/2406.17601v1",
    "pdf_url": "http://arxiv.org/pdf/2406.17601v1",
    "published_date": "2024-06-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods",
    "authors": [
      "Jonas Kulhanek",
      "Torsten Sattler"
    ],
    "abstract": "Novel view synthesis is an important problem with many applications, including AR/VR, gaming, and simulations for robotics. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. Our experiments support this claim by showing that tiny differences in evaluation protocols of various methods can lead to inconsistent reported metrics. To address these issues, we propose a framework called NerfBaselines, which simplifies the installation of various methods, provides consistent benchmarking tools, and ensures reproducibility. We validate our implementation experimentally by reproducing numbers reported in the original papers. To further improve the accessibility, we release a web platform where commonly used methods are compared on standard benchmarks. Web: https://jkulhanek.com/nerfbaselines",
    "arxiv_url": "http://arxiv.org/abs/2406.17345v1",
    "pdf_url": "http://arxiv.org/pdf/2406.17345v1",
    "published_date": "2024-06-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "vr",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reducing the Memory Footprint of 3D Gaussian Splatting",
    "authors": [
      "Panagiotis Papantonakis",
      "Georgios Kopanas",
      "Bernhard Kerbl",
      "Alexandre Lanvin",
      "George Drettakis"
    ],
    "abstract": "3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and real-time rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolution-aware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a 27 reduction in overall size on disk on the standard datasets we tested, along with a 1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device.",
    "arxiv_url": "http://arxiv.org/abs/2406.17074v1",
    "pdf_url": "http://arxiv.org/pdf/2406.17074v1",
    "published_date": "2024-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Perfect to Noisy World Simulation: Customizable Embodied Multi-modal Perturbations for SLAM Robustness Benchmarking",
    "authors": [
      "Xiaohao Xu",
      "Tianyi Zhang",
      "Sibo Wang",
      "Xiang Li",
      "Yongqi Chen",
      "Ye Li",
      "Bhiksha Raj",
      "Matthew Johnson-Roberson",
      "Xiaonan Huang"
    ],
    "abstract": "Embodied agents require robust navigation systems to operate in unstructured environments, making the robustness of Simultaneous Localization and Mapping (SLAM) models critical to embodied agent autonomy. While real-world datasets are invaluable, simulation-based benchmarks offer a scalable approach for robustness evaluations. However, the creation of a challenging and controllable noisy world with diverse perturbations remains under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. The pipeline comprises a comprehensive taxonomy of sensor and motion perturbations for embodied multi-modal (specifically RGB-D) sensing, categorized by their sources and propagation order, allowing for procedural composition. We also provide a toolbox for synthesizing these perturbations, enabling the transformation of clean environments into challenging noisy simulations. Utilizing the pipeline, we instantiate the large-scale Noisy-Replica benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced RGB-D SLAM models. Our extensive analysis uncovers the susceptibilities of both neural (NeRF and Gaussian Splatting -based) and non-neural SLAM models to disturbances, despite their demonstrated accuracy in standard benchmarks. Our code is publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation.",
    "arxiv_url": "http://arxiv.org/abs/2406.16850v1",
    "pdf_url": "http://arxiv.org/pdf/2406.16850v1",
    "published_date": "2024-06-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/Xiaohao-Xu/SLAM-under-Perturbation",
    "keywords": [
      "localization",
      "motion",
      "mapping",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians",
    "authors": [
      "Yufei Liu",
      "Junshu Tang",
      "Chu Zheng",
      "Shijie Zhang",
      "Jinkun Hao",
      "Junwei Zhu",
      "Dongjin Huang"
    ],
    "abstract": "High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.",
    "arxiv_url": "http://arxiv.org/abs/2406.16815v1",
    "pdf_url": "http://arxiv.org/pdf/2406.16815v1",
    "published_date": "2024-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "high-fidelity",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LGS: A Light-weight 4D Gaussian Splatting for Efficient Surgical Scene Reconstruction",
    "authors": [
      "Hengyu Liu",
      "Yifan Liu",
      "Chenxin Li",
      "Wuyang Li",
      "Yixuan Yuan"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3D-GS) techniques and their dynamic scene modeling variants, 4D-GS, offers promising prospects for real-time rendering of dynamic surgical scenarios. However, the prerequisite for modeling dynamic scenes by a large number of Gaussian units, the high-dimensional Gaussian attributes and the high-resolution deformation fields, all lead to serve storage issues that hinder real-time rendering in resource-limited surgical equipment. To surmount these limitations, we introduce a Lightweight 4D Gaussian Splatting framework (LGS) that can liberate the efficiency bottlenecks of both rendering and storage for dynamic endoscopic reconstruction. Specifically, to minimize the redundancy of Gaussian quantities, we propose Deformation-Aware Pruning by gauging the impact of each Gaussian on deformation. Concurrently, to reduce the redundancy of Gaussian attributes, we simplify the representation of textures and lighting in non-crucial areas by pruning the dimensions of Gaussian attributes. We further resolve the feature field redundancy caused by the high resolution of 4D neural spatiotemporal encoder for modeling dynamic scenes via a 4D feature field condensation. Experiments on public benchmarks demonstrate efficacy of LGS in terms of a compression rate exceeding 9 times while maintaining the pleasing visual quality and real-time rendering efficiency. LGS confirms a substantial step towards its application in robotic surgical services.",
    "arxiv_url": "http://arxiv.org/abs/2406.16073v1",
    "pdf_url": "http://arxiv.org/pdf/2406.16073v1",
    "published_date": "2024-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "efficient",
      "lighting",
      "deformation",
      "lightweight",
      "3d gaussian",
      "real-time rendering",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Taming 3DGS: High-Quality Radiance Fields with Limited Resources",
    "authors": [
      "Saswat Subhajyoti Mallick",
      "Rahul Goel",
      "Bernhard Kerbl",
      "Francisco Vicente Carrasco",
      "Markus Steinberger",
      "Fernando De La Torre"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has transformed novel-view synthesis with its fast, interpretable, and high-fidelity rendering. However, its resource requirements limit its usability. Especially on constrained devices, training performance degrades quickly and often cannot complete due to excessive memory consumption of the model. The method converges with an indefinite number of Gaussians -- many of them redundant -- making rendering unnecessarily slow and preventing its usage in downstream tasks that expect fixed-size inputs. To address these issues, we tackle the challenges of training and rendering 3DGS models on a budget. We use a guided, purely constructive densification process that steers densification toward Gaussians that raise the reconstruction quality. Model size continuously increases in a controlled manner towards an exact budget, using score-based densification of Gaussians with training-time priors that measure their contribution. We further address training speed obstacles: following a careful analysis of 3DGS' original pipeline, we derive faster, numerically equivalent solutions for gradient computation and attribute updates, including an alternative parallelization for efficient backpropagation. We also propose quality-preserving approximations where suitable to reduce training time even further. Taken together, these enhancements yield a robust, scalable solution with reduced training times, lower compute and memory requirements, and high quality. Our evaluation shows that in a budgeted setting, we obtain competitive quality metrics with 3DGS while achieving a 4--5x reduction in both model size and training time. With more generous budgets, our measured quality surpasses theirs. These advances open the door for novel-view synthesis in constrained environments, e.g., mobile devices.",
    "arxiv_url": "http://arxiv.org/abs/2406.15643v1",
    "pdf_url": "http://arxiv.org/pdf/2406.15643v1",
    "published_date": "2024-06-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation",
    "authors": [
      "Chubin Zhang",
      "Hongliang Song",
      "Yi Wei",
      "Yu Chen",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "In this work, we introduce the Geometry-Aware Large Reconstruction Model (GeoLRM), an approach which can predict high-quality assets with 512k Gaussians and 21 input images in only 11 GB GPU memory. Previous works neglect the inherent sparsity of 3D structure and do not utilize explicit geometric relationships between 3D and 2D images. This limits these methods to a low-resolution representation and makes it difficult to scale up to the dense views for better quality. GeoLRM tackles these issues by incorporating a novel 3D-aware transformer structure that directly processes 3D points and uses deformable cross-attention mechanisms to effectively integrate image features into 3D representations. We implement this solution through a two-stage pipeline: initially, a lightweight proposal network generates a sparse set of 3D anchor points from the posed image inputs; subsequently, a specialized reconstruction transformer refines the geometry and retrieves textural details. Extensive experimental results demonstrate that GeoLRM significantly outperforms existing models, especially for dense view inputs. We also demonstrate the practical applicability of our model with 3D generation tasks, showcasing its versatility and potential for broader adoption in real-world applications. The project page: https://linshan-bin.github.io/GeoLRM/.",
    "arxiv_url": "http://arxiv.org/abs/2406.15333v2",
    "pdf_url": "http://arxiv.org/pdf/2406.15333v2",
    "published_date": "2024-06-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "lightweight",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks",
    "authors": [
      "Alex Quach",
      "Makram Chahine",
      "Alexander Amini",
      "Ramin Hasani",
      "Daniela Rus"
    ],
    "abstract": "Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target manoeuvres curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.",
    "arxiv_url": "http://arxiv.org/abs/2406.15149v2",
    "pdf_url": "http://arxiv.org/pdf/2406.15149v2",
    "published_date": "2024-06-21",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "68T40, 68U20, 93C85",
      "I.2.9; I.2.6"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "outdoor",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "E2GS: Event Enhanced Gaussian Splatting",
    "authors": [
      "Hiroyuki Deguchi",
      "Mana Masuda",
      "Takuya Nakabayashi",
      "Hideo Saito"
    ],
    "abstract": "Event cameras, known for their high dynamic range, absence of motion blur, and low energy usage, have recently found a wide range of applications thanks to these attributes. In the past few years, the field of event-based 3D reconstruction saw remarkable progress, with the Neural Radiance Field (NeRF) based approach demonstrating photorealistic view synthesis results. However, the volume rendering paradigm of NeRF necessitates extensive training and rendering times. In this paper, we introduce Event Enhanced Gaussian Splatting (E2GS), a novel method that incorporates event data into Gaussian Splatting, which has recently made significant advances in the field of novel view synthesis. Our E2GS effectively utilizes both blurry images and event data, significantly improving image deblurring and producing high-quality novel view synthesis. Our comprehensive experiments on both synthetic and real-world datasets demonstrate our E2GS can generate visually appealing renderings while offering faster training and rendering speed (140 FPS). Our code is available at https://github.com/deguchihiroyuki/E2GS.",
    "arxiv_url": "http://arxiv.org/abs/2406.14978v1",
    "pdf_url": "http://arxiv.org/pdf/2406.14978v1",
    "published_date": "2024-06-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/deguchihiroyuki/E2GS",
    "keywords": [
      "motion",
      "fast",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation",
    "authors": [
      "Junhao Cai",
      "Yuji Yang",
      "Weihao Yuan",
      "Yisheng He",
      "Zilong Dong",
      "Liefeng Bo",
      "Hui Cheng",
      "Qifeng Chen"
    ],
    "abstract": "This paper studies the problem of estimating physical properties (system identification) through visual observations. To facilitate geometry-aware guidance in physical property estimation, we introduce a novel hybrid framework that leverages 3D Gaussian representation to not only capture explicit shapes but also enable the simulated continuum to render object masks as 2D shape surrogates during training. We propose a new dynamic 3D Gaussian framework based on motion factorization to recover the object as 3D Gaussian point sets across different time states. Furthermore, we develop a coarse-to-fine filling strategy to generate the density fields of the object from the Gaussian reconstruction, allowing for the extraction of object continuums along with their surfaces and the integration of Gaussian attributes into these continuum. In addition to the extracted object surfaces, the Gaussian-informed continuum also enables the rendering of object masks during simulations, serving as 2D-shape guidance for physical property estimation. Extensive experimental evaluations demonstrate that our pipeline achieves state-of-the-art performance across multiple benchmarks and metrics. Additionally, we illustrate the effectiveness of the proposed method through real-world demonstrations, showcasing its practical utility. Our project page is at https://jukgei.github.io/project/gic.",
    "arxiv_url": "http://arxiv.org/abs/2406.14927v3",
    "pdf_url": "http://arxiv.org/pdf/2406.14927v3",
    "published_date": "2024-06-21",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatter a Video: Video Gaussian Representation for Versatile Processing",
    "authors": [
      "Yang-Tian Sun",
      "Yi-Hua Huang",
      "Lin Ma",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ],
    "abstract": "Video representation is a long-standing problem that is crucial for various down-stream tasks, such as tracking,depth prediction,segmentation,view synthesis,and editing. However, current methods either struggle to model complex motions due to the absence of 3D structure or rely on implicit 3D representations that are ill-suited for manipulation tasks. To address these challenges, we introduce a novel explicit 3D representation-video Gaussian representation -- that embeds a video into 3D Gaussians. Our proposed representation models video appearance in a 3D canonical space using explicit Gaussians as proxies and associates each Gaussian with 3D motions for video motion. This approach offers a more intrinsic and explicit representation than layered atlas or volumetric pixel matrices. To obtain such a representation, we distill 2D priors, such as optical flow and depth, from foundation models to regularize learning in this ill-posed setting. Extensive applications demonstrate the versatility of our new video representation. It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation. Project page: https://sunyangtian.github.io/spatter_a_video_web/",
    "arxiv_url": "http://arxiv.org/abs/2406.13870v2",
    "pdf_url": "http://arxiv.org/pdf/2406.13870v2",
    "published_date": "2024-06-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models",
    "authors": [
      "Paul Henderson",
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Titas Anciukeviƒçius"
    ],
    "abstract": "We present a latent diffusion model over 3D scenes, that can be trained using only 2D image data. To achieve this, we first design an autoencoder that maps multi-view images to 3D Gaussian splats, and simultaneously builds a compressed latent representation of these splats. Then, we train a multi-view diffusion model over the latent space to learn an efficient generative model. This pipeline does not require object masks nor depths, and is suitable for complex scenes with arbitrary camera positions. We conduct careful experiments on two large-scale datasets of complex real-world scenes -- MVImgNet and RealEstate10K. We show that our approach enables generating 3D scenes in as little as 0.2 seconds, either from scratch, from a single input view, or from sparse input views. It produces diverse and high-quality results while running an order of magnitude faster than non-latent diffusion models and earlier NeRF-based generative models",
    "arxiv_url": "http://arxiv.org/abs/2406.13099v1",
    "pdf_url": "http://arxiv.org/pdf/2406.13099v1",
    "published_date": "2024-06-18",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors",
    "authors": [
      "Panwang Pan",
      "Zhuo Su",
      "Chenguo Lin",
      "Zhen Fan",
      "Yongjie Zhang",
      "Zeming Li",
      "Tingting Shen",
      "Yadong Mu",
      "Yebin Liu"
    ],
    "abstract": "Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we present HumanSplat which predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner. In particular, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is further designed to achieve high-fidelity texture modeling and better constrain the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2406.12459v2",
    "pdf_url": "http://arxiv.org/pdf/2406.12459v2",
    "published_date": "2024-06-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets",
    "authors": [
      "Bernhard Kerbl",
      "Andr√©as Meuleman",
      "Georgios Kopanas",
      "Michael Wimmer",
      "Alexandre Lanvin",
      "George Drettakis"
    ],
    "abstract": "Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels.We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour. Project Page: https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/",
    "arxiv_url": "http://arxiv.org/abs/2406.12080v1",
    "pdf_url": "http://arxiv.org/pdf/2406.12080v1",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians",
    "authors": [
      "Bingling Li",
      "Shengyi Chen",
      "Luchao Wang",
      "Kaimin Liao",
      "Sijie Yan",
      "Yuanjun Xiong"
    ],
    "abstract": "In this work, we explore the possibility of training high-parameter 3D Gaussian splatting (3DGS) models on large-scale, high-resolution datasets. We design a general model parallel training method for 3DGS, named RetinaGS, which uses a proper rendering equation and can be applied to any scene and arbitrary distribution of Gaussian primitives. It enables us to explore the scaling behavior of 3DGS in terms of primitive numbers and training resolutions that were difficult to explore before and surpass previous state-of-the-art reconstruction quality. We observe a clear positive trend of increasing visual quality when increasing primitive numbers with our method. We also demonstrate the first attempt at training a 3DGS model with more than one billion primitives on the full MatrixCity dataset that attains a promising visual quality.",
    "arxiv_url": "http://arxiv.org/abs/2406.11836v2",
    "pdf_url": "http://arxiv.org/pdf/2406.11836v2",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting",
    "authors": [
      "Junha Hyung",
      "Susung Hong",
      "Sungwon Hwang",
      "Jaeseong Lee",
      "Jaegul Choo",
      "Jin-Hwa Kim"
    ],
    "abstract": "3D reconstruction from multi-view images is one of the fundamental challenges in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising technique capable of real-time rendering with high-quality 3D reconstruction. This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying. Despite its potential, 3DGS encounters challenges such as needle-like artifacts, suboptimal geometries, and inaccurate normals caused by the Gaussians converging into anisotropic shapes with one dominant variance. We propose using the effective rank analysis to examine the shape statistics of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like shapes with the effective rank 1. To address this, we introduce the effective rank as a regularization, which constrains the structure of the Gaussians. Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts. The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity. The project page is available at https://junhahyung.github.io/erankgs.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2406.11672v3",
    "pdf_url": "http://arxiv.org/pdf/2406.11672v3",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Projecting Radiance Fields to Mesh Surfaces",
    "authors": [
      "Adrian Xuan Wei Lim",
      "Lynnette Hui Xian Ng",
      "Nicholas Kyger",
      "Tomo Michigami",
      "Faraz Baghernezhad"
    ],
    "abstract": "Radiance fields produce high fidelity images with high rendering speed, but are difficult to manipulate. We effectively perform avatar texture transfer across different appearances by combining benefits from radiance fields and mesh surfaces. We represent the source as a radiance field using 3D Gaussian Splatter, then project the Gaussians on the target mesh. Our pipeline consists of Source Preconditioning, Target Vectorization and Texture Projection. The projection completes in 1.12s in a pure CPU compute, compared to baselines techniques of Per Face Texture Projection and Ray Casting (31s, 4.1min). This method lowers the computational requirements, which makes it applicable to a broader range of devices from low-end mobiles to high end computers.",
    "arxiv_url": "http://arxiv.org/abs/2406.11570v1",
    "pdf_url": "http://arxiv.org/pdf/2406.11570v1",
    "published_date": "2024-06-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray casting",
      "face",
      "3d gaussian",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods",
    "authors": [
      "Milena T. Bagdasarian",
      "Paul Knoll",
      "Yi-Hsin Li",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert",
      "Wieland Morgenstern"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a cutting-edge technique for real-time radiance field rendering, offering state-of-the-art performance in terms of both quality and speed. 3DGS models a scene as a collection of three-dimensional Gaussians, with additional attributes optimized to conform to the scene's geometric and visual properties. Despite its advantages in rendering speed and image fidelity, 3DGS is limited by its significant storage and memory demands. These high demands make 3DGS impractical for mobile devices or headsets, reducing its applicability in important areas of computer graphics. To address these challenges and advance the practicality of 3DGS, this survey provides a comprehensive and detailed examination of compression and compaction techniques developed to make 3DGS more efficient. We classify existing methods into two categories: compression, which focuses on reducing file size, and compaction, which aims to minimize the number of Gaussians. Both methods aim to maintain or improve quality, each by minimizing its respective attribute: file size for compression and Gaussian count for compaction. We introduce the basic mathematical concepts underlying the analyzed methods, as well as key implementation details and design choices. Our report thoroughly discusses similarities and differences among the methods, as well as their respective advantages and disadvantages. We establish a consistent framework for comparing the surveyed methods based on key performance metrics and datasets. Specifically, since these methods have been developed in parallel and over a short period of time, currently, no comprehensive comparison exists. This survey, for the first time, presents a unified framework to evaluate 3DGS compression techniques. We maintain a website that will be regularly updated with emerging methods: https://w-m.github.io/3dgs-compression-survey/ .",
    "arxiv_url": "http://arxiv.org/abs/2407.09510v5",
    "pdf_url": "http://arxiv.org/pdf/2407.09510v5",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "survey",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics",
    "authors": [
      "Jad Abou-Chakra",
      "Krishan Rana",
      "Feras Dayoub",
      "Niko S√ºnderhauf"
    ],
    "abstract": "For robots to robustly understand and interact with the physical world, it is highly beneficial to have a comprehensive representation - modelling geometry, physics, and visual observations - that informs perception, planning, and control algorithms. We propose a novel dual Gaussian-Particle representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates visual forces that correct the particle positions while respecting known physical constraints. By integrating predictive physical modelling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality. Our system runs in realtime at 30Hz using only 3 cameras. We validate our approach on 2D and 3D tracking tasks as well as photometric reconstruction quality. Videos are found at https://embodied-gaussians.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2406.10788v1",
    "pdf_url": "http://arxiv.org/pdf/2406.10788v1",
    "published_date": "2024-06-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "tracking",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections",
    "authors": [
      "Jiacong Xu",
      "Yiqun Mei",
      "Vishal M. Patel"
    ],
    "abstract": "Photographs captured in unstructured tourist environments frequently exhibit variable appearances and transient occlusions, challenging accurate scene reconstruction and inducing artifacts in novel view synthesis. Although prior approaches have integrated the Neural Radiance Field (NeRF) with additional learnable modules to handle the dynamic appearances and eliminate transient objects, their extensive training demands and slow rendering speeds limit practical deployments. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising alternative to NeRF, offering superior training and inference efficiency along with better rendering quality. This paper presents Wild-GS, an innovative adaptation of 3DGS optimized for unconstrained photo collections while preserving its efficiency benefits. Wild-GS determines the appearance of each 3D Gaussian by their inherent material attributes, global illumination and camera properties per image, and point-level local variance of reflectance. Unlike previous methods that model reference features in image space, Wild-GS explicitly aligns the pixel appearance features to the corresponding local Gaussians by sampling the triplane extracted from the reference image. This novel design effectively transfers the high-frequency detailed appearance of the reference view to 3D space and significantly expedites the training process. Furthermore, 2D visibility maps and depth regularization are leveraged to mitigate the transient effects and constrain the geometry, respectively. Extensive experiments demonstrate that Wild-GS achieves state-of-the-art rendering performance and the highest efficiency in both training and inference among all the existing techniques.",
    "arxiv_url": "http://arxiv.org/abs/2406.10373v1",
    "pdf_url": "http://arxiv.org/pdf/2406.10373v1",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "global illumination",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting",
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Vasu Singla",
      "Mayuka Jayawardhana",
      "Matthias Zwicker",
      "Tom Goldstein"
    ],
    "abstract": "Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.",
    "arxiv_url": "http://arxiv.org/abs/2406.10219v3",
    "pdf_url": "http://arxiv.org/pdf/2406.10219v3",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L4GM: Large 4D Gaussian Reconstruction Model",
    "authors": [
      "Jiawei Ren",
      "Kevin Xie",
      "Ashkan Mirzaei",
      "Hanxue Liang",
      "Xiaohui Zeng",
      "Karsten Kreis",
      "Ziwei Liu",
      "Antonio Torralba",
      "Sanja Fidler",
      "Seung Wook Kim",
      "Huan Ling"
    ],
    "abstract": "We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input -- in a single feed-forward pass that takes only a second. Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. We keep our L4GM simple for scalability and build directly on top of LGM, a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input. L4GM outputs a per-frame 3D Gaussian Splatting representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. We showcase that L4GM that is only trained on synthetic data generalizes extremely well on in-the-wild videos, producing high quality animated 3D assets.",
    "arxiv_url": "http://arxiv.org/abs/2406.10324v1",
    "pdf_url": "http://arxiv.org/pdf/2406.10324v1",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "4d",
      "3d gaussian",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors",
    "authors": [
      "Xiqian Yu",
      "Hanxin Zhu",
      "Tianyu He",
      "Zhibo Chen"
    ],
    "abstract": "Achieving high-resolution novel view synthesis (HRNVS) from low-resolution input views is a challenging task due to the lack of high-resolution data. Previous methods optimize high-resolution Neural Radiance Field (NeRF) from low-resolution input views but suffer from slow rendering speed. In this work, we base our method on 3D Gaussian Splatting (3DGS) due to its capability of producing high-quality images at a faster rendering speed. To alleviate the shortage of data for higher-resolution synthesis, we propose to leverage off-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with Score Distillation Sampling (SDS). Nevertheless, applying SDS directly to Gaussian-based 3D super-resolution leads to undesirable and redundant 3D Gaussian primitives, due to the randomness brought by generative priors. To mitigate this issue, we introduce two simple yet effective techniques to reduce stochastic disturbances introduced by SDS. Specifically, we 1) shrink the range of diffusion timestep in SDS with an annealing strategy; 2) randomly discard redundant Gaussian primitives during densification. Extensive experiments have demonstrated that our proposed GaussainSR can attain high-quality results for HRNVS with only low-resolution inputs on both synthetic and real-world datasets. Project page: https://chchnii.github.io/GaussianSR/",
    "arxiv_url": "http://arxiv.org/abs/2406.10111v1",
    "pdf_url": "http://arxiv.org/pdf/2406.10111v1",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion",
    "authors": [
      "Trapoom Ukarapol",
      "Kevin Pruvost"
    ],
    "abstract": "Text-to-3D generation has shown promising results, yet common challenges such as the Multi-face Janus problem and extended generation time for high-quality assets. In this paper, we address these issues by introducing a novel three-stage training pipeline called GradeADreamer. This pipeline is capable of producing high-quality assets with a total generation time of under 30 minutes using only a single RTX 3090 GPU. Our proposed method employs a Multi-view Diffusion Model, MVDream, to generate Gaussian Splats as a prior, followed by refining geometry and texture using StableDiffusion. Experimental results demonstrate that our approach significantly mitigates the Multi-face Janus problem and achieves the highest average user preference ranking compared to previous state-of-the-art methods. The project code is available at https://github.com/trapoom555/GradeADreamer.",
    "arxiv_url": "http://arxiv.org/abs/2406.09850v1",
    "pdf_url": "http://arxiv.org/pdf/2406.09850v1",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/trapoom555/GradeADreamer",
    "keywords": [
      "geometry",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Unified Gaussian Primitives for Scene Representation and Rendering",
    "authors": [
      "Yang Zhou",
      "Songyin Wu",
      "Ling-Qi Yan"
    ],
    "abstract": "Searching for a unified scene representation remains a research challenge in computer graphics. Traditional mesh-based representations are unsuitable for dense, fuzzy elements, and introduce additional complexity for filtering and differentiable rendering. Conversely, voxel-based representations struggle to model hard surfaces and suffer from intensive memory requirement. We propose a general-purpose rendering primitive based on 3D Gaussian distribution for unified scene representation, featuring versatile appearance ranging from glossy surfaces to fuzzy elements, as well as physically based scattering to enable accurate global illumination. We formulate the rendering theory for the primitive based on non-exponential transport and derive efficient rendering operations to be compatible with Monte Carlo path tracing. The new representation can be converted from different sources, including meshes and 3D Gaussian splatting, and further refined via transmittance optimization thanks to its differentiability. We demonstrate the versatility of our representation in various rendering applications such as global illumination and appearance editing, while supporting arbitrary lighting conditions by nature. Additionally, we compare our representation to existing volumetric representations, highlighting its efficiency to reproduce details.",
    "arxiv_url": "http://arxiv.org/abs/2406.09733v2",
    "pdf_url": "http://arxiv.org/pdf/2406.09733v2",
    "published_date": "2024-06-14",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "path tracing",
      "lighting",
      "global illumination",
      "face",
      "3d gaussian",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Modeling Ambient Scene Dynamics for Free-view Synthesis",
    "authors": [
      "Meng-Li Shih",
      "Jia-Bin Huang",
      "Changil Kim",
      "Rajvi Shah",
      "Johannes Kopf",
      "Chen Gao"
    ],
    "abstract": "We introduce a novel method for dynamic free-view synthesis of an ambient scenes from a monocular capture bringing a immersive quality to the viewing experience. Our method builds upon the recent advancements in 3D Gaussian Splatting (3DGS) that can faithfully reconstruct complex static scenes. Previous attempts to extend 3DGS to represent dynamics have been confined to bounded scenes or require multi-camera captures, and often fail to generalize to unseen motions, limiting their practical application. Our approach overcomes these constraints by leveraging the periodicity of ambient motions to learn the motion trajectory model, coupled with careful regularization. We also propose important practical strategies to improve the visual quality of the baseline 3DGS static reconstructions and to improve memory efficiency critical for GPU-memory intensive learning. We demonstrate high-quality photorealistic novel view synthesis of several ambient natural scenes with intricate textures and fine structural elements.",
    "arxiv_url": "http://arxiv.org/abs/2406.09395v1",
    "pdf_url": "http://arxiv.org/pdf/2406.09395v1",
    "published_date": "2024-06-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GGHead: Fast and Generalizable 3D Gaussian Heads",
    "authors": [
      "Tobias Kirschstein",
      "Simon Giebenhain",
      "Jiapeng Tang",
      "Markos Georgopoulos",
      "Matthias Nie√üner"
    ],
    "abstract": "Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time. Project Website: https://tobias-kirschstein.github.io/gghead",
    "arxiv_url": "http://arxiv.org/abs/2406.09377v2",
    "pdf_url": "http://arxiv.org/pdf/2406.09377v2",
    "published_date": "2024-06-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis",
    "authors": [
      "Swapnil Bhosale",
      "Haosen Yang",
      "Diptesh Kanojia",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets.",
    "arxiv_url": "http://arxiv.org/abs/2406.08920v3",
    "pdf_url": "http://arxiv.org/pdf/2406.08920v3",
    "published_date": "2024-06-13",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling",
    "authors": [
      "Fengyi Zhang",
      "Yadan Luo",
      "Tianjun Zhang",
      "Lin Zhang",
      "Zi Huang"
    ],
    "abstract": "The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes will be available at https://github.com/Xian-Bei/GaussianForest.",
    "arxiv_url": "http://arxiv.org/abs/2406.08759v2",
    "pdf_url": "http://arxiv.org/pdf/2406.08759v2",
    "published_date": "2024-06-13",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/Xian-Bei/GaussianForest",
    "keywords": [
      "efficient",
      "ray tracing",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ICE-G: Image Conditional Editing of 3D Gaussian Splats",
    "authors": [
      "Vishnu Jaganathan",
      "Hannah Hanyun Huang",
      "Muhammad Zubair Irshad",
      "Varun Jampani",
      "Amit Raj",
      "Zsolt Kira"
    ],
    "abstract": "Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing. Project page: ice-gaussian.github.io",
    "arxiv_url": "http://arxiv.org/abs/2406.08488v1",
    "pdf_url": "http://arxiv.org/pdf/2406.08488v1",
    "published_date": "2024-06-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "high quality",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models",
    "authors": [
      "Yuxuan Xue",
      "Xianghui Xie",
      "Riccardo Marin",
      "Gerard Pons-Moll"
    ],
    "abstract": "Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on https://yuxuan-xue.com/human-3diffusion.",
    "arxiv_url": "http://arxiv.org/abs/2406.08475v2",
    "pdf_url": "http://arxiv.org/pdf/2406.08475v2",
    "published_date": "2024-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Chaos to Clarity: 3DGS in the Dark",
    "authors": [
      "Zhihao Li",
      "Yufei Wang",
      "Alex Kot",
      "Bihan Wen"
    ],
    "abstract": "Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images. However, the inherent noise in unprocessed raw images compromises the accuracy of 3D scene representation. Our study reveals that 3D Gaussian Splatting (3DGS) is particularly susceptible to this noise, leading to numerous elongated Gaussian shapes that overfit the noise, thereby significantly degrading reconstruction quality and reducing inference speed, especially in scenarios with limited views. To address these issues, we introduce a novel self-supervised learning framework designed to reconstruct HDR 3DGS from a limited number of noisy raw images. This framework enhances 3DGS by integrating a noise extractor and employing a noise-robust reconstruction loss that leverages a noise distribution prior. Experimental results show that our method outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised and supervised pre-trained models in both reconstruction quality and inference speed on the RawNeRF dataset across a broad range of training views. Code can be found in \\url{https://lizhihao6.github.io/Raw3DGS}.",
    "arxiv_url": "http://arxiv.org/abs/2406.08300v1",
    "pdf_url": "http://arxiv.org/pdf/2406.08300v1",
    "published_date": "2024-06-12",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trim 3D Gaussian Splatting for Accurate Geometry Representation",
    "authors": [
      "Lue Fan",
      "Yuxue Yang",
      "Minxing Li",
      "Hongsheng Li",
      "Zhaoxiang Zhang"
    ],
    "abstract": "In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality. Our project page is https://trimgs.github.io",
    "arxiv_url": "http://arxiv.org/abs/2406.07499v1",
    "pdf_url": "http://arxiv.org/pdf/2406.07499v1",
    "published_date": "2024-06-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field",
    "authors": [
      "Chao Wang",
      "Krzysztof Wolski",
      "Bernhard Kerbl",
      "Ana Serrano",
      "Mojtaba Bemana",
      "Hans-Peter Seidel",
      "Karol Myszkowski",
      "Thomas Leimk√ºhler"
    ],
    "abstract": "Radiance field methods represent the state of the art in reconstructing complex scenes from multi-view photos. However, these reconstructions often suffer from one or both of the following limitations: First, they typically represent scenes in low dynamic range (LDR), which restricts their use to evenly lit environments and hinders immersive viewing experiences. Secondly, their reliance on a pinhole camera model, assuming all scene elements are in focus in the input images, presents practical challenges and complicates refocusing during novel-view synthesis. Addressing these limitations, we present a lightweight method based on 3D Gaussian Splatting that utilizes multi-view LDR images of a scene with varying exposure times, apertures, and focus distances as input to reconstruct a high-dynamic-range (HDR) radiance field. By incorporating analytical convolutions of Gaussians based on a thin-lens camera model as well as a tonemapping module, our reconstructions enable the rendering of HDR content with flexible refocusing capabilities. We demonstrate that our combined treatment of HDR and depth of field facilitates real-time cinematic rendering, outperforming the state of the art.",
    "arxiv_url": "http://arxiv.org/abs/2406.07329v4",
    "pdf_url": "http://arxiv.org/pdf/2406.07329v4",
    "published_date": "2024-06-11",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Gaussian Splatting for Unbounded 3D City Generation",
    "authors": [
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ],
    "abstract": "3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation. However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2. In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass. Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation. 2) Spatial-aware Gaussian Attribute Decoder: We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points. Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS).",
    "arxiv_url": "http://arxiv.org/abs/2406.06526v3",
    "pdf_url": "http://arxiv.org/pdf/2406.06526v3",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction",
    "authors": [
      "Danpeng Chen",
      "Hai Li",
      "Weicai Ye",
      "Yifan Wang",
      "Weijian Xie",
      "Shangjin Zhai",
      "Nan Wang",
      "Haomin Liu",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due to its high-quality rendering, and ultra-fast training and rendering speed. However, due to the unstructured and irregular nature of Gaussian point clouds, it is difficult to guarantee geometric reconstruction accuracy and multi-view consistency simply by relying on image reconstruction loss. Although many studies on surface reconstruction based on 3DGS have emerged recently, the quality of their meshes is generally unsatisfactory. To address this problem, we propose a fast planar-based Gaussian splatting reconstruction representation (PGSR) to achieve high-fidelity surface reconstruction while ensuring high-quality rendering. Specifically, we first introduce an unbiased depth rendering method, which directly renders the distance from the camera origin to the Gaussian plane and the corresponding normal map based on the Gaussian distribution of the point cloud, and divides the two to obtain the unbiased depth. We then introduce single-view geometric, multi-view photometric, and geometric regularization to preserve global geometric accuracy. We also propose a camera exposure compensation model to cope with scenes with large illumination variations. Experiments on indoor and outdoor scenes show that our method achieves fast training and rendering while maintaining high-fidelity rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2406.06521v2",
    "pdf_url": "http://arxiv.org/pdf/2406.06521v2",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "efficient",
      "high-fidelity",
      "outdoor",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGamba: Unify 3D Content Generation as State Space Sequence Modeling",
    "authors": [
      "Xuanyu Yi",
      "Zike Wu",
      "Qiuhong Shen",
      "Qingshan Xu",
      "Pan Zhou",
      "Joo-Hwee Lim",
      "Shuicheng Yan",
      "Xinchao Wang",
      "Hanwang Zhang"
    ],
    "abstract": "Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (e.g., Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\times$ of the model size.",
    "arxiv_url": "http://arxiv.org/abs/2406.06367v3",
    "pdf_url": "http://arxiv.org/pdf/2406.06367v3",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis",
    "authors": [
      "Xin Jin",
      "Pengyi Jiao",
      "Zheng-Peng Duan",
      "Xingchao Yang",
      "Chun-Le Guo",
      "Bo Ren",
      "Chongyi Li"
    ],
    "abstract": "Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes. While, they suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in distant views; 2) the limited representation capacity of spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM, and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for downstream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous volumetric rendering based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can be found in https://github.com/Srameo/LE3D .",
    "arxiv_url": "http://arxiv.org/abs/2406.06216v1",
    "pdf_url": "http://arxiv.org/pdf/2406.06216v1",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Srameo/LE3D",
    "keywords": [
      "lighting",
      "fast",
      "motion",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for Direct Rendering",
    "authors": [
      "Yueyu Hu",
      "Ran Gong",
      "Yao Wang"
    ],
    "abstract": "Point cloud is a promising 3D representation for volumetric streaming in emerging AR/VR applications. Despite recent advances in point cloud compression, decoding and rendering high-quality images from lossy compressed point clouds is still challenging in terms of quality and complexity, making it a major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In this paper, we address this problem by developing a point cloud compression scheme that generates a bit stream that can be directly decoded to renderable 3D Gaussians. The encoder and decoder are jointly optimized to consider both bit-rates and rendering quality. It significantly improves the rendering quality while substantially reducing decoding and rendering time, compared to existing point cloud compression methods. Furthermore, the proposed scheme generates a scalable bit stream, allowing multiple levels of details at different bit-rate ranges. Our method supports real-time color decoding and rendering of high quality point clouds, thus paving the way for interactive 3D streaming applications with free view points.",
    "arxiv_url": "http://arxiv.org/abs/2406.05915v2",
    "pdf_url": "http://arxiv.org/pdf/2406.05915v2",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "high quality",
      "3d gaussian",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InfoGaussian: Structure-Aware Dynamic Gaussians through Lightweight Information Shaping",
    "authors": [
      "Yunchao Zhang",
      "Guandao Yang",
      "Leonidas Guibas",
      "Yanchao Yang"
    ],
    "abstract": "3D Gaussians, as a low-level scene representation, typically involve thousands to millions of Gaussians. This makes it difficult to control the scene in ways that reflect the underlying dynamic structure, where the number of independent entities is typically much smaller. In particular, it can be challenging to animate and move objects in the scene, which requires coordination among many Gaussians. To address this issue, we develop a mutual information shaping technique that enforces movement resonance between correlated Gaussians in a motion network. Such correlations can be learned from putative 2D object masks in different views. By approximating the mutual information with the Jacobians of the motions, our method ensures consistent movements of the Gaussians composing different objects under various perturbations. In particular, we develop an efficient contrastive training pipeline with lightweight optimization to shape the motion network, avoiding the need for re-shaping throughout the motion sequence. Notably, our training only touches a small fraction of all Gaussians in the scene yet attains the desired compositional behavior according to the underlying dynamic structure. The proposed technique is evaluated on challenging scenes and demonstrates significant performance improvement in promoting consistent movements and 3D object segmentation while inducing low computation and memory requirements.",
    "arxiv_url": "http://arxiv.org/abs/2406.05897v2",
    "pdf_url": "http://arxiv.org/pdf/2406.05897v2",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "segmentation",
      "3d gaussian",
      "ar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Simplicits: Mesh-Free, Geometry-Agnostic, Elastic Simulation",
    "authors": [
      "Vismay Modi",
      "Nicholas Sharp",
      "Or Perel",
      "Shinjiro Sueda",
      "David I. W. Levin"
    ],
    "abstract": "The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.",
    "arxiv_url": "http://arxiv.org/abs/2407.09497v1",
    "pdf_url": "http://arxiv.org/pdf/2407.09497v1",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "deformation",
      "geometry",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for Realistic Rendering",
    "authors": [
      "Rui Zhang",
      "Tianyue Luo",
      "Weidong Yang",
      "Ben Fei",
      "Jingyi Xu",
      "Qingyuan Zhou",
      "Keyi Liu",
      "Ying He"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of neural rendering, 3D scene reconstruction, and novel view synthesis. Nevertheless, 3D-GS encounters the main challenge when it comes to accurately representing physical reflections, especially in the case of total reflection and semi-reflection that are commonly found in real-world scenes. This limitation causes reflections to be mistakenly treated as independent elements with physical presence, leading to imprecise reconstructions. Herein, to tackle this challenge, we propose RefGaussian to disentangle reflections from 3D-GS for realistically modeling reflections. Specifically, we propose to split a scene into transmitted and reflected components and represent these components using two Spherical Harmonics (SH). Given that this decomposition is not fully determined, we employ local regularization techniques to ensure local smoothness for both the transmitted and reflected components, thereby achieving more plausible decomposition outcomes than 3D-GS. Experimental results demonstrate that our approach achieves superior novel view synthesis and accurate depth estimation outcomes. Furthermore, it enables the utilization of scene editing applications, ensuring both high-quality results and physical coherence.",
    "arxiv_url": "http://arxiv.org/abs/2406.05852v1",
    "pdf_url": "http://arxiv.org/pdf/2406.05852v1",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction",
    "authors": [
      "Hanlin Chen",
      "Fangyin Wei",
      "Chen Li",
      "Tianxin Huang",
      "Yunsong Wang",
      "Gim Hee Lee"
    ],
    "abstract": "Although 3D Gaussian Splatting has been widely studied because of its realistic and efficient novel-view synthesis, it is still challenging to extract a high-quality surface from the point-based representation. Previous works improve the surface by incorporating geometric priors from the off-the-shelf normal estimator. However, there are two main limitations: 1) Supervising normals rendered from 3D Gaussians effectively updates the rotation parameter but is less effective for other geometric parameters; 2) The inconsistency of predicted normal maps across multiple views may lead to severe reconstruction artifacts. In this paper, we propose a Depth-Normal regularizer that directly couples normal with other geometric parameters, leading to full updates of the geometric parameters from normal regularization. We further propose a confidence term to mitigate inconsistencies of normal predictions across multiple views. Moreover, we also introduce a densification and splitting strategy to regularize the size and distribution of 3D Gaussians for more accurate surface modeling. Compared with Gaussian-based baselines, experiments show that our approach obtains better reconstruction quality and maintains competitive appearance quality at faster training speed and 100+ FPS rendering.",
    "arxiv_url": "http://arxiv.org/abs/2406.05774v2",
    "pdf_url": "http://arxiv.org/pdf/2406.05774v2",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image",
    "authors": [
      "Stanislaw Szymanowicz",
      "Eldar Insafutdinov",
      "Chuanxia Zheng",
      "Dylan Campbell",
      "Jo√£o F. Henriques",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "We propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a \"foundation\" model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input. Code, models, demo, and more results are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.",
    "arxiv_url": "http://arxiv.org/abs/2406.04343v2",
    "pdf_url": "http://arxiv.org/pdf/2406.04343v2",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion",
    "authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Shunyu Yao",
      "Shengjun Zhang",
      "Jie Zhou",
      "Yueqi Duan"
    ],
    "abstract": "In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors. However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world. To accurately simulate physics-aligned dynamics, it is essential to predict the physical properties of materials and incorporate them into the behavior prediction process. Nonetheless, predicting the diverse materials of real-world objects is still challenging due to the complex nature of their physical attributes. In this paper, we propose \\textbf{Physics3D}, a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities. Moreover, we distill the physical priors from a video diffusion model that contains more understanding of realistic object materials. Extensive experiments demonstrate the effectiveness of our method with both elastic and plastic materials. Physics3D shows great potential for bridging the gap between the physical world and virtual neural space, providing a better integration and application of realistic physical principles in virtual environments. Project page: https://liuff19.github.io/Physics3D.",
    "arxiv_url": "http://arxiv.org/abs/2406.04338v3",
    "pdf_url": "http://arxiv.org/pdf/2406.04338v3",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "understanding",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation",
    "authors": [
      "Ruihe Wang",
      "Yukang Cao",
      "Kai Han",
      "Kwan-Yee K. Wong"
    ],
    "abstract": "3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.",
    "arxiv_url": "http://arxiv.org/abs/2406.04253v1",
    "pdf_url": "http://arxiv.org/pdf/2406.04253v1",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "survey",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Gaussian Splatting with Localized Points Management",
    "authors": [
      "Haosen Yang",
      "Chenhao Zhang",
      "Wenqing Wang",
      "Marco Volino",
      "Adrian Hilton",
      "Li Zhang",
      "Xiatian Zhu"
    ],
    "abstract": "Point management is critical for optimizing 3D Gaussian Splatting models, as point initiation (e.g., via structure from motion) is often distributionally inappropriate. Typically, Adaptive Density Control (ADC) algorithm is adopted, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset. We reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) due to inability of identifying all 3D zones requiring point densification, and lacking an appropriate mechanism to handle ill-conditioned points with negative impacts (e.g., occlusion due to false high opacity). To address these limitations, we propose a Localized Point Management (LPM) strategy, capable of identifying those error-contributing zones in greatest need for both point addition and geometry calibration. Zone identification is achieved by leveraging the underlying multiview geometry constraints, subject to image rendering errors. We apply point densification in the identified zones and then reset the opacity of the points in front of these regions, creating a new opportunity to correct poorly conditioned points. Serving as a versatile plugin, LPM can be seamlessly integrated into existing static 3D and dynamic 4D Gaussian Splatting models with minimal additional cost. Experimental evaluations validate the efficacy of our LPM in boosting a variety of existing 3D/4D models both quantitatively and qualitatively. Notably, LPM improves both static 3DGS and dynamic SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, excelling on challenging datasets such as Tanks & Temples and the Neural 3D Video dataset.",
    "arxiv_url": "http://arxiv.org/abs/2406.04251v3",
    "pdf_url": "http://arxiv.org/pdf/2406.04251v3",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "geometry",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction",
    "authors": [
      "Diwen Wan",
      "Ruijie Lu",
      "Gang Zeng"
    ],
    "abstract": "Rendering novel view images in dynamic scenes is a crucial yet challenging task. Current methods mainly utilize NeRF-based methods to represent the static scene and an additional time-variant MLP to model scene deformations, resulting in relatively low rendering quality as well as slow inference speed. To tackle these challenges, we propose a novel framework named Superpoint Gaussian Splatting (SP-GS). Specifically, our framework first employs explicit 3D Gaussians to reconstruct the scene and then clusters Gaussians with similar properties (e.g., rotation, translation, and location) into superpoints. Empowered by these superpoints, our method manages to extend 3D Gaussian splatting to dynamic scenes with only a slight increase in computational expense. Apart from achieving state-of-the-art visual quality and real-time rendering under high resolutions, the superpoint representation provides a stronger manipulation capability. Extensive experiments demonstrate the practicality and effectiveness of our approach on both synthetic and real-world datasets. Please see our project page at https://dnvtmf.github.io/SP_GS.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2406.03697v1",
    "pdf_url": "http://arxiv.org/pdf/2406.03697v1",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Primitives for Deformable Image Registration",
    "authors": [
      "Jihe Li",
      "Xiang Liu",
      "Fabian Zhang",
      "Xia Li",
      "Xixin Cao",
      "Ye Zhang",
      "Joachim Buhmann"
    ],
    "abstract": "Deformable Image Registration (DIR) is essential for aligning medical images that exhibit anatomical variations, facilitating applications such as disease tracking and radiotherapy planning. While classical iterative methods and deep learning approaches have achieved success in DIR, they are often hindered by computational inefficiency or poor generalization. In this paper, we introduce GaussianDIR, a novel, case-specific optimization DIR method inspired by 3D Gaussian splatting. In general, GaussianDIR represents image deformations using a sparse set of mobile and flexible Gaussian primitives, each defined by a center position, covariance, and local rigid transformation. This compact and explicit representation reduces noise and computational overhead while improving interpretability. Furthermore, the movement of individual voxel is derived via blending the local rigid transformation of the neighboring Gaussian primitives. By this, GaussianDIR captures both global smoothness and local rigidity as well as reduces the computational burden. To address varying levels of deformation complexity, GaussianDIR also integrates an adaptive density control mechanism that dynamically adjusts the density of Gaussian primitives. Additionally, we employ multi-scale Gaussian primitives to capture both coarse and fine deformations, reducing optimization to local minima. Experimental results on brain MRI, lung CT, and cardiac MRI datasets demonstrate that GaussianDIR outperforms existing DIR methods in both accuracy and efficiency, highlighting its potential for clinical applications. Finally, as a training-free approach, it challenges the stereotype that iterative methods are inherently slow and transcend the limitations of poor generalization.",
    "arxiv_url": "http://arxiv.org/abs/2406.03394v2",
    "pdf_url": "http://arxiv.org/pdf/2406.03394v2",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "tracking",
      "lighting",
      "deformation",
      "medical",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic 3D Gaussian Fields for Urban Areas",
    "authors": [
      "Tobias Fischer",
      "Jonas Kulhanek",
      "Samuel Rota Bul√≤",
      "Lorenzo Porzi",
      "Marc Pollefeys",
      "Peter Kontschieder"
    ],
    "abstract": "We present an efficient neural 3D scene representation for novel-view synthesis (NVS) in large-scale, dynamic urban areas. Existing works are not well suited for applications like mixed-reality or closed-loop simulation due to their limited visual quality and non-interactive rendering speeds. Recently, rasterization-based approaches have achieved high-quality NVS at impressive speeds. However, these methods are limited to small-scale, homogeneous data, i.e. they cannot handle severe appearance and geometry variations due to weather, season, and lighting and do not scale to larger, dynamic areas with thousands of images. We propose 4DGF, a neural scene representation that scales to large-scale dynamic urban areas, handles heterogeneous input data, and substantially improves rendering speeds. We use 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. We integrate scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. This decomposed approach enables flexible scene composition suitable for real-world applications. In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than 200 times in rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2406.03175v2",
    "pdf_url": "http://arxiv.org/pdf/2406.03175v2",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "4d",
      "ar",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion",
    "authors": [
      "Tianyi Xiong",
      "Jiayi Wu",
      "Botao He",
      "Cornelia Fermuller",
      "Yiannis Aloimonos",
      "Heng Huang",
      "Christopher A. Metzler"
    ],
    "abstract": "By combining differentiable rendering with explicit point-based scene representations, 3D Gaussian Splatting (3DGS) has demonstrated breakthrough 3D reconstruction capabilities. However, to date 3DGS has had limited impact on robotics, where high-speed egomotion is pervasive: Egomotion introduces motion blur and leads to artifacts in existing frame-based 3DGS reconstruction methods. To address this challenge, we introduce Event3DGS, an {\\em event-based} 3DGS framework. By exploiting the exceptional temporal resolution of event cameras, Event3GDS can reconstruct high-fidelity 3D structure and appearance under high-speed egomotion. Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of Event3DGS compared with existing event-based dense 3D scene reconstruction frameworks; Event3DGS substantially improves reconstruction quality (+3dB) while reducing computational costs by 95\\%. Our framework also allows one to incorporate a few motion-blurred frame-based measurements into the reconstruction process to further improve appearance fidelity without loss of structural accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2406.02972v4",
    "pdf_url": "http://arxiv.org/pdf/2406.02972v4",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSGAN: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats",
    "authors": [
      "Sangeek Hyun",
      "Jae-Pil Heo"
    ],
    "abstract": "Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a na\\\"ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability. Project page: https://hse1032.github.io/gsgan.",
    "arxiv_url": "http://arxiv.org/abs/2406.02968v2",
    "pdf_url": "http://arxiv.org/pdf/2406.02968v2",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray casting",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-HGS: 3D Half-Gaussian Splatting",
    "authors": [
      "Haolin Li",
      "Jinyang Liu",
      "Mario Sznaier",
      "Octavia Camps"
    ],
    "abstract": "Photo-realistic image rendering from 3D scene reconstruction has advanced significantly with neural rendering techniques. Among these, 3D Gaussian Splatting (3D-GS) outperforms Neural Radiance Fields (NeRFs) in quality and speed but struggles with shape and color discontinuities. We propose 3D Half-Gaussian (3D-HGS) kernels as a plug-and-play solution to address these limitations. Our experiments show that 3D-HGS enhances existing 3D-GS methods, achieving state-of-the-art rendering quality without compromising speed.",
    "arxiv_url": "http://arxiv.org/abs/2406.02720v4",
    "pdf_url": "http://arxiv.org/pdf/2406.02720v4",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
    "authors": [
      "Inkyu Shin",
      "Qihang Yu",
      "Xiaohui Shen",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ],
    "abstract": "Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos.",
    "arxiv_url": "http://arxiv.org/abs/2406.02541v4",
    "pdf_url": "http://arxiv.org/pdf/2406.02541v4",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection Ensembles for Satellite Feature Recognition",
    "authors": [
      "Van Minh Nguyen",
      "Emma Sandidge",
      "Trupti Mahendrakar",
      "Ryan T. White"
    ],
    "abstract": "On-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possibly unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. In this article, we present an approach for mapping geometries and high-confidence detection of components of unknown, non-cooperative satellites on orbit. We implement accelerated 3D Gaussian splatting to learn a 3D representation of the satellite, render virtual views of the target, and ensemble the YOLOv5 object detector over the virtual views, resulting in reliable, accurate, and precise satellite component detections. The full pipeline capable of running on-board and stand to enable downstream machine intelligence tasks necessary for autonomous guidance, navigation, and control tasks.",
    "arxiv_url": "http://arxiv.org/abs/2406.02533v1",
    "pdf_url": "http://arxiv.org/pdf/2406.02533v1",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "mapping",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Xiao Chen",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods. While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering. We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy. Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods.",
    "arxiv_url": "http://arxiv.org/abs/2406.02518v2",
    "pdf_url": "http://arxiv.org/pdf/2406.02518v2",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections",
    "authors": [
      "Yuze Wang",
      "Junyi Wang",
      "Yue Qi"
    ],
    "abstract": "Novel View Synthesis (NVS) from unconstrained photo collections is challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has shown promise for photorealistic and real-time NVS of static scenes. Building on 3DGS, we propose an efficient point-based differentiable rendering framework for scene reconstruction from photo collections. Our key innovation is a residual-based spherical harmonic coefficients transfer module that adapts 3DGS to varying lighting conditions and photometric post-processing. This lightweight module can be pre-computed and ensures efficient gradient propagation from rendered images to 3D Gaussian attributes. Additionally, we observe that the appearance encoder and the transient mask predictor, the two most critical parts of NVS from unconstrained photo collections, can be mutually beneficial. We introduce a plug-and-play lightweight spatial attention module to simultaneously predict transient occluders and latent appearance representation for each image. After training and preprocessing, our method aligns with the standard 3DGS format and rendering pipeline, facilitating seamlessly integration into various 3DGS applications. Extensive experiments on diverse datasets show our approach outperforms existing approaches on the rendering quality of novel view and appearance synthesis with high converge and rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2406.02407v1",
    "pdf_url": "http://arxiv.org/pdf/2406.02407v1",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning",
    "authors": [
      "Jiaxu Wang",
      "Ziyi Zhang",
      "Qiang Zhang",
      "Jia Li",
      "Jingkai Sun",
      "Mingyuan Sun",
      "Junhao He",
      "Renjing Xu"
    ],
    "abstract": "Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene representation learning. However, these NeRF-related methods struggle to perceive 3D structural information due to the inefficient dense sampling in volumetric rendering. Moreover, they lack fine-grained semantic information included in their scene representation vectors because they evenly consider free and occupied spaces. Both of them can destroy the performance of downstream RL tasks. To address the above challenges, we propose a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to learn 3D scene representation for the first time. In brief, we present the Query-based Generalizable 3DGS to bridge the 3DGS technique and scene representations with more geometrical awareness than those in NeRFs. Moreover, we present the Hierarchical Semantics Encoding to ground the fine-grained semantic features to 3D Gaussians and further distilled to the scene representation vectors. We conduct extensive experiments on two RL platforms including Maniskill2 and Robomimic across 10 different tasks. The results show that our method outperforms the other 5 baselines by a large margin. We achieve the best success rates on 8 tasks and the second-best on the other two tasks.",
    "arxiv_url": "http://arxiv.org/abs/2406.02370v4",
    "pdf_url": "http://arxiv.org/pdf/2406.02370v4",
    "published_date": "2024-06-04",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding",
    "authors": [
      "Yanmin Wu",
      "Jiarui Meng",
      "Haijie Li",
      "Chenming Wu",
      "Yahao Shi",
      "Xinhua Cheng",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Jian Zhang"
    ],
    "abstract": "This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) capable of 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level. Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. The source code is available at our project page: https://3d-aigc.github.io/OpenGaussian",
    "arxiv_url": "http://arxiv.org/abs/2406.02058v2",
    "pdf_url": "http://arxiv.org/pdf/2406.02058v2",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping",
    "authors": [
      "Yuzhou Ji",
      "He Zhu",
      "Junshu Tang",
      "Wuyi Liu",
      "Zhizhong Zhang",
      "Xin Tan",
      "Yuan Xie"
    ],
    "abstract": "The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems.",
    "arxiv_url": "http://arxiv.org/abs/2406.01916v4",
    "pdf_url": "http://arxiv.org/pdf/2406.01916v4",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "mapping",
      "understanding",
      "segmentation",
      "high quality",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting",
    "authors": [
      "Shaojie Ma",
      "Yawei Luo",
      "Wei Yang",
      "Yi Yang"
    ],
    "abstract": "3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation.",
    "arxiv_url": "http://arxiv.org/abs/2406.01593v2",
    "pdf_url": "http://arxiv.org/pdf/2406.01593v2",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tetrahedron Splatting for 3D Generation",
    "authors": [
      "Chun Gu",
      "Zeyu Yang",
      "Zijie Pan",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "3D representation is essential to the significant advance of 3D generation with 2D diffusion priors. As a flexible representation, NeRF has been first adopted for 3D representation. With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges. Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction. In this work, we introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering simultaneously. This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve generation quality and stability. Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge. Our TeT-Splatting can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization. Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.",
    "arxiv_url": "http://arxiv.org/abs/2406.01579v2",
    "pdf_url": "http://arxiv.org/pdf/2406.01579v2",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaDe-GS: Rasterizing Depth in Gaussian Splatting",
    "authors": [
      "Baowen Zhang",
      "Chuan Fang",
      "Rakesh Shrestha",
      "Yixun Liang",
      "Xiaoxiao Long",
      "Ping Tan"
    ],
    "abstract": "Gaussian Splatting (GS) has proven to be highly effective in novel view synthesis, achieving high-quality and real-time rendering. However, its potential for reconstructing detailed 3D shapes has not been fully explored. Existing methods often suffer from limited shape accuracy due to the discrete and unstructured nature of Gaussian splats, which complicates the shape extraction. While recent techniques like 2D GS have attempted to improve shape reconstruction, they often reformulate the Gaussian primitives in ways that reduce both rendering quality and computational efficiency. To address these problems, our work introduces a rasterized approach to render the depth maps and surface normal maps of general 3D Gaussian splats. Our method not only significantly enhances shape reconstruction accuracy but also maintains the computational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and maintains similar computational efficiency as the original 3D GS methods. Our method is a significant advancement in Gaussian Splatting and can be directly integrated into existing Gaussian Splatting-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2406.01467v2",
    "pdf_url": "http://arxiv.org/pdf/2406.01467v2",
    "published_date": "2024-06-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using Gaussian Splatting",
    "authors": [
      "Fang Li",
      "Hao Zhang",
      "Narendra Ahuja"
    ],
    "abstract": "Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. > hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at https://github.com/fangli333/SC-4DGS.",
    "arxiv_url": "http://arxiv.org/abs/2406.01042v3",
    "pdf_url": "http://arxiv.org/pdf/2406.01042v3",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/fangli333/SC-4DGS",
    "keywords": [
      "high-fidelity",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
    "authors": [
      "Yuan Shen",
      "Duygu Ceylan",
      "Paul Guerrero",
      "Zexiang Xu",
      "Niloy J. Mitra",
      "Shenlong Wang",
      "Anna Fr√ºhst√ºck"
    ],
    "abstract": "We present a simple, modular, and generic method that upsamples coarse 3D models by adding geometric and appearance details. While generative 3D models now exist, they do not yet match the quality of their counterparts in image and video domains. We demonstrate that it is possible to directly repurpose existing (pretrained) video models for 3D super-resolution and thus sidestep the problem of the shortage of large repositories of high-quality 3D training models. We describe how to repurpose video upsampling models, which are not 3D consistent, and combine them with 3D consolidation to produce 3D-consistent results. As output, we produce high quality Gaussian Splat models, which are object centric and effective. Our method is category agnostic and can be easily incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian on a variety of 3D inputs, which are diverse both in terms of complexity and representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our simple method significantly improves the fidelity of the final 3D models. Check our project website for details: supergaussian.github.io",
    "arxiv_url": "http://arxiv.org/abs/2406.00609v4",
    "pdf_url": "http://arxiv.org/pdf/2406.00609v4",
    "published_date": "2024-06-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "high quality",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture",
    "authors": [
      "Xuanchen Li",
      "Yuhao Cheng",
      "Xingyu Ren",
      "Haozhe Jia",
      "Di Xu",
      "Wenhan Zhu",
      "Yichao Yan"
    ],
    "abstract": "4D head capture aims to generate dynamic topological meshes and corresponding texture maps from videos, which is widely utilized in movies and games for its ability to simulate facial muscle movements and recover dynamic textures in pore-squeezing. The industry often adopts the method involving multi-view stereo and non-rigid alignment. However, this approach is prone to errors and heavily reliant on time-consuming manual processing by artists. To simplify this process, we propose Topo4D, a novel framework for automatic geometry and texture generation, which optimizes densely aligned 4D heads and 8K texture maps directly from calibrated multi-view time-series images. Specifically, we first represent the time-series faces as a set of dynamic 3D Gaussians with fixed topology in which the Gaussian centers are bound to the mesh vertices. Afterward, we perform alternative geometry and texture optimization frame-by-frame for high-quality geometry and texture learning while maintaining temporal topology stability. Finally, we can extract dynamic facial meshes in regular wiring arrangement and high-fidelity textures with pore-level details from the learned Gaussians. Extensive experiments show that our method achieves superior results than the current SOTA face reconstruction methods both in the quality of meshes and textures. Project page: https://xuanchenli.github.io/Topo4D/.",
    "arxiv_url": "http://arxiv.org/abs/2406.00440v3",
    "pdf_url": "http://arxiv.org/pdf/2406.00440v3",
    "published_date": "2024-06-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos with Depth Priors",
    "authors": [
      "Qingming Liu",
      "Yuan Liu",
      "Jiepeng Wang",
      "Xianqiang Lyv",
      "Peng Wang",
      "Wenping Wang",
      "Junhui Hou"
    ],
    "abstract": "In this paper, we propose MoDGS, a new pipeline to render novel views of dy namic scenes from a casually captured monocular video. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid move ment of input cameras to construct multiview consistency but struggle to recon struct dynamic scenes on casually captured input videos whose cameras are either static or move slowly. To address this challenging task, MoDGS adopts recent single-view depth estimation methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization method is proposed to learn a reasonable deformation field and a new robust depth loss is proposed to guide the learning of dynamic scene geometry. Comprehensive experiments demonstrate that MoDGS is able to render high-quality novel view images of dynamic scenes from just a casually captured monocular video, which outperforms state-of-the-art meth ods by a significant margin. The code will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2406.00434v3",
    "pdf_url": "http://arxiv.org/pdf/2406.00434v3",
    "published_date": "2024-06-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "geometry",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting",
    "authors": [
      "Yumeng He",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce MetaGS to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the Blinn-Phong reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps.",
    "arxiv_url": "http://arxiv.org/abs/2405.20791v2",
    "pdf_url": "http://arxiv.org/pdf/2405.20791v2",
    "published_date": "2024-05-31",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "reflection",
      "relighting",
      "lighting",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model",
    "authors": [
      "Yufei Wang",
      "Zhihao Li",
      "Lanqing Guo",
      "Wenhan Yang",
      "Alex C. Kot",
      "Bihan Wen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity. However, the large number of Gaussians and their associated attributes require effective compression techniques. Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence. Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work. We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency. To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed. Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2405.20721v1",
    "pdf_url": "http://arxiv.org/pdf/2405.20721v1",
    "published_date": "2024-05-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction",
    "authors": [
      "Ruyi Zha",
      "Tao Jun Lin",
      "Yuanhao Cai",
      "Jiwen Cao",
      "Yanhao Zhang",
      "Hongdong Li"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown integration bias in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12$\\times$ faster than NeRF-based methods and on par with traditional algorithms. Code and models are available on the project page https://github.com/Ruyi-Zha/r2_gaussian.",
    "arxiv_url": "http://arxiv.org/abs/2405.20693v2",
    "pdf_url": "http://arxiv.org/pdf/2405.20693v2",
    "published_date": "2024-05-31",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "https://github.com/Ruyi-Zha/r2_gaussian",
    "keywords": [
      "sparse-view",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Fourier Score Distillation for Efficient One Image to 3D Object Generation",
    "authors": [
      "Shuzhou Yang",
      "Yu Wang",
      "Haijie Li",
      "Jiarui Meng",
      "Yanmin Wu",
      "Xiandong Meng",
      "Jian Zhang"
    ],
    "abstract": "Single image-to-3D generation is pivotal for crafting controllable 3D assets. Given its under-constrained nature, we attempt to leverage 3D geometric priors from a novel view diffusion model and 2D appearance priors from an image generation model to guide the optimization process. We note that there is a disparity between the generation priors of these two diffusion models, leading to their different appearance outputs. Specifically, image generation models tend to deliver more detailed visuals, whereas novel view models produce consistent yet over-smooth results across different views. Directly combining them leads to suboptimal effects due to their appearance conflicts. Hence, we propose a 2D-3D hybrid Fourier Score Distillation objective function, hy-FSD. It optimizes 3D Gaussians using 3D priors in spatial domain to ensure geometric consistency, while exploiting 2D priors in the frequency domain through Fourier transform for better visual quality. hy-FSD can be integrated into existing 3D generation methods and produce significant performance gains. With this technique, we further develop an image-to-3D generation pipeline to create high-quality 3D objects within one minute, named Fourier123. Extensive experiments demonstrate that Fourier123 excels in efficient generation with rapid convergence speed and visually-friendly generation results.",
    "arxiv_url": "http://arxiv.org/abs/2405.20669v2",
    "pdf_url": "http://arxiv.org/pdf/2405.20669v2",
    "published_date": "2024-05-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving",
    "authors": [
      "Nan Huang",
      "Xiaobao Wei",
      "Wenzhao Zheng",
      "Pengju An",
      "Ming Lu",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ],
    "abstract": "Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving. Despite the efficacy of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting (3DGS) emerges as a promising direction due to its faster speed and more explicit representation. However, most existing street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios. To facilitate efficient 3D scene reconstruction without costly annotations, we propose a self-supervised street Gaussian ($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from 4D consistency. We represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics. We conduct extensive experiments on the challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our $\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves the best performance without using 3D annotations. Code is available at: https://github.com/nnanhuang/S3Gaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2405.20323v1",
    "pdf_url": "http://arxiv.org/pdf/2405.20323v1",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/nnanhuang/S3Gaussian/",
    "keywords": [
      "efficient",
      "autonomous driving",
      "fast",
      "3d gaussian",
      "4d",
      "3d reconstruction",
      "ar",
      "nerf",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D Reconstruction",
    "authors": [
      "Jianghao Shen",
      "Nan Xue",
      "Tianfu Wu"
    ],
    "abstract": "Learning 3D scene representation from a single-view image is a long-standing fundamental problem in computer vision, with the inherent ambiguity in predicting contents unseen from the input view. Built on the recently proposed 3D Gaussian Splatting (3DGS), the Splatter Image method has made promising progress on fast single-image novel view synthesis via learning a single 3D Gaussian for each pixel based on the U-Net feature map of an input image. However, it has limited expressive power to represent occluded components that are not observable in the input view. To address this problem, this paper presents a Hierarchical Splatter Image method in which a pixel is worth more than one 3D Gaussians. Specifically, each pixel is represented by a parent 3D Gaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are learned as done in the vanilla Splatter Image. Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron (MLP) which takes as input the projected image features of a parent 3D Gaussian and the embedding of a target camera view. Both parent and child 3D Gaussians are learned end-to-end in a stage-wise way. The joint condition of input image features from eyes of the parent Gaussians and the target camera position facilitates learning to allocate child Gaussians to ``see the unseen'', recovering the occluded details that are often missed by parent Gaussians.   In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D datasets with state-of-the-art performance obtained, especially showing promising capabilities of reconstructing occluded contents in the input view.",
    "arxiv_url": "http://arxiv.org/abs/2405.20310v3",
    "pdf_url": "http://arxiv.org/pdf/2405.20310v3",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "gaussian splatting",
      "3d reconstruction",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-centric Reconstruction and Tracking of Dynamic Unknown Objects using 3D Gaussian Splatting",
    "authors": [
      "Kuldeep R Barad",
      "Antoine Richard",
      "Jan Dentler",
      "Miguel Olivares-Mendez",
      "Carol Martinez"
    ],
    "abstract": "Generalizable perception is one of the pillars of high-level autonomy in space robotics. Estimating the structure and motion of unknown objects in dynamic environments is fundamental for such autonomous systems. Traditionally, the solutions have relied on prior knowledge of target objects, multiple disparate representations, or low-fidelity outputs unsuitable for robotic operations. This work proposes a novel approach to incrementally reconstruct and track a dynamic unknown object using a unified representation -- a set of 3D Gaussian blobs that describe its geometry and appearance. The differentiable 3D Gaussian Splatting framework is adapted to a dynamic object-centric setting. The input to the pipeline is a sequential set of RGB-D images. 3D reconstruction and 6-DoF pose tracking tasks are tackled using first-order gradient-based optimization. The formulation is simple, requires no pre-training, assumes no prior knowledge of the object or its motion, and is suitable for online applications. The proposed approach is validated on a dataset of 10 unknown spacecraft of diverse geometry and texture under arbitrary relative motion. The experiments demonstrate successful 3D reconstruction and accurate 6-DoF tracking of the target object in proximity operations over a short to medium duration. The causes of tracking drift are discussed and potential solutions are outlined.",
    "arxiv_url": "http://arxiv.org/abs/2405.20104v2",
    "pdf_url": "http://arxiv.org/pdf/2405.20104v2",
    "published_date": "2024-05-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "tracking",
      "motion",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis",
    "authors": [
      "Shuhong Liu",
      "Tianchen Deng",
      "Heng Zhou",
      "Liuzhuozheng Li",
      "Hongyu Wang",
      "Danwei Wang",
      "Mingrui Li"
    ],
    "abstract": "Gaussian Splatting SLAMs have made significant advancements in improving the efficiency and fidelity of real-time reconstructions. However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles. To address this challenge, we present Manhattan Gaussian SLAM, an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness. By seamlessly integrating fused line segments derived from structured scenes, our method ensures robust tracking in textureless indoor areas. Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion. Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems.",
    "arxiv_url": "http://arxiv.org/abs/2405.20031v3",
    "pdf_url": "http://arxiv.org/pdf/2405.20031v3",
    "published_date": "2024-05-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "face",
      "geometry",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
    "authors": [
      "Qiaowei Miao",
      "JinSheng Quan",
      "Kehan Li",
      "Yawei Luo"
    ],
    "abstract": "Previous text-to-4D methods have leveraged multiple Score Distillation Sampling (SDS) techniques, combining motion priors from video-based diffusion models (DMs) with geometric priors from multiview DMs to implicitly guide 4D renderings. However, differences in these priors result in conflicting gradient directions during optimization, causing trade-offs between motion fidelity and geometry accuracy, and requiring substantial optimization time to reconcile the models. In this paper, we introduce \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignment for text-driven \\textbf{4D} Gaussian splatting (PLA4D) to resolve this motion-geometry conflict. PLA4D provides an anchor reference, i.e., text-generated video, to align the rendering process conditioned by different DMs in pixel space. For static alignment, our approach introduces a focal alignment method and Gaussian-Mesh contrastive learning to iteratively adjust focal lengths and provide explicit geometric priors at each timestep. At the dynamic level, a motion alignment technique and T-MV refinement method are employed to enforce both pose alignment and motion continuity across unknown viewpoints, ensuring intrinsic geometric consistency across views. With such pixel-level multi-DM alignment, our PLA4D framework is able to generate 4D objects with superior geometric, motion, and semantic consistency. Fully implemented with open-source tools, PLA4D offers an efficient and accessible solution for high-quality 4D digital content creation with significantly reduced generation time.",
    "arxiv_url": "http://arxiv.org/abs/2405.19957v4",
    "pdf_url": "http://arxiv.org/pdf/2405.19957v4",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "semantic",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPrediction: Dynamic 3D Gaussian Prediction for Motion Extrapolation and Free View Synthesis",
    "authors": [
      "Boming Zhao",
      "Yuan Li",
      "Ziyu Sun",
      "Lin Zeng",
      "Yujun Shen",
      "Rui Ma",
      "Yinda Zhang",
      "Hujun Bao",
      "Zhaopeng Cui"
    ],
    "abstract": "Forecasting future scenarios in dynamic environments is essential for intelligent decision-making and navigation, a challenge yet to be fully realized in computer vision and robotics. Traditional approaches like video prediction and novel-view synthesis either lack the ability to forecast from arbitrary viewpoints or to predict temporal dynamics. In this paper, we introduce GaussianPrediction, a novel framework that empowers 3D Gaussian representations with dynamic scene modeling and future scenario synthesis in dynamic environments. GaussianPrediction can forecast future states from any viewpoint, using video observations of dynamic scenes. To this end, we first propose a 3D Gaussian canonical space with deformation modeling to capture the appearance and geometry of dynamic scenes, and integrate the lifecycle property into Gaussians for irreversible deformations. To make the prediction feasible and efficient, a concentric motion distillation approach is developed by distilling the scene motion with key points. Finally, a Graph Convolutional Network is employed to predict the motions of key points, enabling the rendering of photorealistic images of future scenarios. Our framework shows outstanding performance on both synthetic and real-world datasets, demonstrating its efficacy in predicting and rendering future environments.",
    "arxiv_url": "http://arxiv.org/abs/2405.19745v1",
    "pdf_url": "http://arxiv.org/pdf/2405.19745v1",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction",
    "authors": [
      "Haodong Xiang",
      "Xinghui Li",
      "Kai Cheng",
      "Xiansong Lai",
      "Wanting Zhang",
      "Zhichao Liao",
      "Long Zeng",
      "Xueping Liu"
    ],
    "abstract": "Embodied intelligence requires precise reconstruction and rendering to simulate large-scale real-world data. Although 3D Gaussian Splatting (3DGS) has recently demonstrated high-quality results with real-time performance, it still faces challenges in indoor scenes with large, textureless regions, resulting in incomplete and noisy reconstructions due to poor point cloud initialization and underconstrained optimization. Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, we propose a unified optimization framework that integrates neural signed distance fields (SDFs) with 3DGS for accurate geometry reconstruction and real-time rendering. This framework incorporates a neural SDF field to guide the densification and pruning of Gaussians, enabling Gaussians to model scenes accurately even with poor initialized point clouds. Simultaneously, the geometry represented by Gaussians improves the efficiency of the SDF field by piloting its point sampling. Additionally, we introduce two regularization terms based on normal and edge priors to resolve geometric ambiguities in textureless areas and enhance detail accuracy. Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2405.19671v2",
    "pdf_url": "http://arxiv.org/pdf/2405.19671v2",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian",
    "authors": [
      "Wei Sun",
      "Qi Zhang",
      "Yanzhao Zhou",
      "Qixiang Ye",
      "Jianbin Jiao",
      "Yuan Li"
    ],
    "abstract": "3D Gaussian splatting has demonstrated impressive performance in real-time novel view synthesis. However, achieving successful reconstruction from RGB images generally requires multiple input views captured under static conditions. To address the challenge of sparse input views, previous approaches have incorporated depth supervision into the training of 3D Gaussians to mitigate overfitting, using dense predictions from pretrained depth networks as pseudo-ground truth. Nevertheless, depth predictions from monocular depth estimation models inherently exhibit significant uncertainty in specific areas. Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental noise from these uncertain areas. In this work, we introduce a novel method to supervise the depth distribution of 3D Gaussians, utilizing depth priors with integrated uncertainty estimates. To address these localized errors in depth predictions, we integrate a patch-wise optimal transport strategy to complement traditional L2 loss in depth supervision. Extensive experiments conducted on the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT, achieves superior novel view synthesis and consistently outperforms state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2405.19657v1",
    "pdf_url": "http://arxiv.org/pdf/2405.19657v1",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM",
    "authors": [
      "Peifeng Jiang",
      "Hong Liu",
      "Xia Li",
      "Ti Wang",
      "Fabian Zhang",
      "Joachim M. Buhmann"
    ],
    "abstract": "The limited robustness of 3D Gaussian Splatting (3DGS) to motion blur and camera noise, along with its poor real-time performance, restricts its application in robotic SLAM tasks. Upon analysis, the primary causes of these issues are the density of views with motion blur and the cumulative errors in dense pose estimation from calculating losses based on noisy original images and rendering results, which increase the difficulty of 3DGS rendering convergence. Thus, a cutting-edge 3DGS-based SLAM system is introduced, leveraging the efficiency and flexibility of 3DGS to achieve real-time performance while remaining robust against sensor noise, motion blur, and the challenges posed by long-session SLAM. Central to this approach is the Fusion Bridge module, which seamlessly integrates tracking-centered ORB Visual Odometry with mapping-centered online 3DGS. Precise pose initialization is enabled by this module through joint optimization of re-projection and rendering loss, as well as strategic view selection, enhancing rendering convergence in large-scale scenes. Extensive experiments demonstrate state-of-the-art rendering quality and localization accuracy, positioning this system as a promising solution for real-world robotics applications that require stable, near-real-time performance. Our project is available at https://ZeldaFromHeaven.github.io/TAMBRIDGE/",
    "arxiv_url": "http://arxiv.org/abs/2405.19614v1",
    "pdf_url": "http://arxiv.org/pdf/2405.19614v1",
    "published_date": "2024-05-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "tracking",
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NPGA: Neural Parametric Gaussian Avatars",
    "authors": [
      "Simon Giebenhain",
      "Tobias Kirschstein",
      "Martin R√ºnz",
      "Lourdes Agapito",
      "Matthias Nie√üner"
    ],
    "abstract": "The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. For increased representational capacity of our avatars, we propose per-Gaussian latent features that condition each primitives dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.",
    "arxiv_url": "http://arxiv.org/abs/2405.19331v2",
    "pdf_url": "http://arxiv.org/pdf/2405.19331v2",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "high-fidelity",
      "head",
      "efficient rendering",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGD: Dynamic 3D Gaussians Distillation",
    "authors": [
      "Isaac Labe",
      "Noam Issachar",
      "Itai Lang",
      "Sagie Benaim"
    ],
    "abstract": "We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/",
    "arxiv_url": "http://arxiv.org/abs/2405.19321v1",
    "pdf_url": "http://arxiv.org/pdf/2405.19321v1",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "face",
      "semantic",
      "fast",
      "3d gaussian",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation",
    "authors": [
      "Weitian Zhang",
      "Yichao Yan",
      "Yunhui Liu",
      "Xingdong Sheng",
      "Xiaokang Yang"
    ],
    "abstract": "This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is https://olivia23333.github.io/E3Gen.",
    "arxiv_url": "http://arxiv.org/abs/2405.19203v2",
    "pdf_url": "http://arxiv.org/pdf/2405.19203v2",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "deformation",
      "body",
      "3d gaussian",
      "ar",
      "animation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LP-3DGS: Learning to Prune 3D Gaussian Splatting",
    "authors": [
      "Zhaoliang Zhang",
      "Tianchen Song",
      "Yongjae Lee",
      "Li Yang",
      "Cheng Peng",
      "Rama Chellappa",
      "Deliang Fan"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream methodologies for novel view synthesis (NVS) due to its high quality and fast rendering speed. However, as a point-based scene representation, 3DGS potentially generates a large number of Gaussians to fit the scene, leading to high memory usage. Improvements that have been proposed require either an empirical and preset pruning ratio or importance score threshold to prune the point cloud. Such hyperparamter requires multiple rounds of training to optimize and achieve the maximum pruning ratio, while maintaining the rendering quality for each scene. In this work, we propose learning-to-prune 3DGS (LP-3DGS), where a trainable binary mask is applied to the importance score that can find optimal pruning ratio automatically. Instead of using the traditional straight-through estimator (STE) method to approximate the binary mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid method, making it differentiable and compatible with the existing training process of 3DGS. Extensive experiments have shown that LP-3DGS consistently produces a good balance that is both efficient and high quality.",
    "arxiv_url": "http://arxiv.org/abs/2405.18784v1",
    "pdf_url": "http://arxiv.org/pdf/2405.18784v1",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images",
    "authors": [
      "Wangbo Yu",
      "Chaoran Feng",
      "Jiye Tang",
      "Jiashu Yang",
      "Zhenyu Tang",
      "Xu Jia",
      "Yuchao Yang",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has demonstrated exceptional capabilities in 3D scene reconstruction and novel view synthesis. However, its training heavily depends on high-quality, sharp images and accurate camera poses. Fulfilling these requirements can be challenging in non-ideal real-world scenarios, where motion-blurred images are commonly encountered in high-speed moving cameras or low-light environments that require long exposure times. To address these challenges, we introduce Event Stream Assisted Gaussian Splatting (EvaGaussians), a novel approach that integrates event streams captured by an event camera to assist in reconstructing high-quality 3D-GS from blurry images. Capitalizing on the high temporal resolution and dynamic range offered by the event camera, we leverage the event streams to explicitly model the formation process of motion-blurred images and guide the deblurring reconstruction of 3D-GS. By jointly optimizing the 3D-GS parameters and recovering camera motion trajectories during the exposure time, our method can robustly facilitate the acquisition of high-fidelity novel views with intricate texture details. We comprehensively evaluated our method and compared it with previous state-of-the-art deblurring rendering methods. Both qualitative and quantitative comparisons demonstrate that our method surpasses existing techniques in restoring fine details from blurry images and producing high-fidelity novel views.",
    "arxiv_url": "http://arxiv.org/abs/2405.20224v3",
    "pdf_url": "http://arxiv.org/pdf/2405.20224v3",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GFlow: Recovering 4D World from Monocular Video",
    "authors": [
      "Shizun Wang",
      "Xingyi Yang",
      "Qiuhong Shen",
      "Zhenxiang Jiang",
      "Xinchao Wang"
    ],
    "abstract": "Recovering 4D world from monocular video is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view videos, known camera parameters, or static scenes. In this paper, we relax all these constraints and tackle a highly ambitious but practical task: With only one monocular video without camera parameters, we aim to recover the dynamic 3D world alongside the camera poses. To solve this, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video to a 4D scene, as a flow of 3D Gaussians through space and time. GFlow starts by segmenting the video into still and moving parts, then alternates between optimizing camera poses and the dynamics of the 3D Gaussian points. This method ensures consistency among adjacent points and smooth transitions between frames. Since dynamic scenes always continually introduce new visual content, we present prior-driven initialization and pixel-wise densification strategy for Gaussian points to integrate new content. By combining all those techniques, GFlow transcends the boundaries of 4D recovery from causal videos; it naturally enables tracking of points and segmentation of moving objects across frames. Additionally, GFlow estimates the camera poses for each frame, enabling novel view synthesis by changing camera pose. This capability facilitates extensive scene-level or object-level editing, highlighting GFlow's versatility and effectiveness. Visit our project page at: https://littlepure2333.github.io/GFlow",
    "arxiv_url": "http://arxiv.org/abs/2405.18426v2",
    "pdf_url": "http://arxiv.org/pdf/2405.18426v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "lighting",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting",
    "authors": [
      "Qihang Zhang",
      "Yinghao Xu",
      "Chaoyang Wang",
      "Hsin-Ying Lee",
      "Gordon Wetzstein",
      "Bolei Zhou",
      "Ceyuan Yang"
    ],
    "abstract": "Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing. Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/.",
    "arxiv_url": "http://arxiv.org/abs/2405.18424v1",
    "pdf_url": "http://arxiv.org/pdf/2405.18424v1",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D StreetUnveiler with Semantic-aware 2DGS -- a simple baseline",
    "authors": [
      "Jingwei Xu",
      "Yikai Wang",
      "Yiqun Zhao",
      "Yanwei Fu",
      "Shenghua Gao"
    ],
    "abstract": "Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scene cases involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications. The project page and more visualizations can be found at: https://streetunveiler.github.io",
    "arxiv_url": "http://arxiv.org/abs/2405.18416v4",
    "pdf_url": "http://arxiv.org/pdf/2405.18416v4",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NegGS: Negative Gaussian Splatting",
    "authors": [
      "Artur Kasymov",
      "Bartosz Czekaj",
      "Marcin Mazur",
      "Jacek Tabor",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "One of the key advantages of 3D rendering is its ability to simulate intricate scenes accurately. One of the most widely used methods for this purpose is Gaussian Splatting, a novel approach that is known for its rapid training and inference capabilities. In essence, Gaussian Splatting involves incorporating data about the 3D objects of interest into a series of Gaussian distributions, each of which can then be depicted in 3D in a manner analogous to traditional meshes. It is regrettable that the use of Gaussians in Gaussian Splatting is currently somewhat restrictive due to their perceived linear nature. In practice, 3D objects are often composed of complex curves and highly nonlinear structures. This issue can to some extent be alleviated by employing a multitude of Gaussian components to reflect the complex, nonlinear structures accurately. However, this approach results in a considerable increase in time complexity. This paper introduces the concept of negative Gaussians, which are interpreted as items with negative colors. The rationale behind this approach is based on the density distribution created by dividing the probability density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian. Such a distribution can be used to approximate structures such as donut and moon-shaped datasets. Experimental findings indicate that the application of these techniques enhances the modeling of high-frequency elements with rapid color transitions. Additionally, it improves the representation of shadows. To the best of our knowledge, this is the first paper to extend the simple elipsoid shapes of Gaussian Splatting to more complex nonlinear structures.",
    "arxiv_url": "http://arxiv.org/abs/2405.18163v2",
    "pdf_url": "http://arxiv.org/pdf/2405.18163v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Grid-Free Fluid Solver based on Gaussian Spatial Representation",
    "authors": [
      "Jingrui Xing",
      "Bin Wang",
      "Mengyu Chu",
      "Baoquan Chen"
    ],
    "abstract": "We present a grid-free fluid solver featuring a novel Gaussian representation. Drawing inspiration from the expressive capabilities of 3D Gaussian Splatting in multi-view image reconstruction, we model the continuous flow velocity as a weighted sum of multiple Gaussian functions. Leveraging this representation, we derive differential operators for the field and implement a time-dependent PDE solver using the traditional operator splitting method. Compared to implicit neural representations as another continuous spatial representation with increasing attention, our method with flexible 3D Gaussians presents enhanced accuracy on vorticity preservation. Moreover, we apply physics-driven strategies to accelerate the optimization-based time integration of Gaussian functions. This temporal evolution surpasses previous work based on implicit neural representation with reduced computational time and memory. Although not surpassing the quality of state-of-the-art Eulerian methods in fluid simulation, experiments and ablation studies indicate the potential of our memory-efficient representation. With enriched spatial information, our method exhibits a distinctive perspective combining the advantages of Eulerian and Lagrangian approaches.",
    "arxiv_url": "http://arxiv.org/abs/2405.18133v1",
    "pdf_url": "http://arxiv.org/pdf/2405.18133v1",
    "published_date": "2024-05-28",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EG4D: Explicit Generation of 4D Object without Score Distillation",
    "authors": [
      "Qi Sun",
      "Zhiyang Guo",
      "Ziyu Wan",
      "Jing Nathan Yan",
      "Shengming Yin",
      "Wengang Zhou",
      "Jing Liao",
      "Houqiang Li"
    ],
    "abstract": "In recent years, the increasing demand for dynamic 3D assets in design and gaming applications has given rise to powerful generative pipelines capable of synthesizing high-quality 4D objects. Previous methods generally rely on score distillation sampling (SDS) algorithm to infer the unseen views and motion of 4D objects, thus leading to unsatisfactory results with defects like over-saturation and Janus problem. Therefore, inspired by recent progress of video diffusion models, we propose to optimize a 4D representation by explicitly generating multi-view videos from one input image. However, it is far from trivial to handle practical challenges faced by such a pipeline, including dramatic temporal inconsistency, inter-frame geometry and texture diversity, and semantic defects brought by video generation results. To address these issues, we propose DG4D, a novel multi-stage framework that generates high-quality and consistent 4D assets without score distillation. Specifically, collaborative techniques and solutions are developed, including an attention injection strategy to synthesize temporal-consistent multi-view videos, a robust and efficient dynamic reconstruction method based on Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration. The qualitative results and user preference study demonstrate that our framework outperforms the baselines in generation quality by a considerable margin. Code will be released at \\url{https://github.com/jasongzy/EG4D}.",
    "arxiv_url": "http://arxiv.org/abs/2405.18132v1",
    "pdf_url": "http://arxiv.org/pdf/2405.18132v1",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jasongzy/EG4D",
    "keywords": [
      "efficient",
      "motion",
      "face",
      "semantic",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields",
    "authors": [
      "Mihnea-Bogdan Jurca",
      "Remco Royen",
      "Ion Giosan",
      "Adrian Munteanu"
    ],
    "abstract": "Gaussian Splatting has revolutionized the world of novel view synthesis by achieving high rendering performance in real-time. Recently, studies have focused on enriching these 3D representations with semantic information for downstream tasks. In this paper, we introduce RT-GS2, the first generalizable semantic segmentation method employing Gaussian Splatting. While existing Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2 demonstrates the ability to generalize to unseen scenes. Our method adopts a new approach by first extracting view-independent 3D Gaussian features in a self-supervised manner, followed by a novel View-Dependent / View-Independent (VDVI) feature fusion to enhance semantic consistency over different views. Extensive experimentation on three different datasets showcases RT-GS2's superiority over the state-of-the-art methods in semantic segmentation quality, exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our method achieves real-time performance of 27.03 FPS, marking an astonishing 901 times speedup compared to existing approaches. This work represents a significant advancement in the field by introducing, to the best of our knowledge, the first real-time generalizable semantic segmentation method for 3D Gaussian representations of radiance fields.",
    "arxiv_url": "http://arxiv.org/abs/2405.18033v2",
    "pdf_url": "http://arxiv.org/pdf/2405.18033v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes",
    "authors": [
      "Yunsong Wang",
      "Tianxin Huang",
      "Hanlin Chen",
      "Gim Hee Lee"
    ],
    "abstract": "Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors.",
    "arxiv_url": "http://arxiv.org/abs/2405.17958v3",
    "pdf_url": "http://arxiv.org/pdf/2405.17958v3",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "large scene",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction",
    "authors": [
      "Bin Zhang",
      "Bi Zeng",
      "Zexin Peng"
    ],
    "abstract": "In recent years, Neural Radiance Fields (NeRF) has revolutionized three-dimensional (3D) reconstruction with its implicit representation. Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the implicit representation of neural networks and instead directly represents scenes as point clouds with Gaussian-shaped distributions. While this shift has notably elevated the rendering quality and speed of radiance fields but inevitably led to a significant increase in memory usage. Additionally, effectively rendering dynamic scenes in 3D-GS has emerged as a pressing challenge. To address these concerns, this paper purposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction. Firstly, we use a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements. Subsequently, we introduce a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, thereby further compressing 3D Gaussian model. Finally, motion noise of points is mitigated through static constraints and motion consistency constraints. Experimental results demonstrate that our method surpasses existing approaches in rendering quality and speed, while significantly reducing the memory usage associated with 3D-GS, making it highly suitable for various tasks such as novel view synthesis, and dynamic mapping.",
    "arxiv_url": "http://arxiv.org/abs/2405.17891v1",
    "pdf_url": "http://arxiv.org/pdf/2405.17891v1",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "mapping",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction",
    "authors": [
      "Haoyu Zhao",
      "Xingyue Zhao",
      "Lingting Zhu",
      "Weixi Zheng",
      "Yongchao Xu"
    ],
    "abstract": "Robot-assisted minimally invasive surgery benefits from enhancing dynamic scene reconstruction, as it improves surgical outcomes. While Neural Radiance Fields (NeRF) have been effective in scene reconstruction, their slow inference speeds and lengthy training durations limit their applicability. To overcome these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as a recent trend, offering rapid inference capabilities and superior 3D quality. However, these methods still struggle with under-reconstruction in both static and dynamic scenes. In this paper, we propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses these challenges from spatial and temporal frequency perspectives. Our approach incorporates deformation fields to better handle dynamic scenes and introduces Spatial High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in spatial frequency spectra between the rendered image and its ground truth. Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction (THF) to enhance dynamic awareness in neural rendering by leveraging flow priors, focusing optimization on motion-intensive parts. Extensive experiments on two widely used benchmarks demonstrate that HFGS achieves superior rendering quality.",
    "arxiv_url": "http://arxiv.org/abs/2405.17872v3",
    "pdf_url": "http://arxiv.org/pdf/2405.17872v3",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "4d",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deform3DGS: Flexible Deformation for Fast Surgical Scene Reconstruction with Gaussian Splatting",
    "authors": [
      "Shuojue Yang",
      "Qian Li",
      "Daiyun Shen",
      "Bingchen Gong",
      "Qi Dou",
      "Yueming Jin"
    ],
    "abstract": "Tissue deformation poses a key challenge for accurate surgical scene reconstruction. Despite yielding high reconstruction quality, existing methods suffer from slow rendering speeds and long training times, limiting their intraoperative applicability. Motivated by recent progress in 3D Gaussian Splatting, an emerging technology in real-time 3D rendering, this work presents a novel fast reconstruction framework, termed Deform3DGS, for deformable tissues during endoscopic surgery. Specifically, we introduce 3D GS into surgical scenes by integrating a point cloud initialization to improve reconstruction. Furthermore, we propose a novel flexible deformation modeling scheme (FDM) to learn tissue deformation dynamics at the level of individual Gaussians. Our FDM can model the surface deformation with efficient representations, allowing for real-time rendering performance. More importantly, FDM significantly accelerates surgical scene reconstruction, demonstrating considerable clinical values, particularly in intraoperative settings where time efficiency is crucial. Experiments on DaVinci robotic surgery videos indicate the efficacy of our approach, showcasing superior reconstruction fidelity PSNR: (37.90) and rendering speed (338.8 FPS) while substantially reducing training time to only 1 minute/scene. Our code is available at https://github.com/jinlab-imvr/Deform3DGS.",
    "arxiv_url": "http://arxiv.org/abs/2405.17835v3",
    "pdf_url": "http://arxiv.org/pdf/2405.17835v3",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jinlab-imvr/Deform3DGS",
    "keywords": [
      "efficient",
      "vr",
      "fast",
      "face",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
    "authors": [
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Yiyu Zhuang",
      "Qi Zhang",
      "Wenbo Hu",
      "Chaopeng Zhang",
      "Yao Yao",
      "Ying Shan",
      "Long Quan"
    ],
    "abstract": "Neural 3D representations such as Neural Radiance Fields (NeRF), excel at producing photo-realistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation. Previous works have attempted to address this issue by deforming a NeRF in canonical space or manipulating the radiance field based on an explicit mesh. However, manipulating NeRF is not highly controllable and requires a long training and inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed. However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality. In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering. We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation. This approach reduces the need to design various algorithms for different types of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering after manipulation. Our approach is capable of handling large deformations, local manipulations, and soft body simulations while keeping high-quality rendering. Furthermore, we demonstrate that our method is also effective with inaccurate meshes extracted from 3DGS. Experiments conducted demonstrate the effectiveness of our method and its superiority over baseline approaches.",
    "arxiv_url": "http://arxiv.org/abs/2405.17811v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17811v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "deformation",
      "body",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic Scene Destruction",
    "authors": [
      "Yongjae Lee",
      "Zhaoliang Zhang",
      "Deliang Fan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis. However, its suboptimal densification process results in the excessively large number of Gaussian primitives, which impacts frame-per-second and increases memory usage, making it unsuitable for low-end devices. To address this issue, many follow-up studies have proposed various pruning techniques with score functions designed to identify and remove less important primitives. Nonetheless, a comprehensive discussion of their effectiveness and implications across all techniques is missing. In this paper, we are the first to categorize 3DGS pruning techniques into two types: Scene-level pruning and Pixel-level pruning, distinguished by their scope for ranking primitives. Our subsequent experiments reveal that, while scene-level pruning leads to disastrous quality drops under extreme decimation of Gaussian primitives, pixel-level pruning not only sustains relatively high rendering quality with minuscule performance degradation but also provides an inherent boundary of pruning, i.e., a safeguard of Gaussian pruning. Building on this observation, we further propose multiple variations of score functions based on the factors of rendering equations and discover that assessing based on color similarity with blending weight is the most effective method for discriminating insignificant primitives. In our experiments, our SafeguardGS with the optimal score function shows the highest PSNR-per-primitive performance under an extreme pruning setting, retaining only about 10% of the primitives from the original 3DGS scene (i.e., 10x compression ratio). We believe our research provides valuable insights for optimizing 3DGS for future works.",
    "arxiv_url": "http://arxiv.org/abs/2405.17793v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17793v2",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam Videos",
    "authors": [
      "Linhan Wang",
      "Kai Cheng",
      "Shuo Lei",
      "Shengkun Wang",
      "Wei Yin",
      "Chenyang Lei",
      "Xiaoxiao Long",
      "Chang-Tien Lu"
    ],
    "abstract": "We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos. While neural rendering techniques have made significant strides in driving scenarios, existing methods are primarily designed for videos collected by autonomous vehicles. However, these videos are limited in both quantity and diversity compared to dash cam videos, which are more widely used across various types of vehicles and capture a broader range of scenarios. Dash cam videos often suffer from severe obstructions such as reflections and occlusions on the windshields, which significantly impede the application of neural rendering techniques. To address this challenge, we develop DC-Gaussian based on the recent real-time neural rendering technique 3D Gaussian Splatting (3DGS). Our approach includes an adaptive image decomposition module to model reflections and occlusions in a unified manner. Additionally, we introduce illumination-aware obstruction modeling to manage reflections and occlusions under varying lighting conditions. Lastly, we employ a geometry-guided Gaussian enhancement strategy to improve rendering details by incorporating additional geometry priors. Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions. See the project page for code, data: https://linhanwang.github.io/dcgaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2405.17705v3",
    "pdf_url": "http://arxiv.org/pdf/2405.17705v3",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "lighting",
      "geometry",
      "3d gaussian",
      "ar",
      "illumination",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane",
    "authors": [
      "Yansong Qu",
      "Shaohui Dai",
      "Xinyang Li",
      "Jianghang Lin",
      "Liujuan Cao",
      "Shengchuan Zhang",
      "Rongrong Ji"
    ],
    "abstract": "3D open-vocabulary scene understanding, crucial for advancing augmented reality and robotic applications, involves interpreting and locating specific regions within a 3D space as directed by natural language instructions. To this end, we introduce GOI, a framework that integrates semantic features from 2D vision-language foundation models into 3D Gaussian Splatting (3DGS) and identifies 3D Gaussians of Interest using an Optimizable Semantic-space Hyperplane. Our approach includes an efficient compression method that utilizes scene priors to condense noisy high-dimensional semantic features into compact low-dimensional vectors, which are subsequently embedded in 3DGS. During the open-vocabulary querying process, we adopt a distinct approach compared to existing methods, which depend on a manually set fixed empirical threshold to select regions based on their semantic feature distance to the query text embedding. This traditional approach often lacks universal accuracy, leading to challenges in precisely identifying specific target areas. Instead, our method treats the feature selection process as a hyperplane division within the feature space, retaining only those features that are highly relevant to the query. We leverage off-the-shelf 2D Referring Expression Segmentation (RES) models to fine-tune the semantic-space hyperplane, enabling a more precise distinction between target regions and others. This fine-tuning substantially improves the accuracy of open-vocabulary queries, ensuring the precise localization of pertinent 3D Gaussians. Extensive experiments demonstrate GOI's superiority over previous state-of-the-art methods. Our project page is available at https://quyans.github.io/GOI-Hyperplane/ .",
    "arxiv_url": "http://arxiv.org/abs/2405.17596v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17596v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving. Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources. To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features. We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics. We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position. We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption. Code is available at: https://github.com/huang-yh/GaussianFormer.",
    "arxiv_url": "http://arxiv.org/abs/2405.17429v1",
    "pdf_url": "http://arxiv.org/pdf/2405.17429v1",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/huang-yh/GaussianFormer",
    "keywords": [
      "efficient",
      "autonomous driving",
      "semantic",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
    "authors": [
      "Jiahui Lei",
      "Yijia Weng",
      "Adam Harley",
      "Leonidas Guibas",
      "Kostas Daniilidis"
    ],
    "abstract": "We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos.",
    "arxiv_url": "http://arxiv.org/abs/2405.17421v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17421v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "geometry",
      "4d",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DOF-GS:Adjustable Depth-of-Field 3D Gaussian Splatting for Post-Capture Refocusing, Defocus Rendering and Blur Removal",
    "authors": [
      "Yujie Wang",
      "Praneeth Chakravarthula",
      "Baoquan Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) techniques have recently enabled high-quality 3D scene reconstruction and real-time novel view synthesis. These approaches, however, are limited by the pinhole camera model and lack effective modeling of defocus effects. Departing from this, we introduce DOF-GS--a new 3DGS-based framework with a finite-aperture camera model and explicit, differentiable defocus rendering, enabling it to function as a post-capture control tool. By training with multi-view images with moderate defocus blur, DOF-GS learns inherent camera characteristics and reconstructs sharp details of the underlying scene, particularly, enabling rendering of varying DOF effects through on-demand aperture and focal distance control, post-capture and optimization. Additionally, our framework extracts circle-of-confusion cues during optimization to identify in-focus regions in input views, enhancing the reconstructed 3D scene details. Experimental results demonstrate that DOF-GS supports post-capture refocusing, adjustable defocus and high-quality all-in-focus rendering, from multi-view images with uncalibrated defocus blur.",
    "arxiv_url": "http://arxiv.org/abs/2405.17351v3",
    "pdf_url": "http://arxiv.org/pdf/2405.17351v3",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Memorize What Matters: Emergent Scene Decomposition from Multitraverse",
    "authors": [
      "Yiming Li",
      "Zehong Wang",
      "Yue Wang",
      "Zhiding Yu",
      "Zan Gojcic",
      "Marco Pavone",
      "Chen Feng",
      "Jose M. Alvarez"
    ],
    "abstract": "Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust differentiable rendering problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residuals mining, and robust optimization, 3DGM jointly performs 2D segmentation and 3D mapping without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.",
    "arxiv_url": "http://arxiv.org/abs/2405.17187v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17187v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "mapping",
      "segmentation",
      "3d gaussian",
      "human",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Joo Chan Lee",
      "Daniel Rho",
      "Jong Hwan Ko",
      "Usman Ali",
      "Eunbyung Park"
    ],
    "abstract": "The neural radiance field (NeRF) has made significant strides in representing 3D scenes and synthesizing novel views. Despite its advancements, the high computational costs of NeRF have posed challenges for its deployment in resource-constrained environments and real-time applications. As an alternative to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers rapid rendering speeds while maintaining excellent image quality. However, as it represents objects and scenes using a myriad of Gaussians, it requires substantial storage to achieve high-quality representation. To mitigate the storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality. Inspired by classical matrix and tensor factorization techniques, our method represents and approximates dense clusters of Gaussians with significantly fewer Gaussians through efficient factorization. We aim to efficiently represent dense 3D Gaussians by approximating them with a limited amount of information for each axis and their combinations. This method allows us to encode a substantially large number of Gaussians along with their essential attributes -- such as color, scale, and rotation -- necessary for rendering using a relatively small number of elements. Extensive experimental results demonstrate that F-3DGS achieves a significant reduction in storage costs while maintaining comparable quality in rendered images.",
    "arxiv_url": "http://arxiv.org/abs/2405.17083v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17083v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction with Geometry Constrain",
    "authors": [
      "Butian Xiong",
      "Xiaoyu Ye",
      "Tze Ho Elden Tse",
      "Kai Han",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "With the emergence of Gaussian Splats, recent efforts have focused on large-scale scene geometric reconstruction. However, most of these efforts either concentrate on memory reduction or spatial space division, neglecting information in the semantic space. In this paper, we propose a novel method, named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware 3D Gaussian Splats. Specifically, we leverage prior information stored in large vision models such as SAM and DINO to generate semantic masks. We then introduce a geometric complexity measurement function to serve as soft regularization, guiding the shape of each Gaussian Splat within specific semantic areas. Additionally, we present a method that estimates the expected number of Gaussian Splats in different semantic areas, effectively providing a lower bound for Gaussian Splats in these areas. Subsequently, we extract the point cloud using a novel probability density-based extraction method, transforming Gaussian Splats into a point cloud crucial for downstream tasks. Our method also offers the potential for detailed semantic inquiries while maintaining high image-based reconstruction results. We provide extensive experiments on publicly available large-scale scene reconstruction datasets with highly accurate point clouds as ground truth and our novel dataset. Our results demonstrate the superiority of our method over current state-of-the-art Gaussian Splats reconstruction methods by a significant margin in terms of geometric-based measurement metrics. Code and additional results will soon be available on our project page.",
    "arxiv_url": "http://arxiv.org/abs/2405.16923v2",
    "pdf_url": "http://arxiv.org/pdf/2405.16923v2",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation",
    "authors": [
      "Zhoujie Fu",
      "Jiacheng Wei",
      "Wenhao Shen",
      "Chaoyue Song",
      "Xiaofeng Yang",
      "Fayao Liu",
      "Xulei Yang",
      "Guosheng Lin"
    ],
    "abstract": "In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos. Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer. We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects. This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes. To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion. This integration is optimized through a displacement loss to ensure reliable and genuine dynamics. Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability. Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency.",
    "arxiv_url": "http://arxiv.org/abs/2405.16849v3",
    "pdf_url": "http://arxiv.org/pdf/2405.16849v3",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "motion",
      "3d gaussian",
      "4d",
      "human",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting has recently been introduced as a potent alternative, achieving both high-fidelity visual results and accelerated rendering performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with challenges. Specifically, large-scale scenes grapples with the integration of objects across multiple scales and disparate viewpoints, which often leads to compromised efficacy as the Gaussians need to balance between detail levels. Furthermore, the generation of initialization points via COLMAP from large-scale dataset is both computationally demanding and prone to incomplete reconstructions. To address these challenges, we present Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization. Our approach represent the scene with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The top level of the pyramid is composed of a few large Gaussians, while each subsequent layer accommodates a denser collection of smaller Gaussians. We effectively initialize these pyramidal Gaussians through sampling a rapidly trained grid-based NeRF at various frequencies. We group these pyramidal Gaussians into clusters and use a compact weighting network to dynamically determine the influence of each pyramid level of each cluster considering camera viewpoint during rendering. Our method achieves a significant performance leap across multiple large-scale datasets and attains a rendering time that is over 400 times faster than current state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2405.16829v3",
    "pdf_url": "http://arxiv.org/pdf/2405.16829v3",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models",
    "authors": [
      "Hanwen Liang",
      "Yuyang Yin",
      "Dejia Xu",
      "Hanxue Liang",
      "Zhangyang Wang",
      "Konstantinos N. Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple image or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, \\textbf{Diffusion4D}, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets within just several minutes. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.",
    "arxiv_url": "http://arxiv.org/abs/2405.16645v1",
    "pdf_url": "http://arxiv.org/pdf/2405.16645v1",
    "published_date": "2024-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "fast",
      "geometry",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians",
    "authors": [
      "Erik Sandstr√∂m",
      "Keisuke Tateno",
      "Michael Oechsle",
      "Michael Niemeyer",
      "Luc Van Gool",
      "Martin R. Oswald",
      "Federico Tombari"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful representation of geometry and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM), as it provides a compact dense map representation while enabling efficient and high-quality map rendering. However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth. In response, we propose the first RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction. Our experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing RGB-only SLAM methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes. The source code is available at https://github.com/eriksandstroem/Splat-SLAM.",
    "arxiv_url": "http://arxiv.org/abs/2405.16544v1",
    "pdf_url": "http://arxiv.org/pdf/2405.16544v1",
    "published_date": "2024-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/eriksandstroem/Splat-SLAM",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "fast",
      "mapping",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors",
    "authors": [
      "Soumava Paul",
      "Christopher Wewer",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ],
    "abstract": "We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest. In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning. Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views. Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations. We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction. Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail.",
    "arxiv_url": "http://arxiv.org/abs/2405.16517v2",
    "pdf_url": "http://arxiv.org/pdf/2405.16517v2",
    "published_date": "2024-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "sparse-view",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature Splatting for Better Novel View Synthesis with Low Overlap",
    "authors": [
      "T. Berriel Martins",
      "Javier Civera"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first \"splatted\" into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. Code available at https://github.com/tberriel/FeatSplat .   Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting",
    "arxiv_url": "http://arxiv.org/abs/2405.15518v2",
    "pdf_url": "http://arxiv.org/pdf/2405.15518v2",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/tberriel/FeatSplat",
    "keywords": [
      "fast",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSDeformer: Direct, Real-time and Extensible Cage-based Deformation for 3D Gaussian Splatting",
    "authors": [
      "Jiajun Huang",
      "Shuolin Xu",
      "Hongchuan Yu",
      "Tong-Yee Lee"
    ],
    "abstract": "We present GSDeformer, a method that enables cage-based deformation on 3D Gaussian Splatting (3DGS). Our approach bridges cage-based deformation and 3DGS by using a proxy point-cloud representation. This point cloud is generated from 3D Gaussians, and deformations applied to the point cloud are translated into transformations on the 3D Gaussians. To handle potential bending caused by deformation, we incorporate a splitting process to approximate it. Our method does not modify or extend the core architecture of 3D Gaussian Splatting, making it compatible with any trained vanilla 3DGS or its variants. Additionally, we automate cage construction for 3DGS and its variants using a render-and-reconstruct approach. Experiments demonstrate that GSDeformer delivers superior deformation results compared to existing methods, is robust under extreme deformations, requires no retraining for editing, runs in real-time, and can be extended to other 3DGS variants. Project Page: https://jhuangbu.github.io/gsdeformer/",
    "arxiv_url": "http://arxiv.org/abs/2405.15491v3",
    "pdf_url": "http://arxiv.org/pdf/2405.15491v3",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media",
    "authors": [
      "Jorge Condor",
      "Sebastien Speierer",
      "Lukas Bode",
      "Aljaz Bozic",
      "Simon Green",
      "Piotr Didyk",
      "Adrian Jarabo"
    ],
    "abstract": "Efficient scene representations are essential for many computer graphics applications. A general unified representation that can handle both surfaces and volumes simultaneously, remains a research challenge. Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for different kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer. We demonstrate our method as a compact and efficient alternative to other forms of volume modeling for forward and inverse rendering of scattering media. Furthermore, we adapt and showcase our method in radiance field optimization and rendering, providing additional flexibility compared to current state of the art given its ray-tracing formulation. We also introduce the Epanechnikov kernel and demonstrate its potential as an efficient alternative to the traditionally-used Gaussian kernel in scene reconstruction tasks. The versatility and physically-based nature of our approach allows us to go beyond radiance fields and bring to kernel-based modeling and rendering any path-tracing enabled functionality such as scattering, relighting and complex camera models.",
    "arxiv_url": "http://arxiv.org/abs/2405.15425v2",
    "pdf_url": "http://arxiv.org/pdf/2405.15425v2",
    "published_date": "2024-05-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "I.3.2; I.3.3; I.3.6; I.3.5; I.3.7"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relighting",
      "lighting",
      "face",
      "3d gaussian",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DisC-GS: Discontinuity-aware Gaussian Splatting",
    "authors": [
      "Haoxuan Qu",
      "Zhuoling Li",
      "Hossein Rahmani",
      "Yujun Cai",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis. In this paper, we highlight a fundamental limitation of Gaussian Splatting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions. To address this issue, we propose a novel framework enabling Gaussian Splatting to perform discontinuity-aware image rendering. Additionally, we introduce a B\\'ezier-boundary gradient approximation strategy within our framework to keep the \"differentiability\" of the proposed discontinuity-aware rendering process. Extensive experiments demonstrate the efficacy of our framework.",
    "arxiv_url": "http://arxiv.org/abs/2405.15196v2",
    "pdf_url": "http://arxiv.org/pdf/2405.15196v2",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting",
    "authors": [
      "Yuanhao Cai",
      "Zihao Xiao",
      "Yixun Liang",
      "Minghan Qin",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Yaoyao Liu",
      "Alan Yuille"
    ],
    "abstract": "High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and only requiring 6.3% training time. Code and recalibrated data will be publicly available at https://github.com/caiyuanhao1998/HDR-GS . A brief video introduction of our work is available at https://youtu.be/wtU7Kcwe7ck",
    "arxiv_url": "http://arxiv.org/abs/2405.15125v4",
    "pdf_url": "http://arxiv.org/pdf/2405.15125v4",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/caiyuanhao1998/HDR-GS",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Hider: Hiding Messages into 3D Gaussian Splatting",
    "authors": [
      "Xuanyu Zhang",
      "Jiarui Meng",
      "Runyi Li",
      "Zhipei Xu",
      "Yongbing Zhang",
      "Jian Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGS's spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: https://xuanyuzhang21.github.io/project/gshider.",
    "arxiv_url": "http://arxiv.org/abs/2405.15118v2",
    "pdf_url": "http://arxiv.org/pdf/2405.15118v2",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting",
    "authors": [
      "Jiaxu Wang",
      "Junhao He",
      "Ziyi Zhang",
      "Mingyuan Sun",
      "Jingkai Sun",
      "Renjing Xu"
    ],
    "abstract": "Event cameras offer promising advantages such as high dynamic range and low latency, making them well-suited for challenging lighting conditions and fast-moving scenarios. However, reconstructing 3D scenes from raw event streams is difficult because event data is sparse and does not carry absolute color information. To release its potential in 3D reconstruction, we propose the first event-based generalizable 3D reconstruction framework, called EvGGS, which reconstructs scenes as 3D Gaussians from only event input in a feedforward manner and can generalize to unseen cases without any retraining. This framework includes a depth estimation module, an intensity reconstruction module, and a Gaussian regression module. These submodules connect in a cascading manner, and we collaboratively train them with a designed joint loss to make them mutually promote. To facilitate related studies, we build a novel event-based 3D dataset with various material objects and calibrated labels of grayscale images, depth maps, camera poses, and silhouettes. Experiments show models that have jointly trained significantly outperform those trained individually. Our approach performs better than all baselines in reconstruction quality, and depth/intensity predictions with satisfactory rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2405.14959v3",
    "pdf_url": "http://arxiv.org/pdf/2405.14959v3",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using Sparse RGB Cameras",
    "authors": [
      "Hanzhang Tu",
      "Ruizhi Shao",
      "Xue Dong",
      "Shunyuan Zheng",
      "Hao Zhang",
      "Lili Chen",
      "Meili Wang",
      "Wenyu Li",
      "Siyan Ma",
      "Shengping Zhang",
      "Boyao Zhou",
      "Yebin Liu"
    ],
    "abstract": "In this paper, we present a low-budget and high-authenticity bidirectional telepresence system, Tele-Aloha, targeting peer-to-peer communication scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms) and robust distant communication. As the core of Tele-Aloha, we propose an efficient novel view synthesis algorithm for upper-body. Firstly, we design a cascaded disparity estimator for obtaining a robust geometry cue. Additionally a neural rasterizer via Gaussian Splatting is introduced to project latent features onto target view and to decode them into a reduced resolution. Further, given the high-quality captured data, we leverage weighted blending mechanism to refine the decoded image into the final resolution of 2K. Exploiting world-leading autostereoscopic display and low-latency iris tracking, users are able to experience a strong three-dimensional sense even without any wearable head-mounted display device. Altogether, our telepresence system demonstrates the sense of co-presence in real-life experiments, inspiring the next generation of communication.",
    "arxiv_url": "http://arxiv.org/abs/2405.14866v1",
    "pdf_url": "http://arxiv.org/pdf/2405.14866v1",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "tracking",
      "body",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LDM: Large Tensorial SDF Model for Textured Mesh Generation",
    "authors": [
      "Rengan Xie",
      "Wenting Zheng",
      "Kai Huang",
      "Yizheng Chen",
      "Qi Wang",
      "Qi Ye",
      "Wei Chen",
      "Yuchi Huo"
    ],
    "abstract": "Previous efforts have managed to generate production-ready 3D assets from text or images. However, these methods primarily employ NeRF or 3D Gaussian representations, which are not adept at producing smooth, high-quality geometries required by modern rendering pipelines. In this paper, we propose LDM, a novel feed-forward framework capable of generating high-fidelity, illumination-decoupled textured mesh from a single image or text prompts. We firstly utilize a multi-view diffusion model to generate sparse multi-view inputs from single images or text prompts, and then a transformer-based model is trained to predict a tensorial SDF field from these sparse multi-view image inputs. Finally, we employ a gradient-based mesh optimization layer to refine this model, enabling it to produce an SDF field from which high-quality textured meshes can be extracted. Extensive experiments demonstrate that our method can generate diverse, high-quality 3D mesh assets with corresponding decomposed RGB textures within seconds.",
    "arxiv_url": "http://arxiv.org/abs/2405.14580v3",
    "pdf_url": "http://arxiv.org/pdf/2405.14580v3",
    "published_date": "2024-05-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes",
    "authors": [
      "Ruiyuan Gao",
      "Kai Chen",
      "Zhihao Li",
      "Lanqing Hong",
      "Zhenguo Li",
      "Qiang Xu"
    ],
    "abstract": "While controllable generative models for images and videos have achieved remarkable success, high-quality models for 3D scenes, particularly in unbounded scenarios like autonomous driving, remain underdeveloped due to high data acquisition costs. In this paper, we introduce MagicDrive3D, a novel pipeline for controllable 3D street scene generation that supports multi-condition control, including BEV maps, 3D objects, and text descriptions. Unlike previous methods that reconstruct before training the generative models, MagicDrive3D first trains a video generation model and then reconstructs from the generated data. This innovative approach enables easily controllable generation and static scene acquisition, resulting in high-quality scene reconstruction. To address the minor errors in generated content, we propose deformable Gaussian splatting with monocular depth initialization and appearance modeling to manage exposure discrepancies across viewpoints. Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality 3D driving scenes that support any-view rendering and enhance downstream tasks like BEV segmentation. Our results demonstrate the framework's superior performance, showcasing its potential for autonomous driving simulation and beyond.",
    "arxiv_url": "http://arxiv.org/abs/2405.14475v3",
    "pdf_url": "http://arxiv.org/pdf/2405.14475v3",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "segmentation",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing",
    "authors": [
      "Teng Xu",
      "Jiamin Chen",
      "Peng Chen",
      "Youjia Zhang",
      "Junqing Yu",
      "Wei Yang"
    ],
    "abstract": "Editing objects within a scene is a critical functionality required across a broad spectrum of applications in computer vision and graphics. As 3D Gaussian Splatting (3DGS) emerges as a frontier in scene representation, the effective modification of 3D Gaussian scenes has become increasingly vital. This process entails accurately retrieve the target objects and subsequently performing modifications based on instructions. Though available in pieces, existing techniques mainly embed sparse semantics into Gaussians for retrieval, and rely on an iterative dataset update paradigm for editing, leading to over-smoothing or inconsistency issues. To this end, this paper proposes a systematic approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and editing. In contrast to the top-down language grounding approach for 3D Gaussians, we adopt a bottom-up language aggregation strategy to generate a denser language embedded 3D Gaussians that supports open-vocabulary retrieval. To overcome the over-smoothing and inconsistency issues in editing, we propose a Coherent Score Distillation (CSD) that aggregates a 2D image editing diffusion model and a multi-view diffusion model for score distillation, producing multi-view consistent editing with much finer details. In various experiments, we demonstrate that our TIGER is able to accomplish more consistent and realistic edits than prior work.",
    "arxiv_url": "http://arxiv.org/abs/2405.14455v2",
    "pdf_url": "http://arxiv.org/pdf/2405.14455v2",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RoGs: Large Scale Road Surface Reconstruction with Meshgrid Gaussian",
    "authors": [
      "Zhiheng Feng",
      "Wenhua Wu",
      "Tianchen Deng",
      "Hesheng Wang"
    ],
    "abstract": "Road surface reconstruction plays a crucial role in autonomous driving, which can be used for road lane perception and autolabeling. Recently, mesh-based road surface reconstruction algorithms have shown promising reconstruction results. However, these mesh-based methods suffer from slow speed and poor reconstruction quality. To address these limitations, we propose a novel large-scale road surface reconstruction approach with meshgrid Gaussian, named RoGs. Specifically, we model the road surface by placing Gaussian surfels in the vertices of a uniformly distributed square mesh, where each surfel stores color, semantic, and geometric information. This square mesh-based layout covers the entire road with fewer Gaussian surfels and reduces the overlap between Gaussian surfels during training. In addition, because the road surface has no thickness, 2D Gaussian surfel is more consistent with the physical reality of the road surface than 3D Gaussian sphere. Then, unlike previous initialization methods that rely on point clouds, we introduce a vehicle pose-based initialization method to initialize the height and rotation of the Gaussian surfel. Thanks to this meshgrid Gaussian modeling and pose-based initialization, our method achieves significant speedups while improving reconstruction quality. We obtain excellent results in reconstruction of road surfaces in a variety of challenging real-world scenes.",
    "arxiv_url": "http://arxiv.org/abs/2405.14342v3",
    "pdf_url": "http://arxiv.org/pdf/2405.14342v3",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "face",
      "semantic",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup",
    "authors": [
      "Joanna Waczy≈Ñska",
      "Piotr Borycki",
      "Joanna Kaleta",
      "S≈Çawomir Tadeja",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to represent the scene's structure and the neural network to model dynamics. Such approaches allow fast rendering and extracting each element of such a dynamic scene. However, modifying such objects over time is challenging. SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue. However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing. Moreover, this task poses additional difficulties regarding the re-productivity of such editing. To address this, we propose Dynamic Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS. Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh. Consequently, we can separately construct new trajectories for the 3D objects composing the scene. Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.",
    "arxiv_url": "http://arxiv.org/abs/2405.14276v3",
    "pdf_url": "http://arxiv.org/pdf/2405.14276v3",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation",
    "authors": [
      "Chaokang Jiang",
      "Dalong Du",
      "Jiuming Liu",
      "Siting Zhu",
      "Zhenqiang Liu",
      "Zhuang Ma",
      "Zhujin Liang",
      "Jie Zhou"
    ],
    "abstract": "Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels at modeling complex non-rigid deformations across varied dynamic scenes. The method begins with an iterative Gaussian cloud soft clustering module, offering structured temporal point cloud representations. The proposed temporal radial basis function Gaussian residual utilizes Gaussian parameter interpolation over time, enabling smooth parameter transitions and capturing temporal residuals of Gaussian distributions. Additionally, a 4D Gaussian deformation field tracks the evolution of these parameters, creating continuous spatiotemporal deformation fields. A 4D neural field transforms low-dimensional spatiotemporal coordinates ($x,y,z,t$) into a high-dimensional latent space. Finally, we adaptively and efficiently fuse the latent features from neural fields and the geometric features from Gaussian deformation fields. NeuroGauss4D-PCI outperforms existing methods in point cloud frame interpolation, delivering leading performance on both object-level (DHB) and large-scale autonomous driving datasets (NL-Drive), with scalability to auto-labeling and point cloud densification tasks. The source code is released at https://github.com/jiangchaokang/NeuroGauss4D-PCI.",
    "arxiv_url": "http://arxiv.org/abs/2405.14241v1",
    "pdf_url": "http://arxiv.org/pdf/2405.14241v1",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/jiangchaokang/NeuroGauss4D-PCI",
    "keywords": [
      "efficient",
      "autonomous driving",
      "deformation",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus",
    "authors": [
      "Yu Chen",
      "Gim Hee Lee"
    ],
    "abstract": "The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into K blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DOGS maintains one global 3DGS model on the master node and K local 3DGS models on the slave nodes. The K local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our code is publicly available at https://github.com/AIBluefisher/DOGS.",
    "arxiv_url": "http://arxiv.org/abs/2405.13943v2",
    "pdf_url": "http://arxiv.org/pdf/2405.13943v2",
    "published_date": "2024-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/AIBluefisher/DOGS",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular Gaussian SLAM with Language Extended Loop Closure",
    "authors": [
      "Tian Lan",
      "Qinwei Lin",
      "Haoqian Wang"
    ],
    "abstract": "Recently,3DGaussianSplattinghasshowngreatpotentialin visual Simultaneous Localization And Mapping (SLAM). Existing methods have achieved encouraging results on RGB-D SLAM, but studies of the monocular case are still scarce. Moreover, they also fail to correct drift errors due to the lack of loop closure and global optimization. In this paper, we present MG-SLAM, a monocular Gaussian SLAM with a language-extended loop closure module capable of performing drift-corrected tracking and high-fidelity reconstruction while achieving a high-level understanding of the environment. Our key idea is to represent the global map as 3D Gaussian and use it to guide the estimation of the scene geometry, thus mitigating the efforts of missing depth information. Further, an additional language-extended loop closure module which is based on CLIP feature is designed to continually perform global optimization to correct drift errors accumulated as the system runs. Our system shows promising results on multiple challenging datasets in both tracking and mapping and even surpasses some existing RGB-D methods.",
    "arxiv_url": "http://arxiv.org/abs/2405.13748v1",
    "pdf_url": "http://arxiv.org/pdf/2405.13748v1",
    "published_date": "2024-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "high-fidelity",
      "tracking",
      "mapping",
      "understanding",
      "geometry",
      "3d gaussian",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances",
    "authors": [
      "Licheng Shen",
      "Ho Ngai Chow",
      "Lingyun Wang",
      "Tong Zhang",
      "Mengqiu Wang",
      "Yuxing Han"
    ],
    "abstract": "Recent advancements in neural rendering techniques have significantly enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D Gaussian Splatting (3DGS) has marked a significant milestone by adopting a discrete scene representation, facilitating efficient training and real-time rendering. Several studies have successfully extended the real-time rendering capability of 3DGS to dynamic scenes. However, a challenge arises when training images are captured under vastly differing weather and lighting conditions. This scenario poses a challenge for 3DGS and its variants in achieving accurate reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown promise in handling such challenging conditions, their computational demands hinder real-time rendering capabilities. In this paper, we present Gaussian Time Machine (GTM) which models the time-dependent attributes of Gaussian primitives with discrete time embedding vectors decoded by a lightweight Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives, we can reconstruct visibility changes of objects. We further propose a decomposed color model for improved geometric consistency. GTM achieved state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles the appearance changes and renders smooth appearance interpolation.",
    "arxiv_url": "http://arxiv.org/abs/2405.13694v1",
    "pdf_url": "http://arxiv.org/pdf/2405.13694v1",
    "published_date": "2024-05-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "fast",
      "lightweight",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "nerf",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction",
    "authors": [
      "Zuo-Liang Zhu",
      "Beibei Wang",
      "Jian Yang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets and reconstructing faithful geometry with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Volumetric signed distance field (SDF) methods provide robust geometry reconstruction, while the expensive ray marching hinders its real-time application and slows the training. Besides, these methods struggle to capture sharp geometric details. To this end, we propose to guide 3DGS and SDF bidirectionally in a complementary manner, including an SDF-aided Gaussian splatting for efficient optimization of the relighting model and a GS-guided SDF enhancement for high-quality geometry reconstruction. At the core of our SDF-aided Gaussian splatting is the mutual supervision of the depth and normal between blended Gaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks to this mutual supervision, the learned blended Gaussians are well-constrained with a minimal time cost. As the Gaussians are rendered in a deferred shading mode, the alpha-blended Gaussians are smooth, while individual Gaussians may still be outliers, yielding floater artifacts. Therefore, we introduce an SDF-aware pruning strategy to remove Gaussian outliers located distant from the surface defined by SDF, avoiding floater issue. This way, our GS framework provides reasonable normal and achieves realistic relighting, while the mesh from depth is still problematic. Therefore, we design a GS-guided SDF refinement, which utilizes the blended normal from Gaussians to finetune SDF. With this enhancement, our method can further provide high-quality meshes for reflective objects at the cost of 17% extra training time.",
    "arxiv_url": "http://arxiv.org/abs/2406.18544v3",
    "pdf_url": "http://arxiv.org/pdf/2406.18544v3",
    "published_date": "2024-05-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray marching",
      "efficient",
      "relightable",
      "efficient rendering",
      "relighting",
      "lighting",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video",
    "authors": [
      "Hongsheng Wang",
      "Xiang Cai",
      "Xi Sun",
      "Jinhong Yue",
      "Zhanyun Tang",
      "Shengyu Zhang",
      "Feng Lin",
      "Fei Wu"
    ],
    "abstract": "Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clo}thed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively. Codes are available at https://wanghongsheng01.github.io/MOSS/.",
    "arxiv_url": "http://arxiv.org/abs/2405.12806v3",
    "pdf_url": "http://arxiv.org/pdf/2405.12806v3",
    "published_date": "2024-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "deformation",
      "body",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LAGA: Layered 3D Avatar Generation and Customization via Gaussian Splatting",
    "authors": [
      "Jia Gong",
      "Shenyu Ji",
      "Lin Geng Foo",
      "Kang Chen",
      "Hossein Rahmani",
      "Jun Liu"
    ],
    "abstract": "Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task. Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments. In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments. By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level. Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself. To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments. Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars. Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans.",
    "arxiv_url": "http://arxiv.org/abs/2405.12663v2",
    "pdf_url": "http://arxiv.org/pdf/2405.12663v2",
    "published_date": "2024-05-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery",
    "authors": [
      "Hongsheng Wang",
      "Weiyue Zhang",
      "Sihao Liu",
      "Xinrui Zhou",
      "Jing Li",
      "Zhanyun Tang",
      "Shengyu Zhang",
      "Fei Wu",
      "Feng Lin"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.",
    "arxiv_url": "http://arxiv.org/abs/2405.12477v3",
    "pdf_url": "http://arxiv.org/pdf/2405.12477v3",
    "published_date": "2024-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "semantic",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details",
    "authors": [
      "Boqian Li",
      "Xuan Li",
      "Ying Jiang",
      "Tianyi Xie",
      "Feng Gao",
      "Huamin Wang",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "abstract": "Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/.",
    "arxiv_url": "http://arxiv.org/abs/2405.12420v1",
    "pdf_url": "http://arxiv.org/pdf/2405.12420v1",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "mapping",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field",
    "authors": [
      "Rong Liu",
      "Rui Xu",
      "Yue Hu",
      "Meida Chen",
      "Andrew Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (https://rongliu-leo.github.io/AtomGS/).",
    "arxiv_url": "http://arxiv.org/abs/2405.12369v3",
    "pdf_url": "http://arxiv.org/pdf/2405.12369v3",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo",
    "authors": [
      "Tianqi Liu",
      "Guangcong Wang",
      "Shoukang Hu",
      "Liao Shen",
      "Xinyi Ye",
      "Yuhang Zang",
      "Zhiguo Cao",
      "Wei Li",
      "Ziwei Liu"
    ],
    "abstract": "We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.",
    "arxiv_url": "http://arxiv.org/abs/2405.12218v3",
    "pdf_url": "http://arxiv.org/pdf/2405.12218v3",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents",
    "authors": [
      "Guanlin Wu",
      "Zhonghao Lyu",
      "Juyong Zhang",
      "Jie Xu"
    ],
    "abstract": "The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences. Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes. Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience. This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks. Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques. We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture, which is suitable for exploiting distributed data and computing resources over 6G networks to train large models representing large-scale scenes. Then, we consider the over-the-air rendering of NeRF and 3D-GS models at wireless network edge. We present three practical rendering architectures, namely local, remote, and co-rendering, respectively, and provide model compression approaches to facilitate the transmission of radiance field models for rendering. We also present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency. In a case study, we propose a new semantic communication enabled 3D content transmission design.",
    "arxiv_url": "http://arxiv.org/abs/2405.12155v2",
    "pdf_url": "http://arxiv.org/pdf/2405.12155v2",
    "published_date": "2024-05-20",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "acceleration",
      "ar",
      "nerf",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization",
    "authors": [
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Lei Huang",
      "Lin Gu",
      "Jin Zheng",
      "Xiao Bai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting rendering. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields, we observe that the two radiance fields exhibit point disagreement and rendering disagreement that can unsupervisedly predict reconstruction quality, stemming from the randomness of densification implementation. We further quantify the two disagreements and demonstrate the negative correlation between them and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurate and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views.",
    "arxiv_url": "http://arxiv.org/abs/2405.12110v2",
    "pdf_url": "http://arxiv.org/pdf/2405.12110v2",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping",
    "authors": [
      "Tianhao Wu",
      "Jing Yang",
      "Zhilin Guo",
      "Jingyi Wan",
      "Fangcheng Zhong",
      "Cengiz Oztireli"
    ],
    "abstract": "By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.",
    "arxiv_url": "http://arxiv.org/abs/2405.12069v2",
    "pdf_url": "http://arxiv.org/pdf/2405.12069v2",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "efficient rendering",
      "mapping",
      "body",
      "geometry",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections",
    "authors": [
      "Jiayue Liu",
      "Xiao Tang",
      "Freeman Cheng",
      "Roy Yang",
      "Zhihao Li",
      "Jianzhuang Liu",
      "Yi Huang",
      "Jiaqi Lin",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Chun Yuan"
    ],
    "abstract": "3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2405.11921v1",
    "pdf_url": "http://arxiv.org/pdf/2405.11921v1",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory Score Matching",
    "authors": [
      "Xingyu Miao",
      "Haoran Duan",
      "Varun Ojha",
      "Jun Song",
      "Tejal Shah",
      "Yang Long",
      "Rajiv Ranjan"
    ],
    "abstract": "In this work, we propose a novel Trajectory Score Matching (TSM) method that aims to solve the pseudo ground truth inconsistency problem caused by the accumulated error in Interval Score Matching (ISM) when using the Denoising Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the inversion process of DDIM to calculate on a single path, our TSM method leverages the inversion process of DDIM to generate two paths from the same starting point for calculation. Since both paths start from the same starting point, TSM can reduce the accumulated error compared to ISM, thus alleviating the problem of pseudo ground truth inconsistency. TSM enhances the stability and consistency of the model's generated paths during the distillation process. We demonstrate this experimentally and further show that ISM is a special case of TSM. Furthermore, to optimize the current multi-stage optimization process from high-resolution text to 3D generation, we adopt Stable Diffusion XL for guidance. In response to the issues of abnormal replication and splitting caused by unstable gradients during the 3D Gaussian splatting process when using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping method. Extensive experiments show that our model significantly surpasses the state-of-the-art models in terms of visual quality and performance. Code: \\url{https://github.com/xingy038/Dreamer-XL}.",
    "arxiv_url": "http://arxiv.org/abs/2405.11252v1",
    "pdf_url": "http://arxiv.org/pdf/2405.11252v1",
    "published_date": "2024-05-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/xingy038/Dreamer-XL",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionGS : Compact Gaussian Splatting SLAM by Motion Filter",
    "authors": [
      "Xinli Guo",
      "Weidong Zhang",
      "Ruonan Liu",
      "Peng Han",
      "Hongtian Chen"
    ],
    "abstract": "With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed tracking is achieved by feature extraction and motion filter on each frame. The joint optimization of poses and 3D Gaussians runs through the entire mapping process. Additionally, the coarse-to-fine pose estimation and compact Gaussian scene representation are implemented by dual keyframe selection and novel loss functions. Experimental results demonstrate that the proposed algorithm not only outperforms the existing methods in tracking and mapping, but also has less memory usage.",
    "arxiv_url": "http://arxiv.org/abs/2405.11129v2",
    "pdf_url": "http://arxiv.org/pdf/2405.11129v2",
    "published_date": "2024-05-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhanced 3D Urban Scene Reconstruction and Point Cloud Densification using Gaussian Splatting and Google Earth Imagery",
    "authors": [
      "Kyle Gao",
      "Dening Lu",
      "Hongjie He",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "3D urban scene reconstruction and modelling is a crucial research area in remote sensing with numerous applications in academia, commerce, industry, and administration. Recent advancements in view synthesis models have facilitated photorealistic 3D reconstruction solely from 2D images. Leveraging Google Earth imagery, we construct a 3D Gaussian Splatting model of the Waterloo region centered on the University of Waterloo and are able to achieve view-synthesis results far exceeding previous 3D view-synthesis results based on neural radiance fields which we demonstrate in our benchmark. Additionally, we retrieved the 3D geometry of the scene using the 3D point cloud extracted from the 3D Gaussian Splatting model which we benchmarked against our Multi- View-Stereo dense reconstruction of the scene, thereby reconstructing both the 3D geometry and photorealistic lighting of the large-scale urban scene through 3D Gaussian Splatting",
    "arxiv_url": "http://arxiv.org/abs/2405.11021v2",
    "pdf_url": "http://arxiv.org/pdf/2405.11021v2",
    "published_date": "2024-05-17",
    "categories": [
      "cs.CV",
      "I.4; I.3"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation",
    "authors": [
      "Pengzhi Li",
      "Chengshuai Tang",
      "Qinxuan Huang",
      "Zhiheng Li"
    ],
    "abstract": "In this paper, we explore the existing challenges in 3D artistic scene generation by introducing ART3D, a novel framework that combines diffusion models and 3D Gaussian splatting techniques. Our method effectively bridges the gap between artistic and realistic images through an innovative image semantic transfer algorithm. By leveraging depth information and an initial artistic image, we generate a point cloud map, addressing domain differences. Additionally, we propose a depth consistency module to enhance 3D scene consistency. Finally, the 3D scene serves as initial points for optimizing Gaussian splats. Experimental results demonstrate ART3D's superior performance in both content and structural consistency metrics when compared to existing methods. ART3D significantly advances the field of AI in art creation by providing an innovative solution for generating high-quality 3D artistic scenes.",
    "arxiv_url": "http://arxiv.org/abs/2405.10508v1",
    "pdf_url": "http://arxiv.org/pdf/2405.10508v1",
    "published_date": "2024-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Planner: A Gaussian-Splatting-based Planning Framework for Active High-Fidelity Reconstruction",
    "authors": [
      "Rui Jin",
      "Yuman Gao",
      "Yingjian Wang",
      "Haojian Lu",
      "Fei Gao"
    ],
    "abstract": "Active reconstruction technique enables robots to autonomously collect scene data for full coverage, relieving users from tedious and time-consuming data capturing process. However, designed based on unsuitable scene representations, existing methods show unrealistic reconstruction results or the inability of online quality evaluation. Due to the recent advancements in explicit radiance field technology, online active high-fidelity reconstruction has become achievable. In this paper, we propose GS-Planner, a planning framework for active high-fidelity reconstruction using 3D Gaussian Splatting. With improvement on 3DGS to recognize unobserved regions, we evaluate the reconstruction quality and completeness of 3DGS map online to guide the robot. Then we design a sampling-based active reconstruction strategy to explore the unobserved areas and improve the reconstruction geometric and textural quality. To establish a complete robot active reconstruction system, we choose quadrotor as the robotic platform for its high agility. Then we devise a safety constraint with 3DGS to generate executable trajectories for quadrotor navigation in the 3DGS map. To validate the effectiveness of our method, we conduct extensive experiments and ablation studies in highly realistic simulation scenes.",
    "arxiv_url": "http://arxiv.org/abs/2405.10142v2",
    "pdf_url": "http://arxiv.org/pdf/2405.10142v2",
    "published_date": "2024-05-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From NeRFs to Gaussian Splats, and Back",
    "authors": [
      "Siming He",
      "Zach Osman",
      "Pratik Chaudhari"
    ],
    "abstract": "For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch.",
    "arxiv_url": "http://arxiv.org/abs/2405.09717v3",
    "pdf_url": "http://arxiv.org/pdf/2405.09717v3",
    "published_date": "2024-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "fast",
      "real-time rendering",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting",
    "authors": [
      "Haodong Chen",
      "Yongle Huang",
      "Haojian Huang",
      "Xiangsheng Ge",
      "Dian Shao"
    ],
    "abstract": "The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing.",
    "arxiv_url": "http://arxiv.org/abs/2405.07472v2",
    "pdf_url": "http://arxiv.org/pdf/2405.07472v2",
    "published_date": "2024-05-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "body",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer",
    "authors": [
      "Siyou Lin",
      "Zhe Li",
      "Zhaoqi Su",
      "Zerong Zheng",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html.",
    "arxiv_url": "http://arxiv.org/abs/2405.07319v1",
    "pdf_url": "http://arxiv.org/pdf/2405.07319v1",
    "published_date": "2024-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "motion",
      "face",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "avatar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Direct Learning of Mesh and Appearance via 3D Gaussian Splatting",
    "authors": [
      "Ancheng Lin",
      "Yusheng Xiang",
      "Paul Kennedy",
      "Jun Li"
    ],
    "abstract": "Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). However, existing methods encounter efficiency issues due to indirect geometry learning and the paradigm of separately modeling geometry and surface appearance. In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of both 3DGS and mesh. Experimental results demonstrate that the learned scene model not only improves efficiency and rendering quality but also enables manipulation via the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance.",
    "arxiv_url": "http://arxiv.org/abs/2405.06945v3",
    "pdf_url": "http://arxiv.org/pdf/2405.06945v3",
    "published_date": "2024-05-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation",
    "authors": [
      "Jinwei Lin"
    ],
    "abstract": "One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source.",
    "arxiv_url": "http://arxiv.org/abs/2405.06547v1",
    "pdf_url": "http://arxiv.org/pdf/2405.06547v1",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "semantic",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions",
    "authors": [
      "Jinwei Lin"
    ],
    "abstract": "3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain an implicit neural learning rendering result than the traditional neural rendering technology but keep the more high-definition fast rendering speed. But it is still difficult to achieve a fast enough efficiency on 3D Gaussian Splatting for the practical applications. To Address this issue, we propose the I3DS, a synthetic model performance improvement evaluation solution and experiments test. From multiple and important levels or dimensions of the original 3D Gaussian Splatting, we made more than two thousand various kinds of experiments to test how the selected different items and components can make an impact on the training efficiency of the 3D Gaussian Splatting model. In this paper, we will share abundant and meaningful experiences and methods about how to improve the training, performance and the impacts caused by different items of the model. A special but normal Integer compression in base 95 and a floating-point compression in base 94 with ASCII encoding and decoding mechanism is presented. Many real and effective experiments and test results or phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can gain excellent performance improvements than the previous one. The project code is available as open source.",
    "arxiv_url": "http://arxiv.org/abs/2405.06408v1",
    "pdf_url": "http://arxiv.org/pdf/2405.06408v1",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "neural rendering",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization",
    "authors": [
      "Pengcheng Zhu",
      "Yaoming Zhuang",
      "Baoquan Chen",
      "Li Li",
      "Chengdong Wu",
      "Zhanlin Liu"
    ],
    "abstract": "This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM based on Gaussian Splatting has shown promising results. However, in monocular scenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit weaker tracking capability. To address these limitations, we jointly optimize sparse visual odometry tracking and 3D Gaussian Splatting scene representation for the first time. We obtain depth maps on visual odometry keyframe windows using a fast Multi-View Stereo (MVS) network for the geometric supervision of Gaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense Adjustment Ring (SDAR) to reduce the negative effect of estimated depth maps and preserve the consistency in scale between the visual odometry and Gaussian maps. We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art. Additionally, it outperforms previous monocular methods in terms of novel view synthesis and geometric reconstruction fidelities.",
    "arxiv_url": "http://arxiv.org/abs/2405.06241v2",
    "pdf_url": "http://arxiv.org/pdf/2405.06241v2",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation",
    "authors": [
      "Sitian Shen",
      "Jing Xu",
      "Yuheng Yuan",
      "Xingyi Yang",
      "Qiuhong Shen",
      "Xinchao Wang"
    ],
    "abstract": "User-friendly 3D object editing is a challenging task that has attracted significant attention recently. The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing. While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing. In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks. Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input. This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing. Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments.",
    "arxiv_url": "http://arxiv.org/abs/2405.05800v1",
    "pdf_url": "http://arxiv.org/pdf/2405.05800v1",
    "published_date": "2024-05-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic Gaussian Splatting",
    "authors": [
      "Yikun Ma",
      "Dandan Zhan",
      "Zhi Jin"
    ],
    "abstract": "Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2405.05768v1",
    "pdf_url": "http://arxiv.org/pdf/2405.05768v1",
    "published_date": "2024-05-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "vr",
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap",
    "authors": [
      "Jingwei Huang",
      "Mingrui Li",
      "Lei Sun",
      "Aaron Xuxiang Tian",
      "Tianchen Deng",
      "Hongyu Wang"
    ],
    "abstract": "SLAM systems based on Gaussian Splatting have garnered attention due to their capabilities for rapid real-time rendering and high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure detection. To address these issues, we introduce NGM-SLAM, the first 3DGS based SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting. We utilize neural radiance field submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps. Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate hole filling and high-quality scene expression, supporting monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance.",
    "arxiv_url": "http://arxiv.org/abs/2405.05702v8",
    "pdf_url": "http://arxiv.org/pdf/2405.05702v8",
    "published_date": "2024-05-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "large scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview",
    "authors": [
      "Yuhang Ming",
      "Xingrui Yang",
      "Weihan Wang",
      "Zheng Chen",
      "Jinglun Feng",
      "Yifan Xing",
      "Guofeng Zhang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D scene representation, offering high-fidelity renderings and reconstructions from a set of sparse and unstructured sensor data. In the context of autonomous robotics, where perception and understanding of the environment are pivotal, NeRF holds immense promise for improving performance. In this paper, we present a comprehensive survey and analysis of the state-of-the-art techniques for utilizing NeRF to enhance the capabilities of autonomous robots. We especially focus on the perception, localization and navigation, and decision-making modules of autonomous robots and delve into tasks crucial for autonomous operation, including 3D reconstruction, segmentation, pose estimation, simultaneous localization and mapping (SLAM), navigation and planning, and interaction. Our survey meticulously benchmarks existing NeRF-based methods, providing insights into their strengths and limitations. Moreover, we explore promising avenues for future research and development in this domain. Notably, we discuss the integration of advanced techniques such as 3D Gaussian splatting (3DGS), large language models (LLM), and generative AIs, envisioning enhanced reconstruction efficiency, scene understanding, decision-making capabilities. This survey serves as a roadmap for researchers seeking to leverage NeRFs to empower autonomous robots, paving the way for innovative solutions that can navigate and interact seamlessly in complex environments.",
    "arxiv_url": "http://arxiv.org/abs/2405.05526v3",
    "pdf_url": "http://arxiv.org/pdf/2405.05526v3",
    "published_date": "2024-05-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "high-fidelity",
      "survey",
      "mapping",
      "understanding",
      "segmentation",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields",
    "authors": [
      "Yuanhao Gong"
    ],
    "abstract": "The 3D Gaussian splatting methods are getting popular. However, they work directly on the signal, leading to a dense representation of the signal. Even with some techniques such as pruning or distillation, the results are still dense. In this paper, we propose to model the gradient of the original signal. The gradients are much sparser than the original signal. Therefore, the gradients use much less Gaussian splats, leading to the more efficient storage and thus higher computational performance during both training and rendering. Thanks to the sparsity, during the view synthesis, only a small mount of pixels are needed, leading to much higher computational performance ($100\\sim 1000\\times$ faster). And the 2D image can be recovered from the gradients via solving a Poisson equation with linear computation complexity. Several experiments are performed to confirm the sparseness of the gradients and the computation performance of the proposed method. The method can be applied various applications, such as human body modeling and indoor environment modeling.",
    "arxiv_url": "http://arxiv.org/abs/2405.05446v1",
    "pdf_url": "http://arxiv.org/pdf/2405.05446v1",
    "published_date": "2024-05-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting",
    "authors": [
      "Ola Shorinwa",
      "Johnathan Tucker",
      "Aliyah Smith",
      "Aiden Swann",
      "Timothy Chen",
      "Roya Firoozi",
      "Monroe Kennedy III",
      "Mac Schwager"
    ],
    "abstract": "We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) ASK-Splat, a GSplat representation that distills semantic and grasp affordance features into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical in many robotics tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a \"digital twin\" of the evolving environment throughout the manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses ASK-Splat and SEE-Splat to propose affordance-aligned candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks and in four multi-stage manipulation tasks, using the edited scene to reflect changes due to prior manipulation stages, which is not possible with existing baselines. Video demonstrations and the code for the project are available at https://splatmover.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2405.04378v4",
    "pdf_url": "http://arxiv.org/pdf/2405.04378v4",
    "published_date": "2024-05-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "motion",
      "semantic",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose",
    "authors": [
      "Kaiwen Jiang",
      "Yang Fu",
      "Mukund Varma T",
      "Yash Belhe",
      "Xiaolong Wang",
      "Hao Su",
      "Ravi Ramamoorthi"
    ],
    "abstract": "Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset. Project page: https://raymondjiangkw.github.io/cogs.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2405.03659v2",
    "pdf_url": "http://arxiv.org/pdf/2405.03659v2",
    "published_date": "2024-05-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review",
    "authors": [
      "Anurag Dalal",
      "Daniel Hagen",
      "Kjell G. Robbersmyr",
      "Kristian Muri Knausg√•rd"
    ],
    "abstract": "Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2405.03417v1",
    "pdf_url": "http://arxiv.org/pdf/2405.03417v1",
    "published_date": "2024-05-06",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.2.10; I.3.6; I.3.7; I.3.8; I.4.5; I.4.8; I.4.10"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft HoloLens 2",
    "authors": [
      "Miriam J√§ger",
      "Theodor Kapler",
      "Michael Fe√üenbecker",
      "Felix Birkelbach",
      "Markus Hillemann",
      "Boris Jutzi"
    ],
    "abstract": "In the fields of photogrammetry, computer vision and computer graphics, the task of neural 3D scene reconstruction has led to the exploration of various techniques. Among these, 3D Gaussian Splatting stands out for its explicit representation of scenes using 3D Gaussians, making it appealing for tasks like 3D point cloud extraction and surface reconstruction. Motivated by its potential, we address the domain of 3D scene reconstruction, aiming to leverage the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting. We present HoloGS, a novel workflow utilizing HoloLens sensor data, which bypasses the need for pre-processing steps like Structure from Motion by instantly accessing the required input data i.e. the images, camera poses and the point cloud from depth sensing. We provide comprehensive investigations, including the training process and the rendering quality, assessed through the Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate our approach on two self-captured scenes: An outdoor scene of a cultural heritage statue and an indoor scene of a fine-structured plant. Our results show that the HoloLens data, including RGB images, corresponding camera poses, and depth sensing based point clouds to initialize the Gaussians, are suitable as input for 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2405.02005v1",
    "pdf_url": "http://arxiv.org/pdf/2405.02005v1",
    "published_date": "2024-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians",
    "authors": [
      "Zhenya Yang",
      "Kai Chen",
      "Yonghao Long",
      "Qi Dou"
    ],
    "abstract": "Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy regularization into the Gaussian learning process. Furthermore, we apply the Material Point Method, which is integrated with physical properties, to the 3D Gaussians to achieve realistic scene deformations. Our method was evaluated on our collected in-house and public surgical videos datasets. Results show that it can reconstruct and simulate surgical scenes from endoscopic videos efficiently-taking only a few minutes to reconstruct the surgical scene-and produce both visually and physically plausible deformations at a speed approaching real-time. The results demonstrate great potential of our proposed method to enhance the efficiency and variety of simulations available for surgical education and robot learning.",
    "arxiv_url": "http://arxiv.org/abs/2405.00956v3",
    "pdf_url": "http://arxiv.org/pdf/2405.00956v3",
    "published_date": "2024-05-02",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "deformation",
      "body",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spectrally Pruned Gaussian Fields with Neural Compensation",
    "authors": [
      "Runyi Yang",
      "Zhenxin Zhu",
      "Zhou Jiang",
      "Baijun Ye",
      "Xiaoxue Chen",
      "Yifei Zhang",
      "Yuantao Chen",
      "Jian Zhao",
      "Hao Zhao"
    ],
    "abstract": "Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/.",
    "arxiv_url": "http://arxiv.org/abs/2405.00676v1",
    "pdf_url": "http://arxiv.org/pdf/2405.00676v1",
    "published_date": "2024-05-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "fast",
      "lightweight",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting",
    "authors": [
      "Zhexi Peng",
      "Tianjia Shao",
      "Yong Liu",
      "Jingke Zhou",
      "Yin Yang",
      "Jingdong Wang",
      "Kun Zhou"
    ],
    "abstract": "We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2404.19706v3",
    "pdf_url": "http://arxiv.org/pdf/2404.19706v3",
    "published_date": "2024-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "face",
      "ar",
      "3d reconstruction",
      "slam",
      "large scene",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting",
    "authors": [
      "Kai Zhang",
      "Sai Bi",
      "Hao Tan",
      "Yuanbo Xiangli",
      "Nanxuan Zhao",
      "Kalyan Sunkavalli",
      "Zexiang Xu"
    ],
    "abstract": "We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ .",
    "arxiv_url": "http://arxiv.org/abs/2404.19702v1",
    "pdf_url": "http://arxiv.org/pdf/2404.19702v1",
    "published_date": "2024-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MicroDreamer: Efficient 3D Generation in $\\sim$20 Seconds by Score-based Iterative Reconstruction",
    "authors": [
      "Luxi Chen",
      "Zhengyi Wang",
      "Zihan Zhou",
      "Tingting Gao",
      "Hang Su",
      "Jun Zhu",
      "Chongxuan Li"
    ],
    "abstract": "Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample and the limitation of optimization confined to latent space. This paper introduces score-based iterative reconstruction (SIR), an efficient and general algorithm mimicking a differentiable 3D reconstruction process to reduce the NFEs and enable optimization in pixel space. Given a single set of images sampled from a multi-view score-based diffusion model, SIR repeatedly optimizes 3D parameters, unlike the single-step optimization in SDS. With other improvements in training, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks. In particular, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field while retaining a comparable performance and takes about 20 seconds to create meshes from 3D Gaussian splatting on a single A100 GPU, halving the time of the fastest optimization-based baseline DreamGaussian with significantly superior performance compared to the measurement standard deviation. Our code is available at https://github.com/ML-GSAI/MicroDreamer.",
    "arxiv_url": "http://arxiv.org/abs/2404.19525v3",
    "pdf_url": "http://arxiv.org/pdf/2404.19525v3",
    "published_date": "2024-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ML-GSAI/MicroDreamer",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Blendshapes for Head Avatar Animation",
    "authors": [
      "Shengjie Ma",
      "Yanlin Weng",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.",
    "arxiv_url": "http://arxiv.org/abs/2404.19398v2",
    "pdf_url": "http://arxiv.org/pdf/2404.19398v2",
    "published_date": "2024-04-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "face",
      "3d gaussian",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGS: Structure-Aware 3D Gaussian Splatting",
    "authors": [
      "Evangelos Ververas",
      "Rolandos Alexandros Potamias",
      "Jifei Song",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ],
    "abstract": "Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene's geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page https://eververas.github.io/SAGS/.",
    "arxiv_url": "http://arxiv.org/abs/2404.19149v1",
    "pdf_url": "http://arxiv.org/pdf/2404.19149v1",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lightweight",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting",
    "authors": [
      "Bo Chen",
      "Shoukang Hu",
      "Qi Chen",
      "Chenpeng Du",
      "Ran Yi",
      "Yanmin Qian",
      "Xie Chen"
    ],
    "abstract": "We present GStalker, a 3D audio-driven talking face generation model with Gaussian Splatting for both fast training (40 minutes) and real-time rendering (125 FPS) with a 3$\\sim$5 minute video for training material, in comparison with previous 2D and 3D NeRF-based modeling frameworks which require hours of training and seconds of rendering per frame. Specifically, GSTalker learns an audio-driven Gaussian deformation field to translate and transform 3D Gaussians to synchronize with audio information, in which multi-resolution hashing grid-based tri-plane and temporal smooth module are incorporated to learn accurate deformation for fine-grained facial details. In addition, a pose-conditioned deformation field is designed to model the stabilized torso. To enable efficient optimization of the condition Gaussian deformation field, we initialize 3D Gaussians by learning a coarse static Gaussian representation. Extensive experiments in person-specific videos with audio tracks validate that GSTalker can generate high-fidelity and audio-lips synchronized results with fast training and real-time rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2404.19040v1",
    "pdf_url": "http://arxiv.org/pdf/2404.19040v1",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "fast",
      "face",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing",
    "authors": [
      "Cong Wang",
      "Di Kang",
      "He-Yi Sun",
      "Shen-Han Qian",
      "Zi-Xuan Wang",
      "Linchao Bao",
      "Song-Hai Zhang"
    ],
    "abstract": "Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing.",
    "arxiv_url": "http://arxiv.org/abs/2404.19026v1",
    "pdf_url": "http://arxiv.org/pdf/2404.19026v1",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "vr",
      "deformation",
      "3d gaussian",
      "ar",
      "avatar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing",
    "authors": [
      "Minghao Chen",
      "Iro Laina",
      "Andrea Vedaldi"
    ],
    "abstract": "We consider the problem of editing 3D objects and scenes based on open-ended language instructions. A common approach to this problem is to use a 2D image generator or editor to guide the 3D editing process, obviating the need for 3D data. However, this process is often inefficient due to the need for iterative updates of costly 3D representations, such as neural radiance fields, either through individual view edits or score distillation sampling. A major disadvantage of this approach is the slow convergence caused by aggregating inconsistent information across views, as the guidance from 2D models is not multi-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two stages. First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent. To do so, we propose a training-free approach that integrates cues from the 3D geometry of the underlying scene. Second, given a multi-view consistent edited sequence of images, we directly and efficiently optimize the 3D representation, which is based on 3D Gaussian Splatting. Because it avoids incremental and iterative edits, DGE is significantly more accurate and efficient than existing approaches and offers additional benefits, such as enabling selective editing of parts of the scene.",
    "arxiv_url": "http://arxiv.org/abs/2404.18929v3",
    "pdf_url": "http://arxiv.org/pdf/2404.18929v3",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bootstrap-GS: Self-Supervised Augmentation for High-Fidelity Gaussian Splatting",
    "authors": [
      "Yifei Gao",
      "Kerui Ren",
      "Jie Ou",
      "Lei Wang",
      "Jiaji Wu",
      "Jun Cheng"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) have established new benchmarks for rendering quality and efficiency in 3D reconstruction. However, 3D-GS faces critical limitations when generating novel views that significantly deviate from those encountered during training. Moreover, issues such as dilation and aliasing arise during zoom operations. These challenges stem from a fundamental issue: training sampling deficiency. In this paper, we introduce a bootstrapping framework to address this problem. Our approach synthesizes pseudo-ground truth from novel views that align with the limited training set and reintegrates these synthesized views into the training pipeline. Experimental results demonstrate that our bootstrapping technique not only reduces artifacts but also improves quantitative metrics. Furthermore, our technique is highly adaptable, allowing various Gaussian-based method to benefit from its integration.",
    "arxiv_url": "http://arxiv.org/abs/2404.18669v3",
    "pdf_url": "http://arxiv.org/pdf/2404.18669v3",
    "published_date": "2024-04-29",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "I.4.8"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting with Deferred Reflection",
    "authors": [
      "Keyang Ye",
      "Qiming Hou",
      "Kun Zhou"
    ],
    "abstract": "The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.",
    "arxiv_url": "http://arxiv.org/abs/2404.18454v2",
    "pdf_url": "http://arxiv.org/pdf/2404.18454v2",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Satellites in 3D from Amateur Telescope Images",
    "authors": [
      "Zhiming Chang",
      "Boyang Liu",
      "Yifei Xia",
      "Youming Guo",
      "Boxin Shi",
      "He Sun"
    ],
    "abstract": "Monitoring space objects is crucial for space situational awareness, yet reconstructing 3D satellite models from ground-based telescope images is challenging due to atmospheric turbulence, long observation distances, limited viewpoints, and low signal-to-noise ratios. In this paper, we propose a novel computational imaging framework that overcomes these obstacles by integrating a hybrid image pre-processing pipeline with a joint pose estimation and 3D reconstruction module based on controlled Gaussian Splatting (GS) and Branch-and-Bound (BnB) search. We validate our approach on both synthetic satellite datasets and on-sky observations of China's Tiangong Space Station and the International Space Station, achieving robust 3D reconstructions of low-Earth orbit satellites from ground-based data. Quantitative evaluations using SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method outperforms state-of-the-art NeRF-based approaches, and ablation studies confirm the critical role of each component. Our framework enables high-fidelity 3D satellite monitoring from Earth, offering a cost-effective alternative for space situational awareness. Project page: https://ai4scientificimaging.org/ReconstructingSatellites",
    "arxiv_url": "http://arxiv.org/abs/2404.18394v3",
    "pdf_url": "http://arxiv.org/pdf/2404.18394v3",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-quality Surface Reconstruction using Gaussian Surfels",
    "authors": [
      "Pinxuan Dai",
      "Jiamin Xu",
      "Wenxiang Xie",
      "Xinguo Liu",
      "Huamin Wang",
      "Weiwei Xu"
    ],
    "abstract": "We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.",
    "arxiv_url": "http://arxiv.org/abs/2404.17774v2",
    "pdf_url": "http://arxiv.org/pdf/2404.17774v2",
    "published_date": "2024-04-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAM for Indoor Mapping of Wide Area Construction Environments",
    "authors": [
      "Vincent Ress",
      "Wei Zhang",
      "David Skuddis",
      "Norbert Haala",
      "Uwe Soergel"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM), i.e., the reconstruction of the environment represented by a (3D) map and the concurrent pose estimation, has made astonishing progress. Meanwhile, large scale applications aiming at the data collection in complex environments like factory halls or construction sites are becoming feasible. However, in contrast to small scale scenarios with building interiors separated to single rooms, shop floors or construction areas require measures at larger distances in potentially texture less areas under difficult illumination. Pose estimation is further aggravated since no GNSS measures are available as it is usual for such indoor applications. In our work, we realize data collection in a large factory hall by a robot system equipped with four stereo cameras as well as a 3D laser scanner. We apply our state-of-the-art LiDAR and visual SLAM approaches and discuss the respective pros and cons of the different sensor types for trajectory estimation and dense map generation in such an environment. Additionally, dense and accurate depth maps are generated by 3D Gaussian splatting, which we plan to use in the context of our project aiming on the automatic construction and site monitoring.",
    "arxiv_url": "http://arxiv.org/abs/2404.17215v1",
    "pdf_url": "http://arxiv.org/pdf/2404.17215v1",
    "published_date": "2024-04-26",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
    "authors": [
      "Shaocong Dong",
      "Lihe Ding",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue",
      "Dan Xu"
    ],
    "abstract": "3D object generation has undergone significant advancements, yielding high-quality results. However, fall short of achieving precise user control, often yielding results that do not align with user expectations, thus limiting their applicability. User-envisioning 3D object generation faces significant challenges in realizing its concepts using current generative models due to limited interaction capabilities. Existing methods mainly offer two approaches: (i) interpreting textual instructions with constrained controllability, or (ii) reconstructing 3D objects from 2D images. Both of them limit customization to the confines of the 2D reference and potentially introduce undesirable artifacts during the 3D lifting process, restricting the scope for direct and versatile 3D modifications. In this work, we introduce Interactive3D, an innovative framework for interactive 3D generation that grants users precise control over the generative process through extensive 3D interaction capabilities. Interactive3D is constructed in two cascading stages, utilizing distinct 3D representations. The first stage employs Gaussian Splatting for direct user interaction, allowing modifications and guidance of the generative direction at any intermediate step through (i) Adding and Removing components, (ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv) Semantic Editing. Subsequently, the Gaussian splats are transformed into InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to further add details and extract the geometry in the second stage. Our experiments demonstrate that Interactive3D markedly improves the controllability and quality of 3D generation. Our project webpage is available at \\url{https://interactive-3d.github.io/}.",
    "arxiv_url": "http://arxiv.org/abs/2404.16510v1",
    "pdf_url": "http://arxiv.org/pdf/2404.16510v1",
    "published_date": "2024-04-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "semantic",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians",
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Han Gao",
      "Xiaoke Jiang",
      "Yao Yuan",
      "Lei Zhang"
    ],
    "abstract": "Recently, Gaussian splatting has demonstrated significant success in novel view synthesis. Current methods often regress Gaussians with pixel or point cloud correspondence, linking each Gaussian with a pixel or a 3D point. This leads to the redundancy of Gaussians being used to overfit the correspondence rather than the objects represented by the 3D Gaussians themselves, consequently wasting resources and lacking accurate geometries or textures. In this paper, we introduce LeanGaussian, a novel approach that treats each query in deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or point cloud correspondence constraints. We leverage deformable decoder to iteratively refine the Gaussians layer-by-layer with the image features as keys and values. Notably, the center of each 3D Gaussian is defined as 3D reference points, which are then projected onto the image for deformable attention in 2D space. On both the ShapeNet SRN dataset (category level) and the Google Scanned Objects dataset (open-category level, trained with the Objaverse dataset), our approach, outperforms prior methods by approximately 6.1%, achieving a PSNR of 25.44 and 22.36, respectively. Additionally, our method achieves a 3D reconstruction speed of 7.2 FPS and rendering speed 500 FPS. Codes are available at https://github.com/jwubz123/LeanGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2404.16323v4",
    "pdf_url": "http://arxiv.org/pdf/2404.16323v4",
    "published_date": "2024-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jwubz123/LeanGaussian",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting",
    "authors": [
      "Kyusun Cho",
      "Joungbin Lee",
      "Heeji Yoon",
      "Yeobin Hong",
      "Jaehoon Ko",
      "Sangjun Ahn",
      "Seungryong Kim"
    ],
    "abstract": "We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker's superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at https://github.com/KU-CVLAB/GaussianTalker/ .",
    "arxiv_url": "http://arxiv.org/abs/2404.16012v2",
    "pdf_url": "http://arxiv.org/pdf/2404.16012v2",
    "published_date": "2024-04-24",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/KU-CVLAB/GaussianTalker",
    "keywords": [
      "head",
      "high-fidelity",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian Segmentation",
    "authors": [
      "Lizhi Wang",
      "Feng Zhou",
      "Bo yu",
      "Pu Cao",
      "Jianqin Yin"
    ],
    "abstract": "Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian Segmentation. Specifically, we proposed a novel 3D target segmentation technique based on 2D Gaussian Splatting, which segments 3D consistent target masks in multi-view scene images and generates a preliminary target model. Moreover, to reconstruct the unseen portions of the target, we propose a novel target replenishment technique driven by large-scale generative diffusion priors. We demonstrate that our method can accurately reconstruct specific targets from large scenes, both quantitatively and qualitatively. Our experiments show that OMEGAS significantly outperforms existing reconstruction methods across various scenarios. Our project page is at: https://github.com/CrystalWlz/OMEGAS",
    "arxiv_url": "http://arxiv.org/abs/2404.15891v4",
    "pdf_url": "http://arxiv.org/pdf/2404.15891v4",
    "published_date": "2024-04-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/CrystalWlz/OMEGAS",
    "keywords": [
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "large scene",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Xin Ning",
      "Jun Zhou",
      "Lin Gu"
    ],
    "abstract": "Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads. However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions. To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis. Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods. Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature. Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions. To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region. Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods.",
    "arxiv_url": "http://arxiv.org/abs/2404.15264v2",
    "pdf_url": "http://arxiv.org/pdf/2404.15264v2",
    "published_date": "2024-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "motion",
      "face",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent",
    "authors": [
      "Cameron Smith",
      "David Charatan",
      "Ayush Tewari",
      "Vincent Sitzmann"
    ],
    "abstract": "This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).",
    "arxiv_url": "http://arxiv.org/abs/2404.15259v3",
    "pdf_url": "http://arxiv.org/pdf/2404.15259v3",
    "published_date": "2024-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses",
    "authors": [
      "Inhee Lee",
      "Byungjun Kim",
      "Hanbyul Joo"
    ],
    "abstract": "In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2404.14410v1",
    "pdf_url": "http://arxiv.org/pdf/2404.14410v1",
    "published_date": "2024-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "few-shot",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and View-consistent 3D Semantic Understanding",
    "authors": [
      "Guibiao Liao",
      "Jiankun Li",
      "Zhenyu Bao",
      "Xiaoqing Ye",
      "Jingdong Wang",
      "Qing Li",
      "Kanglin Liu"
    ],
    "abstract": "The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2404.14249v1",
    "pdf_url": "http://arxiv.org/pdf/2404.14249v1",
    "published_date": "2024-04-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "semantic",
      "understanding",
      "segmentation",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting",
    "authors": [
      "Hongyun Yu",
      "Zhan Qu",
      "Qihang Yu",
      "Jianchuan Chen",
      "Zhonghua Jiang",
      "Zhiwen Chen",
      "Shengyu Zhang",
      "Jimin Xu",
      "Fei Wu",
      "Chengfei Lv",
      "Gang Yu"
    ],
    "abstract": "Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms.",
    "arxiv_url": "http://arxiv.org/abs/2404.14037v3",
    "pdf_url": "http://arxiv.org/pdf/2404.14037v3",
    "published_date": "2024-04-22",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal",
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Guofeng Zhang",
      "Dan Xu"
    ],
    "abstract": "This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds.",
    "arxiv_url": "http://arxiv.org/abs/2404.13679v1",
    "pdf_url": "http://arxiv.org/pdf/2404.13679v1",
    "published_date": "2024-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learn2Talk: 3D Talking Face Learns from 2D Talking Face",
    "authors": [
      "Yixiang Zhuang",
      "Baoping Cheng",
      "Yao Cheng",
      "Yuntao Jin",
      "Renshuai Liu",
      "Chengyang Li",
      "Xuan Cheng",
      "Jing Liao",
      "Juncong Lin"
    ],
    "abstract": "Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation.",
    "arxiv_url": "http://arxiv.org/abs/2404.12888v1",
    "pdf_url": "http://arxiv.org/pdf/2404.12888v1",
    "published_date": "2024-04-19",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation",
    "authors": [
      "Myrna C. Silva",
      "Mahtab Dahaghin",
      "Matteo Toso",
      "Alessio Del Bue"
    ],
    "abstract": "We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\\%$ over the state of the art. Code and trained models will be released soon.",
    "arxiv_url": "http://arxiv.org/abs/2404.12784v1",
    "pdf_url": "http://arxiv.org/pdf/2404.12784v1",
    "published_date": "2024-04-19",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation",
    "authors": [
      "Wenkai Liu",
      "Tao Guan",
      "Bin Zhu",
      "Lili Ju",
      "Zikai Song",
      "Dan Li",
      "Yuesong Wang",
      "Wei Yang"
    ],
    "abstract": "In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology. However, its application to large-scale, high-resolution scenes (exceeding 4k$\\times$4k pixels) is hindered by the excessive computational requirements for managing a large number of Gaussians. Addressing this, we introduce 'EfficientGS', an advanced approach that optimizes 3DGS for high-resolution, large-scale scenes. We analyze the densification process in 3DGS and identify areas of Gaussian over-proliferation. We propose a selective strategy, limiting Gaussian increase to key primitives, thereby enhancing the representational efficiency. Additionally, we develop a pruning mechanism to remove redundant Gaussians, those that are merely auxiliary to adjacent ones. For further enhancement, we integrate a sparse order increment for Spherical Harmonics (SH), designed to alleviate storage constraints and reduce training overhead. Our empirical evaluations, conducted on a range of datasets including extensive 4K+ aerial images, demonstrate that 'EfficientGS' not only expedites training and rendering times but also achieves this with a model size approximately tenfold smaller than conventional 3DGS while maintaining high rendering fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2404.12777v1",
    "pdf_url": "http://arxiv.org/pdf/2404.12777v1",
    "published_date": "2024-04-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Alternatives to SFM Point Cloud Initialization for Gaussian Splatting",
    "authors": [
      "Yalda Foroutan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian Splatting has recently been embraced as a versatile and effective method for scene reconstruction and novel view synthesis, owing to its high-quality results and compatibility with hardware rasterization. Despite its advantages, Gaussian Splatting's reliance on high-quality point cloud initialization by Structure-from-Motion (SFM) algorithms is a significant limitation to be overcome. To this end, we investigate various initialization strategies for Gaussian Splatting and delve into how volumetric reconstructions from Neural Radiance Fields (NeRF) can be utilized to bypass the dependency on SFM data. Our findings demonstrate that random initialization can perform much better if carefully designed and that by employing a combination of improved initialization strategies and structure distillation from low-cost NeRF models, it is possible to achieve equivalent results, or at times even superior, to those obtained from SFM initialization. Source code is available at https://theialab.github.io/nerf-3dgs .",
    "arxiv_url": "http://arxiv.org/abs/2404.12547v3",
    "pdf_url": "http://arxiv.org/pdf/2404.12547v3",
    "published_date": "2024-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes",
    "authors": [
      "Isabella Liu",
      "Hao Su",
      "Xiaolong Wang"
    ],
    "abstract": "Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of detailed structure and time consistency from dynamic observations. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh from dynamic input. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from dynamic observations. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: https://www.liuisabella.com/DG-Mesh",
    "arxiv_url": "http://arxiv.org/abs/2404.12379v3",
    "pdf_url": "http://arxiv.org/pdf/2404.12379v3",
    "published_date": "2024-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "efficient rendering",
      "deformation",
      "geometry",
      "3d gaussian",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior",
    "authors": [
      "Zhiheng Liu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ka Leong Cheng",
      "Jie Xiao",
      "Kai Zhu",
      "Nan Xue",
      "Yu Liu",
      "Yujun Shen",
      "Yang Cao"
    ],
    "abstract": "3D Gaussians have recently emerged as an efficient representation for novel view synthesis. This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering. Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions. To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image. Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior. Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios. We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion.",
    "arxiv_url": "http://arxiv.org/abs/2404.11613v1",
    "pdf_url": "http://arxiv.org/pdf/2404.11613v1",
    "published_date": "2024-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering",
    "authors": [
      "Xianqiang Lyu",
      "Hui Liu",
      "Junhui Hou"
    ],
    "abstract": "We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2404.11401v1",
    "pdf_url": "http://arxiv.org/pdf/2404.11401v1",
    "published_date": "2024-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeblurGS: Gaussian Splatting for Camera Motion Blur",
    "authors": [
      "Jeongtaek Oh",
      "Jaeyoung Chung",
      "Dongwoo Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.",
    "arxiv_url": "http://arxiv.org/abs/2404.11358v2",
    "pdf_url": "http://arxiv.org/pdf/2404.11358v2",
    "published_date": "2024-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Application of 3D Gaussian Splatting for Cinematic Anatomy on Consumer Class Devices",
    "authors": [
      "Simon Niedermayr",
      "Christoph Neuhauser",
      "Kaloian Petkov",
      "Klaus Engel",
      "R√ºdiger Westermann"
    ],
    "abstract": "Interactive photorealistic rendering of 3D anatomy is used in medical education to explain the structure of the human body. It is currently restricted to frontal teaching scenarios, where even with a powerful GPU and high-speed access to a large storage device where the data set is hosted, interactive demonstrations can hardly be achieved. We present the use of novel view synthesis via compressed 3D Gaussian Splatting (3DGS) to overcome this restriction, and to even enable students to perform cinematic anatomy on lightweight and mobile devices. Our proposed pipeline first finds a set of camera poses that captures all potentially seen structures in the data. High-quality images are then generated with path tracing and converted into a compact 3DGS representation, consuming < 70 MB even for data sets of multiple GBs. This allows for real-time photorealistic novel view synthesis that recovers structures up to the voxel resolution and is almost indistinguishable from the path-traced images",
    "arxiv_url": "http://arxiv.org/abs/2404.11285v2",
    "pdf_url": "http://arxiv.org/pdf/2404.11285v2",
    "published_date": "2024-04-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "path tracing",
      "body",
      "lightweight",
      "medical",
      "3d gaussian",
      "human",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded Scenes",
    "authors": [
      "Zehao Yu",
      "Torsten Sattler",
      "Andreas Geiger"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and adaptive surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing Marching Tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to or even outperforms, neural implicit methods in both quality and speed.",
    "arxiv_url": "http://arxiv.org/abs/2404.10772v2",
    "pdf_url": "http://arxiv.org/pdf/2404.10772v2",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks",
    "authors": [
      "Florian Barthel",
      "Arian Beckmann",
      "Wieland Morgenstern",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes. Project page: florian-barthel.github.io/gaussian_decoder",
    "arxiv_url": "http://arxiv.org/abs/2404.10625v2",
    "pdf_url": "http://arxiv.org/pdf/2404.10625v2",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction",
    "authors": [
      "Sinisa Stekovic",
      "Arslan Artykov",
      "Stefan Ainetter",
      "Mattia D'Urso",
      "Friedrich Fraundorfer"
    ],
    "abstract": "We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects and their parameters from images using interpretable shape programs. Unlike traditional CAD model retrieval, shape programs allow reasoning about semantic parameters, editing, and a low memory footprint. Despite their potential, shape programs for 3D scene understanding have been largely overlooked. Our key contribution is enabling gradient-based optimization by parsing shape programs, or more precisely procedural models designed in Blender, into efficient PyTorch code. While there are many possible applications of our PyTochGeoNodes, we show that a combination of PyTorchGeoNodes with genetic algorithm is a method of choice to optimize both discrete and continuous shape program parameters for 3D reconstruction and understanding of 3D object parameters. Our modular framework can be further integrated with other reconstruction algorithms, and we demonstrate one such integration to enable procedural Gaussian splatting. Our experiments on the ScanNet dataset show that our method achieves accurate reconstructions while enabling, until now, unseen level of 3D scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2404.10620v2",
    "pdf_url": "http://arxiv.org/pdf/2404.10620v2",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "efficient",
      "semantic",
      "understanding",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AbsGS: Recovering Fine Details for 3D Gaussian Splatting",
    "authors": [
      "Zongxin Ye",
      "Wenyu Li",
      "Sidun Liu",
      "Peng Qiao",
      "Yong Dou"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: https://ty424.github.io/AbsGS.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2404.10484v1",
    "pdf_url": "http://arxiv.org/pdf/2404.10484v1",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SRGS: Super-Resolution 3D Gaussian Splatting",
    "authors": [
      "Xiang Feng",
      "Yongbo He",
      "Yubo Wang",
      "Yan Yang",
      "Wen Li",
      "Yifei Chen",
      "Zhenzhong Kuang",
      "Jiajun ding",
      "Jianping Fan",
      "Yu Jun"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes will be released upon acceptance.",
    "arxiv_url": "http://arxiv.org/abs/2404.10318v2",
    "pdf_url": "http://arxiv.org/pdf/2404.10318v2",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives",
    "authors": [
      "Jiadi Cui",
      "Junming Cao",
      "Fuqiang Zhao",
      "Zhipeng He",
      "Yifan Chen",
      "Yuhui Zhong",
      "Lan Xu",
      "Yujiao Shi",
      "Yingliang Zhang",
      "Jingyi Yu"
    ],
    "abstract": "Large garages are ubiquitous yet intricate scenes that present unique challenges due to their monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction often fail in these environments due to poor correspondence construction. To address these challenges, we introduce LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate data acquisition. Using this Polar device, we present the GarageWorld dataset, consisting of eight expansive garage scenes with diverse geometric structures, which will be made publicly available for further research. Our approach demonstrates that LiDAR point clouds collected by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We introduce a novel depth regularizer that effectively eliminates floating artifacts in rendered images. Additionally, we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail (LOD) rendering. This includes adapted scaling factors for individual levels and a random-resolution-level training scheme to optimize the Gaussians across different resolutions. This representation enables efficient rendering of large-scale garage scenes on lightweight devices via a web-based renderer. Experimental results on our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the superiority of our method in terms of rendering quality and resource efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2404.09748v3",
    "pdf_url": "http://arxiv.org/pdf/2404.09748v3",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "motion",
      "face",
      "lightweight",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting as Markov Chain Monte Carlo",
    "authors": [
      "Shakiba Kheradmand",
      "Daniel Rebain",
      "Gopal Sharma",
      "Weiwei Sun",
      "Jeff Tseng",
      "Hossam Isack",
      "Abhishek Kar",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi"
    ],
    "abstract": "While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which can lead to poor-quality renderings, and reliance on a good initialization. In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene-in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply introducing noise. We then rewrite the densification and pruning strategies in 3D Gaussian Splatting as simply a deterministic state transition of MCMC samples, removing these heuristics from the framework. To do so, we revise the 'cloning' of Gaussians into a relocalization scheme that approximately preserves sample probability. To encourage efficient use of Gaussians, we introduce a regularizer that promotes the removal of unused Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.",
    "arxiv_url": "http://arxiv.org/abs/2404.09591v3",
    "pdf_url": "http://arxiv.org/pdf/2404.09591v3",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "3d gaussian",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting",
    "authors": [
      "Xiangrui Liu",
      "Xinju Wu",
      "Pingping Zhang",
      "Shiqi Wang",
      "Zhu Li",
      "Sam Kwong"
    ],
    "abstract": "Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research.",
    "arxiv_url": "http://arxiv.org/abs/2404.09458v1",
    "pdf_url": "http://arxiv.org/pdf/2404.09458v1",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred Shading",
    "authors": [
      "Tong Wu",
      "Jia-Mu Sun",
      "Yu-Kun Lai",
      "Yuewen Ma",
      "Leif Kobbelt",
      "Lin Gao"
    ],
    "abstract": "Reconstructing and editing 3D objects and scenes both play crucial roles in computer graphics and computer vision. Neural radiance fields (NeRFs) can achieve realistic reconstruction and editing results but suffer from inefficiency in rendering. Gaussian splatting significantly accelerates rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting utilizes a single Spherical Harmonic (SH) function to model both texture and lighting, limiting independent editing capabilities of these components. Recently, attempts have been made to decouple texture and lighting with the Gaussian splatting representation but may fail to produce plausible geometry and decomposition results on reflective scenes. Additionally, the forward shading technique they employ introduces noticeable blending artifacts during relighting, as the geometry attributes of Gaussians are optimized under the original illumination and may not be suitable for novel lighting conditions. To address these issues, we introduce DeferredGS, a method for decoupling and editing the Gaussian splatting representation using deferred shading. To achieve successful decoupling, we model the illumination with a learnable environment map and define additional attributes such as texture parameters and normal direction on Gaussians, where the normal is distilled from a jointly trained signed distance function. More importantly, we apply deferred shading, resulting in more realistic relighting effects compared to previous methods. Both qualitative and quantitative experiments demonstrate the superior performance of DeferredGS in novel view synthesis and editing tasks.",
    "arxiv_url": "http://arxiv.org/abs/2404.09412v2",
    "pdf_url": "http://arxiv.org/pdf/2404.09412v2",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "relighting",
      "lighting",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling",
    "authors": [
      "Yueming Zhao",
      "Xuening Yuan",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Recent advances in text-to-3D creation integrate the potent prior of Diffusion Models from text-to-image generation into 3D domain. Nevertheless, generating 3D scenes with multiple objects remains challenging. Therefore, we present DreamScape, a method for generating 3D scenes from text. Utilizing Gaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian Guide that encodes semantic primitives, spatial transformations and relationships from text using LLMs, enabling local-to-global optimization. Progressive scale control is tailored during local object generation, addressing training instability issue arising from simple blending in the global optimization stage. Collision relationships between objects are modeled at the global level to mitigate biases in LLMs priors, ensuring physical correctness. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we design specialized sparse initialization and densification strategy. Experiments demonstrate that DreamScape achieves state-of-the-art performance, enabling high-fidelity, controllable 3D scene generation.",
    "arxiv_url": "http://arxiv.org/abs/2404.09227v3",
    "pdf_url": "http://arxiv.org/pdf/2404.09227v3",
    "published_date": "2024-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGGS: Edge Guided Gaussian Splatting for Radiance Fields",
    "authors": [
      "Yuanhao Gong"
    ],
    "abstract": "The Gaussian splatting methods are getting popular. However, their loss function only contains the $\\ell_1$ norm and the structural similarity between the rendered and input images, without considering the edges in these images. It is well-known that the edges in an image provide important information. Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS) method that leverages the edges in the input images. More specifically, we give the edge region a higher weight than the flat region. With such edge guidance, the resulting Gaussian particles focus more on the edges instead of the flat regions. Moreover, such edge guidance does not crease the computation cost during the training and rendering stage. The experiments confirm that such simple edge-weighted loss function indeed improves about $1\\sim2$ dB on several difference data sets. With simply plugging in the edge guidance, the proposed method can improve all Gaussian splatting methods in different scenarios, such as human head modeling, building 3D reconstruction, etc.",
    "arxiv_url": "http://arxiv.org/abs/2404.09105v2",
    "pdf_url": "http://arxiv.org/pdf/2404.09105v2",
    "published_date": "2024-04-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "human",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field",
    "authors": [
      "Jiyang Li",
      "Lechao Cheng",
      "Zhangye Wang",
      "Tingting Mu",
      "Jingxuan He"
    ],
    "abstract": "Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at https://pokerlishao.github.io/LoopGaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2404.08966v2",
    "pdf_url": "http://arxiv.org/pdf/2404.08966v2",
    "published_date": "2024-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering",
    "authors": [
      "Jingrui Ye",
      "Zongkai Zhang",
      "Yujiao Jiang",
      "Qingmin Liao",
      "Wenming Yang",
      "Zongqing Lu"
    ],
    "abstract": "Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes.",
    "arxiv_url": "http://arxiv.org/abs/2404.08449v3",
    "pdf_url": "http://arxiv.org/pdf/2404.08449v3",
    "published_date": "2024-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh",
    "authors": [
      "Jing Wen",
      "Xiaoming Zhao",
      "Zhongzheng Ren",
      "Alexander G. Schwing",
      "Shenlong Wang"
    ],
    "abstract": "We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).",
    "arxiv_url": "http://arxiv.org/abs/2404.07991v1",
    "pdf_url": "http://arxiv.org/pdf/2404.07991v1",
    "published_date": "2024-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "real-time rendering",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion",
    "authors": [
      "Jaidev Shriram",
      "Alex Trevithick",
      "Lingjie Liu",
      "Ravi Ramamoorthi"
    ],
    "abstract": "We introduce RealmDreamer, a technique for generating forward-facing 3D scenes from text descriptions. Our method optimizes a 3D Gaussian Splatting representation to match complex text prompts using pretrained diffusion models. Our key insight is to leverage 2D inpainting diffusion models conditioned on an initial scene estimate to provide low variance supervision for unknown regions during 3D distillation. In conjunction, we imbue high-fidelity geometry with geometric distillation from a depth diffusion model, conditioned on samples from the inpainting model. We find that the initialization of the optimization is crucial, and provide a principled methodology for doing so. Notably, our technique doesn't require video or multi-view data and can synthesize various high-quality 3D scenes in different styles with complex layouts. Further, the generality of our method allows 3D synthesis from a single image. As measured by a comprehensive user study, our method outperforms all existing approaches, preferred by 88-95%. Project Page: https://realmdreamer.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2404.07199v2",
    "pdf_url": "http://arxiv.org/pdf/2404.07199v2",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and LiDAR-Inertial-Camera Fusion",
    "authors": [
      "Xiaolei Lang",
      "Laijian Li",
      "Chenming Wu",
      "Chen Zhao",
      "Lina Liu",
      "Yong Liu",
      "Jiajun Lv",
      "Xingxing Zuo"
    ],
    "abstract": "In this paper, we present a real-time photo-realistic SLAM method based on marrying Gaussian Splatting with LiDAR-Inertial-Camera SLAM. Most existing radiance-field-based SLAM systems mainly focus on bounded indoor environments, equipped with RGB-D or RGB sensors. However, they are prone to decline when expanding to unbounded scenes or encountering adverse conditions, such as violent motions and changing illumination. In contrast, oriented to general scenarios, our approach additionally tightly fuses LiDAR, IMU, and camera for robust pose estimation and photo-realistic online mapping. To compensate for regions unobserved by the LiDAR, we propose to integrate both the triangulated visual points from images and LiDAR points for initializing 3D Gaussians. In addition, the modeling of the sky and varying camera exposure have been realized for high-quality rendering. Notably, we implement our system purely with C++ and CUDA, and meticulously design a series of strategies to accelerate the online optimization of the Gaussian-based scene representation. Extensive experiments demonstrate that our method outperforms its counterparts while maintaining real-time capability. Impressively, regarding photo-realistic mapping, our method with our estimated poses even surpasses all the compared approaches that utilize privileged ground-truth poses for mapping. Our code will be released on project page https://xingxingzuo.github.io/gaussian_lic.",
    "arxiv_url": "http://arxiv.org/abs/2404.06926v2",
    "pdf_url": "http://arxiv.org/pdf/2404.06926v2",
    "published_date": "2024-04-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "illumination",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting",
    "authors": [
      "Shijie Zhou",
      "Zhiwen Fan",
      "Dejia Xu",
      "Haoran Chang",
      "Pradyumna Chari",
      "Tejas Bharadwaj",
      "Suya You",
      "Zhangyang Wang",
      "Achuta Kadambi"
    ],
    "abstract": "The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2404.06903v2",
    "pdf_url": "http://arxiv.org/pdf/2404.06903v2",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection",
    "authors": [
      "Mathis Kruse",
      "Marco Rudolph",
      "Dominik Woiwode",
      "Bodo Rosenhahn"
    ],
    "abstract": "Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set.",
    "arxiv_url": "http://arxiv.org/abs/2404.06832v1",
    "pdf_url": "http://arxiv.org/pdf/2404.06832v1",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors",
    "authors": [
      "Tianxin Huang",
      "Zhiwen Yan",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ],
    "abstract": "3D point clouds directly collected from objects through sensors are often incomplete due to self-occlusion. Conventional methods for completing these partial point clouds rely on manually organized training sets and are usually limited to object categories seen during training. In this work, we propose a test-time framework for completing partial point clouds across unseen categories without any requirement for training. Leveraging point rendering via Gaussian Splatting, we develop techniques of Partial Gaussian Initialization, Zero-shot Fractal Completion, and Point Cloud Extraction that utilize priors from pre-trained 2D diffusion models to infer missing regions and extract uniform completed point clouds. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects. Our project page is at \\url{https://tianxinhuang.github.io/projects/ComPC/}.",
    "arxiv_url": "http://arxiv.org/abs/2404.06814v2",
    "pdf_url": "http://arxiv.org/pdf/2404.06814v2",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera",
    "authors": [
      "Gaole Dai",
      "Zhenyu Wang",
      "Qinwen Xu",
      "Ming Lu",
      "Wen Chen",
      "Boxin Shi",
      "Shanghang Zhang",
      "Tiejun Huang"
    ],
    "abstract": "One of the most critical factors in achieving sharp Novel View Synthesis (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) is the quality of the training images. However, Conventional RGB cameras are susceptible to motion blur. In contrast, neuromorphic cameras like event and spike cameras inherently capture more comprehensive temporal information, which can provide a sharp representation of the scene as additional training data. Recent methods have explored the integration of event cameras to improve the quality of NVS. The event-RGB approaches have some limitations, such as high training costs and the inability to work effectively in the background. Instead, our study introduces a new method that uses the spike camera to overcome these limitations. By considering texture reconstruction from spike streams as ground truth, we design the Texture from Spike (TfS) loss. Since the spike camera relies on temporal integration instead of temporal differentiation used by event cameras, our proposed TfS loss maintains manageable training costs. It handles foreground objects with backgrounds simultaneously. We also provide a real-world dataset captured with our spike-RGB camera system to facilitate future research endeavors. We conduct extensive experiments using synthetic and real-world datasets to demonstrate that our design can enhance novel view synthesis across NeRF and 3DGS. The code and dataset will be made available for public access.",
    "arxiv_url": "http://arxiv.org/abs/2404.06710v3",
    "pdf_url": "http://arxiv.org/pdf/2404.06710v3",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "End-to-End Rate-Distortion Optimized 3D Gaussian Representation",
    "authors": [
      "Henan Wang",
      "Hanxin Zhu",
      "Tianyu He",
      "Runsen Feng",
      "Jiajun Deng",
      "Jiang Bian",
      "Zhibo Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40x, and surpasses existing methods in rate-distortion performance.",
    "arxiv_url": "http://arxiv.org/abs/2406.01597v2",
    "pdf_url": "http://arxiv.org/pdf/2406.01597v2",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis",
    "authors": [
      "Zhicheng Lu",
      "Xiang Guo",
      "Le Hui",
      "Tianrui Chen",
      "Min Yang",
      "Xiao Tang",
      "Feng Zhu",
      "Yuchao Dai"
    ],
    "abstract": "In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at https://npucvr.github.io/GaGS/",
    "arxiv_url": "http://arxiv.org/abs/2404.06270v2",
    "pdf_url": "http://arxiv.org/pdf/2404.06270v2",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction",
    "authors": [
      "Sierra Bonilla",
      "Shuai Zhang",
      "Dimitrios Psychogyios",
      "Danail Stoyanov",
      "Francisco Vasconcelos",
      "Sophia Bano"
    ],
    "abstract": "Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.",
    "arxiv_url": "http://arxiv.org/abs/2404.06128v2",
    "pdf_url": "http://arxiv.org/pdf/2404.06128v2",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "face",
      "fast",
      "mapping",
      "3d gaussian",
      "3d reconstruction",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Revising Densification in Gaussian Splatting",
    "authors": [
      "Samuel Rota Bul√≤",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ],
    "abstract": "In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2404.06109v1",
    "pdf_url": "http://arxiv.org/pdf/2404.06109v1",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hash3D: Training-free Acceleration for 3D Generation",
    "authors": [
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.",
    "arxiv_url": "http://arxiv.org/abs/2404.06091v1",
    "pdf_url": "http://arxiv.org/pdf/2404.06091v1",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "acceleration",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StylizedGS: Controllable Stylization for 3D Gaussian Splatting",
    "authors": [
      "Dingxi Zhang",
      "Yu-Jie Yuan",
      "Zhuoxun Chen",
      "Fang-Lue Zhang",
      "Zhenliang He",
      "Shiguang Shan",
      "Lin Gao"
    ],
    "abstract": "As XR technology continues to advance rapidly, 3D generation and editing are increasingly crucial. Among these, stylization plays a key role in enhancing the appearance of 3D models. By utilizing stylization, users can achieve consistent artistic effects in 3D editing using a single reference style image, making it a user-friendly editing method. However, recent NeRF-based 3D stylization methods encounter efficiency issues that impact the user experience, and their implicit nature limits their ability to accurately transfer geometric pattern styles. Additionally, the ability for artists to apply flexible control over stylized scenes is considered highly desirable to foster an environment conducive to creative exploration. To address the above issues, we introduce StylizedGS, an efficient 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. We propose a filter-based refinement to eliminate floaters that affect the stylization effects in the scene reconstruction process. The nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale, and regions during the stylization to possess customization capabilities. Our method achieves high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference speed.",
    "arxiv_url": "http://arxiv.org/abs/2404.05220v2",
    "pdf_url": "http://arxiv.org/pdf/2404.05220v2",
    "published_date": "2024-04-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dual-Camera Smooth Zoom on Mobile Phones",
    "authors": [
      "Renlong Wu",
      "Zhilu Zhang",
      "Yu Yang",
      "Wangmeng Zuo"
    ],
    "abstract": "When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user's zoom experience. In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection. To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera. With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom images without ground-truth for evaluation. Extensive experiments are conducted with multiple FI methods. The results show that the fine-tuned FI models achieve a significant performance improvement over the original ones on DCSZ task. The datasets, codes, and pre-trained models will are available at https://github.com/ZcsrenlongZ/ZoomGS.",
    "arxiv_url": "http://arxiv.org/abs/2404.04908v2",
    "pdf_url": "http://arxiv.org/pdf/2404.04908v2",
    "published_date": "2024-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ZcsrenlongZ/ZoomGS",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with Expansive Lidar Image Dataset Using 3DGS and NeRF",
    "authors": [
      "Butian Xiong",
      "Nanjun Zheng",
      "Junhua Liu",
      "Zhen Li"
    ],
    "abstract": "We introduce a novel, multimodal large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields (NeRF). Our expansive U-Scene dataset surpasses any previously existing real large-scale outdoor LiDAR and image dataset in both area and point count. GauU-Scene encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. Additionally, we are the first to propose a LiDAR and image alignment method for a drone-based dataset. Our assessment of GauU-Scene includes a detailed analysis across various novel viewpoints, employing image-based metrics such as SSIM, LPIPS, and PSNR on NeRF and Gaussian Splatting based methods. This analysis reveals contradictory results when applying geometric-based metrics like Chamfer distance. The experimental results on our multimodal dataset highlight the unreliability of current image-based metrics and reveal significant drawbacks in geometric reconstruction using the current Gaussian Splatting-based method, further illustrating the necessity of our dataset for assessing geometry reconstruction tasks. We also provide detailed supplementary information on data collection protocols and make the dataset available on the following anonymous project page",
    "arxiv_url": "http://arxiv.org/abs/2404.04880v2",
    "pdf_url": "http://arxiv.org/pdf/2404.04880v2",
    "published_date": "2024-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion",
    "authors": [
      "Ziyuan Qu",
      "Omkar Vengurlekar",
      "Mohamad Qadri",
      "Kevin Zhang",
      "Michael Kaess",
      "Christopher Metzler",
      "Suren Jayasuriya",
      "Adithya Pediredla"
    ],
    "abstract": "Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent technique in computer vision and graphics for reconstructing 3D scenes. GS represents a scene as a set of 3D Gaussians with varying opacities and employs a computationally efficient splatting operation along with analytical derivatives to compute the 3D Gaussian parameters given scene images captured from various viewpoints. Unfortunately, capturing surround view ($360^{\\circ}$ viewpoint) images is impossible or impractical in many real-world imaging scenarios, including underwater imaging, rooms inside a building, and autonomous navigation. In these restricted baseline imaging scenarios, the GS algorithm suffers from a well-known 'missing cone' problem, which results in poor reconstruction along the depth axis. In this manuscript, we demonstrate that using transient data (from sonars) allows us to address the missing cone problem by sampling high-frequency data along the depth axis. We extend the Gaussian splatting algorithms for two commonly used sonars and propose fusion algorithms that simultaneously utilize RGB camera data and sonar data. Through simulations, emulations, and hardware experiments across various imaging scenarios, we show that the proposed fusion algorithms lead to significantly better novel view synthesis (5 dB improvement in PSNR) and 3D geometry reconstruction (60% lower Chamfer distance).",
    "arxiv_url": "http://arxiv.org/abs/2404.04687v2",
    "pdf_url": "http://arxiv.org/pdf/2404.04687v2",
    "published_date": "2024-04-06",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations",
    "authors": [
      "Yang Zheng",
      "Qingqing Zhao",
      "Guandao Yang",
      "Wang Yifan",
      "Donglai Xiang",
      "Florian Dubost",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ],
    "abstract": "Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for spatio-temporal mesh tracking as well as a physically based inverse renderer to estimate the intrinsic material properties. PhysAvatar integrates a physics simulator to estimate the physical parameters of the garments using gradient-based optimization in a principled manner. These novel capabilities enable PhysAvatar to create high-quality novel-view renderings of avatars dressed in loose-fitting clothes under motions and lighting conditions not seen in the training data. This marks a significant advancement towards modeling photorealistic digital humans using physically based inverse rendering with physics in the loop. Our project website is at: https://qingqing-zhao.github.io/PhysAvatar",
    "arxiv_url": "http://arxiv.org/abs/2404.04421v2",
    "pdf_url": "http://arxiv.org/pdf/2404.04421v2",
    "published_date": "2024-04-05",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "lighting",
      "4d",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Gaussian Splatting",
    "authors": [
      "Fran√ßois Darmon",
      "Lorenzo Porzi",
      "Samuel Rota-Bul√≤",
      "Peter Kontschieder"
    ],
    "abstract": "In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.",
    "arxiv_url": "http://arxiv.org/abs/2404.04211v1",
    "pdf_url": "http://arxiv.org/pdf/2404.04211v1",
    "published_date": "2024-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "shadow",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes",
    "authors": [
      "Chenyang Wu",
      "Yifan Duan",
      "Xinran Zhang",
      "Yu Sheng",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "abstract": "Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2404.04026v1",
    "pdf_url": "http://arxiv.org/pdf/2404.04026v1",
    "published_date": "2024-04-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "outdoor",
      "fast",
      "mapping",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer",
    "authors": [
      "Zijie Wu",
      "Chaohui Yu",
      "Yanqin Jiang",
      "Chenjie Cao",
      "Fan Wang",
      "Xiang Bai"
    ],
    "abstract": "Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.",
    "arxiv_url": "http://arxiv.org/abs/2404.03736v2",
    "pdf_url": "http://arxiv.org/pdf/2404.03736v2",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "nerf",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting",
    "authors": [
      "Jeongmin Bae",
      "Seoha Kim",
      "Youngsik Yun",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions. Project page: https://jeongminb.github.io/e-d3dgs/",
    "arxiv_url": "http://arxiv.org/abs/2404.03613v5",
    "pdf_url": "http://arxiv.org/pdf/2404.03613v5",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling",
    "authors": [
      "Haoran Li",
      "Haolin Shi",
      "Wenli Zhang",
      "Wenjun Wu",
      "Yong Liao",
      "Lin Wang",
      "Lik-hang Lee",
      "Pengyuan Zhou"
    ],
    "abstract": "Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io .",
    "arxiv_url": "http://arxiv.org/abs/2404.03575v2",
    "pdf_url": "http://arxiv.org/pdf/2404.03575v2",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "fast",
      "semantic",
      "high quality",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniGS: Fast Radiance Field Reconstruction using Omnidirectional Gaussian Splatting",
    "authors": [
      "Longwei Li",
      "Huajian Huang",
      "Sai-Kit Yeung",
      "Hui Cheng"
    ],
    "abstract": "Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in various domains. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. We realize differentiable optimization of the omnidirectional radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. The code will be publicly available.",
    "arxiv_url": "http://arxiv.org/abs/2404.03202v5",
    "pdf_url": "http://arxiv.org/pdf/2404.03202v5",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis",
    "authors": [
      "Emmanouil Nikolakakis",
      "Utkarsh Gupta",
      "Jonathan Vengosh",
      "Justin Bui",
      "Razvan Marinescu"
    ],
    "abstract": "We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.",
    "arxiv_url": "http://arxiv.org/abs/2404.03126v1",
    "pdf_url": "http://arxiv.org/pdf/2404.03126v1",
    "published_date": "2024-04-04",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Cheng Zhao",
      "Su Sun",
      "Ruoyu Wang",
      "Yuliang Guo",
      "Jun-Jun Wan",
      "Zhou Huang",
      "Xinyu Huang",
      "Yingjie Victor Chen",
      "Liu Ren"
    ],
    "abstract": "Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2404.02410v2",
    "pdf_url": "http://arxiv.org/pdf/2404.02410v2",
    "published_date": "2024-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "fast",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sketch3D: Style-Consistent Guidance for Sketch-to-3D Generation",
    "authors": [
      "Wangguandong Zheng",
      "Haifeng Xia",
      "Rui Chen",
      "Ming Shao",
      "Siyu Xia",
      "Zhengming Ding"
    ],
    "abstract": "Recently, image-to-3D approaches have achieved significant results with a natural image as input. However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available. Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content. To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description. Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process. Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians. Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss. Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input.",
    "arxiv_url": "http://arxiv.org/abs/2404.01843v2",
    "pdf_url": "http://arxiv.org/pdf/2404.01843v2",
    "published_date": "2024-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views",
    "authors": [
      "Yaniv Wolf",
      "Amit Bracha",
      "Ron Kimmel"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for accurately representing scenes. However, despite its superior novel view synthesis capabilities, extracting the geometry of the scene directly from the Gaussian properties remains a challenge, as those are optimized based on a photometric loss. While some concurrent models have tried adding geometric constraints during the Gaussian optimization process, they still produce noisy, unrealistic surfaces.   We propose a novel approach for bridging the gap between the noisy 3DGS representation and the smooth 3D mesh representation, by injecting real-world knowledge into the depth extraction process. Instead of extracting the geometry of the scene directly from the Gaussian properties, we instead extract the geometry through a pre-trained stereo-matching model. We render stereo-aligned pairs of images corresponding to the original training poses, feed the pairs into a stereo model to get a depth profile, and finally fuse all of the profiles together to get a single mesh.   The resulting reconstruction is smoother, more accurate and shows more intricate details compared to other methods for surface reconstruction from Gaussian Splatting, while only requiring a small overhead on top of the fairly short 3DGS optimization process.   We performed extensive testing of the proposed method on in-the-wild scenes, obtained using a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the method on the Tanks and Temples and DTU benchmarks, achieving state-of-the-art results.",
    "arxiv_url": "http://arxiv.org/abs/2404.01810v2",
    "pdf_url": "http://arxiv.org/pdf/2404.01810v2",
    "published_date": "2024-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing",
    "authors": [
      "Ri-Zhao Qiu",
      "Ge Yang",
      "Weijia Zeng",
      "Xiaolong Wang"
    ],
    "abstract": "Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2404.01223v1",
    "pdf_url": "http://arxiv.org/pdf/2404.01223v1",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting",
    "authors": [
      "Jiarui Meng",
      "Haijie Li",
      "Yanmin Wu",
      "Qiankun Gao",
      "Shuzhou Yang",
      "Jian Zhang",
      "Siwei Ma"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has significantly advanced 3D scene reconstruction and novel view synthesis. However, like Neural Radiance Fields (NeRF), 3DGS struggles with accurately modeling physical reflections, particularly in mirrors, leading to incorrect reconstructions and inconsistent reflective properties. To address this challenge, we introduce Mirror-3DGS, a novel framework designed to accurately handle mirror geometries and reflections, thereby generating realistic mirror reflections. By incorporating mirror attributes into 3DGS and leveraging plane mirror imaging principles, Mirror-3DGS simulates a mirrored viewpoint from behind the mirror, enhancing the realism of scene renderings. Extensive evaluations on both synthetic and real-world scenes demonstrate that our method can render novel views with improved fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF, especially in mirror regions.",
    "arxiv_url": "http://arxiv.org/abs/2404.01168v2",
    "pdf_url": "http://arxiv.org/pdf/2404.01168v2",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians",
    "authors": [
      "Yang Liu",
      "He Guan",
      "Chuanchen Luo",
      "Lue Fan",
      "Naiyan Wang",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at https://dekuliutesla.github.io/citygs/.",
    "arxiv_url": "http://arxiv.org/abs/2404.01133v3",
    "pdf_url": "http://arxiv.org/pdf/2404.01133v3",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior",
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ],
    "abstract": "We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.",
    "arxiv_url": "http://arxiv.org/abs/2404.01053v2",
    "pdf_url": "http://arxiv.org/pdf/2404.01053v2",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements",
    "authors": [
      "Lisong C. Sun",
      "Neel P. Bhatt",
      "Jonathan C. Liu",
      "Zhiwen Fan",
      "Zhangyang Wang",
      "Todd E. Humphreys",
      "Ufuk Topcu"
    ],
    "abstract": "Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: https://vita-group.github.io/MM3DGS-SLAM",
    "arxiv_url": "http://arxiv.org/abs/2404.00923v1",
    "pdf_url": "http://arxiv.org/pdf/2404.00923v1",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "fast",
      "mapping",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting",
    "authors": [
      "Xiaoyang Lyu",
      "Yang-Tian Sun",
      "Yi-Hua Huang",
      "Xiuzhe Wu",
      "Ziyi Yang",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ],
    "abstract": "In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR.",
    "arxiv_url": "http://arxiv.org/abs/2404.00409v2",
    "pdf_url": "http://arxiv.org/pdf/2404.00409v2",
    "published_date": "2024-03-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/CVMI-Lab/3DGSR",
    "keywords": [
      "efficient",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InstantSplat: Sparse-view Gaussian Splatting in Seconds",
    "authors": [
      "Zhiwen Fan",
      "Kairun Wen",
      "Wenyan Cong",
      "Kevin Wang",
      "Jian Zhang",
      "Xinghao Ding",
      "Danfei Xu",
      "Boris Ivanovic",
      "Marco Pavone",
      "Georgios Pavlakos",
      "Zhangyang Wang",
      "Yue Wang"
    ],
    "abstract": "While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering. The optimization process is initialized with a large-scale trained geometric foundation model, which provides dense priors that yield initial points through model inference, after which we further optimize all scene parameters using photometric errors. To mitigate redundancy introduced by the prior model, we propose a co-visibility-based geometry initialization, and a Gaussian-based bundle adjustment is employed to rapidly adapt both the scene representation and camera parameters without relying on a complex adaptive density control process. Overall, InstantSplat is compatible with multiple point-based representations for view synthesis and surface reconstruction. It achieves an acceleration of over 30x in reconstruction and improves visual quality (SSIM) from 0.3755 to 0.7624 compared to traditional SfM with 3D-GS.",
    "arxiv_url": "http://arxiv.org/abs/2403.20309v5",
    "pdf_url": "http://arxiv.org/pdf/2403.20309v5",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "fast",
      "face",
      "geometry",
      "acceleration",
      "3d reconstruction",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces",
    "authors": [
      "Mauro Comi",
      "Alessio Tonioni",
      "Max Yang",
      "Jonathan Tremblay",
      "Valts Blukis",
      "Yijiong Lin",
      "Nathan F. Lepora",
      "Laurence Aitchison"
    ],
    "abstract": "Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object's geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality.",
    "arxiv_url": "http://arxiv.org/abs/2403.20275v1",
    "pdf_url": "http://arxiv.org/pdf/2403.20275v1",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes",
    "authors": [
      "Ke Wu",
      "Kaizhao Zhang",
      "Zhiwei Zhang",
      "Shanshuai Yuan",
      "Muer Tie",
      "Julong Wei",
      "Zijun Xu",
      "Jieru Zhao",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed.",
    "arxiv_url": "http://arxiv.org/abs/2403.20159v1",
    "pdf_url": "http://arxiv.org/pdf/2403.20159v1",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "face",
      "mapping",
      "understanding",
      "3d gaussian",
      "ar",
      "nerf",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior",
    "authors": [
      "Zhongrui Yu",
      "Haoran Wang",
      "Jinze Yang",
      "Hanzhang Wang",
      "Zeke Xie",
      "Yunfeng Cai",
      "Jiale Cao",
      "Zhong Ji",
      "Mingming Sun"
    ],
    "abstract": "Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.",
    "arxiv_url": "http://arxiv.org/abs/2403.20079v1",
    "pdf_url": "http://arxiv.org/pdf/2403.20079v1",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes",
    "authors": [
      "Zhuopeng Li",
      "Yilin Zhang",
      "Chenming Wu",
      "Jianke Zhu",
      "Liangjun Zhang"
    ],
    "abstract": "The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural rendering, enabling real-time production of high-quality renderings. However, the previous 3DGS-based methods have limitations in urban scenes due to reliance on initial Structure-from-Motion(SfM) points and difficulties in rendering distant, sky and low-texture areas. To overcome these challenges, we propose a hybrid optimization method named HO-Gaussian, which combines a grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency on SfM point initialization, allowing for rendering of urban scenes, and incorporates the Point Densitification to enhance rendering quality in problematic regions during training. Furthermore, we introduce Gaussian Direction Encoding as an alternative for spherical harmonics in the rendering pipeline, which enables view-dependent color representation. To account for multi-camera systems, we introduce neural warping to enhance object consistency across different cameras. Experimental results on widely used autonomous driving datasets demonstrate that HO-Gaussian achieves photo-realistic rendering in real-time on multi-camera urban datasets.",
    "arxiv_url": "http://arxiv.org/abs/2403.20032v1",
    "pdf_url": "http://arxiv.org/pdf/2403.20032v1",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "motion",
      "3d gaussian",
      "ar",
      "neural rendering",
      "urban scene",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond",
    "authors": [
      "Chongjie Ye",
      "Yinyu Nie",
      "Jiahao Chang",
      "Yuantao Chen",
      "Yihao Zhi",
      "Xiaoguang Han"
    ],
    "abstract": "We present GauStudio, a novel modular framework for modeling 3D Gaussian Splatting (3DGS) to provide standardized, plug-and-play components for users to easily customize and implement a 3DGS pipeline. Supported by our framework, we propose a hybrid Gaussian representation with foreground and skyball background models. Experiments demonstrate this representation reduces artifacts in unbounded outdoor scenes and improves novel view synthesis. Finally, we propose Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse approach for high-fidelity mesh reconstruction from 3DGS inputs without fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS approach enhance 3DGS modeling and rendering capabilities, enabling higher-quality novel view synthesis and surface reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2403.19632v1",
    "pdf_url": "http://arxiv.org/pdf/2403.19632v1",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing",
    "authors": [
      "Xiaowei Song",
      "Jv Zheng",
      "Shiran Yuan",
      "Huan-ang Gao",
      "Jingwei Zhao",
      "Xiang He",
      "Weihao Gu",
      "Hao Zhao"
    ],
    "abstract": "In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at https://github.com/zsy1987/SA-GS.",
    "arxiv_url": "http://arxiv.org/abs/2403.19615v1",
    "pdf_url": "http://arxiv.org/pdf/2403.19615v1",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zsy1987/SA-GS",
    "keywords": [
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering",
    "authors": [
      "Shuai Zhang",
      "Huangxuan Zhao",
      "Zhenghong Zhou",
      "Guanjun Wu",
      "Chuansheng Zheng",
      "Xinggang Wang",
      "Wenyu Liu"
    ],
    "abstract": "Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the opacity offsets of the Gaussian, using these opacity-varying Gaussians to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art render quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code is available at https://github.com/hustvl/TOGS.",
    "arxiv_url": "http://arxiv.org/abs/2403.19586v2",
    "pdf_url": "http://arxiv.org/pdf/2403.19586v2",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/hustvl/TOGS",
    "keywords": [
      "sparse view",
      "head",
      "medical",
      "4d",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians",
    "authors": [
      "Avinash Paliwal",
      "Wei Ye",
      "Jinhui Xiong",
      "Dmytro Kotovenko",
      "Rakesh Ranjan",
      "Vikas Chandra",
      "Nima Khademi Kalantari"
    ],
    "abstract": "The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.",
    "arxiv_url": "http://arxiv.org/abs/2403.19495v2",
    "pdf_url": "http://arxiv.org/pdf/2403.19495v2",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction",
    "authors": [
      "Qiuhong Shen",
      "Zike Wu",
      "Xuanyu Yi",
      "Pan Zhou",
      "Hanwang Zhang",
      "Shuicheng Yan",
      "Xinchao Wang"
    ],
    "abstract": "We tackle the challenge of efficiently reconstructing a 3D asset from a single image at millisecond speed. Existing methods for single-image 3D reconstruction are primarily based on Score Distillation Sampling (SDS) with Neural 3D representations. Despite promising results, these approaches encounter practical limitations due to lengthy optimizations and significant memory consumption. In this work, we introduce Gamba, an end-to-end 3D reconstruction model from a single-view image, emphasizing two main insights: (1) Efficient Backbone Design: introducing a Mamba-based GambaFormer network to model 3D Gaussian Splatting (3DGS) reconstruction as sequential prediction with linear scalability of token length, thereby accommodating a substantial number of Gaussians; (2) Robust Gaussian Constraints: deriving radial mask constraints from multi-view masks to eliminate the need for warmup supervision of 3D point clouds in training. We trained Gamba on Objaverse and assessed it against existing optimization-based and feed-forward 3D reconstruction approaches on the GSO Dataset, among which Gamba is the only end-to-end trained single-view reconstruction model with 3DGS. Experimental results demonstrate its competitive generation capabilities both qualitatively and quantitatively and highlight its remarkable speed: Gamba completes reconstruction within 0.05 seconds on a single NVIDIA A100 GPU, which is about $1,000\\times$ faster than optimization-based methods. Please see our project page at https://florinshen.github.io/gamba-project.",
    "arxiv_url": "http://arxiv.org/abs/2403.18795v3",
    "pdf_url": "http://arxiv.org/pdf/2403.18795v3",
    "published_date": "2024-03-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface",
    "authors": [
      "Jiahao Luo",
      "Jing Liu",
      "James Davis"
    ],
    "abstract": "We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.",
    "arxiv_url": "http://arxiv.org/abs/2403.18784v3",
    "pdf_url": "http://arxiv.org/pdf/2403.18784v3",
    "published_date": "2024-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "human",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Modeling uncertainty for Gaussian Splatting",
    "authors": [
      "Luca Savant",
      "Diego Valsesia",
      "Enrico Magli"
    ],
    "abstract": "We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications.",
    "arxiv_url": "http://arxiv.org/abs/2403.18476v1",
    "pdf_url": "http://arxiv.org/pdf/2403.18476v1",
    "published_date": "2024-03-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EgoLifter: Open-world 3D Segmentation for Egocentric Perception",
    "authors": [
      "Qiao Gu",
      "Zhaoyang Lv",
      "Duncan Frost",
      "Simon Green",
      "Julian Straub",
      "Chris Sweeney"
    ],
    "abstract": "In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.",
    "arxiv_url": "http://arxiv.org/abs/2403.18118v2",
    "pdf_url": "http://arxiv.org/pdf/2403.18118v2",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians",
    "authors": [
      "Kerui Ren",
      "Lihan Jiang",
      "Tao Lu",
      "Mulin Yu",
      "Linning Xu",
      "Zhangkai Ni",
      "Bo Dai"
    ],
    "abstract": "The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.",
    "arxiv_url": "http://arxiv.org/abs/2403.17898v2",
    "pdf_url": "http://arxiv.org/pdf/2403.17898v2",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "large scene",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
    "authors": [
      "Binbin Huang",
      "Zehao Yu",
      "Anpei Chen",
      "Andreas Geiger",
      "Shenghua Gao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-correct 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.",
    "arxiv_url": "http://arxiv.org/abs/2403.17888v3",
    "pdf_url": "http://arxiv.org/pdf/2403.17888v3",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "high quality",
      "3d gaussian",
      "real-time rendering",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing",
    "authors": [
      "Matias Turkulainen",
      "Xuqian Ren",
      "Iaroslav Melekhov",
      "Otto Seiskari",
      "Esa Rahtu",
      "Juho Kannala"
    ],
    "abstract": "High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and AR applications. 3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. In this work, we explore the use of readily accessible geometric cues to enhance Gaussian splatting optimization in challenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use off-the-shelf monocular networks to achieve better alignment with the true scene geometry. We propose an adaptive depth loss based on the gradient of color images, improving depth estimation and novel view synthesis results over various baselines. Our simple yet effective regularization technique enables direct mesh extraction from the Gaussian representation, yielding more physically accurate reconstructions of indoor scenes.",
    "arxiv_url": "http://arxiv.org/abs/2403.17822v3",
    "pdf_url": "http://arxiv.org/pdf/2403.17822v3",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "vr",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion",
    "authors": [
      "Yuanze Lin",
      "Ronald Clark",
      "Philip Torr"
    ],
    "abstract": "We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.",
    "arxiv_url": "http://arxiv.org/abs/2403.17237v1",
    "pdf_url": "http://arxiv.org/pdf/2403.17237v1",
    "published_date": "2024-03-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction",
    "authors": [
      "Mulin Yu",
      "Tao Lu",
      "Linning Xu",
      "Lihan Jiang",
      "Yuanbo Xiangli",
      "Bo Dai"
    ],
    "abstract": "Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry.",
    "arxiv_url": "http://arxiv.org/abs/2403.16964v2",
    "pdf_url": "http://arxiv.org/pdf/2403.16964v2",
    "published_date": "2024-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction",
    "authors": [
      "Christopher Wewer",
      "Kevin Raj",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ],
    "abstract": "We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not scale to large scenes and resolutions, or are limited to interpolation of close input views. latentSplat combines the strengths of regression-based and generative approaches while being trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient splatting and a fast, generative decoder. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.",
    "arxiv_url": "http://arxiv.org/abs/2403.16292v2",
    "pdf_url": "http://arxiv.org/pdf/2403.16292v2",
    "published_date": "2024-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "3d gaussian",
      "ar",
      "3d reconstruction",
      "large scene"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field",
    "authors": [
      "Jiarui Hu",
      "Xianhao Chen",
      "Boyin Feng",
      "Guanglin Li",
      "Liangjing Yang",
      "Hujun Bao",
      "Guofeng Zhang",
      "Zhaopeng Cui"
    ],
    "abstract": "Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam.",
    "arxiv_url": "http://arxiv.org/abs/2403.16095v1",
    "pdf_url": "http://arxiv.org/pdf/2403.16095v1",
    "published_date": "2024-03-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "face",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections",
    "authors": [
      "Dongbin Zhang",
      "Chuming Wang",
      "Weitao Wang",
      "Peihao Li",
      "Minghan Qin",
      "Haoqian Wang"
    ],
    "abstract": "Novel view synthesis from unconstrained in-the-wild images remains a meaningful but challenging task. The photometric variation and transient occluders in those unconstrained images make it difficult to reconstruct the original scene accurately. Previous approaches tackle the problem by introducing a global appearance feature in Neural Radiance Fields (NeRF). However, in the real world, the unique appearance of each tiny point in a scene is determined by its independent intrinsic material attributes and the varying environmental impacts it receives. Inspired by this fact, we propose Gaussian in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the scene and introduces separated intrinsic and dynamic appearance feature for each point, capturing the unchanged scene appearance along with dynamic variation like illumination and weather. Additionally, an adaptive sampling strategy is presented to allow each Gaussian point to focus on the local and detailed information more effectively. We also reduce the impact of transient occluders using a 2D visibility map. More experiments have demonstrated better reconstruction quality and details of GS-W compared to NeRF-based methods, with a faster rendering speed. Video results and code are available at https://eastbeanzhang.github.io/GS-W/.",
    "arxiv_url": "http://arxiv.org/abs/2403.15704v2",
    "pdf_url": "http://arxiv.org/pdf/2403.15704v2",
    "published_date": "2024-03-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting",
    "authors": [
      "Jun Guo",
      "Xiaojian Ma",
      "Yue Fan",
      "Huaping Liu",
      "Qing Li"
    ],
    "abstract": "Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, with wide-ranging applications in embodied agents and augmented reality systems. Existing methods adopt neurel rendering methods as 3D representations and jointly optimize color and semantic features to achieve rendering and scene understanding simultaneously. In this paper, we introduce Semantic Gaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our key idea is to distill knowledge from 2D pre-trained models to 3D Gaussians. Unlike existing methods, we design a versatile projection approach that maps various 2D semantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, which is based on spatial relationship and need no additional training. We further build a 3D semantic network that directly predicts the semantic component from raw 3D Gaussians for fast inference. The quantitative results on ScanNet segmentation and LERF object localization demonstates the superior performance of our method. Additionally, we explore several applications of Semantic Gaussians including object part segmentation, instance segmentation, scene editing, and spatiotemporal segmentation with better qualitative results over 2D and 3D baselines, highlighting its versatility and effectiveness on supporting diverse downstream tasks.",
    "arxiv_url": "http://arxiv.org/abs/2403.15624v2",
    "pdf_url": "http://arxiv.org/pdf/2403.15624v2",
    "published_date": "2024-03-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "lighting",
      "fast",
      "semantic",
      "understanding",
      "segmentation",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhang",
      "Wenbo Hu",
      "Yixing Lao",
      "Tong He",
      "Hengshuang Zhao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.",
    "arxiv_url": "http://arxiv.org/abs/2403.15530v1",
    "pdf_url": "http://arxiv.org/pdf/2403.15530v1",
    "published_date": "2024-03-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting",
    "authors": [
      "Kailing Wang",
      "Chen Yang",
      "Yuehao Wang",
      "Sikuang Li",
      "Yan Wang",
      "Qi Dou",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "abstract": "Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com",
    "arxiv_url": "http://arxiv.org/abs/2403.15124v1",
    "pdf_url": "http://arxiv.org/pdf/2403.15124v1",
    "published_date": "2024-03-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "mapping",
      "body",
      "medical",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians",
    "authors": [
      "Yifei Zeng",
      "Yanqin Jiang",
      "Siyu Zhu",
      "Yuanxun Lu",
      "Youtian Lin",
      "Hao Zhu",
      "Weiming Hu",
      "Xun Cao",
      "Yao Yao"
    ],
    "abstract": "Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video.",
    "arxiv_url": "http://arxiv.org/abs/2403.14939v1",
    "pdf_url": "http://arxiv.org/pdf/2403.14939v1",
    "published_date": "2024-03-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images",
    "authors": [
      "Yuedong Chen",
      "Haofei Xu",
      "Chuanxia Zheng",
      "Bohan Zhuang",
      "Marc Pollefeys",
      "Andreas Geiger",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ],
    "abstract": "We introduce MVSplat, an efficient model that, given sparse multi-view images as input, predicts clean feed-forward 3D Gaussians. To accurately localize the Gaussian centers, we build a cost volume representation via plane sweeping, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We also learn other Gaussian primitives' parameters jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussians via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat achieves state-of-the-art performance with the fastest feed-forward inference speed (22~fps). More impressively, compared to the latest state-of-the-art method pixelSplat, MVSplat uses $10\\times$ fewer parameters and infers more than $2\\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.",
    "arxiv_url": "http://arxiv.org/abs/2403.14627v2",
    "pdf_url": "http://arxiv.org/pdf/2403.14627v2",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation",
    "authors": [
      "Yinghao Xu",
      "Zifan Shi",
      "Wang Yifan",
      "Hansheng Chen",
      "Ceyuan Yang",
      "Sida Peng",
      "Yujun Shen",
      "Gordon Wetzstein"
    ],
    "abstract": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/.",
    "arxiv_url": "http://arxiv.org/abs/2403.14621v1",
    "pdf_url": "http://arxiv.org/pdf/2403.14621v1",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering",
    "authors": [
      "Antoine Gu√©don",
      "Vincent Lepetit"
    ],
    "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/",
    "arxiv_url": "http://arxiv.org/abs/2403.14554v1",
    "pdf_url": "http://arxiv.org/pdf/2403.14554v1",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression",
    "authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: https://github.com/YihangChen-ee/HAC",
    "arxiv_url": "http://arxiv.org/abs/2403.14530v3",
    "pdf_url": "http://arxiv.org/pdf/2403.14530v3",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/YihangChen-ee/HAC",
    "keywords": [
      "3d gaussian",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SyncTweedies: A General Generative Framework Based on Synchronized Diffusions",
    "authors": [
      "Jaihoon Kim",
      "Juil Koo",
      "Kyeongmin Yeo",
      "Minhyuk Sung"
    ],
    "abstract": "We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie's formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2403.14370v4",
    "pdf_url": "http://arxiv.org/pdf/2403.14370v4",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Yuanhao Gong",
      "Lantao Yu",
      "Guanghui Yue"
    ],
    "abstract": "The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\\bf 100X} faster without losing the geometry representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.",
    "arxiv_url": "http://arxiv.org/abs/2403.14244v1",
    "pdf_url": "http://arxiv.org/pdf/2403.14244v1",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "3d gaussian",
      "geometry",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians",
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ],
    "abstract": "In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through intersection preserving and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our Mini-Splatting integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. \\href{https://github.com/fatPeter/mini-splatting}{Code is available}.",
    "arxiv_url": "http://arxiv.org/abs/2403.14166v3",
    "pdf_url": "http://arxiv.org/pdf/2403.14166v3",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/fatPeter/mini-splatting",
    "keywords": [
      "efficient",
      "lighting",
      "compression",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS",
    "authors": [
      "Michael Niemeyer",
      "Fabian Manhardt",
      "Marie-Julie Rakotosaona",
      "Michael Oechsle",
      "Daniel Duckworth",
      "Rama Gosula",
      "Keisuke Tateno",
      "John Bates",
      "Dominik Kaeser",
      "Federico Tombari"
    ],
    "abstract": "Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS.",
    "arxiv_url": "http://arxiv.org/abs/2403.13806v2",
    "pdf_url": "http://arxiv.org/pdf/2403.13806v2",
    "published_date": "2024-03-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lightweight",
      "high quality",
      "real-time rendering",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion",
    "authors": [
      "Otto Seiskari",
      "Jerry Ylilammi",
      "Valtteri Kaatrasalo",
      "Pekka Rantalankila",
      "Matias Turkulainen",
      "Juho Kannala",
      "Esa Rahtu",
      "Arno Solin"
    ],
    "abstract": "High-quality scene reconstruction and novel view synthesis based on Gaussian Splatting (3DGS) typically require steady, high-quality photographs, often impractical to capture with handheld cameras. We present a method that adapts to camera motion and allows high-quality scene reconstruction with handheld video data suffering from motion blur and rolling shutter distortion. Our approach is based on detailed modelling of the physical image formation process and utilizes velocities estimated using visual-inertial odometry (VIO). Camera poses are considered non-static during the exposure time of a single image frame and camera poses are further optimized in the reconstruction process. We formulate a differentiable rendering pipeline that leverages screen space approximation to efficiently incorporate rolling-shutter and motion blur effects into the 3DGS framework. Our results with both synthetic and real data demonstrate superior performance in mitigating camera motion over existing methods, thereby advancing 3DGS in naturalistic settings.",
    "arxiv_url": "http://arxiv.org/abs/2403.13327v3",
    "pdf_url": "http://arxiv.org/pdf/2403.13327v3",
    "published_date": "2024-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
    "authors": [
      "Xianglong He",
      "Junyi Chen",
      "Sida Peng",
      "Di Huang",
      "Yangguang Li",
      "Xiaoshui Huang",
      "Chun Yuan",
      "Wanli Ouyang",
      "Tong He"
    ],
    "abstract": "In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency. Our project page is: https://gvgen.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2403.12957v2",
    "pdf_url": "http://arxiv.org/pdf/2403.12957v2",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting",
    "authors": [
      "Hongyu Zhou",
      "Jiahao Shao",
      "Lu Xu",
      "Dongfeng Bai",
      "Weichao Qiu",
      "Bingbing Liu",
      "Yue Wang",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2403.12722v1",
    "pdf_url": "http://arxiv.org/pdf/2403.12722v1",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "semantic",
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGBD GS-ICP SLAM",
    "authors": [
      "Seongbo Ha",
      "Jiung Yeon",
      "Hyeonwoo Yu"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.",
    "arxiv_url": "http://arxiv.org/abs/2403.12550v2",
    "pdf_url": "http://arxiv.org/pdf/2403.12550v2",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "high-fidelity",
      "tracking",
      "robotics",
      "vr",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization",
    "authors": [
      "Shuo Sun",
      "Malcolm Mielle",
      "Achim J. Lilienthal",
      "Martin Magnusson"
    ],
    "abstract": "We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM.",
    "arxiv_url": "http://arxiv.org/abs/2403.12535v2",
    "pdf_url": "http://arxiv.org/pdf/2403.12535v2",
    "published_date": "2024-03-19",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation",
    "authors": [
      "Quankai Gao",
      "Qiangeng Xu",
      "Zhe Cao",
      "Ben Mildenhall",
      "Wenchao Ma",
      "Le Chen",
      "Danhang Tang",
      "Ulrich Neumann"
    ],
    "abstract": "Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method's effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: https://zerg-overmind.github.io/GaussianFlow.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2403.12365v2",
    "pdf_url": "http://arxiv.org/pdf/2403.12365v2",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "4d",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model",
    "authors": [
      "Qi Zuo",
      "Xiaodong Gu",
      "Lingteng Qiu",
      "Yuan Dong",
      "Zhengyi Zhao",
      "Weihao Yuan",
      "Rui Peng",
      "Siyu Zhu",
      "Zilong Dong",
      "Liefeng Bo",
      "Qixing Huang"
    ],
    "abstract": "Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV.",
    "arxiv_url": "http://arxiv.org/abs/2403.12010v1",
    "pdf_url": "http://arxiv.org/pdf/2403.12010v1",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reinforcement Learning with Generalizable Gaussian Splatting",
    "authors": [
      "Jiaxu Wang",
      "Qiang Zhang",
      "Jingkai Sun",
      "Jiahang Cao",
      "Gang Han",
      "Wen Zhao",
      "Weining Zhang",
      "Yecheng Shao",
      "Yijie Guo",
      "Renjing Xu"
    ],
    "abstract": "An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box\", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL.",
    "arxiv_url": "http://arxiv.org/abs/2404.07950v3",
    "pdf_url": "http://arxiv.org/pdf/2404.07950v3",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Consistent 3D Editing with Gaussian Splatting",
    "authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Zike Wu",
      "Na Zhao",
      "Long Chen",
      "Hanwang Zhang"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing, offering efficient, high-fidelity rendering and enabling precise local manipulations. Currently, diffusion-based 2D editing models are harnessed to modify multi-view rendered images, which then guide the editing of 3DGS models. However, this approach faces a critical issue of multi-view inconsistency, where the guidance images exhibit significant discrepancies across views, leading to mode collapse and visual artifacts of 3DGS. To this end, we introduce View-consistent Editing (VcEdit), a novel framework that seamlessly incorporates 3DGS into image editing processes, ensuring multi-view consistency in edited guidance images and effectively mitigating mode collapse issues. VcEdit employs two innovative consistency modules: the Cross-attention Consistency Module and the Editing Consistency Module, both designed to reduce inconsistencies in edited images. By incorporating these consistency modules into an iterative pattern, VcEdit proficiently resolves the issue of multi-view inconsistency, facilitating high-quality 3DGS editing across a diverse range of scenes. Further video results are shown in http://vcedit.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2403.11868v10",
    "pdf_url": "http://arxiv.org/pdf/2403.11868v10",
    "published_date": "2024-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting",
    "authors": [
      "Lingzhe Zhao",
      "Peng Wang",
      "Peidong Liu"
    ],
    "abstract": "While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at https://lingzhezhao.github.io/BAD-Gaussians/",
    "arxiv_url": "http://arxiv.org/abs/2403.11831v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11831v2",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting",
    "authors": [
      "Yiming Ji",
      "Yang Liu",
      "Guanghu Xie",
      "Boyu Ma",
      "Zongwu Xie"
    ],
    "abstract": "We propose NEDS-SLAM, a dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time. In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption. Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View Pruning method to eliminate outlier gaussians, thereby effectively enhancing the quality of scene representations. Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping.",
    "arxiv_url": "http://arxiv.org/abs/2403.11679v3",
    "pdf_url": "http://arxiv.org/pdf/2403.11679v3",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "tracking",
      "semantic",
      "mapping",
      "lightweight",
      "segmentation",
      "3d gaussian",
      "slam",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussNav: Gaussian Splatting for Visual Navigation",
    "authors": [
      "Xiaohan Lei",
      "Min Wang",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary challenge of IIN arises from the need to recognize the target object across varying viewpoints while ignoring potential distractors. Existing map-based navigation methods typically use Bird's Eye View (BEV) maps, which lack detailed texture representation of a scene. Consequently, while BEV maps are effective for semantic-level visual navigation, they are struggling for instance-level tasks. To this end, we propose a new framework for IIN, Gaussian Splatting for Visual Navigation (GaussNav), which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The GaussNav framework enables the agent to memorize both the geometry and semantic information of the scene, as well as retain the textural features of objects. By matching renderings of similar objects with the target, the agent can accurately identify, ground, and navigate to the specified object. Our GaussNav framework demonstrates a significant performance improvement, with Success weighted by Path Length (SPL) increasing from 0.347 to 0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset. The source code is publicly available at the link: https://github.com/XiaohanLei/GaussNav.",
    "arxiv_url": "http://arxiv.org/abs/2403.11625v3",
    "pdf_url": "http://arxiv.org/pdf/2403.11625v3",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/XiaohanLei/GaussNav",
    "keywords": [
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling",
    "authors": [
      "Yujiao Jiang",
      "Qingmin Liao",
      "Xiaoyu Li",
      "Li Ma",
      "Qi Zhang",
      "Chaopeng Zhang",
      "Zongqing Lu",
      "Ying Shan"
    ],
    "abstract": "Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.",
    "arxiv_url": "http://arxiv.org/abs/2403.11589v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11589v1",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "deformation",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration",
    "authors": [
      "Quentin Herau",
      "Moussab Bennehar",
      "Arthur Moreau",
      "Nathan Piasco",
      "Luis Roldao",
      "Dzmitry Tsishkou",
      "Cyrille Migniot",
      "Pascal Vasseur",
      "C√©dric Demonceaux"
    ],
    "abstract": "Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.",
    "arxiv_url": "http://arxiv.org/abs/2403.11577v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11577v2",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning",
    "authors": [
      "Teppei Suzuki"
    ],
    "abstract": "In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with federated learning. Existing city-scale reconstruction methods typically adopt a centralized approach, which gathers all data in a central server and reconstructs scenes. The approach hampers scalability because it places a heavy load on the server and demands extensive data storage when reconstructing scenes on a scale beyond city-scale. In pursuit of a more scalable 3D reconstruction, we propose a federated learning framework with 3DGS, which is a decentralized framework and can potentially use distributed computational resources across millions of clients. We tailor a distillation-based model update scheme for 3DGS and introduce appearance modeling for handling non-IID data in the scenario of 3D reconstruction with federated learning. We simulate our method on several large-scale benchmarks, and our method demonstrates rendered image quality comparable to centralized approaches. In addition, we also simulate our method with data collected in different seasons, demonstrating that our framework can reflect changes in the scenes and our appearance modeling captures changes due to seasonal variations.",
    "arxiv_url": "http://arxiv.org/abs/2403.11460v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11460v1",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Explicit Representation for Ultra-Realistic Head Avatars",
    "authors": [
      "Hongrui Cai",
      "Yuting Xiao",
      "Xuan Wang",
      "Jiafei Li",
      "Yudong Guo",
      "Yanbo Fan",
      "Shenghua Gao",
      "Juyong Zhang"
    ],
    "abstract": "We introduce a novel approach to creating ultra-realistic head avatars and rendering them in real-time (>30fps at $2048 \\times 1334$ resolution). First, we propose a hybrid explicit representation that combines the advantages of two primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized to capture sharp and rich textures on smooth surfaces, while 3D Gaussian Splatting is employed to represent complex geometric structures. In the pipeline of modeling an avatar, after tracking parametric models based on captured multi-view RGB videos, our goal is to simultaneously optimize the texture and opacity map of mesh, as well as a set of 3D Gaussian splats localized and rigged onto the mesh facets. Specifically, we perform $\\alpha$-blending on the color and opacity values based on the merged and re-ordered z-buffer from the rasterization results of mesh and 3DGS. This process involves the mesh and 3DGS adaptively fitting the captured visual information to outline a high-fidelity digital avatar. To avoid artifacts caused by Gaussian splats crossing the mesh facets, we design a stable hybrid depth sorting strategy. Experiments illustrate that our modeled results exceed those of state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2403.11453v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11453v2",
    "published_date": "2024-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "efficient rendering",
      "tracking",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction",
    "authors": [
      "Zhiyang Guo",
      "Wengang Zhou",
      "Li Li",
      "Min Wang",
      "Houqiang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction. However, existing methods focus mainly on extending static 3DGS into a time-variant representation, while overlooking the rich motion information carried by 2D observations, thus suffering from performance degradation and model redundancy. To address the above problem, we propose a novel motion-aware enhancement framework for dynamic scene reconstruction, which mines useful motion cues from optical flow to improve different paradigms of dynamic 3DGS. Specifically, we first establish a correspondence between 3D Gaussian movements and pixel-level flow. Then a novel flow augmentation method is introduced with additional insights into uncertainty and loss collaboration. Moreover, for the prevalent deformation-based paradigm that presents a harder optimization problem, a transient-aware deformation auxiliary module is proposed. We conduct extensive experiments on both multi-view and monocular scenes to verify the merits of our work. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2403.11447v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11447v1",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors",
    "authors": [
      "Tingyang Zhang",
      "Qingzhe Gao",
      "Weiyu Li",
      "Libin Liu",
      "Baoquan Chen"
    ],
    "abstract": "Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation. Recently, some studies have successfully constructed animatable 3D models from monocular videos. However, these approaches require sufficient view coverage of the object within the input video and typically necessitate significant time and computational costs for training and rendering. This limitation restricts the practical applications. In this work, we propose a method to build animatable 3D Gaussian Splatting from monocular video with diffusion priors. The 3D Gaussian representations significantly accelerate the training and rendering process, and the diffusion priors allow the method to learn 3D models with limited viewpoints. We also present the rigid regularization to enhance the utilization of the priors. We perform an extensive evaluation across various real-world videos, demonstrating its superior performance compared to the current state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2403.11427v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11427v1",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF",
    "authors": [
      "Guangyi Liu",
      "Wen Jiang",
      "Boshu Lei",
      "Vivek Pandey",
      "Kostas Daniilidis",
      "Nader Motee"
    ],
    "abstract": "The active view acquisition problem has been extensively studied in the context of robot navigation using NeRF and 3D Gaussian Splatting. To enhance scene reconstruction efficiency and ensure robot safety, we propose the Risk-aware Environment Masking (RaEM) framework. RaEM leverages coherent risk measures to dynamically prioritize safety-critical regions of the unknown environment, guiding active view acquisition algorithms toward identifying the next-best-view (NBV). Integrated with FisherRF, which selects the NBV by maximizing expected information gain, our framework achieves a dual objective: improving robot safety and increasing efficiency in risk-aware 3D scene reconstruction and understanding. Extensive high-fidelity experiments validate the effectiveness of our approach, demonstrating its ability to establish a robust and safety-focused framework for active robot exploration and 3D scene understanding.",
    "arxiv_url": "http://arxiv.org/abs/2403.11396v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11396v2",
    "published_date": "2024-03-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "understanding",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization",
    "authors": [
      "Peng Jiang",
      "Gaurav Pandey",
      "Srikanth Saripalli"
    ],
    "abstract": "This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment. By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate. To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree. This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC). Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique. The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset.",
    "arxiv_url": "http://arxiv.org/abs/2403.11367v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11367v1",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "mapping",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Seamless 3D Maps Using Radiance Fields",
    "authors": [
      "Sai Tarun Sathyan",
      "Thomas B. Kinsman"
    ],
    "abstract": "It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning. The traditional methods of creating 3D maps, (such as photogrammetry), require a large number of images and odometry. Additionally, traditional methods have difficulty with reflective surfaces and specular reflections; windows and chrome in the scene can be problematic. Google Road View is a familiar application, which uses traditional methods to fuse a collection of 2D input images into the illusion of a 3D map. However, Google Road View does not create an actual 3D object model, only a collection of views. The objective of this work is to create an actual 3D object model using updated techniques. Neural Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the capability to produce more precise and intricate 3D maps. Gaussian Splatting[4] is another contemporary technique. This investigation compares Neural Radiance Fields to Gaussian Splatting, and describes some of their inner workings. Our primary contribution is a method for improving the results of the 3D reconstructed models. Our results indicate that Gaussian Splatting was superior to the NeRF technique.",
    "arxiv_url": "http://arxiv.org/abs/2403.11364v1",
    "pdf_url": "http://arxiv.org/pdf/2403.11364v1",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "face",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering",
    "authors": [
      "Yanyan Li",
      "Chenyu Lyu",
      "Yan Di",
      "Guangyao Zhai",
      "Gim Hee Lee",
      "Federico Tombari"
    ],
    "abstract": "During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces. This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data. To mitigate this issue, we propose a novel approach called GeoGaussian. Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy. Finally, the pipeline ensures that the scene's geometry and texture are maintained through constrained optimization processes with explicit geometry constraints. Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions. Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets.",
    "arxiv_url": "http://arxiv.org/abs/2403.11324v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11324v2",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis",
    "authors": [
      "Lutao Jiang",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Jiazhou Zhou",
      "Lin Wang"
    ],
    "abstract": "Text-to-3D synthesis has recently seen intriguing advances by combining the text-to-image priors with 3D representation methods, e.g., 3D Gaussian Splatting (3D GS), via Score Distillation Sampling (SDS). However, a hurdle of existing methods is the low efficiency, per-prompt optimization for a single 3D object. Therefore, it is imperative for a paradigm shift from per-prompt optimization to feed-forward generation for any unseen text prompts, which yet remains challenging. An obstacle is how to directly generate a set of millions of 3D Gaussians to represent a 3D object. This paper presents BrightDreamer, an end-to-end feed-forward approach that can achieve generalizable and fast (77 ms) text-to-3D generation. Our key idea is to formulate the generation process as estimating the 3D deformation from an anchor shape with predefined positions. For this, we first propose a Text-guided Shape Deformation (TSD) network to predict the deformed shape and its new positions, used as the centers (one attribute) of 3D Gaussians. To estimate the other four attributes (i.e., scaling, rotation, opacity, and SH), we then design a novel Text-guided Triplane Generator (TTG) to generate a triplane representation for a 3D object. The center of each Gaussian enables us to transform the spatial feature into the four attributes. The generated 3D Gaussians can be finally rendered at 705 frames per second. Extensive experiments demonstrate the superiority of our method over existing methods. Also, BrightDreamer possesses a strong semantic understanding capability even for complex text prompts. The code is available in the project page.",
    "arxiv_url": "http://arxiv.org/abs/2403.11273v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11273v2",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "deformation",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Gaussian Splatting For Dense Visual SLAM",
    "authors": [
      "Tianchen Deng",
      "Yaohui Chen",
      "Leyan Zhang",
      "Jianfei Yang",
      "Shenghai Yuan",
      "Jiuming Liu",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes. However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed. To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids. A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids. Then we observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian geometric attributes, i.e., the parameters. Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss. Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.",
    "arxiv_url": "http://arxiv.org/abs/2403.11247v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11247v2",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recent Advances in 3D Gaussian Splatting",
    "authors": [
      "Tong Wu",
      "Yu-Jie Yuan",
      "Ling-Xiao Zhang",
      "Jie Yang",
      "Yan-Pei Cao",
      "Ling-Qi Yan",
      "Lin Gao"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis. Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique. This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation.",
    "arxiv_url": "http://arxiv.org/abs/2403.11134v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11134v2",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "survey",
      "fast",
      "understanding",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration",
    "authors": [
      "Zhihao Liang",
      "Qi Zhang",
      "Wenbo Hu",
      "Ying Feng",
      "Lei Zhu",
      "Kui Jia"
    ],
    "abstract": "The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering. However, 3DGS is not alias-free, and its rendering at varying resolutions could produce severe blurring or jaggies. This is because 3DGS treats each pixel as an isolated, single point rather than as an area, causing insensitivity to changes in the footprints of pixels. Consequently, this discrete sampling scheme inevitably results in aliasing, owing to the restricted sampling bandwidth. In this paper, we derive an analytical solution to address this issue. More specifically, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral by subtracting the CDFs. We then introduce this approximation in the two-dimensional pixel shading, and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel. Moreover, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions. Experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2403.11056v2",
    "pdf_url": "http://arxiv.org/pdf/2403.11056v2",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sim2Real within 5 Minutes: Efficient Domain Transfer with Stylized Gaussian Splatting for Endoscopic Images",
    "authors": [
      "Junyang Wu",
      "Yun Gu",
      "Guang-Zhong Yang"
    ],
    "abstract": "Robot assisted endoluminal intervention is an emerging technique for both benign and malignant luminal lesions. With vision-based navigation, when combined with pre-operative imaging data as priors, it is possible to recover position and pose of the endoscope without the need of additional sensors. In practice, however, aligning pre-operative and intra-operative domains is complicated by significant texture differences. Although methods such as style transfer can be used to address this issue, they require large datasets from both source and target domains with prolonged training times. This paper proposes an efficient domain transfer method based on stylized Gaussian splatting, only requiring a few of real images (10 images) with very fast training time. Specifically, the transfer process includes two phases. In the first phase, the 3D models reconstructed from CT scans are represented as differential Gaussian point clouds. In the second phase, only color appearance related parameters are optimized to transfer the style and preserve the visual content. A novel structure consistency loss is applied to latent features and depth levels to enhance the stability of the transferred images. Detailed validation was performed to demonstrate the performance advantages of the proposed method compared to that of the current state-of-the-art, highlighting the potential for intra-operative surgical navigation.",
    "arxiv_url": "http://arxiv.org/abs/2403.10860v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10860v2",
    "published_date": "2024-03-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark",
    "authors": [
      "Tianyi Zhang",
      "Kaining Huang",
      "Weiming Zhi",
      "Matthew Johnson-Roberson"
    ],
    "abstract": "Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments.",
    "arxiv_url": "http://arxiv.org/abs/2403.10814v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10814v2",
    "published_date": "2024-03-16",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "relighting",
      "lighting",
      "3d gaussian",
      "human",
      "ar",
      "illumination"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with 3D Gaussian Splatting",
    "authors": [
      "Dingding Cai",
      "Janne Heikkil√§",
      "Esa Rahtu"
    ],
    "abstract": "This paper introduces GS-Pose, a unified framework for localizing and estimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB images of a previously unseen object and builds three distinct representations stored in a database. At inference, GS-Pose operates sequentially by locating the object in the input image, estimating its initial 6D pose using a retrieval approach, and refining the pose with a render-and-compare method. The key insight is the application of the appropriate object representation at each stage of the process. In particular, for the refinement step, we leverage 3D Gaussian splatting, a novel differentiable rendering technique that offers high rendering speed and relatively low optimization time. Off-the-shelf toolchains and commodity hardware, such as mobile phones, can be used to capture new objects to be added to the database. Extensive evaluations on the LINEMOD and OnePose-LowTexture datasets demonstrate excellent performance, establishing the new state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose.",
    "arxiv_url": "http://arxiv.org/abs/2403.10683v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10683v2",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians",
    "authors": [
      "Hiba Dahmani",
      "Moussab Bennehar",
      "Nathan Piasco",
      "Luis Roldao",
      "Dzmitry Tsishkou"
    ],
    "abstract": "Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2403.10427v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10427v2",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoGS3D: Single-view 3D Reconstruction via Geometric-aware Diffusion Model and Gaussian Splatting",
    "authors": [
      "Qijun Feng",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "We introduce GeoGS3D, a novel two-stage framework for reconstructing detailed 3D objects from single-view images. Inspired by the success of pre-trained 2D diffusion models, our method incorporates an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, facilitating the generation of multi-view consistent images. During the following Gaussian Splatting, these images are fused with epipolar attention, fully utilizing the geometric correlations across views. Moreover, we propose a novel metric, Gaussian Divergence Significance (GDS), to prune unnecessary operations during optimization, significantly accelerating the reconstruction process. Extensive experiments demonstrate that GeoGS3D generates images with high consistency across views and reconstructs high-quality 3D objects, both qualitatively and quantitatively.",
    "arxiv_url": "http://arxiv.org/abs/2403.10242v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10242v2",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GGRt: Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time",
    "authors": [
      "Hao Li",
      "Yuanyuan Gao",
      "Chenming Wu",
      "Dingwen Zhang",
      "Yalun Dai",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Junwei Han"
    ],
    "abstract": "This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\\ge$ 5 FPS and real-time rendering at $\\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences.",
    "arxiv_url": "http://arxiv.org/abs/2403.10147v2",
    "pdf_url": "http://arxiv.org/pdf/2403.10147v2",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing",
    "authors": [
      "Tian-Xing Xu",
      "Wenbo Hu",
      "Yu-Kun Lai",
      "Ying Shan",
      "Song-Hai Zhang"
    ],
    "abstract": "3D Gaussian splatting, emerging as a groundbreaking approach, has drawn increasing attention for its capabilities of high-fidelity reconstruction and real-time rendering. However, it couples the appearance and geometry of the scene within the Gaussian attributes, which hinders the flexibility of editing operations, such as texture swapping. To address this issue, we propose a novel approach, namely Texture-GS, to disentangle the appearance from the geometry by representing it as a 2D texture mapped onto the 3D surface, thereby facilitating appearance editing. Technically, the disentanglement is achieved by our proposed texture mapping module, which consists of a UV mapping MLP to learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian intersections, and a learnable texture to capture the fine-grained appearance. Extensive experiments on the DTU dataset demonstrate that our method not only facilitates high-fidelity appearance editing but also achieves real-time rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.",
    "arxiv_url": "http://arxiv.org/abs/2403.10050v1",
    "pdf_url": "http://arxiv.org/pdf/2403.10050v1",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "face",
      "mapping",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting",
    "authors": [
      "Zhiqi Li",
      "Yiming Chen",
      "Lingzhe Zhao",
      "Peidong Liu"
    ],
    "abstract": "While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Project page: https://lizhiqi49.github.io/MVControl/.",
    "arxiv_url": "http://arxiv.org/abs/2403.09981v3",
    "pdf_url": "http://arxiv.org/pdf/2403.09981v3",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience",
    "authors": [
      "Xiaohang Yu",
      "Zhengxian Yang",
      "Shi Pan",
      "Yuqi Han",
      "Haoxiang Wang",
      "Jun Zhang",
      "Shi Yan",
      "Borong Lin",
      "Lei Yang",
      "Tao Yu",
      "Lu Fang"
    ],
    "abstract": "We have built a custom mobile multi-camera large-space dense light field capture system, which provides a series of high-quality and sufficiently dense light field images for various scenarios. Our aim is to contribute to the development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF, and 3D Gaussian splitting. More importantly, the collected dataset, which is much denser than existing datasets, may also inspire space-oriented light field reconstruction, which is potentially different from object-centric 3D reconstruction, for immersive VR/AR experiences. We utilized a total of 40 GoPro 10 cameras, capturing images of 5k resolution. The number of photos captured for each scene is no less than 1000, and the average density (view number within a unit sphere) is 134.68. It is also worth noting that our system is capable of efficiently capturing large outdoor scenes. Addressing the current lack of large-space and dense light field datasets, we made efforts to include elements such as sky, reflections, lights and shadows that are of interest to researchers in the field of 3D reconstruction during the data capture process. Finally, we validated the effectiveness of our provided dataset on three popular algorithms and also integrated the reconstructed 3DGS results into the Unity engine, demonstrating the potential of utilizing our datasets to enhance the realism of virtual reality (VR) and create feasible interactive spaces. The dataset is available at our project website.",
    "arxiv_url": "http://arxiv.org/abs/2403.09973v1",
    "pdf_url": "http://arxiv.org/pdf/2403.09973v1",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "reflection",
      "outdoor",
      "shadow",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting",
    "authors": [
      "Aiden Swann",
      "Matthew Strong",
      "Won Kyung Do",
      "Gadiel Sznaier Camps",
      "Mac Schwager",
      "Monroe Kennedy III"
    ],
    "abstract": "In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective and transparent objects. Please see our project page at http://armlabstanford.github.io/touch-gs",
    "arxiv_url": "http://arxiv.org/abs/2403.09875v3",
    "pdf_url": "http://arxiv.org/pdf/2403.09875v3",
    "published_date": "2024-03-14",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "face",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping",
    "authors": [
      "Yuhang Zheng",
      "Xiangyu Chen",
      "Yupeng Zheng",
      "Songen Gu",
      "Runyi Yang",
      "Bu Jin",
      "Pengfei Li",
      "Chengliang Zhong",
      "Zengmao Wang",
      "Lina Liu",
      "Chao Yang",
      "Dawei Wang",
      "Zhen Chen",
      "Xiaoxiao Long",
      "Meiqing Wang"
    ],
    "abstract": "Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.",
    "arxiv_url": "http://arxiv.org/abs/2403.09637v1",
    "pdf_url": "http://arxiv.org/pdf/2403.09637v1",
    "published_date": "2024-03-14",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "https://github.com/MrSecant/GaussianGrasper",
    "keywords": [
      "robotics",
      "efficient",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians",
    "authors": [
      "Licheng Zhong",
      "Hong-Xing Yu",
      "Jiajun Wu",
      "Yunzhu Li"
    ],
    "abstract": "Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, model 3D appearance and geometry, but lack the ability to estimate physical properties for objects and simulate them. The core challenge lies in integrating an expressive yet efficient physical dynamics model. We propose Spring-Gaus, a 3D physical object representation for reconstructing and simulating elastic objects from videos of the object from multiple viewpoints. In particular, we develop and integrate a 3D Spring-Mass model into 3D Gaussian kernels, enabling the reconstruction of the visual appearance, shape, and physical dynamics of the object. Our approach enables future prediction and simulation under various initial states and environmental properties. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. Project page: https://zlicheng.com/spring_gaus/.",
    "arxiv_url": "http://arxiv.org/abs/2403.09434v3",
    "pdf_url": "http://arxiv.org/pdf/2403.09434v3",
    "published_date": "2024-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting",
    "authors": [
      "Jaewoo Jung",
      "Jisang Han",
      "Honggyu An",
      "Jiwon Kang",
      "Seonghoon Park",
      "Seungryong Kim"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When the quality of the initial point cloud deteriorates, such as in the presence of noise or when using randomly initialized point cloud, 3DGS often undergoes large performance drops. To address this limitation, we propose a novel optimization strategy dubbed RAIN-GS (Relaing Accurate Initialization Constraint for 3D Gaussian Splatting). Our approach is based on an in-depth analysis of the original 3DGS optimization scheme and the analysis of the SfM initialization in the frequency domain. Leveraging simple modifications based on our analyses, RAIN-GS successfully trains 3D Gaussians from sub-optimal point cloud (e.g., randomly initialized point cloud), effectively relaxing the need for accurate initialization. We demonstrate the efficacy of our strategy through quantitative and qualitative comparisons on multiple datasets, where RAIN-GS trained with random point cloud achieves performance on-par with or even better than 3DGS trained with accurate SfM point cloud. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS.",
    "arxiv_url": "http://arxiv.org/abs/2403.09413v2",
    "pdf_url": "http://arxiv.org/pdf/2403.09413v2",
    "published_date": "2024-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph",
    "authors": [
      "Donglin Di",
      "Jiahui Yang",
      "Chaofan Luo",
      "Zhou Xue",
      "Wei Chen",
      "Xun Yang",
      "Yue Gao"
    ],
    "abstract": "Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: https://github.com/yjhboy/Hyper3DG)",
    "arxiv_url": "http://arxiv.org/abs/2403.09236v2",
    "pdf_url": "http://arxiv.org/pdf/2403.09236v2",
    "published_date": "2024-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/yjhboy/Hyper3DG",
    "keywords": [
      "geometry",
      "3d gaussian",
      "head",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A New Split Algorithm for 3D Gaussian Splatting",
    "authors": [
      "Qiyuan Feng",
      "Gengchen Cao",
      "Haoxiang Chen",
      "Tai-Jiang Mu",
      "Ralph R. Martin",
      "Shi-Min Hu"
    ],
    "abstract": "3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.",
    "arxiv_url": "http://arxiv.org/abs/2403.09143v1",
    "pdf_url": "http://arxiv.org/pdf/2403.09143v1",
    "published_date": "2024-03-14",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing",
    "authors": [
      "Jing Wu",
      "Jia-Wang Bian",
      "Xinghui Li",
      "Guangrun Wang",
      "Ian Reid",
      "Philip Torr",
      "Victor Adrian Prisacariu"
    ],
    "abstract": "We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2403.08733v4",
    "pdf_url": "http://arxiv.org/pdf/2403.08733v4",
    "published_date": "2024-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting",
    "authors": [
      "Xinjie Zhang",
      "Xingtong Ge",
      "Tongda Xu",
      "Dailan He",
      "Yan Wang",
      "Hongwei Qin",
      "Guo Lu",
      "Jing Geng",
      "Jun Zhang"
    ],
    "abstract": "Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 2000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding. Code is available at https://github.com/Xinjie-Q/GaussianImage.",
    "arxiv_url": "http://arxiv.org/abs/2403.08551v5",
    "pdf_url": "http://arxiv.org/pdf/2403.08551v5",
    "published_date": "2024-03-13",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/Xinjie-Q/GaussianImage",
    "keywords": [
      "compression",
      "fast",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting in Style",
    "authors": [
      "Abhishek Saroha",
      "Mariia Gladkova",
      "Cecilia Curreli",
      "Dominik Muhle",
      "Tarun Yenamandra",
      "Daniel Cremers"
    ],
    "abstract": "3D scene stylization extends the work of neural style transfer to 3D. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across multiple views. A vast majority of the previous works achieve this by training a 3D model for every stylized image and a set of multi-view images. In contrast, we propose a novel architecture trained on a collection of style images that, at test time, produces real time high-quality stylized novel views. We choose the underlying 3D scene representation for our model as 3D Gaussian splatting. We take the 3D Gaussians and process them using a multi-resolution hash grid and a tiny MLP to obtain stylized views. The MLP is conditioned on different style codes for generalization to different styles during test time. The explicit nature of 3D Gaussians gives us inherent advantages over NeRF-based methods, including geometric consistency and a fast training and rendering regime. This enables our method to be useful for various practical use cases, such as augmented or virtual reality. We demonstrate that our method achieves state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.",
    "arxiv_url": "http://arxiv.org/abs/2403.08498v2",
    "pdf_url": "http://arxiv.org/pdf/2403.08498v2",
    "published_date": "2024-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
    "authors": [
      "Guanxing Lu",
      "Shiyi Zhang",
      "Ziwei Wang",
      "Changliu Liu",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\\% in average success rate. Project page: https://guanxinglu.github.io/ManiGaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2403.08321v2",
    "pdf_url": "http://arxiv.org/pdf/2403.08321v2",
    "published_date": "2024-03-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting",
    "authors": [
      "Kunhao Liu",
      "Fangneng Zhan",
      "Muyu Xu",
      "Christian Theobalt",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image's style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: https://kunhao-liu.github.io/StyleGaussian/",
    "arxiv_url": "http://arxiv.org/abs/2403.07807v1",
    "pdf_url": "http://arxiv.org/pdf/2403.07807v1",
    "published_date": "2024-03-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM",
    "authors": [
      "Siting Zhu",
      "Renjie Qin",
      "Guangming Wang",
      "Jiuming Liu",
      "Hesheng Wang"
    ],
    "abstract": "We propose SemGauss-SLAM, a dense semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering simultaneously. In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation. Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization. In addition, to reduce cumulative drift in tracking and improve semantic reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging multi-frame semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to low-drift tracking and accurate mapping. Our SemGauss-SLAM method demonstrates superior performance over existing radiance field-based SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in high-precision semantic segmentation and dense semantic mapping.",
    "arxiv_url": "http://arxiv.org/abs/2403.07494v3",
    "pdf_url": "http://arxiv.org/pdf/2403.07494v3",
    "published_date": "2024-03-12",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "semantic",
      "mapping",
      "3d gaussian",
      "gaussian splatting",
      "slam",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Xin Ning",
      "Jun Zhou",
      "Lin Gu"
    ],
    "abstract": "Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2403.06912v3",
    "pdf_url": "http://arxiv.org/pdf/2403.06912v3",
    "published_date": "2024-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "few-shot",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization",
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Muyu Xu",
      "Shijian Lu",
      "Eric Xing"
    ],
    "abstract": "3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.",
    "arxiv_url": "http://arxiv.org/abs/2403.06908v2",
    "pdf_url": "http://arxiv.org/pdf/2403.06908v2",
    "published_date": "2024-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "V3D: Video Diffusion Models are Effective 3D Generators",
    "authors": [
      "Zilong Chen",
      "Yikai Wang",
      "Feng Wang",
      "Zhengyi Wang",
      "Huaping Liu"
    ],
    "abstract": "Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at https://github.com/heheyas/V3D",
    "arxiv_url": "http://arxiv.org/abs/2403.06738v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06738v1",
    "published_date": "2024-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/heheyas/V3D",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting",
    "authors": [
      "Francesco Palandra",
      "Andrea Sanchietti",
      "Daniele Baieri",
      "Emanuele Rodol√†"
    ],
    "abstract": "We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model. The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.",
    "arxiv_url": "http://arxiv.org/abs/2403.05154v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05154v2",
    "published_date": "2024-03-08",
    "categories": [
      "cs.CV",
      "cs.GR",
      "68T45",
      "I.2.10; I.3.8"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting",
    "authors": [
      "Zhijing Shao",
      "Zhaolong Wang",
      "Zhuang Li",
      "Duotun Wang",
      "Xiangru Lin",
      "Yu Zhang",
      "Mingming Fan",
      "Zeyu Wang"
    ],
    "abstract": "We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.",
    "arxiv_url": "http://arxiv.org/abs/2403.05087v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05087v1",
    "published_date": "2024-03-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "face",
      "deformation",
      "body",
      "geometry",
      "human",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling",
    "authors": [
      "Cheng Peng",
      "Yutao Tang",
      "Yifan Zhou",
      "Nengyu Wang",
      "Xijun Liu",
      "Deming Li",
      "Rama Chellappa"
    ],
    "abstract": "Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated benchmarks; however, images captured in real life are often blurry. In this work, we analyze the robustness of Gaussian-Splatting-based methods against various image blur, such as motion blur, defocus blur, downscaling blur, \\etc. Under these degradations, Gaussian-Splatting-based methods tend to overfit and produce worse results than Neural-Radiance-Field-based methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling capacities such that a 3D-consistent and high quality scene can be reconstructed despite image-wise blur. Specifically, we model blur by estimating per-pixel convolution kernels from a Blur Proposal Network (BPN). BPN is designed to consider spatial, color, and depth variations of the scene to maximize modeling capacity. Additionally, BPN also proposes a quality-assessing mask, which indicates regions where blur occur. Finally, we introduce a coarse-to-fine kernel optimization scheme; this optimization scheme is fast and avoids sub-optimal solutions due to a sparse point cloud initialization, which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate that BAGS achieves photorealistic renderings under various challenging blur conditions and imaging geometry, while significantly improving upon existing approaches.",
    "arxiv_url": "http://arxiv.org/abs/2403.04926v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04926v2",
    "published_date": "2024-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "fast",
      "high quality",
      "3d gaussian",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis",
    "authors": [
      "Yuanhao Cai",
      "Yixun Liang",
      "Jiahao Wang",
      "Angtian Wang",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zongwei Zhou",
      "Alan Yuille"
    ],
    "abstract": "X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73x inference speed. The application on sparse-view CT reconstruction also reveals the practical values of our method. Code is publicly available at https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .",
    "arxiv_url": "http://arxiv.org/abs/2403.04116v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04116v3",
    "published_date": "2024-03-07",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "https://github.com/caiyuanhao1998/X-Gaussian",
    "keywords": [
      "sparse-view",
      "efficient",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Online Photon Guiding with 3D Gaussians for Caustics Rendering",
    "authors": [
      "Jiawei Huang",
      "Hajime Tanaka",
      "Taku Komura",
      "Yoshifumi Kitamura"
    ],
    "abstract": "In production rendering systems, caustics are typically rendered via photon mapping and gathering, a process often hindered by insufficient photon density. In this paper, we propose a novel photon guiding method to improve the photon density and overall quality for caustic rendering. The key insight of our approach is the application of a global 3D Gaussian mixture model, used in conjunction with an adaptive light sampler. This combination effectively guides photon emission in expansive 3D scenes with multiple light sources. By employing a global 3D Gaussian mixture, our method precisely models the distribution of the points of interest. To sample emission directions from the distribution at any observation point, we introduce a novel directional transform of the 3D Gaussian, which ensures accurate photon emission guiding. Furthermore, our method integrates a global light cluster tree, which models the contribution distribution of light sources to the image, facilitating effective light source selection. We conduct experiments demonstrating that our approach robustly outperforms existing photon guiding techniques across a variety of scenarios, significantly advancing the quality of caustic rendering.",
    "arxiv_url": "http://arxiv.org/abs/2403.03641v2",
    "pdf_url": "http://arxiv.org/pdf/2403.03641v2",
    "published_date": "2024-03-06",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps",
    "authors": [
      "Timothy Chen",
      "Ola Shorinwa",
      "Joseph Bruno",
      "Aiden Swann",
      "Javier Yu",
      "Weijia Zeng",
      "Keiko Nagami",
      "Philip Dames",
      "Mac Schwager"
    ],
    "abstract": "We present Splat-Nav, a real-time robot navigation pipeline for Gaussian Splatting (GSplat) scenes, a powerful new 3D scene representation. Splat-Nav consists of two components: 1) Splat-Plan, a safe planning module, and 2) Splat-Loc, a robust vision-based pose estimation module. Splat-Plan builds a safe-by-construction polytope corridor through the map based on mathematically rigorous collision constraints and then constructs a B\\'ezier curve trajectory through this corridor. Splat-Loc provides real-time recursive state estimates given only an RGB feed from an on-board camera, leveraging the point-cloud representation inherent in GSplat scenes. Working together, these modules give robots the ability to recursively re-plan smooth and safe trajectories to goal locations. Goals can be specified with position coordinates, or with language commands by using a semantic GSplat. We demonstrate improved safety compared to point cloud-based methods in extensive simulation experiments. In a total of 126 hardware flights, we demonstrate equivalent safety and speed compared to motion capture and visual odometry, but without a manual frame alignment required by those methods. We show online re-planning at more than 2 Hz and pose estimation at about 25 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation. We provide experiment videos on our project page at https://chengine.github.io/splatnav/. Our codebase and ROS nodes can be found at https://github.com/chengine/splatnav.",
    "arxiv_url": "http://arxiv.org/abs/2403.02751v3",
    "pdf_url": "http://arxiv.org/pdf/2403.02751v3",
    "published_date": "2024-03-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "https://github.com/chengine/splatnav",
    "keywords": [
      "motion",
      "fast",
      "semantic",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos",
    "authors": [
      "Jiakai Sun",
      "Han Jiao",
      "Guangyuan Li",
      "Zhanjie Zhang",
      "Lei Zhao",
      "Wei Xing"
    ],
    "abstract": "Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.",
    "arxiv_url": "http://arxiv.org/abs/2403.01444v4",
    "pdf_url": "http://arxiv.org/pdf/2403.01444v4",
    "published_date": "2024-03-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "neural rendering",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Model for Animation and Texturing",
    "authors": [
      "Xiangzhi Eric Wang",
      "Zackary P. T. Sin"
    ],
    "abstract": "3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance. Despite this achievement, however, it is not readily applicable to developing interactive applications. Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model. We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer. By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications. It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering. Our work, on the other hand, only requires an approximated geometry for rendering an object in high fidelity. Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians. We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting.",
    "arxiv_url": "http://arxiv.org/abs/2402.19441v1",
    "pdf_url": "http://arxiv.org/pdf/2402.19441v1",
    "published_date": "2024-02-29",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "animation",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction",
    "authors": [
      "Jiaqi Lin",
      "Zhihao Li",
      "Xiao Tang",
      "Jianzhuang Liu",
      "Shiyong Liu",
      "Jiayue Liu",
      "Yangdi Lu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Youliang Yan",
      "Wenming Yang"
    ],
    "abstract": "Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.",
    "arxiv_url": "http://arxiv.org/abs/2402.17427v1",
    "pdf_url": "http://arxiv.org/pdf/2402.17427v1",
    "published_date": "2024-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "large scene",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos",
    "authors": [
      "Xinqi Liu",
      "Chenming Wu",
      "Jialun Liu",
      "Xing Liu",
      "Jinbo Wu",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.",
    "arxiv_url": "http://arxiv.org/abs/2402.16607v2",
    "pdf_url": "http://arxiv.org/pdf/2402.16607v2",
    "published_date": "2024-02-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting",
    "authors": [
      "Ziyi Yang",
      "Xinyu Gao",
      "Yangtian Sun",
      "Yihua Huang",
      "Xiaoyang Lyu",
      "Wen Zhou",
      "Shaohui Jiao",
      "Xiaojuan Qi",
      "Xiaogang Jin"
    ],
    "abstract": "The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. Project page is https://ingra14m.github.io/Spec-Gaussian-website/.",
    "arxiv_url": "http://arxiv.org/abs/2402.15870v2",
    "pdf_url": "http://arxiv.org/pdf/2402.15870v2",
    "published_date": "2024-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation",
    "authors": [
      "Kai Cheng",
      "Xiaoxiao Long",
      "Kaizhi Yang",
      "Yao Yao",
      "Wei Yin",
      "Yuexin Ma",
      "Wenping Wang",
      "Xuejin Chen"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.",
    "arxiv_url": "http://arxiv.org/abs/2402.14650v1",
    "pdf_url": "http://arxiv.org/pdf/2402.14650v1",
    "published_date": "2024-02-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
    "authors": [
      "Joongho Jo",
      "Hyeongwon Kim",
      "Jongsun Park"
    ],
    "abstract": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
    "arxiv_url": "http://arxiv.org/abs/2402.13827v2",
    "pdf_url": "http://arxiv.org/pdf/2402.13827v2",
    "published_date": "2024-02-21",
    "categories": [
      "cs.CV",
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey",
    "authors": [
      "Fabio Tosi",
      "Youmin Zhang",
      "Ziren Gong",
      "Erik Sandstr√∂m",
      "Stefano Mattoccia",
      "Martin R. Oswald",
      "Matteo Poggi"
    ],
    "abstract": "Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.",
    "arxiv_url": "http://arxiv.org/abs/2402.13255v3",
    "pdf_url": "http://arxiv.org/pdf/2402.13255v3",
    "published_date": "2024-02-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "survey",
      "lighting",
      "mapping",
      "body",
      "3d gaussian",
      "slam",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians",
    "authors": [
      "Haimin Luo",
      "Min Ouyang",
      "Zijun Zhao",
      "Suyi Jiang",
      "Longwen Zhang",
      "Qixuan Zhang",
      "Wei Yang",
      "Lan Xu",
      "Jingyi Yu"
    ],
    "abstract": "Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the \"GaussianHair Scattering Model\", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.",
    "arxiv_url": "http://arxiv.org/abs/2402.10483v1",
    "pdf_url": "http://arxiv.org/pdf/2402.10483v1",
    "published_date": "2024-02-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "efficient",
      "high-fidelity",
      "relighting",
      "lighting",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "illumination",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting",
    "authors": [
      "Chen Yang",
      "Sikuang Li",
      "Jiemin Fang",
      "Ruofan Liang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination, which explicitly inject structure priors into the initial optimization process to help build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. We further design a COLMAP-free variant, where pre-given accurate camera poses are not required, which achieves competitive quality and facilitates wider applications. GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed images, achieving superior performance from only four views and significantly outperforming previous SOTA methods. Our demo is available at https://gaussianobject.github.io/, and the code has been released at https://github.com/GaussianObject/GaussianObject.",
    "arxiv_url": "http://arxiv.org/abs/2402.10259v4",
    "pdf_url": "http://arxiv.org/pdf/2402.10259v4",
    "published_date": "2024-02-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/GaussianObject/GaussianObject",
    "keywords": [
      "illumination",
      "sparse view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering",
    "authors": [
      "Abdullah Hamdi",
      "Luke Melas-Kyriazi",
      "Jinjie Mai",
      "Guocheng Qian",
      "Ruoshi Liu",
      "Carl Vondrick",
      "Bernard Ghanem",
      "Andrea Vedaldi"
    ],
    "abstract": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .",
    "arxiv_url": "http://arxiv.org/abs/2402.10128v2",
    "pdf_url": "http://arxiv.org/pdf/2402.10128v2",
    "published_date": "2024-02-15",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Magic-Me: Identity-Specific Video Customized Diffusion",
    "authors": [
      "Ze Ma",
      "Daquan Zhou",
      "Chun-Hsiao Yeh",
      "Xue-She Wang",
      "Xiuyu Li",
      "Huanrui Yang",
      "Zhen Dong",
      "Kurt Keutzer",
      "Jiashi Feng"
    ],
    "abstract": "Creating content with specified identities (ID) has attracted significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven creation has achieved great progress with the identity controlled via reference images. However, its extension to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified identity defined by a few images, VCD reinforces the identity characteristics and injects frame-wise correlation at the initialization stage for stable video outputs. To achieve this, we propose three novel components that are essential for high-quality identity preservation and stable video generation: 1) a noise initialization method with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID module based on extended Textual Inversion trained with the cropped identity to disentangle the ID information from the background 3) Face VCD and Tiled VCD modules to reinforce faces and upscale the video to higher resolution while preserving the identity's features. We conducted extensive experiments to verify that VCD is able to generate stable videos with better ID over the baselines. Besides, with the transferability of the encoded identity in the ID module, VCD is also working well with personalized text-to-image models available publicly. The codes are available at https://github.com/Zhen-Dong/Magic-Me.",
    "arxiv_url": "http://arxiv.org/abs/2402.09368v2",
    "pdf_url": "http://arxiv.org/pdf/2402.09368v2",
    "published_date": "2024-02-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/Zhen-Dong/Magic-Me",
    "keywords": [
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
    "authors": [
      "Luke Melas-Kyriazi",
      "Iro Laina",
      "Christian Rupprecht",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Oran Gafni",
      "Filippos Kokkinos"
    ],
    "abstract": "Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.",
    "arxiv_url": "http://arxiv.org/abs/2402.08682v1",
    "pdf_url": "http://arxiv.org/pdf/2402.08682v1",
    "published_date": "2024-02-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
    "authors": [
      "Xiaoyu Zhou",
      "Xingjian Ran",
      "Yajiao Xiong",
      "Jinlin He",
      "Zhiwei Lin",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an instance-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. The source codes and models will be available at gala3d.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2402.07207v2",
    "pdf_url": "http://arxiv.org/pdf/2402.07207v2",
    "published_date": "2024-02-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian as a New Era: A Survey",
    "authors": [
      "Ben Fei",
      "Jingyi Xu",
      "Rui Zhang",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ying He"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section.",
    "arxiv_url": "http://arxiv.org/abs/2402.07181v2",
    "pdf_url": "http://arxiv.org/pdf/2402.07181v2",
    "published_date": "2024-02-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "survey",
      "mapping",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deepfake for the Good: Generating Avatars through Face-Swapping with Implicit Deepfake Generation",
    "authors": [
      "Georgii Stanishevskii",
      "Jakub Steczkiewicz",
      "Tomasz Szczepanik",
      "S≈Çawomir Tadeja",
      "Jacek Tabor",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object's shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object's characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Deepfakes refers to artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such simple strategies can produce plausible 3D deepfake-based avatars.",
    "arxiv_url": "http://arxiv.org/abs/2402.06390v2",
    "pdf_url": "http://arxiv.org/pdf/2402.06390v2",
    "published_date": "2024-02-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data",
    "authors": [
      "Haoyuan Li",
      "Yanpeng Zhou",
      "Yihan Zeng",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "abstract": "3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object's surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results.",
    "arxiv_url": "http://arxiv.org/abs/2402.06198v2",
    "pdf_url": "http://arxiv.org/pdf/2402.06198v2",
    "published_date": "2024-02-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting",
    "authors": [
      "Zhenglin Zhou",
      "Fan Ma",
      "Hehe Fan",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \\textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: https://github.com/ZhenglinZhou/HeadStudio.",
    "arxiv_url": "http://arxiv.org/abs/2402.06149v2",
    "pdf_url": "http://arxiv.org/pdf/2402.06149v2",
    "published_date": "2024-02-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ZhenglinZhou/HeadStudio",
    "keywords": [
      "avatar",
      "efficient",
      "head",
      "face",
      "semantic",
      "geometry",
      "3d gaussian",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mesh-based Gaussian Splatting for Real-time Large-scale Deformation",
    "authors": [
      "Lin Gao",
      "Jie Yang",
      "Bo-Tao Zhang",
      "Jia-Mu Sun",
      "Yu-Jie Yuan",
      "Hongbo Fu",
      "Yu-Kun Lai"
    ],
    "abstract": "Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).",
    "arxiv_url": "http://arxiv.org/abs/2402.04796v1",
    "pdf_url": "http://arxiv.org/pdf/2402.04796v1",
    "published_date": "2024-02-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos",
    "authors": [
      "Alfredo Rivero",
      "ShahRukh Athar",
      "Zhixin Shu",
      "Dimitris Samaras"
    ],
    "abstract": "Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html",
    "arxiv_url": "http://arxiv.org/abs/2402.03723v1",
    "pdf_url": "http://arxiv.org/pdf/2402.03723v1",
    "published_date": "2024-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "vr",
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes",
    "authors": [
      "Yuanxing Duan",
      "Fangyin Wei",
      "Qiyu Dai",
      "Yuhang He",
      "Wenzheng Chen",
      "Baoquan Chen"
    ],
    "abstract": "We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details--especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.",
    "arxiv_url": "http://arxiv.org/abs/2402.03307v3",
    "pdf_url": "http://arxiv.org/pdf/2402.03307v3",
    "published_date": "2024-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "motion",
      "deformation",
      "3d gaussian",
      "acceleration",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM",
    "authors": [
      "Mingrui Li",
      "Shuhong Liu",
      "Heng Zhou",
      "Guohao Zhu",
      "Na Cheng",
      "Tianchen Deng",
      "Hongyu Wang"
    ],
    "abstract": "We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian Splatting. It incorporates appearance, geometry, and semantic features through multi-channel optimization, addressing the oversmoothing limitations of neural implicit SLAM systems in high-quality rendering, scene understanding, and object-level geometry. We introduce a unique semantic feature loss that effectively compensates for the shortcomings of traditional depth and color losses in object optimization. Through a semantic-guided keyframe selection strategy, we prevent erroneous reconstructions caused by cumulative errors. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, precise semantic segmentation, and object-level geometric accuracy, while ensuring real-time rendering capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2402.03246v6",
    "pdf_url": "http://arxiv.org/pdf/2402.03246v6",
    "published_date": "2024-02-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "geometry",
      "gaussian splatting",
      "real-time rendering",
      "slam",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting",
    "authors": [
      "Joanna Waczy≈Ñska",
      "Piotr Borycki",
      "S≈Çawomir Tadeja",
      "Jacek Tabor",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and real-time rendering. The main drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity of conditioning several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS.",
    "arxiv_url": "http://arxiv.org/abs/2402.01459v4",
    "pdf_url": "http://arxiv.org/pdf/2402.01459v4",
    "published_date": "2024-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "face",
      "real-time rendering",
      "ar",
      "animation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianStyle: Gaussian Head Avatar via StyleGAN",
    "authors": [
      "Pinxin Liu",
      "Luchuan Song",
      "Daoan Zhang",
      "Hang Hua",
      "Yunlong Tang",
      "Huaijin Tu",
      "Jiebo Luo",
      "Chenliang Xu"
    ],
    "abstract": "Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.",
    "arxiv_url": "http://arxiv.org/abs/2402.00827v3",
    "pdf_url": "http://arxiv.org/pdf/2402.00827v3",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "animation",
      "head",
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming",
    "authors": [
      "Jiayang Bai",
      "Letian Huang",
      "Jie Guo",
      "Wen Gong",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2402.00763v1",
    "pdf_url": "http://arxiv.org/pdf/2402.00763v1",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection Strategy",
    "authors": [
      "Letian Huang",
      "Jiayang Bai",
      "Jie Guo",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance, and robustness in sparse viewpoints, leading to various improvements. However, there has been a notable lack of attention to the fundamental problem of projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which can accommodate a variety of camera models. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.",
    "arxiv_url": "http://arxiv.org/abs/2402.00752v4",
    "pdf_url": "http://arxiv.org/pdf/2402.00752v4",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering",
    "authors": [
      "Lukas Radl",
      "Michael Steiner",
      "Mathias Parger",
      "Alexander Weinrauch",
      "Bernhard Kerbl",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.",
    "arxiv_url": "http://arxiv.org/abs/2402.00525v3",
    "pdf_url": "http://arxiv.org/pdf/2402.00525v3",
    "published_date": "2024-02-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGD: Boundary-Enhanced Segment Anything in 3D Gaussian via Gaussian Decomposition",
    "authors": [
      "Xu Hu",
      "Yuxi Wang",
      "Lue Fan",
      "Chuanchen Luo",
      "Junsong Fan",
      "Zhen Lei",
      "Qing Li",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "3D Gaussian Splatting has emerged as an alternative 3D representation for novel view synthesis, benefiting from its high-quality rendering results and real-time rendering speed. However, the 3D Gaussians learned by 3D-GS have ambiguous structures without any geometry constraints. This inherent issue in 3D-GS leads to a rough boundary when segmenting individual objects. To remedy these problems, we propose SAGD, a conceptually simple yet effective boundary-enhanced segmentation pipeline for 3D-GS to improve segmentation accuracy while preserving segmentation speed. Specifically, we introduce a Gaussian Decomposition scheme, which ingeniously utilizes the special structure of 3D Gaussian, finds out, and then decomposes the boundary Gaussians. Moreover, to achieve fast interactive 3D segmentation, we introduce a novel training-free pipeline by lifting a 2D foundation model to 3D-GS. Extensive experiments demonstrate that our approach achieves high-quality 3D segmentation without rough boundary issues, which can be easily applied to other scene editing tasks.",
    "arxiv_url": "http://arxiv.org/abs/2401.17857v4",
    "pdf_url": "http://arxiv.org/pdf/2401.17857v4",
    "published_date": "2024-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality",
    "authors": [
      "Ying Jiang",
      "Chang Yu",
      "Tianyi Xie",
      "Xuan Li",
      "Yutao Feng",
      "Huamin Wang",
      "Minchen Li",
      "Henry Lau",
      "Feng Gao",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "abstract": "As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain momentum, there's a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for content creation, editing, and interaction within these virtual spaces are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting in a Virtual Reality setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our Virtual Reality system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.Our project page is available at: https://yingjiang96.github.io/VR-GS/.",
    "arxiv_url": "http://arxiv.org/abs/2401.16663v2",
    "pdf_url": "http://arxiv.org/pdf/2401.16663v2",
    "published_date": "2024-01-30",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "shadow",
      "deformation",
      "body",
      "segmentation",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems",
    "authors": [
      "Liang Zhang",
      "Jionghao Lin",
      "Conrad Borchers",
      "Meng Cao",
      "Xiangen Hu"
    ],
    "abstract": "Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG framework effectively generated scalable, personalized simulations of learning performance. Comparative analysis revealed GAN's superior reliability over GPT-4 in this context, underscoring its potential in addressing data sparsity challenges in ITSs and contributing to the advancement of personalized educational technology.",
    "arxiv_url": "http://arxiv.org/abs/2402.01746v1",
    "pdf_url": "http://arxiv.org/pdf/2402.01746v1",
    "published_date": "2024-01-29",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Beilei Cui",
      "Long Bai",
      "Ziqi Guo",
      "Mengya Xu",
      "Mobarakol Islam",
      "Hongliang Ren"
    ],
    "abstract": "In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes but are hampered by slow inference speed, prolonged training, and inconsistent depth estimation. Some previous work utilizes ground truth depth for optimization but is hard to acquire in the surgical domain. To overcome these obstacles, we present Endo-4DGS, a real-time endoscopic dynamic reconstruction approach that utilizes 3D Gaussian Splatting (GS) for 3D representation. Specifically, we propose lightweight MLPs to capture temporal dynamics with Gaussian deformation fields. To obtain a satisfactory Gaussian Initialization, we exploit a powerful depth estimation foundation model, Depth-Anything, to generate pseudo-depth maps as a geometry prior. We additionally propose confidence-guided learning to tackle the ill-pose problems in monocular depth estimation and enhance the depth-guided reconstruction with surface normal constraints and depth regularization. Our approach has been validated on two surgical datasets, where it can effectively render in real-time, compute efficiently, and reconstruct with remarkable accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2401.16416v4",
    "pdf_url": "http://arxiv.org/pdf/2401.16416v4",
    "published_date": "2024-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "deformation",
      "lightweight",
      "geometry",
      "3d gaussian",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering",
    "authors": [
      "Yutao Feng",
      "Xiang Feng",
      "Yintong Shang",
      "Ying Jiang",
      "Chang Yu",
      "Zeshun Zong",
      "Tianjia Shao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ],
    "abstract": "We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \\url{https://gaussiansplashing.github.io/}.",
    "arxiv_url": "http://arxiv.org/abs/2401.15318v2",
    "pdf_url": "http://arxiv.org/pdf/2401.15318v2",
    "published_date": "2024-01-27",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "motion",
      "face",
      "deformation",
      "3d gaussian",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts",
    "authors": [
      "Jingyu Zhuang",
      "Di Kang",
      "Yan-Pei Cao",
      "Guanbin Li",
      "Liang Lin",
      "Ying Shan"
    ],
    "abstract": "Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.",
    "arxiv_url": "http://arxiv.org/abs/2401.14828v3",
    "pdf_url": "http://arxiv.org/pdf/2401.14828v3",
    "published_date": "2024-01-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting",
    "authors": [
      "Butian Xiong",
      "Zhuo Li",
      "Zhen Li"
    ],
    "abstract": "We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information",
    "arxiv_url": "http://arxiv.org/abs/2401.14032v1",
    "pdf_url": "http://arxiv.org/pdf/2401.14032v1",
    "published_date": "2024-01-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "3d reconstruction",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction",
    "authors": [
      "Yangsen Chen",
      "Hao Wang"
    ],
    "abstract": "The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.",
    "arxiv_url": "http://arxiv.org/abs/2401.13352v1",
    "pdf_url": "http://arxiv.org/pdf/2401.13352v1",
    "published_date": "2024-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "vr",
      "body",
      "medical",
      "3d reconstruction",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting",
    "authors": [
      "Zhongyuan Zhao",
      "Zhenyu Bao",
      "Qing Li",
      "Guoping Qiu",
      "Kanglin Liu"
    ],
    "abstract": "Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency. Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a resolution of 512 $\\times$ 512 ).",
    "arxiv_url": "http://arxiv.org/abs/2401.12900v5",
    "pdf_url": "http://arxiv.org/pdf/2401.12900v5",
    "published_date": "2024-01-23",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGaussian: Real-time Gaussian Splatting for Dynamic Endoscopic Scene Reconstruction",
    "authors": [
      "Yifan Liu",
      "Chenxin Li",
      "Chen Yang",
      "Yixuan Yuan"
    ],
    "abstract": "Reconstructing deformable tissues from endoscopic videos is essential in many downstream surgical applications. However, existing methods suffer from slow rendering speed, greatly limiting their practical use. In this paper, we introduce EndoGaussian, a real-time endoscopic scene reconstruction framework built on 3D Gaussian Splatting (3DGS). By integrating the efficient Gaussian representation and highly-optimized rendering engine, our framework significantly boosts the rendering speed to a real-time level. To adapt 3DGS for endoscopic scenes, we propose two strategies, Holistic Gaussian Initialization (HGI) and Spatio-temporal Gaussian Tracking (SGT), to handle the non-trivial Gaussian initialization and tissue deformation problems, respectively. In HGI, we leverage recent depth estimation models to predict depth maps of input binocular/monocular image sequences, based on which pixels are re-projected and combined for holistic initialization. In SPT, we propose to model surface dynamics using a deformation field, which is composed of an efficient encoding voxel and a lightweight deformation decoder, allowing for Gaussian tracking with minor training and rendering burden. Experiments on public datasets demonstrate our efficacy against prior SOTAs in many aspects, including better rendering speed (195 FPS real-time, 100$\\times$ gain), better rendering quality (37.848 PSNR), and less training overhead (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \\url{https://yifliu3.github.io/EndoGaussian/}.",
    "arxiv_url": "http://arxiv.org/abs/2401.12561v2",
    "pdf_url": "http://arxiv.org/pdf/2401.12561v2",
    "published_date": "2024-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "tracking",
      "face",
      "deformation",
      "lightweight",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting",
    "authors": [
      "Lingting Zhu",
      "Zhao Wang",
      "Jiahao Cui",
      "Zhenchao Jin",
      "Guying Lin",
      "Lequan Yu"
    ],
    "abstract": "Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision with spatial-temporal weight masks to optimize 3D targets with tool occlusion from a single viewpoint, and surface-aligned regularization terms to capture the much better geometry. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at https://github.com/HKU-MedAI/EndoGS.",
    "arxiv_url": "http://arxiv.org/abs/2401.11535v3",
    "pdf_url": "http://arxiv.org/pdf/2401.11535v3",
    "published_date": "2024-01-21",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/HKU-MedAI/EndoGS",
    "keywords": [
      "face",
      "deformation",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting",
    "authors": [
      "Mengtian Li",
      "Shengxiang Yao",
      "Zhifeng Xie",
      "Keyu Chen"
    ],
    "abstract": "In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2401.09720v2",
    "pdf_url": "http://arxiv.org/pdf/2401.09720v2",
    "published_date": "2024-01-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video",
    "authors": [
      "Zijie Pan",
      "Zeyu Yang",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "Generating dynamic 3D object from a single-view video is challenging due to the lack of 4D labeled data. An intuitive approach is to extend previous image-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling.However, this approach would be slow and expensive to scale due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly reconstruct the 4D content through a 4D Gaussian splatting model. Importantly, our method can achieve real-time rendering under continuous camera trajectories. To enable robust reconstruction under sparse views, we introduce inconsistency-aware confidence-weighted loss design, along with a lightly weighted score distillation loss. Extensive experiments on both synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the quality of novel view synthesis. For example, Efficient4D takes only 10 minutes to model a dynamic object, vs 120 minutes by the previous art model Consistent4D.",
    "arxiv_url": "http://arxiv.org/abs/2401.08742v3",
    "pdf_url": "http://arxiv.org/pdf/2401.08742v3",
    "published_date": "2024-01-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "fast",
      "4d",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities",
    "authors": [
      "Xu Yan",
      "Haiming Zhang",
      "Yingjie Cai",
      "Jingming Guo",
      "Weichao Qiu",
      "Bin Gao",
      "Kaiqiang Zhou",
      "Yue Zhao",
      "Huan Jin",
      "Jiantao Gao",
      "Zhen Li",
      "Lihui Jiang",
      "Wei Zhang",
      "Hongbo Zhang",
      "Dengxin Dai",
      "Bingbing Liu"
    ],
    "abstract": "The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.",
    "arxiv_url": "http://arxiv.org/abs/2401.08045v1",
    "pdf_url": "http://arxiv.org/pdf/2401.08045v1",
    "published_date": "2024-01-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zhanghm1995/Forge_VFM4AD",
    "keywords": [
      "autonomous driving",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Shadow Casting for Neural Characters",
    "authors": [
      "Luis Bolanos",
      "Shih-Yang Su",
      "Helge Rhodin"
    ],
    "abstract": "Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.",
    "arxiv_url": "http://arxiv.org/abs/2401.06116v1",
    "pdf_url": "http://arxiv.org/pdf/2401.06116v1",
    "published_date": "2024-01-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "outdoor",
      "relighting",
      "lighting",
      "shadow",
      "motion",
      "geometry",
      "ar",
      "neural rendering",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Linus Franke",
      "Darius R√ºckert",
      "Laura Fink",
      "Marc Stamminger"
    ],
    "abstract": "Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage.   The project page is located at: https://lfranke.github.io/trips/",
    "arxiv_url": "http://arxiv.org/abs/2401.06003v2",
    "pdf_url": "http://arxiv.org/pdf/2401.06003v2",
    "published_date": "2024-01-11",
    "categories": [
      "cs.CV",
      "cs.GR",
      "I.3; I.4"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Segmented 3D Gaussians via Efficient Feature Unprojection for Zero-shot Neural Scene Segmentation",
    "authors": [
      "Bin Dou",
      "Tianyu Zhang",
      "Zhaohui Wang",
      "Yongjia Ma",
      "Zejian Yuan"
    ],
    "abstract": "Zero-shot neural scene segmentation, which reconstructs 3D neural segmentation field without manual annotations, serves as an effective way for scene understanding. However, existing models, especially the efficient 3D Gaussian-based methods, struggle to produce compact segmentation results. This issue stems primarily from their redundant learnable attributes assigned on individual Gaussians, leading to a lack of robustness against the 3D-inconsistencies in zero-shot generated raw labels. To address this problem, our work, named Compact Segmented 3D Gaussians (CoSegGaussians), proposes the Feature Unprojection and Fusion module as the segmentation field, which utilizes a shallow decoder generalizable for all Gaussians based on high-level features. Specifically, leveraging the learned Gaussian geometric parameters, semantic-aware image-based features are introduced into the scene via our unprojection technique. The lifted features, together with spatial information, are fed into the multi-scale aggregation decoder to generate segmentation identities for all Gaussians. Furthermore, we design CoSeg Loss to boost model robustness against 3D-inconsistent noises. Experimental results show that our model surpasses baselines on zero-shot semantic segmentation task, improving by ~10% mIoU over the best baseline. Code and more results will be available at https://David-Dou.github.io/CoSegGaussians.",
    "arxiv_url": "http://arxiv.org/abs/2401.05925v4",
    "pdf_url": "http://arxiv.org/pdf/2401.05925v4",
    "published_date": "2024-01-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "compact",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
    "authors": [
      "Dejia Xu",
      "Ye Yuan",
      "Morteza Mardani",
      "Sifei Liu",
      "Jiaming Song",
      "Zhangyang Wang",
      "Arash Vahdat"
    ],
    "abstract": "Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: https://ir1d.github.io/AGG/",
    "arxiv_url": "http://arxiv.org/abs/2401.04099v1",
    "pdf_url": "http://arxiv.org/pdf/2401.04099v1",
    "published_date": "2024-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on 3D Gaussian Splatting",
    "authors": [
      "Guikun Chen",
      "Wenguan Wang"
    ],
    "abstract": "3D Gaussian splatting (GS) has emerged as a transformative technique in explicit radiance field and computer graphics. This innovative approach, characterized by the use of millions of learnable 3D Gaussians, represents a significant departure from mainstream neural radiance field approaches, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representation and differentiable rendering algorithm, not only promises real-time rendering capability but also introduces unprecedented levels of editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the emergence of 3D GS, laying the groundwork for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By enabling unprecedented rendering speed, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in explicit radiance field.",
    "arxiv_url": "http://arxiv.org/abs/2401.03890v6",
    "pdf_url": "http://arxiv.org/pdf/2401.03890v6",
    "published_date": "2024-01-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human",
    "authors": [
      "Song Bai",
      "Jie Li"
    ],
    "abstract": "While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language inputs to transcend into human motion outputs. This paper aims to provide a comprehensive overview and summary of the relevant papers published mostly during the latter half year of 2023. It will begin by discussing the AI generated object models in 3D, followed by the generated 3D human models, and finally, the generated 3D human motions, culminating in a conclusive summary and a vision for the future.",
    "arxiv_url": "http://arxiv.org/abs/2401.02620v1",
    "pdf_url": "http://arxiv.org/pdf/2401.02620v1",
    "published_date": "2024-01-05",
    "categories": [
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting",
    "authors": [
      "Van Minh Nguyen",
      "Emma Sandidge",
      "Trupti Mahendrakar",
      "Ryan T. White"
    ],
    "abstract": "The accelerating deployment of spacecraft in orbit have generated interest in on-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possible unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. This requires robust characterization of the target's geometry. In this article, we present an approach for mapping geometries of satellites on orbit based on 3D Gaussian Splatting that can run on computing resources available on current spaceflight hardware. We demonstrate model training and 3D rendering performance on a hardware-in-the-loop satellite mock-up under several realistic lighting and motion conditions. Our model is shown to be capable of training on-board and rendering higher quality novel views of an unknown satellite nearly 2 orders of magnitude faster than previous NeRF-based algorithms. Such on-board capabilities are critical to enable downstream machine intelligence tasks necessary for autonomous guidance, navigation, and control tasks.",
    "arxiv_url": "http://arxiv.org/abs/2401.02588v1",
    "pdf_url": "http://arxiv.org/pdf/2401.02588v1",
    "published_date": "2024-01-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "fast",
      "motion",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DoF Object Pose Dataset Generation",
    "authors": [
      "Lukas Meyer",
      "Floris Erich",
      "Yusuke Yoshiyasu",
      "Marc Stamminger",
      "Noriaki Ando",
      "Yukiyasu Domae"
    ],
    "abstract": "We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting.   Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. <i>PEGASUS</i> allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted.   Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS.",
    "arxiv_url": "http://arxiv.org/abs/2401.02281v2",
    "pdf_url": "http://arxiv.org/pdf/2401.02281v2",
    "published_date": "2024-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
    "authors": [
      "Xingxing Zuo",
      "Pouya Samangouei",
      "Yunwen Zhou",
      "Yan Di",
      "Mingyang Li"
    ],
    "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of the same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the project page.",
    "arxiv_url": "http://arxiv.org/abs/2401.01970v2",
    "pdf_url": "http://arxiv.org/pdf/2401.01970v2",
    "published_date": "2024-01-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting",
    "authors": [
      "Yunzhi Yan",
      "Haotong Lin",
      "Chenxu Zhou",
      "Weijie Wang",
      "Haiyang Sun",
      "Kun Zhan",
      "Xianpeng Lang",
      "Xiaowei Zhou",
      "Sida Peng"
    ],
    "abstract": "This paper aims to tackle the problem of modeling dynamic urban streets for autonomous driving scenes. Recent methods extend NeRF by incorporating tracked vehicle poses to animate vehicles, enabling photo-realistic view synthesis of dynamic urban street scenes. However, significant limitations are their slow training and rendering speed. We introduce Street Gaussians, a new explicit scene representation that tackles these limitations. Specifically, the dynamic urban scene is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background. To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a 4D spherical harmonics model for the dynamic appearance. The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 135 FPS (1066 $\\times$ 1600 resolution) within half an hour of training. The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets. Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets. The code will be released to ensure reproducibility.",
    "arxiv_url": "http://arxiv.org/abs/2401.01339v3",
    "pdf_url": "http://arxiv.org/pdf/2401.01339v3",
    "published_date": "2024-01-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "semantic",
      "4d",
      "3d gaussian",
      "ar",
      "nerf",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deblurring 3D Gaussian Splatting",
    "authors": [
      "Byeonghyeon Lee",
      "Howoong Lee",
      "Xiangyu Sun",
      "Usman Ali",
      "Eunbyung Park"
    ],
    "abstract": "Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, Deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While Deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring. Qualitative results are available at https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/",
    "arxiv_url": "http://arxiv.org/abs/2401.00834v3",
    "pdf_url": "http://arxiv.org/pdf/2401.00834v3",
    "published_date": "2024-01-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency",
    "authors": [
      "Yuyang Yin",
      "Dejia Xu",
      "Zhangyang Wang",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize the entire dynamic 3D scene. However, as these pipelines generate 4D content from text or image inputs directly, they are constrained by limited motion capabilities and depend on unreliable prompt engineering for desired results. To address these problems, this work introduces \\textbf{4DGen}, a novel framework for grounded 4D content creation. We identify monocular video sequences as a key component in constructing the 4D content. Our pipeline facilitates controllable 4D generation, enabling users to specify the motion via monocular video or adopt image-to-video generations, thus offering superior control over content creation. Furthermore, we construct our 4D representation using dynamic 3D Gaussians, which permits efficient, high-resolution supervision through rendering during training, thereby facilitating high-quality 4D generation. Additionally, we employ spatial-temporal pseudo labels on anchor frames, along with seamless consistency priors implemented through 3D-aware score distillation sampling and smoothness regularizations. Compared to existing video-to-4D baselines, our approach yields superior results in faithfully reconstructing input signals and realistically inferring renderings from novel viewpoints and timesteps. More importantly, compared to previous image-to-4D and text-to-4D works, 4DGen supports grounded generation, offering users enhanced control and improved motion generation capabilities, a feature difficult to achieve with previous methods. Project page: https://vita-group.github.io/4DGen/",
    "arxiv_url": "http://arxiv.org/abs/2312.17225v3",
    "pdf_url": "http://arxiv.org/pdf/2312.17225v3",
    "published_date": "2023-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamGaussian4D: Generative 4D Gaussian Splatting",
    "authors": [
      "Jiawei Ren",
      "Liang Pan",
      "Jiaxiang Tang",
      "Chi Zhang",
      "Ang Cao",
      "Gang Zeng",
      "Ziwei Liu"
    ],
    "abstract": "4D content generation has achieved remarkable progress recently. However, existing methods suffer from long optimization times, a lack of motion controllability, and a low quality of details. In this paper, we introduce DreamGaussian4D (DG4D), an efficient 4D generation framework that builds on Gaussian Splatting (GS). Our key insight is that combining explicit modeling of spatial transformations with static GS makes an efficient and powerful representation for 4D generation. Moreover, video generation methods have the potential to offer valuable spatial-temporal priors, enhancing the high-quality 4D generation. Specifically, we propose an integral framework with two major modules: 1) Image-to-4D GS - we initially generate static GS with DreamGaussianHD, followed by HexPlane-based dynamic generation with Gaussian deformation; and 2) Video-to-Video Texture Refinement - we refine the generated UV-space texture maps and meanwhile enhance their temporal consistency by utilizing a pre-trained image-to-video diffusion model. Notably, DG4D reduces the optimization time from several hours to just a few minutes, allows the generated 3D motion to be visually controlled, and produces animated meshes that can be realistically rendered in 3D engines.",
    "arxiv_url": "http://arxiv.org/abs/2312.17142v3",
    "pdf_url": "http://arxiv.org/pdf/2312.17142v3",
    "published_date": "2023-12-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis",
    "authors": [
      "Zhan Li",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu"
    ],
    "abstract": "Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.",
    "arxiv_url": "http://arxiv.org/abs/2312.16812v2",
    "pdf_url": "http://arxiv.org/pdf/2312.16812v2",
    "published_date": "2023-12-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/oppo-us-research/SpacetimeGaussians",
    "keywords": [
      "motion",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSplat: 3D Language Gaussian Splatting",
    "authors": [
      "Minghan Qin",
      "Wanhua Li",
      "Jiawei Zhou",
      "Haoqian Wang",
      "Hanspeter Pfister"
    ],
    "abstract": "Humans live in a 3D world and commonly use natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experimental results show that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a 199 $\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2312.16084v2",
    "pdf_url": "http://arxiv.org/pdf/2312.16084v2",
    "published_date": "2023-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D-Guided 3D Gaussian Segmentation",
    "authors": [
      "Kun Lan",
      "Haoran Li",
      "Haolin Shi",
      "Wenjun Wu",
      "Yong Liao",
      "Lin Wang",
      "Pengyuan Zhou"
    ],
    "abstract": "Recently, 3D Gaussian, as an explicit 3D representation method, has demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms of expressing complex scenes and training duration. These advantages signal a wide range of applications for 3D Gaussians in 3D understanding and editing. Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The existing segmentation methods are not only cumbersome but also incapable of segmenting multiple objects simultaneously in a short amount of time. In response, this paper introduces a 3D Gaussian segmentation method implemented with 2D segmentation as supervision. This approach uses input 2D segmentation maps to guide the learning of the added 3D Gaussian semantic information, while nearest neighbor clustering and statistical filtering refine the segmentation results. Experiments show that our concise method can achieve comparable performances on mIOU and mAcc for multi-object segmentation as previous single-object segmentation methods.",
    "arxiv_url": "http://arxiv.org/abs/2312.16047v1",
    "pdf_url": "http://arxiv.org/pdf/2312.16047v1",
    "published_date": "2023-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "3d gaussian",
      "ar",
      "nerf",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatMesh: Interactive 3D Segmentation and Editing Using Mesh-Based Gaussian Splatting",
    "authors": [
      "Kaichen Zhou",
      "Lanqing Hong",
      "Xinhai Chang",
      "Yingji Zhong",
      "Enze Xie",
      "Hao Dong",
      "Zhihao Li",
      "Yongxin Yang",
      "Zhenguo Li",
      "Wei Zhang"
    ],
    "abstract": "A key challenge in fine-grained 3D-based interactive editing is the absence of an efficient representation that balances diverse modifications with high-quality view synthesis under a given memory constraint. While 3D meshes provide robustness for various modifications, they often yield lower-quality view synthesis compared to 3D Gaussian Splatting, which, in turn, suffers from instability during extensive editing. A straightforward combination of these two representations results in suboptimal performance and fails to meet memory constraints. In this paper, we introduce SplatMesh, a novel fine-grained interactive 3D segmentation and editing algorithm that integrates 3D Gaussian Splat with a precomputed mesh and could adjust the memory request based on the requirement. Specifically, given a mesh, \\method simplifies it while considering both color and shape, ensuring it meets memory constraints. Then, SplatMesh aligns Gaussian splats with the simplified mesh by treating each triangle as a new reference point. By segmenting and editing the simplified mesh, we can effectively edit the Gaussian splats as well, which will lead to extensive experiments on real and synthetic datasets, coupled with illustrative visual examples, highlighting the superiority of our approach in terms of representation quality and editing performance. Code of our paper can be found here: https://github.com/kaichen-z/SplatMesh.",
    "arxiv_url": "http://arxiv.org/abs/2312.15856v3",
    "pdf_url": "http://arxiv.org/pdf/2312.15856v3",
    "published_date": "2023-12-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "https://github.com/kaichen-z/SplatMesh",
    "keywords": [
      "efficient",
      "lighting",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGR-CT: Sparse-View CT Reconstruction with a 3D Gaussian Representation",
    "authors": [
      "Yingtai Li",
      "Xueming Fu",
      "Han Li",
      "Shang Zhao",
      "Ruiyang Jin",
      "S. Kevin Zhou"
    ],
    "abstract": "Sparse-view computed tomography (CT) reduces radiation exposure by acquiring fewer projections, making it a valuable tool in clinical scenarios where low-dose radiation is essential. However, this often results in increased noise and artifacts due to limited data. In this paper we propose a novel 3D Gaussian representation (3DGR) based method for sparse-view CT reconstruction. Inspired by recent success in novel view synthesis driven by 3D Gaussian splatting, we leverage the efficiency and expressiveness of 3D Gaussian representation as an alternative to implicit neural representation. To unleash the potential of 3DGR for CT imaging scenario, we propose two key innovations: (i) FBP-image-guided Guassian initialization and (ii) efficient integration with a differentiable CT projector. Extensive experiments and ablations on diverse datasets demonstrate the proposed 3DGR-CT consistently outperforms state-of-the-art counterpart methods, achieving higher reconstruction accuracy with faster convergence. Furthermore, we showcase the potential of 3DGR-CT for real-time physical simulation, which holds important clinical applications while challenging for implicit neural representations.",
    "arxiv_url": "http://arxiv.org/abs/2312.15676v2",
    "pdf_url": "http://arxiv.org/pdf/2312.15676v2",
    "published_date": "2023-12-25",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human101: Training 100+FPS Human Gaussians in 100s from 1 View",
    "authors": [
      "Mingwei Li",
      "Jiachen Tao",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101.",
    "arxiv_url": "http://arxiv.org/abs/2312.15258v1",
    "pdf_url": "http://arxiv.org/pdf/2312.15258v1",
    "published_date": "2023-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/longxiang-ai/Human101",
    "keywords": [
      "animation",
      "efficient",
      "high-fidelity",
      "body",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable 3D Gaussian Splatting for Animatable Human Avatars",
    "authors": [
      "HyunJun Jung",
      "Nikolas Brasch",
      "Jifei Song",
      "Eduardo Perez-Pellitero",
      "Yiren Zhou",
      "Zhihao Li",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "abstract": "Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually.",
    "arxiv_url": "http://arxiv.org/abs/2312.15059v1",
    "pdf_url": "http://arxiv.org/pdf/2312.15059v1",
    "published_date": "2023-12-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models",
    "authors": [
      "Huan Ling",
      "Seung Wook Kim",
      "Antonio Torralba",
      "Sanja Fidler",
      "Karsten Kreis"
    ],
    "abstract": "Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.",
    "arxiv_url": "http://arxiv.org/abs/2312.13763v2",
    "pdf_url": "http://arxiv.org/pdf/2312.13763v2",
    "published_date": "2023-12-21",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "geometry",
      "4d",
      "3d gaussian",
      "ar",
      "animation",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting with NeRF-based Color and Opacity",
    "authors": [
      "Dawid Malarz",
      "Weronika Smolak",
      "Jacek Tabor",
      "S≈Çawomir Tadeja",
      "Przemys≈Çaw Spurek"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects. By encoding the shape and color information within neural network weights, NeRFs excel at producing strikingly sharp novel views of 3D objects. Recently, numerous generalizations of NeRFs utilizing generative models have emerged, expanding its versatility. In contrast, Gaussian Splatting (GS) offers a similar render quality with faster training and inference as it does not need neural networks to work. It encodes information about the 3D objects in the set of Gaussian distributions that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are difficult to condition since they usually require circa hundred thousand Gaussian components. To mitigate the caveats of both models, we propose a hybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS representation of the 3D object's shape and NeRF-based encoding of color and opacity. Our model uses Gaussian distributions with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and a neural network that takes Gaussian parameters and viewing direction to produce changes in the said color and opacity. As a result, our model better describes shadows, light reflections, and the transparency of 3D objects without adding additional texture and light components.",
    "arxiv_url": "http://arxiv.org/abs/2312.13729v5",
    "pdf_url": "http://arxiv.org/pdf/2312.13729v5",
    "published_date": "2023-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "reflection",
      "shadow",
      "fast",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction",
    "authors": [
      "Stanislaw Szymanowicz",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "We introduce the \\method, an ultra-efficient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, maps the input image to one 3D Gaussian per pixel. The resulting set of Gaussians thus has the form an image, the Splatter Image. We further extend the method take several images as input via cross-view attention. Owning to the speed of the renderer (588 FPS), we use a single GPU for training while generating entire images at each iteration to optimize perceptual metrics like LPIPS. On several synthetic, real, multi-category and large-scale benchmark datasets, we achieve better results in terms of PSNR, LPIPS, and other metrics while training and evaluating much faster than prior works. Code, models, demo and more results are available at https://szymanowiczs.github.io/splatter-image.",
    "arxiv_url": "http://arxiv.org/abs/2312.13150v2",
    "pdf_url": "http://arxiv.org/pdf/2312.13150v2",
    "published_date": "2023-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "authors": [
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Perez-Pellitero"
    ],
    "abstract": "Novel view synthesis has shown rapid progress recently, with methods capable of producing increasingly photorealistic results. 3D Gaussian Splatting has emerged as a promising method, producing high-quality renderings of scenes and enabling interactive viewing at real-time frame rates. However, it is limited to static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct dynamic scenes. We model a scene's dynamics using dynamic MLPs, learning deformations from temporally-local canonical representations to per-frame 3D Gaussians. To disentangle static and dynamic regions, tuneable parameters weigh each Gaussian's respective MLP parameters, improving the dynamics modelling of imbalanced scenes. We introduce a sliding window training strategy that partitions the sequence into smaller manageable windows to handle arbitrary length scenes while maintaining high rendering quality. We propose an adaptive sampling strategy to determine appropriate window size hyperparameters based on the scene's motion, balancing training overhead with visual quality. Training a separate dynamic 3D Gaussian model for each sliding window allows the canonical representation to change, enabling the reconstruction of scenes with significant geometric changes. Temporal consistency is enforced using a fine-tuning step with self-supervising consistency loss on randomly sampled novel views. As a result, our method produces high-quality renderings of general dynamic scenes with competitive quantitative performance, which can be viewed in real-time in our dynamic interactive viewer.",
    "arxiv_url": "http://arxiv.org/abs/2312.13308v2",
    "pdf_url": "http://arxiv.org/pdf/2312.13308v2",
    "published_date": "2023-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "deformation",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids",
    "authors": [
      "Wieland Morgenstern",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a highly promising technique for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it utilizes efficient rasterization allowing for very fast rendering at high-quality. However, the storage size is significantly higher, which hinders practical deployment, e.g. on resource constrained devices. In this paper, we introduce a compact scene representation organizing the parameters of 3D Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a drastic reduction in storage requirements without compromising visual quality during rendering. Central to our idea is the explicit exploitation of perceptual redundancies present in natural scenes. In essence, the inherent nature of a scene allows for numerous permutations of Gaussian parameters to equivalently represent it. To this end, we propose a novel highly parallel algorithm that regularly arranges the high-dimensional Gaussian parameters into a 2D grid while preserving their neighborhood structure. During training, we further enforce local smoothness between the sorted parameters in the grid. The uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless integration with established renderers. Our method achieves a reduction factor of 17x to 42x in size for complex scenes with no increase in training time, marking a substantial leap forward in the domain of 3D scene distribution and consumption. Additional information can be found on our project page: https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/",
    "arxiv_url": "http://arxiv.org/abs/2312.13299v2",
    "pdf_url": "http://arxiv.org/pdf/2312.13299v2",
    "published_date": "2023-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction",
    "authors": [
      "David Charatan",
      "Sizhe Li",
      "Andrea Tagliasacchi",
      "Vincent Sitzmann"
    ],
    "abstract": "We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.",
    "arxiv_url": "http://arxiv.org/abs/2312.12337v4",
    "pdf_url": "http://arxiv.org/pdf/2312.12337v4",
    "published_date": "2023-12-19",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "fast",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning",
    "authors": [
      "Ye Yuan",
      "Xueting Li",
      "Yangyi Huang",
      "Shalini De Mello",
      "Koki Nagano",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.",
    "arxiv_url": "http://arxiv.org/abs/2312.11461v2",
    "pdf_url": "http://arxiv.org/pdf/2312.11461v2",
    "published_date": "2023-12-18",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "fast",
      "body",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis",
    "authors": [
      "Yiqing Liang",
      "Numair Khan",
      "Zhengqin Li",
      "Thu Nguyen-Phuoc",
      "Douglas Lanman",
      "James Tompkin",
      "Lei Xiao"
    ],
    "abstract": "We propose a method that achieves state-of-the-art rendering quality and efficiency on monocular dynamic scene reconstruction using deformable 3D Gaussians. Implicit deformable representations commonly model motion with a canonical space and time-dependent backward-warping deformation field. Our method, GauFRe, uses a forward-warping deformation to explicitly model non-rigid transformations of scene geometry. Specifically, we propose a template set of 3D Gaussians residing in a canonical space, and a time-dependent forward-warping deformation field to model dynamic objects. Additionally, we tailor a 3D Gaussian-specific static component supported by an inductive bias-aware initialization approach which allows the deformation field to focus on moving scene regions, improving the rendering of complex real-world motion. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Experiments show our method achieves competitive results and higher efficiency than both previous state-of-the-art NeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20 mins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website: https://lynl7130.github.io/gaufre/index.html",
    "arxiv_url": "http://arxiv.org/abs/2312.11458v3",
    "pdf_url": "http://arxiv.org/pdf/2312.11458v3",
    "published_date": "2023-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring the Feasibility of Generating Realistic 3D Models of Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's Impact on Model Generation",
    "authors": [
      "Selcuk Anil Karatopak",
      "Deniz Sen"
    ],
    "abstract": "Many species face the threat of extinction. It's important to study these species and gather information about them as much as possible to preserve biodiversity. Due to the rarity of endangered species, there is a limited amount of data available, making it difficult to apply data requiring generative AI methods to this domain. We aim to study the feasibility of generating consistent and real-like 3D models of endangered animals using limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion models that can generate a 3D model out of a single image of the target species. This paper investigates the intricate relationship between elevation angle and the output quality of 3D model generation, focusing on the innovative approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing Generative Gaussian Splatting along with novel mesh extraction and refinement algorithms, serves as the focal point of our study. We conduct a comprehensive analysis, analyzing the effect of varying elevation angles on DreamGaussian's ability to reconstruct 3D scenes accurately. Through an empirical evaluation, we demonstrate how changes in elevation angle impact the generated images' spatial coherence, structural integrity, and perceptual realism. We observed that giving a correct elevation angle with the input image significantly affects the result of the generated 3D model. We hope this study to be influential for the usability of AI to preserve endangered animals; while the penultimate aim is to obtain a model that can output biologically consistent 3D models via small samples, the qualitative interpretation of an existing state-of-the-art model such as DreamGaussian will be a step forward in our goal.",
    "arxiv_url": "http://arxiv.org/abs/2312.09682v1",
    "pdf_url": "http://arxiv.org/pdf/2312.09682v1",
    "published_date": "2023-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text2Immersion: Generative Immersive Scene with 3D Gaussians",
    "authors": [
      "Hao Ouyang",
      "Kathryn Heal",
      "Stephen Lombardi",
      "Tiancheng Sun"
    ],
    "abstract": "We introduce Text2Immersion, an elegant method for producing high-quality 3D immersive scenes from text prompts. Our proposed pipeline initiates by progressively generating a Gaussian cloud using pre-trained 2D diffusion and depth estimation models. This is followed by a refining stage on the Gaussian cloud, interpolating and refining it to enhance the details of the generated scene. Distinct from prevalent methods that focus on single object or indoor scenes, or employ zoom-out trajectories, our approach generates diverse scenes with various objects, even extending to the creation of imaginary scenes. Consequently, Text2Immersion can have wide-ranging implications for various applications such as virtual reality, game development, and automated content creation. Extensive evaluations demonstrate that our system surpasses other methods in rendering quality and diversity, further progressing towards text-driven 3D scene generation. We will make the source code publicly accessible at the project page.",
    "arxiv_url": "http://arxiv.org/abs/2312.09242v1",
    "pdf_url": "http://arxiv.org/pdf/2312.09242v1",
    "published_date": "2023-12-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting",
    "authors": [
      "Zhiyin Qian",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Andreas Geiger",
      "Siyu Tang"
    ],
    "abstract": "We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.",
    "arxiv_url": "http://arxiv.org/abs/2312.09228v3",
    "pdf_url": "http://arxiv.org/pdf/2312.09228v3",
    "published_date": "2023-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "deformation",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers",
    "authors": [
      "Zi-Xin Zou",
      "Zhipeng Yu",
      "Yuan-Chen Guo",
      "Yangguang Li",
      "Ding Liang",
      "Yan-Pei Cao",
      "Song-Hai Zhang"
    ],
    "abstract": "Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.",
    "arxiv_url": "http://arxiv.org/abs/2312.09147v2",
    "pdf_url": "http://arxiv.org/pdf/2312.09147v2",
    "published_date": "2023-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "face",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching",
    "authors": [
      "Yuan Sun",
      "Xuan Wang",
      "Yunfan Zhang",
      "Jie Zhang",
      "Caigui Jiang",
      "Yu Guo",
      "Fei Wang"
    ],
    "abstract": "We present a method named iComMa to address the 6D camera pose estimation problem in computer vision. Conventional pose estimation methods typically rely on the target's CAD model or necessitate specific network training tailored to particular object classes. Some existing methods have achieved promising results in mesh-free object and scene pose estimation by inverting the Neural Radiance Fields (NeRF). However, they still struggle with adverse initializations such as large rotations and translations. To address this issue, we propose an efficient method for accurate camera pose estimation by inverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based differentiable framework optimizes camera pose by minimizing the residual between the query image and the rendered image, requiring no training. An end-to-end matching module is designed to enhance the model's robustness against adverse initializations, while minimizing pixel-level comparing loss aids in precise pose estimation. Experimental results on synthetic and complex real-world data demonstrate the effectiveness of the proposed approach in challenging conditions and the accuracy of camera pose estimation.",
    "arxiv_url": "http://arxiv.org/abs/2312.09031v2",
    "pdf_url": "http://arxiv.org/pdf/2312.09031v2",
    "published_date": "2023-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes",
    "authors": [
      "Xiaoyu Zhou",
      "Zhiwei Lin",
      "Xiaojun Shan",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes. For complex scenes with moving objects, we first sequentially and progressively model the static background of the entire scene with incremental static 3D Gaussians. We then leverage a composite dynamic Gaussian graph to handle multiple moving objects, individually reconstructing each object and restoring their accurate positions and occlusion relationships within the scene. We further use a LiDAR prior for Gaussian Splatting to reconstruct scenes with greater details and maintain panoramic consistency. DrivingGaussian outperforms existing methods in dynamic driving scene reconstruction and enables photorealistic surround-view synthesis with high-fidelity and multi-camera consistency. Our project page is at: https://github.com/VDIGPKU/DrivingGaussian.",
    "arxiv_url": "http://arxiv.org/abs/2312.07920v3",
    "pdf_url": "http://arxiv.org/pdf/2312.07920v3",
    "published_date": "2023-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/VDIGPKU/DrivingGaussian",
    "keywords": [
      "efficient",
      "high-fidelity",
      "autonomous driving",
      "3d gaussian",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COLMAP-Free 3D Gaussian Splatting",
    "authors": [
      "Yang Fu",
      "Sifei Liu",
      "Amey Kulkarni",
      "Jan Kautz",
      "Alexei A. Efros",
      "Xiaolong Wang"
    ],
    "abstract": "While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs",
    "arxiv_url": "http://arxiv.org/abs/2312.07504v2",
    "pdf_url": "http://arxiv.org/pdf/2312.07504v2",
    "published_date": "2023-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting SLAM",
    "authors": [
      "Hidenobu Matsuki",
      "Riku Murai",
      "Paul H. J. Kelly",
      "Andrew J. Davison"
    ],
    "abstract": "We present the first application of 3D Gaussian Splatting in monocular SLAM, the most fundamental but the hardest setup for Visual SLAM. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to RGB-D SLAM when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.",
    "arxiv_url": "http://arxiv.org/abs/2312.06741v2",
    "pdf_url": "http://arxiv.org/pdf/2312.06741v2",
    "published_date": "2023-12-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "motion",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering",
    "authors": [
      "Haokai Pang",
      "Heming Zhu",
      "Adam Kortylewski",
      "Christian Theobalt",
      "Marc Habermann"
    ],
    "abstract": "Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.",
    "arxiv_url": "http://arxiv.org/abs/2312.05941v2",
    "pdf_url": "http://arxiv.org/pdf/2312.05941v2",
    "published_date": "2023-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoGS: Controllable Gaussian Splatting",
    "authors": [
      "Heng Yu",
      "Joel Julin",
      "Zolt√°n √Å. Milacski",
      "Koichiro Niinuma",
      "L√°szl√≥ A. Jeni"
    ],
    "abstract": "Capturing and re-animating the 3D structure of articulated objects present significant barriers. On one hand, methods requiring extensively calibrated multi-view setups are prohibitively complex and resource-intensive, limiting their practical applicability. On the other hand, while single-camera Neural Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive training and rendering costs. 3D Gaussian Splatting would be a suitable alternative but for two reasons. Firstly, existing methods for 3D dynamic Gaussians require synchronized multi-view cameras, and secondly, the lack of controllability in dynamic scenarios. We present CoGS, a method for Controllable Gaussian Splatting, that enables the direct manipulation of scene elements, offering real-time control of dynamic scenes without the prerequisite of pre-computing control signals. We evaluated CoGS using both synthetic and real-world datasets that include dynamic objects that differ in degree of difficulty. In our evaluations, CoGS consistently outperformed existing dynamic and controllable neural representations in terms of visual fidelity.",
    "arxiv_url": "http://arxiv.org/abs/2312.05664v2",
    "pdf_url": "http://arxiv.org/pdf/2312.05664v2",
    "published_date": "2023-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization",
    "authors": [
      "Yahao Shi",
      "Yanmin Wu",
      "Chenming Wu",
      "Xing Liu",
      "Chen Zhao",
      "Haocheng Feng",
      "Jian Zhang",
      "Bin Zhou",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing 3D Gaussian representations to effectively factorize the scene into material properties, light, and geometry. The key contributions lie in three-fold. We compute the normal of each 3D Gaussian using the shortest eigenvector, with a directional masking scheme forcing accurate normal estimation without external supervision. We adopt an efficient voxel-based indirect illumination tracing scheme that stores direction-aware outgoing radiance in each 3D Gaussian to disentangle secondary illumination for approximating multi-bounce light transport. To further enhance the illumination disentanglement, we represent a high-resolution environmental map with a learnable low-resolution map and a lightweight, fully convolutional network. Our method achieves state-of-the-art performance in both relighting and novel view synthesis tasks among the recently proposed inverse rendering methods while achieving real-time rendering. This substantiates our proposed method's efficacy and broad applicability, highlighting its potential as an influential tool in various real-time interactive graphics applications such as material editing and relighting. The code will be released at https://github.com/guduxiaolang/GIR.",
    "arxiv_url": "http://arxiv.org/abs/2312.05133v2",
    "pdf_url": "http://arxiv.org/pdf/2312.05133v2",
    "published_date": "2023-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/guduxiaolang/GIR",
    "keywords": [
      "efficient",
      "relightable",
      "relighting",
      "lighting",
      "light transport",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "illumination",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting",
    "authors": [
      "Xiaofeng Yang",
      "Yiwen Chen",
      "Cheng Chen",
      "Chi Zhang",
      "Yi Xu",
      "Xulei Yang",
      "Fayao Liu",
      "Guosheng Lin"
    ],
    "abstract": "We propose a unified framework aimed at enhancing the diffusion priors for 3D generation tasks. Despite the critical importance of these tasks, existing methodologies often struggle to generate high-caliber results. We begin by examining the inherent limitations in previous diffusion priors. We identify a divergence between the diffusion priors and the training procedures of diffusion models that substantially impairs the quality of 3D generation. To address this issue, we propose a novel, unified framework that iteratively optimizes both the 3D model and the diffusion prior. Leveraging the different learnable parameters of the diffusion prior, our approach offers multiple configurations, affording various trade-offs between performance and implementation complexity. Notably, our experimental results demonstrate that our method markedly surpasses existing techniques, establishing new state-of-the-art in the realm of text-to-3D generation. Furthermore, our approach exhibits impressive performance on both NeRF and the newly introduced 3D Gaussian Splatting backbones. Additionally, our framework yields insightful contributions to the understanding of recent score distillation methods, such as the VSD and DDS loss.",
    "arxiv_url": "http://arxiv.org/abs/2312.04820v1",
    "pdf_url": "http://arxiv.org/pdf/2312.04820v1",
    "published_date": "2023-12-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
    "authors": [
      "Sharath Girish",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce per-point memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach develops a pruning stage which results in scene representations with fewer Gaussians, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce storage memory by more than an order of magnitude all while preserving the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x lesser memory and faster training/inference speed. Project page and code is available https://efficientgaussian.github.io",
    "arxiv_url": "http://arxiv.org/abs/2312.04564v3",
    "pdf_url": "http://arxiv.org/pdf/2312.04564v3",
    "published_date": "2023-12-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
    "authors": [
      "Yufan Chen",
      "Lizhen Wang",
      "Qijing Li",
      "Hongjiang Xiao",
      "Shengping Zhang",
      "Hongxun Yao",
      "Yebin Liu"
    ],
    "abstract": "The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.",
    "arxiv_url": "http://arxiv.org/abs/2312.04558v1",
    "pdf_url": "http://arxiv.org/pdf/2312.04558v1",
    "published_date": "2023-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "deformation",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relightable Gaussian Codec Avatars",
    "authors": [
      "Shunsuke Saito",
      "Gabriel Schwartz",
      "Tomas Simon",
      "Junxuan Li",
      "Giljoo Nam"
    ],
    "abstract": "The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.",
    "arxiv_url": "http://arxiv.org/abs/2312.03704v2",
    "pdf_url": "http://arxiv.org/pdf/2312.03704v2",
    "published_date": "2023-12-06",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "efficient",
      "high-fidelity",
      "relightable",
      "head",
      "vr",
      "reflection",
      "relighting",
      "lighting",
      "global illumination",
      "face",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "illumination",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting",
    "authors": [
      "Yuheng Jiang",
      "Zhehao Shen",
      "Penghao Wang",
      "Zhuo Su",
      "Yu Hong",
      "Yingliang Zhang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.",
    "arxiv_url": "http://arxiv.org/abs/2312.03461v2",
    "pdf_url": "http://arxiv.org/pdf/2312.03461v2",
    "published_date": "2023-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "tracking",
      "motion",
      "deformation",
      "4d",
      "3d gaussian",
      "human",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle",
    "authors": [
      "Youtian Lin",
      "Zuozhuo Dai",
      "Siyu Zhu",
      "Yao Yao"
    ],
    "abstract": "We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow",
    "arxiv_url": "http://arxiv.org/abs/2312.03431v1",
    "pdf_url": "http://arxiv.org/pdf/2312.03431v1",
    "published_date": "2023-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "3d reconstruction",
      "4d",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting",
    "authors": [
      "Vladimir Yugay",
      "Yue Li",
      "Theo Gevers",
      "Martin R. Oswald"
    ],
    "abstract": "We present a dense simultaneous localization and mapping (SLAM) method that uses 3D Gaussians as a scene representation. Our approach enables interactive-time reconstruction and photo-realistic rendering from real-world single-camera RGBD videos. To this end, we propose a novel effective strategy for seeding new Gaussians for newly explored areas and their effective online optimization that is independent of the scene size and thus scalable to larger scenes. This is achieved by organizing the scene into sub-maps which are independently optimized and do not need to be kept in memory. We further accomplish frame-to-model camera tracking by minimizing photometric and geometric losses between the input and rendered frames. The Gaussian representation allows for high-quality photo-realistic real-time rendering of real-world scenes. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance in mapping, tracking, and rendering compared to existing neural dense SLAM methods.",
    "arxiv_url": "http://arxiv.org/abs/2312.10070v2",
    "pdf_url": "http://arxiv.org/pdf/2312.10070v2",
    "published_date": "2023-12-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "mapping",
      "3d gaussian",
      "real-time rendering",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields",
    "authors": [
      "Shijie Zhou",
      "Haoran Chang",
      "Sicheng Jiang",
      "Zhiwen Fan",
      "Zehao Zhu",
      "Dejia Xu",
      "Pradyumna Chari",
      "Suya You",
      "Zhangyang Wang",
      "Achuta Kadambi"
    ],
    "abstract": "3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2312.03203v3",
    "pdf_url": "http://arxiv.org/pdf/2312.03203v3",
    "published_date": "2023-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "segmentation",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing",
    "authors": [
      "Yushi Lan",
      "Feitong Tan",
      "Di Qiu",
      "Qiangeng Xu",
      "Kyle Genova",
      "Zeng Huang",
      "Sean Fanello",
      "Rohit Pandey",
      "Thomas Funkhouser",
      "Chen Change Loy",
      "Yinda Zhang"
    ],
    "abstract": "We present a novel framework for generating photorealistic 3D human head and subsequently manipulating and reposing them with remarkable flexibility. The proposed approach leverages an implicit function representation of 3D human heads, employing 3D Gaussians anchored on a parametric face model. To enhance representational capabilities and encode spatial information, we embed a lightweight tri-plane payload within each Gaussian rather than directly storing color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space via a 3DMM, enabling effective utilization of the diffusion model for 3D head avatar generation. Our method facilitates the creation of diverse and realistic 3D human heads with fine-grained editing over facial features and expressions. Extensive experiments demonstrate the effectiveness of our method.",
    "arxiv_url": "http://arxiv.org/abs/2312.03763v3",
    "pdf_url": "http://arxiv.org/pdf/2312.03763v3",
    "published_date": "2023-12-05",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
    "authors": [
      "Shoukang Hu",
      "Ziwei Liu"
    ],
    "abstract": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.",
    "arxiv_url": "http://arxiv.org/abs/2312.02973v1",
    "pdf_url": "http://arxiv.org/pdf/2312.02973v1",
    "published_date": "2023-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting",
    "authors": [
      "Helisa Dhamo",
      "Yinyu Nie",
      "Arthur Moreau",
      "Jifei Song",
      "Richard Shaw",
      "Yiren Zhou",
      "Eduardo P√©rez-Pellitero"
    ],
    "abstract": "3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2dB, while accelerating rendering speed by over x10.",
    "arxiv_url": "http://arxiv.org/abs/2312.02902v2",
    "pdf_url": "http://arxiv.org/pdf/2312.02902v2",
    "published_date": "2023-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "animation",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HHAvatar: Gaussian Head Avatar with Dynamic Hairs",
    "authors": [
      "Zhanfeng Liao",
      "Yuelang Xu",
      "Zhe Li",
      "Qijing Li",
      "Boyao Zhou",
      "Ruifeng Bai",
      "Di Xu",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head",
    "arxiv_url": "http://arxiv.org/abs/2312.03029v3",
    "pdf_url": "http://arxiv.org/pdf/2312.03029v3",
    "published_date": "2023-12-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "sparse-view",
      "head",
      "high-fidelity",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "avatar",
      "dynamic",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis",
    "authors": [
      "Shunyuan Zheng",
      "Boyao Zhou",
      "Ruizhi Shao",
      "Boning Liu",
      "Shengping Zhang",
      "Liqiang Nie",
      "Yebin Liu"
    ],
    "abstract": "We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2312.02155v3",
    "pdf_url": "http://arxiv.org/pdf/2312.02155v3",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MANUS: Markerless Grasp Capture using Articulated 3D Gaussians",
    "authors": [
      "Chandradeep Pokhariya",
      "Ishaan N Shah",
      "Angela Xing",
      "Zekun Li",
      "Kefan Chen",
      "Avinash Sharma",
      "Srinath Sridhar"
    ],
    "abstract": "Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that does not represent hand shape accurately resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.",
    "arxiv_url": "http://arxiv.org/abs/2312.02137v2",
    "pdf_url": "http://arxiv.org/pdf/2312.02137v2",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "efficient",
      "high-fidelity",
      "understanding",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Re-Nerfing: Improving Novel View Synthesis through Novel View Synthesis",
    "authors": [
      "Felix Tristram",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "Recent neural rendering and reconstruction techniques, such as NeRFs or Gaussian Splatting, have shown remarkable novel view synthesis capabilities but require hundreds of images of the scene from diverse viewpoints to render high-quality novel views. With fewer images available, these methods start to fail since they can no longer correctly triangulate the underlying 3D geometry and converge to a non-optimal solution. These failures can manifest as floaters or blurry renderings in sparsely observed areas of the scene. In this paper, we propose Re-Nerfing, a simple and general add-on approach that leverages novel view synthesis itself to tackle this problem. Using an already trained NVS method, we render novel views between existing ones and augment the training data to optimize a second model. This introduces additional multi-view constraints and allows the second model to converge to a better solution. With Re-Nerfing we achieve significant improvements upon multiple pipelines based on NeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and LLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra supervision signals, making it a flexible and practical add-on.",
    "arxiv_url": "http://arxiv.org/abs/2312.02255v3",
    "pdf_url": "http://arxiv.org/pdf/2312.02255v3",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "geometry",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians",
    "authors": [
      "Liangxiao Hu",
      "Hongwen Zhang",
      "Yuxiang Zhang",
      "Boyao Zhou",
      "Boning Liu",
      "Shengping Zhang",
      "Liqiang Nie"
    ],
    "abstract": "We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2312.02134v3",
    "pdf_url": "http://arxiv.org/pdf/2312.02134v3",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "mapping",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM",
    "authors": [
      "Nikhil Keetha",
      "Jay Karhade",
      "Krishna Murthy Jatavallabhula",
      "Gengshan Yang",
      "Sebastian Scherer",
      "Deva Ramanan",
      "Jonathon Luiten"
    ],
    "abstract": "Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM achieves up to 2x superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity SLAM applications.",
    "arxiv_url": "http://arxiv.org/abs/2312.02126v3",
    "pdf_url": "http://arxiv.org/pdf/2312.02126v3",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "high-fidelity",
      "tracking",
      "fast",
      "mapping",
      "3d gaussian",
      "slam",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mathematical Supplement for the $\\texttt{gsplat}$ Library",
    "authors": [
      "Vickie Ye",
      "Angjoo Kanazawa"
    ],
    "abstract": "This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .",
    "arxiv_url": "http://arxiv.org/abs/2312.02121v1",
    "pdf_url": "http://arxiv.org/pdf/2312.02121v1",
    "published_date": "2023-12-04",
    "categories": [
      "cs.MS",
      "cs.CV",
      "cs.GR",
      "cs.NA",
      "math.NA"
    ],
    "github_url": "https://github.com/nerfstudio-project/gsplat",
    "keywords": [
      "nerf",
      "efficient",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians",
    "authors": [
      "Shenhan Qian",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Davide Davoli",
      "Simon Giebenhain",
      "Matthias Nie√üner"
    ],
    "abstract": "We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.",
    "arxiv_url": "http://arxiv.org/abs/2312.02069v2",
    "pdf_url": "http://arxiv.org/pdf/2312.02069v2",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "head",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes",
    "authors": [
      "Yi-Hua Huang",
      "Yang-Tian Sun",
      "Ziyi Yang",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ],
    "abstract": "Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications. Project page: https://yihua7.github.io/SC-GS-web/",
    "arxiv_url": "http://arxiv.org/abs/2312.14937v3",
    "pdf_url": "http://arxiv.org/pdf/2312.14937v3",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation",
    "authors": [
      "Jie Wang",
      "Jiu-Cheng Xie",
      "Xianyan Li",
      "Feng Xu",
      "Chi-Man Pun",
      "Hao Gao"
    ],
    "abstract": "Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging. This paper presents GaussianHead, which models the actional human head with anisotropic 3D Gaussians. In our framework, a motion deformation field and multi-resolution tri-plane are constructed respectively to deal with the head's dynamic geometry and complex texture. Notably, we impose an exclusive derivation scheme on each Gaussian, which generates its multiple doppelgangers through a set of learnable parameters for position transformation. With this design, we can compactly and accurately encode the appearance information of Gaussians, even those fitting the head's particular components with sophisticated structures. In addition, an inherited derivation strategy for newly added Gaussians is adopted to facilitate training acceleration. Extensive experiments show that our method can produce high-fidelity renderings, outperforming state-of-the-art approaches in reconstruction, cross-identity reenactment, and novel view synthesis tasks. Our code is available at: https://github.com/chiehwangs/gaussian-head.",
    "arxiv_url": "http://arxiv.org/abs/2312.01632v5",
    "pdf_url": "http://arxiv.org/pdf/2312.01632v5",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chiehwangs/gaussian-head",
    "keywords": [
      "avatar",
      "head",
      "high-fidelity",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "acceleration",
      "human",
      "ar",
      "animation",
      "compact",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "abstract": "Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model moving 3-D scenes. However, current methods are resource intensive and challenging to compress. To address this, we present WavePlanes, a fast and more compact hex plane representation, applicable to both Neural Radiance Fields and Gaussian Splatting methods. Rather than modeling many feature scales separately (as done previously), we use the inverse discrete wavelet transform to reconstruct features at varying scales. This leads to a more compact representation and allows us to explore wavelet-based compression schemes for further gains. The proposed compression scheme exploits the sparsity of wavelet coefficients, by applying hard thresholding to the wavelet planes and storing nonzero coefficients and their locations on each plane in a Hash Map. Compared to the state-of-the-art (SotA), WavePlanes is significantly smaller, less resource demanding and competitive in reconstruction quality. Compared to small SotA models, WavePlanes outperforms methods in both model size and quality of novel views.",
    "arxiv_url": "http://arxiv.org/abs/2312.02218v4",
    "pdf_url": "http://arxiv.org/pdf/2312.02218v4",
    "published_date": "2023-12-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "dynamic",
      "ar",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding",
    "authors": [
      "Jun Xiang",
      "Xuan Gao",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "abstract": "We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/",
    "arxiv_url": "http://arxiv.org/abs/2312.02214v2",
    "pdf_url": "http://arxiv.org/pdf/2312.02214v2",
    "published_date": "2023-12-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "avatar",
      "lightweight"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction",
    "authors": [
      "Devikalyan Das",
      "Christopher Wewer",
      "Raza Yunus",
      "Eddy Ilg",
      "Jan Eric Lenssen"
    ],
    "abstract": "Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, high-quality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object's deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.",
    "arxiv_url": "http://arxiv.org/abs/2312.01196v2",
    "pdf_url": "http://arxiv.org/pdf/2312.01196v2",
    "published_date": "2023-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D",
    "authors": [
      "Pengsheng Guo",
      "Hans Hao",
      "Adam Caccavale",
      "Zhongzheng Ren",
      "Edward Zhang",
      "Qi Shan",
      "Aditya Sankar",
      "Alexander G. Schwing",
      "Alex Colburn",
      "Fangchang Ma"
    ],
    "abstract": "In the realm of text-to-3D generation, utilizing 2D diffusion models through score distillation sampling (SDS) frequently leads to issues such as blurred appearances and multi-faced geometry, primarily due to the intrinsically noisy nature of the SDS loss. Our analysis identifies the core of these challenges as the interaction among noise levels in the 2D diffusion process, the architecture of the diffusion network, and the 3D model representation. To overcome these limitations, we present StableDreamer, a methodology incorporating three advances. First, inspired by InstructNeRF2NeRF, we formalize the equivalence of the SDS generative prior and a simple supervised L2 reconstruction loss. This finding provides a novel tool to debug SDS, which we use to show the impact of time-annealing noise levels on reducing multi-faced geometries. Second, our analysis shows that while image-space diffusion contributes to geometric precision, latent-space diffusion is crucial for vivid color rendition. Based on this observation, StableDreamer introduces a two-stage training strategy that effectively combines these aspects, resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance the overall quality, reduce memory usage during training, and accelerate rendering speeds, and better capture semi-transparent objects. StableDreamer reduces multi-face geometries, generates fine details, and converges stably.",
    "arxiv_url": "http://arxiv.org/abs/2312.02189v1",
    "pdf_url": "http://arxiv.org/pdf/2312.02189v1",
    "published_date": "2023-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines",
    "authors": [
      "Sankeerth Durvasula",
      "Adrian Zhao",
      "Fan Chen",
      "Ruofan Liang",
      "Pawan Kumar Sanjaya",
      "Nandita Vijaykumar"
    ],
    "abstract": "Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent. Recent works (e.g. 3D Gaussian Splatting) use a rasterization pipeline to enable rendering high quality photo-realistic imagery at high speeds from these learned 3D models. These methods have been demonstrated to be very promising, providing state-of-art quality for many important tasks. However, training a model to represent a scene is still a time-consuming task even when using powerful GPUs. In this work, we observe that the gradient computation phase during training is a significant bottleneck on GPUs due to the large number of atomic operations that need to be processed. These atomic operations overwhelm atomic units in the L2 partitions causing stalls. To address this challenge, we leverage the observations that during the gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amounts of atomic traffic (since some threads may be inactive). We propose DISTWAR, a software-approach to accelerate atomic operations based on two key ideas: First, we enable warp-level reduction of threads at the SM sub-cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute the atomic computation between the warp-level reduction at the SM and the L2 atomic units to increase the throughput of atomic computation. Warps with many threads performing atomic updates to the same memory locations are scheduled at the SM, and the rest using L2 atomic units. We implement DISTWAR using existing warp-level primitives. We evaluate DISTWAR on widely used raster-based differentiable rendering workloads. We demonstrate significant speedups of 2.44x on average (up to 5.7x).",
    "arxiv_url": "http://arxiv.org/abs/2401.05345v1",
    "pdf_url": "http://arxiv.org/pdf/2401.05345v1",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.PF"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment Any 3D Gaussians",
    "authors": [
      "Jiazhong Cen",
      "Jiemin Fang",
      "Chen Yang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D promptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D visual prompts as input, SAGA can segment the corresponding 3D target represented by 3D Gaussians within 4 ms. This is achieved by attaching an scale-gated affinity feature to each 3D Gaussian to endow it a new property towards multi-granularity segmentation. Specifically, a scale-aware contrastive training strategy is proposed for the scale-gated affinity feature learning. It 1) distills the segmentation capability of the Segment Anything Model (SAM) from 2D masks into the affinity features and 2) employs a soft scale gate mechanism to deal with multi-granularity ambiguity in 3D segmentation through adjusting the magnitude of each feature channel according to a specified 3D physical scale. Evaluations demonstrate that SAGA achieves real-time multi-granularity segmentation with quality comparable to state-of-the-art methods. As one of the first methods addressing promptable segmentation in 3D-GS, the simplicity and effectiveness of SAGA pave the way for future advancements in this field. Our code will be released.",
    "arxiv_url": "http://arxiv.org/abs/2312.00860v3",
    "pdf_url": "http://arxiv.org/pdf/2312.00860v3",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "gaussian splatting",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
    "authors": [
      "Mingqiao Ye",
      "Martin Danelljan",
      "Fisher Yu",
      "Lei Ke"
    ],
    "abstract": "The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by Segment Anything Model (SAM), along with introduced 3D spatial consistency regularization. Compared to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization, style transfer and scene recomposition. Our code and models are at https://github.com/lkeab/gaussian-grouping.",
    "arxiv_url": "http://arxiv.org/abs/2312.00732v2",
    "pdf_url": "http://arxiv.org/pdf/2312.00732v2",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/lkeab/gaussian-grouping",
    "keywords": [
      "understanding",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting",
    "authors": [
      "Zehao Zhu",
      "Zhiwen Fan",
      "Yifan Jiang",
      "Zhangyang Wang"
    ],
    "abstract": "Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.",
    "arxiv_url": "http://arxiv.org/abs/2312.00451v2",
    "pdf_url": "http://arxiv.org/pdf/2312.00451v2",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance",
    "authors": [
      "Hanlin Chen",
      "Chen Li",
      "Yunsong Wang",
      "Gim Hee Lee"
    ],
    "abstract": "Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization. However, the reconstruction results still lack fine details because of the over-smoothed depth map or sparse point cloud. In this work, we propose a neural implicit surface reconstruction pipeline with guidance from 3D Gaussian Splatting to recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is that it can generate dense point clouds with detailed structure. Nonetheless, a naive adoption of 3D Gaussian Splatting can fail since the generated points are the centers of 3D Gaussians that do not necessarily lie on the surface. We thus introduce a scale regularizer to pull the centers close to the surface by enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine the point cloud from 3D Gaussians Splatting with the normal priors from the surface predicted by neural implicit models instead of using a fixed set of points as guidance. Consequently, the quality of surface reconstruction improves from the guidance of the more accurate 3D Gaussian splatting. By jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our approach benefits from both representations and generates complete surfaces with intricate details. Experiments on Tanks and Temples verify the effectiveness of our proposed method.",
    "arxiv_url": "http://arxiv.org/abs/2312.00846v2",
    "pdf_url": "http://arxiv.org/pdf/2312.00846v2",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparseGS: Real-Time 360¬∞ Sparse View Synthesis using Gaussian Splatting",
    "authors": [
      "Haolin Xiong",
      "Sairisheek Muttukuru",
      "Rishi Upadhyay",
      "Pradyumna Chari",
      "Achuta Kadambi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently enabled real-time rendering of unbounded 3D scenes for novel view synthesis. However, this technique requires dense training views to accurately reconstruct 3D geometry. A limited number of input views will significantly degrade reconstruction quality, resulting in artifacts such as \"floaters\" and \"background collapse\" at unseen viewpoints. In this work, we introduce SparseGS, an efficient training pipeline designed to address the limitations of 3DGS in scenarios with sparse training views. SparseGS incorporates depth priors, novel depth rendering techniques, and a pruning heuristic to mitigate floater artifacts, alongside an Unseen Viewpoint Regularization module to alleviate background collapses. Our extensive evaluations on the Mip-NeRF360, LLFF, and DTU datasets demonstrate that SparseGS achieves high-quality reconstruction in both unbounded and forward-facing scenarios, with as few as 12 and 3 input images, respectively, while maintaining fast training and real-time rendering capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2312.00206v3",
    "pdf_url": "http://arxiv.org/pdf/2312.00206v3",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting",
    "authors": [
      "Agelos Kratimenos",
      "Jiahui Lei",
      "Kostas Daniilidis"
    ],
    "abstract": "Accurately and efficiently modeling dynamic scenes and motions is considered so challenging a task due to temporal dynamics and motion complexity. To address these challenges, we propose DynMF, a compact and efficient representation that decomposes a dynamic scene into a few neural trajectories. We argue that the per-point motions of a dynamic scene can be decomposed into a small set of explicit or learned trajectories. Our carefully designed neural framework consisting of a tiny set of learned basis queried only in time allows for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while at the same time, requiring only double the storage compared to static scenes. Our neural representation adequately constrains the inherently underconstrained motion field of a dynamic scene leading to effective and fast optimization. This is done by biding each point to motion coefficients that enforce the per-point sharing of basis trajectories. By carefully applying a sparsity loss to the motion coefficients, we are able to disentangle the motions that comprise the scene, independently control them, and generate novel motion combinations that have never been seen before. We can reach state-of-the-art render quality within just 5 minutes of training and in less than half an hour, we can synthesize novel views of dynamic scenes with superior photorealistic quality. Our representation is interpretable, efficient, and expressive enough to offer real-time view synthesis of complex dynamic scene motions, in monocular and multi-view scenarios.",
    "arxiv_url": "http://arxiv.org/abs/2312.00112v2",
    "pdf_url": "http://arxiv.org/pdf/2312.00112v2",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "fast",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object Manipulation",
    "authors": [
      "Bardienus P. Duisterhof",
      "Zhao Mandi",
      "Yunchao Yao",
      "Jia-Wei Liu",
      "Jenny Seidenschwarz",
      "Mike Zheng Shou",
      "Deva Ramanan",
      "Shuran Song",
      "Stan Birchfield",
      "Bowen Wen",
      "Jeffrey Ichnowski"
    ],
    "abstract": "Teaching robots to fold, drape, or reposition deformable objects such as cloth will unlock a variety of automation applications. While remarkable progress has been made for rigid object manipulation, manipulating deformable objects poses unique challenges, including frequent occlusions, infinite-dimensional state spaces and complex dynamics. Just as object pose estimation and tracking have aided robots for rigid manipulation, dense 3D tracking (scene flow) of highly deformable objects will enable new applications in robotics while aiding existing approaches, such as imitation learning or creating digital twins with real2sim transfer. We propose DeformGS, an approach to recover scene flow in highly deformable scenes, using simultaneous video captures of a dynamic scene from multiple cameras. DeformGS builds on recent advances in Gaussian splatting, a method that learns the properties of a large number of Gaussians for state-of-the-art and fast novel-view synthesis. DeformGS learns a deformation function to project a set of Gaussians with canonical properties into world space. The deformation function uses a neural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow scalar. We enforce physics-inspired regularization terms based on conservation of momentum and isometry, which leads to trajectories with smaller trajectory errors. We also leverage existing foundation models SAM and XMEM to produce noisy masks, and learn a per-Gaussian mask for better physics-inspired regularization. DeformGS achieves high-quality 3D tracking on highly deformable scenes with shadows and occlusions. In experiments, DeformGS improves 3D tracking by an average of 55.8% compared to the state-of-the-art. With sufficient texture, DeformGS achieves a median tracking error of 3.3 mm on a cloth of 1.5 x 1.5 m in area. Website: https://deformgs.github.io",
    "arxiv_url": "http://arxiv.org/abs/2312.00583v2",
    "pdf_url": "http://arxiv.org/pdf/2312.00583v2",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "tracking",
      "shadow",
      "fast",
      "deformation",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering",
    "authors": [
      "Tao Lu",
      "Mulin Yu",
      "Linning Xu",
      "Yuanbo Xiangli",
      "Limin Wang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved the state-of-the-art rendering quality and speed combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less area and lighting effects. We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrates an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing the rendering speed.",
    "arxiv_url": "http://arxiv.org/abs/2312.00109v1",
    "pdf_url": "http://arxiv.org/pdf/2312.00109v1",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "geometry",
      "3d gaussian",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering",
    "authors": [
      "Yurui Chen",
      "Chun Gu",
      "Junzhe Jiang",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent and large scene representation learning with sparse training data, we introduce a novel temporal smoothing mechanism and a position-aware adaptive control strategy respectively. Extensive experiments on Waymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 900-fold acceleration in rendering over the best alternative.",
    "arxiv_url": "http://arxiv.org/abs/2311.18561v2",
    "pdf_url": "http://arxiv.org/pdf/2311.18561v2",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "acceleration",
      "real-time rendering",
      "ar",
      "large scene",
      "urban scene",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding",
    "authors": [
      "Jin-Chuan Shi",
      "Miao Wang",
      "Hao-Bin Duan",
      "Shao-Hua Guan"
    ],
    "abstract": "Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language-embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language-embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.",
    "arxiv_url": "http://arxiv.org/abs/2311.18482v1",
    "pdf_url": "http://arxiv.org/pdf/2311.18482v1",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "semantic",
      "understanding",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization",
    "authors": [
      "KL Navaneet",
      "Kossar Pourahmadi Meibodi",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a new method for modeling and rendering 3D radiance fields that achieves much faster learning and rendering time compared to SOTA NeRF methods. However, it comes with a drawback in the much larger storage demand compared to NeRF methods since it needs to store the parameters for several 3D Gaussians. We notice that many Gaussians may share similar parameters, so we introduce a simple vector quantization method based on K-means to quantize the Gaussian parameters while optimizing them. Then, we store the small codebook along with the index of the code for each Gaussian. We compress the indices further by sorting them and using a method similar to run-length encoding. Moreover, we use a simple regularizer to encourage zero opacity (invisible Gaussians) to reduce the storage and rendering time by a large factor through reducing the number of Gaussians. We do extensive experiments on standard benchmarks as well as an existing 3D dataset that is an order of magnitude larger than the standard benchmarks used in this field. We show that our simple yet effective method can reduce the storage cost for 3DGS by 40 to 50x and rendering time by 2 to 3x with a very small drop in the quality of rendered images.",
    "arxiv_url": "http://arxiv.org/abs/2311.18159v3",
    "pdf_url": "http://arxiv.org/pdf/2311.18159v3",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HUGS: Human Gaussian Splats",
    "authors": [
      "Muhammed Kocabas",
      "Jen-Hao Rick Chang",
      "James Gabriel",
      "Oncel Tuzel",
      "Anurag Ranjan"
    ],
    "abstract": "Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",
    "arxiv_url": "http://arxiv.org/abs/2311.17910v1",
    "pdf_url": "http://arxiv.org/pdf/2311.17910v1",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/apple/ml-hugs",
    "keywords": [
      "fast",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "animation",
      "neural rendering",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting",
    "authors": [
      "Alexander Vilesov",
      "Pradyumna Chari",
      "Achuta Kadambi"
    ],
    "abstract": "With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.",
    "arxiv_url": "http://arxiv.org/abs/2311.17907v1",
    "pdf_url": "http://arxiv.org/pdf/2311.17907v1",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "face",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Shell Maps for Efficient 3D Human Generation",
    "authors": [
      "Rameen Abdal",
      "Wang Yifan",
      "Zifan Shi",
      "Yinghao Xu",
      "Ryan Po",
      "Zhengfei Kuang",
      "Qifeng Chen",
      "Dit-Yan Yeung",
      "Gordon Wetzstein"
    ],
    "abstract": "Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \\times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",
    "arxiv_url": "http://arxiv.org/abs/2311.17857v1",
    "pdf_url": "http://arxiv.org/pdf/2311.17857v1",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces",
    "authors": [
      "Yingwenqi Jiang",
      "Jiadong Tu",
      "Yuan Liu",
      "Xifeng Gao",
      "Xiaoxiao Long",
      "Wenping Wang",
      "Yuexin Ma"
    ],
    "abstract": "The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds. However, the explicit and discrete representation encounters challenges when applied to scenes featuring reflective surfaces. In this paper, we present GaussianShader, a novel method that applies a simplified shading function on 3D Gaussians to enhance the neural rendering in scenes with reflective surfaces while preserving the training and rendering efficiency. The main challenge in applying the shading function lies in the accurate normal estimation on discrete 3D Gaussians. Specifically, we proposed a novel normal estimation framework based on the shortest axis directions of 3D Gaussians with a delicately designed loss to make the consistency between the normals and the geometries of Gaussian spheres. Experiments show that GaussianShader strikes a commendable balance between efficiency and visual quality. Our method surpasses Gaussian Splatting in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When compared to prior works handling reflective surfaces, such as Ref-NeRF, our optimization time is significantly accelerated (23h vs. 0.58h). Please click on our project website to see more results.",
    "arxiv_url": "http://arxiv.org/abs/2311.17977v1",
    "pdf_url": "http://arxiv.org/pdf/2311.17977v1",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS",
    "authors": [
      "Zhiwen Fan",
      "Kevin Wang",
      "Kairun Wen",
      "Zehao Zhu",
      "Dejia Xu",
      "Zhangyang Wang"
    ],
    "abstract": "Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce LightGaussian, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussian's global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average 15x compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities.",
    "arxiv_url": "http://arxiv.org/abs/2311.17245v6",
    "pdf_url": "http://arxiv.org/pdf/2311.17245v6",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "motion",
      "3d gaussian",
      "ar",
      "nerf",
      "neural rendering",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting",
    "authors": [
      "Xian Liu",
      "Xiaohang Zhan",
      "Jiaxiang Tang",
      "Ying Shan",
      "Gang Zeng",
      "Dahua Lin",
      "Xihui Liu",
      "Ziwei Liu"
    ],
    "abstract": "Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian",
    "arxiv_url": "http://arxiv.org/abs/2311.17061v2",
    "pdf_url": "http://arxiv.org/pdf/2311.17061v2",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields",
    "authors": [
      "Jiajun Huang",
      "Hongchuan Yu"
    ],
    "abstract": "We propose Point'n Move, a method that achieves interactive scene object manipulation with exposed region inpainting. Interactivity here further comes from intuitive object selection and real-time editing. To achieve this, we adopt Gaussian Splatting Radiance Field as the scene representation and fully leverage its explicit nature and speed advantage. Its explicit representation formulation allows us to devise a 2D prompt points to 3D mask dual-stage self-prompting segmentation algorithm, perform mask refinement and merging, minimize change as well as provide good initialization for scene inpainting and perform editing in real-time without per-editing training, all leads to superior quality and performance. We test our method by performing editing on both forward-facing and 360 scenes. We also compare our method against existing scene object removal methods, showing superior quality despite being more capable and having a speed advantage.",
    "arxiv_url": "http://arxiv.org/abs/2311.16737v1",
    "pdf_url": "http://arxiv.org/pdf/2311.16737v1",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human Gaussian Splatting: Real-time Rendering of Animatable Avatars",
    "authors": [
      "Arthur Moreau",
      "Jifei Song",
      "Helisa Dhamo",
      "Richard Shaw",
      "Yiren Zhou",
      "Eduardo P√©rez-Pellitero"
    ],
    "abstract": "This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose an animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. The body is represented by a set of gaussian primitives in a canonical space which is deformed with a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (HuGS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4 dataset while being able to render in real-time (80 fps for 512x512 resolution).",
    "arxiv_url": "http://arxiv.org/abs/2311.17113v2",
    "pdf_url": "http://arxiv.org/pdf/2311.17113v2",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "body",
      "3d gaussian",
      "real-time rendering",
      "human",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering",
    "authors": [
      "Zhiwen Yan",
      "Weng Fei Low",
      "Yu Chen",
      "Gim Hee Lee"
    ],
    "abstract": "3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\\%-66\\% PSNR and 160\\%-2400\\% rendering speed improvement at 4$\\times$-128$\\times$ scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splitting. Our code and more results are available on our project website https://jokeryan.github.io/projects/ms-gs/",
    "arxiv_url": "http://arxiv.org/abs/2311.17089v2",
    "pdf_url": "http://arxiv.org/pdf/2311.17089v2",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GART: Gaussian Articulated Template Models",
    "authors": [
      "Jiahui Lei",
      "Yufu Wang",
      "Georgios Pavlakos",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "abstract": "We introduce Gaussian Articulated Template Model GART, an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos. GART utilizes a mixture of moving 3D Gaussians to explicitly approximate a deformable subject's geometry and appearance. It takes advantage of a categorical template model prior (SMPL, SMAL, etc.) with learnable forward skinning while further generalizing to more complex non-rigid deformations with novel latent bones. GART can be reconstructed via differentiable rendering from monocular videos in seconds or minutes and rendered in novel poses faster than 150fps.",
    "arxiv_url": "http://arxiv.org/abs/2311.16099v1",
    "pdf_url": "http://arxiv.org/pdf/2311.16099v1",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling",
    "authors": [
      "Zhe Li",
      "Yipengjing Sun",
      "Zerong Zheng",
      "Lizhen Wang",
      "Shengping Zhang",
      "Yebin Liu"
    ],
    "abstract": "Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front & back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. To tackle the realistic relighting of animatable avatars, we introduce physically-based rendering into the avatar representation for decomposing avatar materials and environment illumination. Overall, our method can create lifelike avatars with dynamic, realistic, generalized and relightable appearances. Experiments show that our method outperforms other state-of-the-art approaches.",
    "arxiv_url": "http://arxiv.org/abs/2311.16096v4",
    "pdf_url": "http://arxiv.org/pdf/2311.16096v4",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "illumination",
      "high-fidelity",
      "relightable",
      "relighting",
      "lighting",
      "3d gaussian",
      "human",
      "ar",
      "nerf",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing",
    "authors": [
      "Jian Gao",
      "Chun Gu",
      "Youtian Lin",
      "Zhihao Li",
      "Hao Zhu",
      "Xun Cao",
      "Li Zhang",
      "Yao Yao"
    ],
    "abstract": "In this paper, we present a novel differentiable point-based rendering framework to achieve photo-realistic relighting. To make the reconstructed scene relightable, we enhance vanilla 3D Gaussians by associating extra properties, including normal vectors, BRDF parameters, and incident lighting from various directions. From a collection of multi-view images, the 3D scene is optimized through 3D Gaussian Splatting while BRDF and lighting are decomposed by physically based differentiable rendering. To produce plausible shadow effects in photo-realistic relighting, we introduce an innovative point-based ray tracing with the bounding volume hierarchies for efficient visibility pre-computation. Extensive experiments demonstrate our improved BRDF estimation, novel view synthesis and relighting results compared to state-of-the-art approaches. The proposed framework showcases the potential to revolutionize the mesh-based graphics pipeline with a point-based pipeline enabling editing, tracing, and relighting.",
    "arxiv_url": "http://arxiv.org/abs/2311.16043v2",
    "pdf_url": "http://arxiv.org/pdf/2311.16043v2",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "ray tracing",
      "relighting",
      "lighting",
      "shadow",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions",
    "authors": [
      "Junjie Wang",
      "Jiemin Fang",
      "Xiaopeng Zhang",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "abstract": "Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes -- 2 hours).",
    "arxiv_url": "http://arxiv.org/abs/2311.16037v2",
    "pdf_url": "http://arxiv.org/pdf/2311.16037v2",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mip-Splatting: Alias-free 3D Gaussian Splatting",
    "authors": [
      "Zehao Yu",
      "Anpei Chen",
      "Binbin Huang",
      "Torsten Sattler",
      "Andreas Geiger"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency. However, strong artifacts can be observed when changing the sampling rate, \\eg, by changing focal length or camera distance. We find that the source for this phenomenon can be attributed to the lack of 3D frequency constraints and the usage of a 2D dilation filter. To address this problem, we introduce a 3D smoothing filter which constrains the size of the 3D Gaussian primitives based on the maximal sampling frequency induced by the input views, eliminating high-frequency artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip filter, which simulates a 2D box filter, effectively mitigates aliasing and dilation issues. Our evaluation, including scenarios such a training on single-scale images and testing on multiple scales, validates the effectiveness of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2311.16493v1",
    "pdf_url": "http://arxiv.org/pdf/2311.16493v1",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars",
    "authors": [
      "Yang Liu",
      "Xiang Huang",
      "Minghan Qin",
      "Qinwei Lin",
      "Haoqian Wang"
    ],
    "abstract": "Neural radiance fields are capable of reconstructing high-quality drivable human avatars but are expensive to train and render and not suitable for multi-human scenes with complex shadows. To reduce consumption, we propose Animatable 3D Gaussian, which learns human avatars from input images and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of skinned 3D Gaussians and a corresponding skeleton in canonical space and deforming 3D Gaussians to posed space according to the input poses. We introduce a multi-head hash encoder for pose-dependent shape and appearance and a time-dependent ambient occlusion module to achieve high-quality reconstructions in scenes containing complex motions and dynamic shadows. On both novel view synthesis and novel pose synthesis tasks, our method achieves higher reconstruction quality than InstantAvatar with less training time (1/60), less GPU memory (1/4), and faster rendering speed (7x). Our method can be easily extended to multi-human scenes and achieve comparable novel view synthesis results on a scene with ten people in only 25 seconds of training.",
    "arxiv_url": "http://arxiv.org/abs/2311.16482v3",
    "pdf_url": "http://arxiv.org/pdf/2311.16482v3",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "motion",
      "shadow",
      "fast",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-IR: 3D Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Zhihao Liang",
      "Qi Zhang",
      "Ying Feng",
      "Ying Shan",
      "Kui Jia"
    ],
    "abstract": "We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian Splatting (GS) that leverages forward mapping volume rendering to achieve photorealistic novel view synthesis and relighting results. Unlike previous works that use implicit neural representations and volume rendering (e.g. NeRF), which suffer from low expressive power and high computational complexity, we extend GS, a top-performance representation for novel view synthesis, to estimate scene geometry, surface material, and environment illumination from multi-view images captured under unknown lighting conditions. There are two main problems when introducing GS to inverse rendering: 1) GS does not support producing plausible normal natively; 2) forward mapping (e.g. rasterization and splatting) cannot trace the occlusion like backward mapping (e.g. ray tracing). To address these challenges, our GS-IR proposes an efficient optimization scheme that incorporates a depth-derivation-based regularization for normal estimation and a baking-based occlusion to model indirect lighting. The flexible and expressive GS representation allows us to achieve fast and compact geometry reconstruction, photorealistic novel view synthesis, and effective physically-based rendering. We demonstrate the superiority of our method over baseline methods through qualitative and quantitative evaluations on various challenging scenes.",
    "arxiv_url": "http://arxiv.org/abs/2311.16473v3",
    "pdf_url": "http://arxiv.org/pdf/2311.16473v3",
    "published_date": "2023-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "efficient",
      "ray tracing",
      "relighting",
      "lighting",
      "fast",
      "face",
      "mapping",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "compact",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting",
    "authors": [
      "Yiwen Chen",
      "Zilong Chen",
      "Chi Zhang",
      "Feng Wang",
      "Xiaofeng Yang",
      "Yikai Wang",
      "Zhongang Cai",
      "Lei Yang",
      "Huaping Liu",
      "Guosheng Lin"
    ],
    "abstract": "3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/",
    "arxiv_url": "http://arxiv.org/abs/2311.14521v4",
    "pdf_url": "http://arxiv.org/pdf/2311.14521v4",
    "published_date": "2023-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Gaussian Representation for Radiance Field",
    "authors": [
      "Joo Chan Lee",
      "Daniel Rho",
      "Xiangyu Sun",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in capturing complex 3D scenes with high fidelity. However, one persistent challenge that hinders the widespread adoption of NeRFs is the computational bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussisan-based representation and adopts the rasterization pipeline to render the images rather than volumetric rendering, achieving very fast rendering speed and promising image quality. However, a significant drawback arises as 3DGS entails a substantial number of 3D Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while preserving high performance. In addition, we propose a compact but effective representation of view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics. Finally, we learn codebooks to compactly represent the geometric attributes of Gaussian by vector quantization. With model compression techniques such as quantization and entropy coding, we consistently show over 25$\\times$ reduced storage and enhanced rendering speed, while maintaining the quality of the scene representation, compared to 3DGS. Our work provides a comprehensive framework for 3D scene representation, achieving high performance, fast training, compactness, and real-time rendering. Our project page is available at https://maincold2.github.io/c3dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2311.13681v2",
    "pdf_url": "http://arxiv.org/pdf/2311.13681v2",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "nerf",
      "compact",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions",
    "authors": [
      "Keyang Ye",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We present a novel animatable 3D Gaussian model for rendering high-fidelity free-view human motions in real time. Compared to existing NeRF-based methods, the model owns better capability in synthesizing high-frequency details without the jittering problem across video frames. The core of our model is a novel augmented 3D Gaussian representation, which attaches each Gaussian with a learnable code. The learnable code serves as a pose-dependent appearance embedding for refining the erroneous appearance caused by geometric transformation of Gaussians, based on which an appearance refinement model is learned to produce residual Gaussian properties to match the appearance in target pose. To force the Gaussians to learn the foreground human only without background interference, we further design a novel alpha loss to explicitly constrain the Gaussians within the human body. We also propose to jointly optimize the human joint parameters to improve the appearance accuracy. The animatable 3D Gaussian model can be learned with shallow MLPs, so new human motions can be synthesized in real time (66 fps on avarage). Experiments show that our model has superior performance over NeRF-based methods.",
    "arxiv_url": "http://arxiv.org/abs/2311.13404v2",
    "pdf_url": "http://arxiv.org/pdf/2311.13404v2",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images",
    "authors": [
      "Jaeyoung Chung",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ],
    "abstract": "In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting. Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality. However, it tends to overfit the training views when only a small number of images are available. To address this issue, we introduce a dense depth map as a geometry guide to mitigate overfitting. We obtained the depth map using a pre-trained monocular depth estimation model and aligning the scale and offset using sparse COLMAP feature points. The adjusted depth aids in the color-based optimization of 3D Gaussian splatting, mitigating floating artifacts, and ensuring adherence to geometric constraints. We verify the proposed method on the NeRF-LLFF dataset with varying numbers of few images. Our approach demonstrates robust geometry compared to the original method that relies solely on images. Project page: robot0321.github.io/DepthRegGS",
    "arxiv_url": "http://arxiv.org/abs/2311.13398v3",
    "pdf_url": "http://arxiv.org/pdf/2311.13398v3",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "geometry",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes",
    "authors": [
      "Jaeyoung Chung",
      "Suyoung Lee",
      "Hyeongjin Nam",
      "Jaerin Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene. Project page: https://luciddreamer-cvlab.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2311.13384v2",
    "pdf_url": "http://arxiv.org/pdf/2311.13384v2",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "vr",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering",
    "authors": [
      "Antoine Gu√©don",
      "Vincent Lepetit"
    ],
    "abstract": "We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",
    "arxiv_url": "http://arxiv.org/abs/2311.12775v3",
    "pdf_url": "http://arxiv.org/pdf/2311.12775v3",
    "published_date": "2023-11-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relighting",
      "lighting",
      "fast",
      "face",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis",
    "authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable success in synthesizing novel views given multiple views of a static scene. Yet, 3DGS faces challenges when applied to dynamic scenes because 3D Gaussian parameters need to be updated per timestep, requiring a large amount of memory and at least a dozen observations per timestep. To address these limitations, we present a compact dynamic 3D Gaussian representation that models positions and rotations as functions of time with a few parameter approximations while keeping other properties of 3DGS including scale, color and opacity invariant. Our method can dramatically reduce memory usage and relax a strict multi-view assumption. In our experiments on monocular and multi-view scenarios, we show that our method not only matches state-of-the-art methods, often linked with slower rendering speeds, in terms of high rendering quality but also significantly surpasses them by achieving a rendering speed of $118$ frames per second (FPS) at a resolution of 1,352$\\times$1,014 on a single GPU.",
    "arxiv_url": "http://arxiv.org/abs/2311.12897v2",
    "pdf_url": "http://arxiv.org/pdf/2311.12897v2",
    "published_date": "2023-11-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d gaussian",
      "ar",
      "compact",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics",
    "authors": [
      "Tianyi Xie",
      "Zeshun Zong",
      "Yuxing Qiu",
      "Xuan Li",
      "Yutao Feng",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "abstract": "We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, \"cage meshes,\" or any other geometry embedding, highlighting the principle of \"what you see is what you simulate (WS$^2$).\" Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. Our project page is at: https://xpandora.github.io/PhysGaussian/",
    "arxiv_url": "http://arxiv.org/abs/2311.12198v3",
    "pdf_url": "http://arxiv.org/pdf/2311.12198v3",
    "published_date": "2023-11-20",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "motion",
      "deformation",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting",
    "authors": [
      "Chi Yan",
      "Delin Qu",
      "Dan Xu",
      "Bin Zhao",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li"
    ],
    "abstract": "In this paper, we introduce \\textbf{GS-SLAM} that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussians in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. Project page: https://gs-slam.github.io/.",
    "arxiv_url": "http://arxiv.org/abs/2311.11700v4",
    "pdf_url": "http://arxiv.org/pdf/2311.11700v4",
    "published_date": "2023-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "tracking",
      "mapping",
      "geometry",
      "3d gaussian",
      "slam",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching",
    "authors": [
      "Yixun Liang",
      "Xin Yang",
      "Jiantao Lin",
      "Haodong Li",
      "Xiaogang Xu",
      "Yingcong Chen"
    ],
    "abstract": "The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2311.11284v3",
    "pdf_url": "http://arxiv.org/pdf/2311.11284v3",
    "published_date": "2023-11-19",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise",
    "authors": [
      "Xinhai Li",
      "Huaibin Wang",
      "Kuo-Kun Tseng"
    ],
    "abstract": "Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain. However, the pixel-wise rendering of NeRF and its ray marching light sampling constrain the rendering speed, impacting its utility in downstream industrial applications. Gaussian Splatting has recently shown a trend of replacing the traditional pointwise sampling technique commonly used in NeRF-based methodologies, and it is changing various aspects of 3D reconstruction. This paper introduces a novel text to 3D content generation framework, Gaussian Diffusion, based on Gaussian Splatting and produces more realistic renderings. The challenge of achieving multi-view consistency in 3D generation significantly impedes modeling complexity and accuracy. Taking inspiration from SJC, we explore employing multi-view noise distributions to perturb images generated by 3D Gaussian Splatting, aiming to rectify inconsistencies in multi-view geometry. We ingeniously devise an efficient method to generate noise that produces Gaussian noise from diverse viewpoints, all originating from a shared noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap models in local minima, causing artifacts like floaters, burrs, or proliferative elements. To mitigate these issues, we propose the variational Gaussian Splatting technique to enhance the quality and stability of 3D appearance. To our knowledge, our approach represents the first comprehensive utilization of Gaussian Diffusion across the entire spectrum of 3D content generation processes.",
    "arxiv_url": "http://arxiv.org/abs/2311.11221v3",
    "pdf_url": "http://arxiv.org/pdf/2311.11221v3",
    "published_date": "2023-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray marching",
      "efficient",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos",
    "authors": [
      "Rohit Jena",
      "Ganesh Subramanian Iyer",
      "Siddharth Choudhary",
      "Brandon Smith",
      "Pratik Chaudhari",
      "James Gee"
    ],
    "abstract": "We propose SplatArmor, a novel approach for recovering detailed and animatable human models by `armoring' a parameterized body model with 3D Gaussians. Our approach represents the human as a set of 3D Gaussians within a canonical space, whose articulation is defined by extending the skinning of the underlying SMPL geometry to arbitrary locations in the canonical space. To account for pose-dependent effects, we introduce a SE(3) field, which allows us to capture both the location and anisotropy of the Gaussians. Furthermore, we propose the use of a neural color field to provide color regularization and 3D supervision for the precise positioning of these Gaussians. We show that Gaussian splatting provides an interesting alternative to neural rendering based methods by leverging a rasterization primitive without facing any of the non-differentiability and optimization challenges typically faced in such approaches. The rasterization paradigms allows us to leverage forward skinning, and does not suffer from the ambiguities associated with inverse skinning and warping. We show compelling results on the ZJU MoCap and People Snapshot datasets, which underscore the effectiveness of our method for controllable human synthesis.",
    "arxiv_url": "http://arxiv.org/abs/2311.10812v1",
    "pdf_url": "http://arxiv.org/pdf/2311.10812v1",
    "published_date": "2023-11-17",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "body",
      "geometry",
      "3d gaussian",
      "human",
      "ar",
      "neural rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
    "authors": [
      "Simon Niedermayr",
      "Josef Stumpfegger",
      "R√ºdiger Westermann"
    ],
    "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to $31\\times$ on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to $4\\times$ higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.",
    "arxiv_url": "http://arxiv.org/abs/2401.02436v2",
    "pdf_url": "http://arxiv.org/pdf/2401.02436v2",
    "published_date": "2023-11-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "lightweight",
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drivable 3D Gaussian Avatars",
    "authors": [
      "Wojciech Zielonka",
      "Timur Bagautdinov",
      "Shunsuke Saito",
      "Michael Zollh√∂fer",
      "Justus Thies",
      "Javier Romero"
    ],
    "abstract": "We present Drivable 3D Gaussian Avatars (D3GA), a multi-layered 3D controllable model for human bodies that utilizes 3D Gaussian primitives embedded into tetrahedral cages. The advantage of using cages compared to commonly employed linear blend skinning (LBS) is that primitives like 3D Gaussians are naturally re-oriented and their kernels are stretched via the deformation gradients of the encapsulating tetrahedron. Additional offsets are modeled for the tetrahedron vertices, effectively decoupling the low-dimensional driving poses from the extensive set of primitives to be rendered. This separation is achieved through the localized influence of each tetrahedron on 3D Gaussians, resulting in improved optimization. Using the cage-based deformation model, we introduce a compositional pipeline that decomposes an avatar into layers, such as garments, hands, or faces, improving the modeling of phenomena like garment sliding. These parts can be conditioned on different driving signals, such as keypoints for facial expressions or joint-angle vectors for garments and the body. Our experiments on two multi-view datasets with varied body shapes, clothes, and motions show higher-quality results. They surpass PSNR and SSIM metrics of other SOTA methods using the same data while offering greater flexibility and compactness.",
    "arxiv_url": "http://arxiv.org/abs/2311.08581v2",
    "pdf_url": "http://arxiv.org/pdf/2311.08581v2",
    "published_date": "2023-11-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "face",
      "deformation",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "avatar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements",
    "authors": [
      "R. James Cotton",
      "Colleen Peyton"
    ],
    "abstract": "Easy access to precise 3D tracking of movement could benefit many aspects of rehabilitation. A challenge to achieving this goal is that while there are many datasets and pretrained algorithms for able-bodied adults, algorithms trained on these datasets often fail to generalize to clinical populations including people with disabilities, infants, and neonates. Reliable movement analysis of infants and neonates is important as spontaneous movement behavior is an important indicator of neurological function and neurodevelopmental disability, which can help guide early interventions. We explored the application of dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our approach leverages semantic segmentation masks to focus on the infant, significantly improving the initialization of the scene. Our results demonstrate the potential of this method in rendering novel views of scenes and tracking infant movements. This work paves the way for advanced movement analysis tools that can be applied to diverse clinical populations, with a particular emphasis on early detection in infants.",
    "arxiv_url": "http://arxiv.org/abs/2310.19441v1",
    "pdf_url": "http://arxiv.org/pdf/2310.19441v1",
    "published_date": "2023-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "semantic",
      "gaussian splatting",
      "ar",
      "dynamic",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting",
    "authors": [
      "Zeyu Yang",
      "Hongye Yang",
      "Zijie Pan",
      "Li Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.",
    "arxiv_url": "http://arxiv.org/abs/2310.10642v3",
    "pdf_url": "http://arxiv.org/pdf/2310.10642v3",
    "published_date": "2023-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "geometry",
      "4d",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models",
    "authors": [
      "Taoran Yi",
      "Jiemin Fang",
      "Junjie Wang",
      "Guanjun Wu",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wenyu Liu",
      "Qi Tian",
      "Xinggang Wang"
    ],
    "abstract": "In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.",
    "arxiv_url": "http://arxiv.org/abs/2310.08529v3",
    "pdf_url": "http://arxiv.org/pdf/2310.08529v3",
    "published_date": "2023-10-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "3d gaussian",
      "ar",
      "avatar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Guanjun Wu",
      "Taoran Yi",
      "Jiemin Fang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Wei",
      "Wenyu Liu",
      "Qi Tian",
      "Xinggang Wang"
    ],
    "abstract": "Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.",
    "arxiv_url": "http://arxiv.org/abs/2310.08528v3",
    "pdf_url": "http://arxiv.org/pdf/2310.08528v3",
    "published_date": "2023-10-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "deformation",
      "lightweight",
      "4d",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation",
    "authors": [
      "Jiaxiang Tang",
      "Jiawei Ren",
      "Hang Zhou",
      "Ziwei Liu",
      "Gang Zeng"
    ],
    "abstract": "Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.",
    "arxiv_url": "http://arxiv.org/abs/2309.16653v2",
    "pdf_url": "http://arxiv.org/pdf/2309.16653v2",
    "published_date": "2023-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "3d gaussian",
      "acceleration",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-to-3D using Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Feng Wang",
      "Yikai Wang",
      "Huaping Liu"
    ],
    "abstract": "Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent state-of-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and addressing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with delicate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our code is available at https://github.com/gsgen3d/gsgen",
    "arxiv_url": "http://arxiv.org/abs/2309.16585v4",
    "pdf_url": "http://arxiv.org/pdf/2309.16585v4",
    "published_date": "2023-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/gsgen3d/gsgen",
    "keywords": [
      "geometry",
      "compact",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction",
    "authors": [
      "Ziyi Yang",
      "Xinyu Gao",
      "Wen Zhou",
      "Shaohui Jiao",
      "Yuqing Zhang",
      "Xiaogang Jin"
    ],
    "abstract": "Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world datasets. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering.",
    "arxiv_url": "http://arxiv.org/abs/2309.13101v2",
    "pdf_url": "http://arxiv.org/pdf/2309.13101v2",
    "published_date": "2023-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "deformation",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "neural rendering",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ChromaDistill: Colorizing Monochrome Radiance Fields with Knowledge Distillation",
    "authors": [
      "Ankit Dhiman",
      "R Srinath",
      "Srinjay Sarkar",
      "Lokesh R Boregowda",
      "R Venkatesh Babu"
    ],
    "abstract": "Colorization is a well-explored problem in the domains of image and video processing. However, extending colorization to 3D scenes presents significant challenges. Recent Neural Radiance Field (NeRF) and Gaussian-Splatting(3DGS) methods enable high-quality novel-view synthesis for multi-view images. However, the question arises: How can we colorize these 3D representations? This work presents a method for synthesizing colorized novel views from input grayscale multi-view images. Using image or video colorization methods to colorize novel views from these 3D representations naively will yield output with severe inconsistencies. We introduce a novel method to use powerful image colorization models for colorizing 3D representations. We propose a distillation-based method that transfers color from these networks trained on natural images to the target 3D representation. Notably, this strategy does not add any additional weights or computational overhead to the original representation during inference. Extensive experiments demonstrate that our method produces high-quality colorized views for indoor and outdoor scenes, showcasing significant cross-view consistency advantages over baseline approaches. Our method is agnostic to the underlying 3D representation and easily generalizable to NeRF and 3DGS methods. Further, we validate the efficacy of our approach in several diverse applications: 1.) Infra-Red (IR) multi-view images and 2.) Legacy grayscale multi-view image sequences. Project Webpage: https://val.cds.iisc.ac.in/chroma-distill.github.io/",
    "arxiv_url": "http://arxiv.org/abs/2309.07668v2",
    "pdf_url": "http://arxiv.org/pdf/2309.07668v2",
    "published_date": "2023-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "nerf",
      "outdoor",
      "head",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flexible Techniques for Differentiable Rendering with 3D Gaussians",
    "authors": [
      "Leonid Keselman",
      "Martial Hebert"
    ],
    "abstract": "Fast, reliable shape reconstruction is an essential ingredient in many computer vision applications. Neural Radiance Fields demonstrated that photorealistic novel view synthesis is within reach, but was gated by performance requirements for fast reconstruction of real scenes and objects. Several recent approaches have built on alternative shape representations, in particular, 3D Gaussians. We develop extensions to these renderers, such as integrating differentiable optical flow, exporting watertight meshes and rendering per-ray normals. Additionally, we show how two of the recent methods are interoperable with each other. These reconstructions are quick, robust, and easily performed on GPU or CPU. For code and visual examples, see https://leonidk.github.io/fmb-plus",
    "arxiv_url": "http://arxiv.org/abs/2308.14737v1",
    "pdf_url": "http://arxiv.org/pdf/2308.14737v1",
    "published_date": "2023-08-28",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "I.2.10; I.3.7; I.4.0"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "3d gaussian",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis",
    "authors": [
      "Jonathon Luiten",
      "Georgios Kopanas",
      "Bastian Leibe",
      "Deva Ramanan"
    ],
    "abstract": "We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.",
    "arxiv_url": "http://arxiv.org/abs/2308.09713v1",
    "pdf_url": "http://arxiv.org/pdf/2308.09713v1",
    "published_date": "2023-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "motion",
      "3d gaussian",
      "4d",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Bernhard Kerbl",
      "Georgios Kopanas",
      "Thomas Leimk√ºhler",
      "George Drettakis"
    ],
    "abstract": "Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.",
    "arxiv_url": "http://arxiv.org/abs/2308.04079v1",
    "pdf_url": "http://arxiv.org/pdf/2308.04079v1",
    "published_date": "2023-08-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "3d gaussian",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decoherence in Neutrino Oscillation between 3D Gaussian Wave Packets",
    "authors": [
      "Haruhi Mitani",
      "Kin-ya Oda"
    ],
    "abstract": "There is renewed attention to whether we can observe the decoherence effect in neutrino oscillation due to the separation of wave packets with different masses in near-future experiments. As a contribution to this endeavor, we extend the existing formulation based on a single 1D Gaussian wave function to an amplitude between two distinct 3D Gaussian wave packets, corresponding to the neutrinos being produced and detected, with different central momenta and spacetime positions and with different widths. We find that the spatial widths-squared for the production and detection appear additively in the (de)coherence length and in the localization factor for governing the propagation of the wave packet, whereas they appear as the reduced one (inverse of the sum of inverse) in the momentum conservation factor. The overall probability is governed by the ratio of the reduced to the sum.",
    "arxiv_url": "http://arxiv.org/abs/2307.12230v2",
    "pdf_url": "http://arxiv.org/pdf/2307.12230v2",
    "published_date": "2023-07-23",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NEAT: Distilling 3D Wireframes from Neural Attraction Fields",
    "authors": [
      "Nan Xue",
      "Bin Tan",
      "Yuxi Xiao",
      "Liang Dong",
      "Gui-Song Xia",
      "Tianfu Wu",
      "Yujun Shen"
    ],
    "abstract": "This paper studies the problem of structured 3D reconstruction using wireframes that consist of line segments and junctions, focusing on the computation of structured boundary geometries of scenes. Instead of leveraging matching-based solutions from 2D wireframes (or line segments) for 3D wireframe reconstruction as done in prior arts, we present NEAT, a rendering-distilling formulation using neural fields to represent 3D line segments with 2D observations, and bipartite matching for perceiving and distilling of a sparse set of 3D global junctions. The proposed {NEAT} enjoys the joint optimization of the neural fields and the global junctions from scratch, using view-dependent 2D observations without precomputed cross-view feature matching. Comprehensive experiments on the DTU and BlendedMVS datasets demonstrate our NEAT's superiority over state-of-the-art alternatives for 3D wireframe reconstruction. Moreover, the distilled 3D global junctions by NEAT, are a better initialization than SfM points, for the recently-emerged 3D Gaussian Splatting for high-fidelity novel view synthesis using about 20 times fewer initial 3D points. Project page: \\url{https://xuenan.net/neat}.",
    "arxiv_url": "http://arxiv.org/abs/2307.10206v2",
    "pdf_url": "http://arxiv.org/pdf/2307.10206v2",
    "published_date": "2023-07-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization",
    "authors": [
      "Aditya Vora",
      "Akshay Gadi Patil",
      "Hao Zhang"
    ],
    "abstract": "We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates to act as surface priors. Our method, coined DiViNet, operates in two stages. It first learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help \"stitch'' the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from a few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views and performs on par, if not better, with competing methods when dense views are employed as inputs.",
    "arxiv_url": "http://arxiv.org/abs/2306.04699v4",
    "pdf_url": "http://arxiv.org/pdf/2306.04699v4",
    "published_date": "2023-06-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "face",
      "geometry",
      "3d gaussian",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Control4D: Efficient 4D Portrait Editing with Text",
    "authors": [
      "Ruizhi Shao",
      "Jingxiang Sun",
      "Cheng Peng",
      "Zerong Zheng",
      "Boyao Zhou",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "We introduce Control4D, an innovative framework for editing dynamic 4D portraits using text instructions. Our method addresses the prevalent challenges in 4D editing, notably the inefficiencies of existing 4D representations and the inconsistent editing effect caused by diffusion-based editors. We first propose GaussianPlanes, a novel 4D representation that makes Gaussian Splatting more structured by applying plane-based decomposition in 3D space and time. This enhances both efficiency and robustness in 4D editing. Furthermore, we propose to leverage a 4D generator to learn a more continuous generation space from inconsistent edited images produced by the diffusion-based editor, which effectively improves the consistency and quality of 4D editing. Comprehensive evaluation demonstrates the superiority of Control4D, including significantly reduced training time, high-quality rendering, and spatial-temporal consistency in 4D portrait editing. The link to our project website is https://control4darxiv.github.io.",
    "arxiv_url": "http://arxiv.org/abs/2305.20082v2",
    "pdf_url": "http://arxiv.org/pdf/2305.20082v2",
    "published_date": "2023-05-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deceptive-NeRF/3DGS: Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction",
    "authors": [
      "Xinhang Liu",
      "Jiaben Chen",
      "Shiu-hong Kao",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "Novel view synthesis via Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS) typically necessitates dense observations with hundreds of input images to circumvent artifacts. We introduce Deceptive-NeRF/3DGS to enhance sparse-view reconstruction with only a limited set of input images, by leveraging a diffusion model pre-trained from multiview datasets. Different from using diffusion priors to regularize representation optimization, our method directly uses diffusion-generated images to train NeRF/3DGS as if they were real input views. Specifically, we propose a deceptive diffusion model turning noisy images rendered from few-view reconstructions into high-quality photorealistic pseudo-observations. To resolve consistency among pseudo-observations and real input views, we develop an uncertainty measure to guide the diffusion model's generation. Our system progressively incorporates diffusion-generated pseudo-observations into the training image sets, ultimately densifying the sparse input observations by 5 to 10 times. Extensive experiments across diverse and challenging datasets validate that our approach outperforms existing state-of-the-art methods and is capable of synthesizing novel views with super-resolution in the few-view setting.",
    "arxiv_url": "http://arxiv.org/abs/2305.15171v4",
    "pdf_url": "http://arxiv.org/pdf/2305.15171v4",
    "published_date": "2023-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NOVUM: Neural Object Volumes for Robust Object Classification",
    "authors": [
      "Artur Jesslen",
      "Guofeng Zhang",
      "Angtian Wang",
      "Wufei Ma",
      "Alan Yuille",
      "Adam Kortylewski"
    ],
    "abstract": "Discriminative models for object classification typically learn image-based representations that do not capture the compositional and 3D nature of objects. In this work, we show that explicitly integrating 3D compositional object representations into deep networks for image classification leads to a largely enhanced generalization in out-of-distribution scenarios. In particular, we introduce a novel architecture, referred to as NOVUM, that consists of a feature extractor and a neural object volume for every target object class. Each neural object volume is a composition of 3D Gaussians that emit feature vectors. This compositional object representation allows for a highly robust and fast estimation of the object class by independently matching the features of the 3D Gaussians of each category to features extracted from an input image. Additionally, the object pose can be estimated via inverse rendering of the corresponding neural object volume. To enable the classification of objects, the neural features at each 3D Gaussian are trained discriminatively to be distinct from (i) the features of 3D Gaussians in other categories, (ii) features of other 3D Gaussians of the same object, and (iii) the background features. Our experiments show that NOVUM offers intriguing advantages over standard architectures due to the 3D compositional structure of the object representation, namely: (1) An exceptional robustness across a spectrum of real-world and synthetic out-of-distribution shifts and (2) an enhanced human interpretability compared to standard models, all while maintaining real-time inference and a competitive accuracy on in-distribution data.",
    "arxiv_url": "http://arxiv.org/abs/2305.14668v4",
    "pdf_url": "http://arxiv.org/pdf/2305.14668v4",
    "published_date": "2023-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "human",
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generation of artificial facial drug abuse images using Deep De-identified anonymous Dataset augmentation through Genetics Algorithm (3DG-GA)",
    "authors": [
      "Hazem Zein",
      "Lou Laurent",
      "R√©gis Fournier",
      "Amine Nait-Ali"
    ],
    "abstract": "In biomedical research and artificial intelligence, access to large, well-balanced, and representative datasets is crucial for developing trustworthy applications that can be used in real-world scenarios. However, obtaining such datasets can be challenging, as they are often restricted to hospitals and specialized facilities. To address this issue, the study proposes to generate highly realistic synthetic faces exhibiting drug abuse traits through augmentation. The proposed method, called \"3DG-GA\", Deep De-identified anonymous Dataset Generation, uses Genetics Algorithm as a strategy for synthetic faces generation. The algorithm includes GAN artificial face generation, forgery detection, and face recognition. Initially, a dataset of 120 images of actual facial drug abuse is used. By preserving, the drug traits, the 3DG-GA provides a dataset containing 3000 synthetic facial drug abuse images. The dataset will be open to the scientific community, which can reproduce our results and benefit from the generated datasets while avoiding legal or ethical restrictions.",
    "arxiv_url": "http://arxiv.org/abs/2304.06106v1",
    "pdf_url": "http://arxiv.org/pdf/2304.06106v1",
    "published_date": "2023-04-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "recognition",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quantitative perfusion and water transport time model from multi b-value diffusion magnetic resonance imaging validated against neutron capture microspheres",
    "authors": [
      "M. Liu",
      "N. Saadat",
      "Y. Jeong",
      "S. Roth",
      "M. Niekrasz",
      "M. Giurcanu",
      "T. Carroll",
      "G. Christoforidis"
    ],
    "abstract": "Intravoxel Incoherent Motion (IVIM) is a non-contrast magnetic resonance imaging diffusion-based scan that uses a multitude of b-values to measure various speeds of molecular perfusion and diffusion, sidestepping inaccuracy of arterial input functions or bolus kinetics in quantitative imaging. We test a new method of IVIM quantification and compare our values to reference standard neutron capture microspheres across normocapnia, CO2 induced hypercapnia, and middle cerebral artery occlusion in a controlled animal model. Perfusion quantification in ml/100g/min compared to microsphere perfusion uses the 3D gaussian probability distribution and defined water transport time as when 50% of the molecules remain in the tissue of interest. Perfusion, water transport time, and infarct volume was compared to reference standards. Simulations were studied to suppress non-specific cerebrospinal fluid (CSF). Linear regression analysis of quantitative perfusion returned correlation (slope = .55, intercept = 52.5, $R^2$= .64). Linear regression for water transport time asymmetry in infarcted tissue was excellent (slope = .59, intercept = .3, $R^2$ = .93). Strong linear agreement also was found for infarct volume (slope = 1.01, $R^2$= .79). Simulation of CSF suppression via inversion recovery returned blood signal reduced by 82% from combined T1 and T2 effects. Intra-physiologic state comparison of perfusion shows potential partial volume effects which require further study especially in disease states. The accuracy and sensitivity of IVIM provides evidence that observed signal changes reflect cytotoxic edema and tissue perfusion. Partial volume contamination of CSF may be better removed during post-processing rather than with inversion recovery to avoid artificial loss of blood signal.",
    "arxiv_url": "http://arxiv.org/abs/2304.01888v1",
    "pdf_url": "http://arxiv.org/pdf/2304.01888v1",
    "published_date": "2023-04-04",
    "categories": [
      "physics.med-ph",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "motion",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Light-Weight Pointcloud Representation with Sparse Gaussian Process",
    "authors": [
      "Mahmoud Ali",
      "Lantao Liu"
    ],
    "abstract": "This paper presents a framework to represent high-fidelity pointcloud sensor observations for efficient communication and storage. The proposed approach exploits Sparse Gaussian Process to encode pointcloud into a compact form. Our approach represents both the free space and the occupied space using only one model (one 2D Sparse Gaussian Process) instead of the existing two-model framework (two 3D Gaussian Mixture Models). We achieve this by proposing a variance-based sampling technique that effectively discriminates between the free and occupied space. The new representation requires less memory footprint and can be transmitted across limitedbandwidth communication channels. The framework is extensively evaluated in simulation and it is also demonstrated using a real mobile robot equipped with a 3D LiDAR. Our method results in a 70 to 100 times reduction in the communication rate compared to sending the raw pointcloud.",
    "arxiv_url": "http://arxiv.org/abs/2301.11251v1",
    "pdf_url": "http://arxiv.org/pdf/2301.11251v1",
    "published_date": "2023-01-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "3d gaussian",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FedGS: Federated Graph-based Sampling with Arbitrary Client Availability",
    "authors": [
      "Zheng Wang",
      "Xiaoliang Fan",
      "Jianzhong Qi",
      "Haibing Jin",
      "Peizhen Yang",
      "Siqi Shen",
      "Cheng Wang"
    ],
    "abstract": "While federated learning has shown strong results in optimizing a machine learning model without direct access to the original data, its performance may be hindered by intermittent client availability which slows down the convergence and biases the final learned model. There are significant challenges to achieve both stable and bias-free training under arbitrary client availability. To address these challenges, we propose a framework named Federated Graph-based Sampling (FedGS), to stabilize the global model update and mitigate the long-term bias given arbitrary client availability simultaneously. First, we model the data correlations of clients with a Data-Distribution-Dependency Graph (3DG) that helps keep the sampled clients data apart from each other, which is theoretically shown to improve the approximation to the optimal model update. Second, constrained by the far-distance in data distribution of the sampled clients, we further minimize the variance of the numbers of times that the clients are sampled, to mitigate long-term bias. To validate the effectiveness of FedGS, we conduct experiments on three datasets under a comprehensive set of seven client availability modes. Our experimental results confirm FedGS's advantage in both enabling a fair client-sampling scheme and improving the model performance under arbitrary client availability. Our code is available at \\url{https://github.com/WwZzz/FedGS}.",
    "arxiv_url": "http://arxiv.org/abs/2211.13975v3",
    "pdf_url": "http://arxiv.org/pdf/2211.13975v3",
    "published_date": "2022-11-25",
    "categories": [
      "cs.LG"
    ],
    "github_url": "https://github.com/WwZzz/FedGS",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching",
    "authors": [
      "Runyu Mao",
      "Chen Bai",
      "Yatong An",
      "Fengqing Zhu",
      "Cheng Lu"
    ],
    "abstract": "We tackle the essential task of finding dense visual correspondences between a pair of images. This is a challenging problem due to various factors such as poor texture, repetitive patterns, illumination variation, and motion blur in practical scenarios. In contrast to methods that use dense correspondence ground-truths as direct supervision for local feature matching training, we train 3DG-STFM: a multi-modal matching model (Teacher) to enforce the depth consistency under 3D dense correspondence supervision and transfer the knowledge to 2D unimodal matching model (Student). Both teacher and student models consist of two transformer-based matching modules that obtain dense correspondences in a coarse-to-fine manner. The teacher model guides the student model to learn RGB-induced depth information for the matching purpose on both coarse and fine branches. We also evaluate 3DG-STFM on a model compression task. To the best of our knowledge, 3DG-STFM is the first student-teacher learning method for the local feature matching task. The experiments show that our method outperforms state-of-the-art methods on indoor and outdoor camera pose estimations, and homography estimation problems. Code is available at: https://github.com/Ryan-prime/3DG-STFM.",
    "arxiv_url": "http://arxiv.org/abs/2207.02375v2",
    "pdf_url": "http://arxiv.org/pdf/2207.02375v2",
    "published_date": "2022-07-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Ryan-prime/3DG-STFM",
    "keywords": [
      "outdoor",
      "motion",
      "ar",
      "illumination",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Contour Generation with Realistic Inter-observer Variation",
    "authors": [
      "Eliana V√°squez Osorio",
      "Jane Shortall",
      "Jennifer Robbins",
      "Marcel van Herk"
    ],
    "abstract": "Contours are used in radiotherapy treatment planning to identify regions to be irradiated with high dose and regions to be spared. Therefore, any contouring uncertainty influences the whole treatment. Even though this is the biggest remaining source of uncertainty when daily IGRT or adaptation is used, it has not been accounted for quantitatively in treatment planning. Using probabilistic planning allows to directly account for contouring uncertainties in plan optimisation. The first step is to create an algorithm that can generate many realistic contours with variation matching actual inter-observer variation.   We propose a methodology to generate random contours, based on measured spatial inter-observer variation, IOV, and a single parameter that controls its geometrical dependency: alpha, the width of the 3D Gaussian used as point spread function (PSF). We used a level set formulation of the median shape, with the level set function defined as the signed distance transform. To create a new contour, we added the median level set and a noise map which was weighted with the IOV map and then convolved with the PSF. Thresholding the level set function reconstructs the newly generated contour.   We used data from 18 patients from the golden atlas, consisting of five prostate delineations on T2-w MRI scans. To evaluate the similarity between the contours, we calculated the maximum distance to agreement to the median shape (maxDTA), and the minimum dose of the contours using an ideal dose distribution. We used the two-sample Kolmogorov-Smirnov test to compare the distributions for maxDTA and minDose between the generated and manually delineated contours.   Only alpha=0.75cm produced maxDTA and minDose distributions that were not significantly different from the manually delineated structures. Accounting for the PSF is essential to correctly simulate inter-observer variation.",
    "arxiv_url": "http://arxiv.org/abs/2204.10098v1",
    "pdf_url": "http://arxiv.org/pdf/2204.10098v1",
    "published_date": "2022-04-21",
    "categories": [
      "physics.med-ph"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Sloan Digital Sky Survey Peculiar Velocity Catalogue",
    "authors": [
      "Cullan Howlett",
      "Khaled Said",
      "John R. Lucey",
      "Matthew Colless",
      "Fei Qin",
      "Yan Lai",
      "R. Brent Tully",
      "Tamara M. Davis"
    ],
    "abstract": "We present a new catalogue of distances and peculiar velocities (PVs) of $34,059$ early-type galaxies derived from Fundamental Plane (FP) measurements using data from the Sloan Digital Sky Survey (SDSS). This $7016\\,\\mathrm{deg}^{2}$ homogeneous sample comprises the largest set of peculiar velocities produced to date and extends the reach of PV surveys up to a redshift limit of $z=0.1$. Our SDSS-based FP distance measurements have a mean uncertainty of 23%. Alongside the data, we produce an ensemble of 2,048 mock galaxy catalogues that reproduce the data selection function, and are used to validate our fitting pipelines and check for systematic errors. We uncover a significant trend between group richness and mean surface brightness within the sample, which may hint at an environmental dependence within the FP or the presence of unresolved systematics, and can result in biased peculiar velocities. This is removed using multiple FP fits as function of group richness, a procedure made tractable through a new analytic derivation for the integral of a 3D Gaussian over non-trivial limits. Our catalogue is calibrated to the zero-point of the CosmicFlows-III sample with an uncertainty of $0.004$ dex (not including cosmic variance or the error within CosmicFlows-III itself), which is validated using independent cross-checks with the predicted zero-point from the 2M++ reconstruction of our local velocity field. Finally, as an example of what is possible with our new catalogue, we obtain preliminary bulk flow measurements up to a depth of $135\\,h^{-1}\\mathrm{Mpc}$. We find a slightly larger-than-expected bulk flow at high redshift, although this could be caused by the presence of the Shapley supercluster which lies outside the SDSS PV footprint.",
    "arxiv_url": "http://arxiv.org/abs/2201.03112v2",
    "pdf_url": "http://arxiv.org/pdf/2201.03112v2",
    "published_date": "2022-01-09",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Yang-Mills heat flow with random distributional initial data",
    "authors": [
      "Sky Cao",
      "Sourav Chatterjee"
    ],
    "abstract": "We construct local solutions to the Yang-Mills heat flow (in the DeTurck gauge) for a certain class of random distributional initial data, which includes the 3D Gaussian free field. The main idea, which goes back to work of Bourgain as well as work of Da Prato-Debussche, is to decompose the solution into a rougher linear part and a smoother nonlinear part, and to control the latter by probabilistic arguments. In a companion work, we use the main results of this paper to propose a way towards the construction of 3D Yang-Mills measures.",
    "arxiv_url": "http://arxiv.org/abs/2111.10652v4",
    "pdf_url": "http://arxiv.org/pdf/2111.10652v4",
    "published_date": "2021-11-20",
    "categories": [
      "math.PR",
      "hep-th",
      "math-ph",
      "math.AP",
      "math.MP",
      "35R60, 35A01, 60G60, 81T13"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Topology and geometry of Gaussian random fields II: on critical points, excursion sets, and persistent homology",
    "authors": [
      "Pratyush Pranav"
    ],
    "abstract": "This paper is second in the series, following Pranav et al. (2019), focused on the characterization of geometric and topological properties of 3D Gaussian random fields. We focus on the formalism of persistent homology, the mainstay of Topological Data Analysis (TDA), in the context of excursion set formalism. We also focus on the structure of critical points of stochastic fields, and their relationship with formation and evolution of structures in the universe.   The topological background is accompanied by an investigation of Gaussian field simulations based on the LCDM spectrum, as well as power-law spectra with varying spectral indices. We present the statistical properties in terms of the intensity and difference maps constructed from the persistence diagrams, as well as their distribution functions. We demonstrate that the intensity maps encapsulate information about the distribution of power across the hierarchies of structures in more detailed than the Betti numbers or the Euler characteristic. In particular, the white noise ($n = 0$) case with flat spectrum stands out as the divide between models with positive and negative spectral index. It has the highest proportion of low significance features. This level of information is not available from the geometric Minkowski functionals or the topological Euler characteristic, or even the Betti numbers, and demonstrates the usefulness of hierarchical topological methods. Another important result is the observation that topological characteristics of Gaussian fields depend on the power spectrum, as opposed to the geometric measures that are insensitive to the power spectrum characteristics.",
    "arxiv_url": "http://arxiv.org/abs/2109.08721v1",
    "pdf_url": "http://arxiv.org/pdf/2109.08721v1",
    "published_date": "2021-09-17",
    "categories": [
      "astro-ph.CO",
      "math.AT"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes",
    "authors": [
      "Youssef A. Mejjati",
      "Isa Milefchik",
      "Aaron Gokaslan",
      "Oliver Wang",
      "Kwang In Kim",
      "James Tompkin"
    ],
    "abstract": "We present an algorithm that learns a coarse 3D representation of objects from unposed multi-view 2D mask supervision, then uses it to generate detailed mask and image texture. In contrast to existing voxel-based methods for unposed object reconstruction, our approach learns to represent the generated shape and pose with a set of self-supervised canonical 3D anisotropic Gaussians via a perspective camera, and a set of per-image transforms. We show that this approach can robustly estimate a 3D space for the camera and object, while recent baselines sometimes struggle to reconstruct coherent 3D spaces in this setting. We show results on synthetic datasets with realistic lighting, and demonstrate object insertion with interactive posing. With our work, we help move towards structured representations that handle more real-world variation in learning-based object reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/2106.13215v1",
    "pdf_url": "http://arxiv.org/pdf/2106.13215v1",
    "published_date": "2021-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "lighting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Probabilistic Localization of Insect-Scale Drones on Floating-Gate Inverter Arrays",
    "authors": [
      "Priyesh Shukla",
      "Ankith Muralidhar",
      "Nick Iliev",
      "Theja Tulabandhula",
      "Sawyer B. Fuller",
      "Amit Ranjan Trivedi"
    ],
    "abstract": "We propose a novel compute-in-memory (CIM)-based ultra-low-power framework for probabilistic localization of insect-scale drones. The conventional probabilistic localization approaches rely on the three-dimensional (3D) Gaussian Mixture Model (GMM)-based representation of a 3D map. A GMM model with hundreds of mixture functions is typically needed to adequately learn and represent the intricacies of the map. Meanwhile, localization using complex GMM map models is computationally intensive. Since insect-scale drones operate under extremely limited area/power budget, continuous localization using GMM models entails much higher operating energy -- thereby, limiting flying duration and/or size of the drone due to a larger battery. Addressing the computational challenges of localization in an insect-scale drone using a CIM approach, we propose a novel framework of 3D map representation using a harmonic mean of \"Gaussian-like\" mixture (HMGM) model. The likelihood function useful for drone localization can be efficiently implemented by connecting many multi-input inverters in parallel, each programmed with the parameters of the 3D map model represented as HMGM. When the depth measurements are projected to the input of the implementation, the summed current of the inverters emulates the likelihood of the measurement. We have characterized our approach on an RGB-D indoor localization dataset. The average localization error in our approach is $\\sim$0.1125 m which is only slightly degraded than software-based evaluation ($\\sim$0.08 m). Meanwhile, our localization framework is ultra-low-power, consuming as little as $\\sim$17 $\\mu$W power while processing a depth frame in 1.33 ms over hundred pose hypotheses in the particle-filtering (PF) algorithm used to localize the drone.",
    "arxiv_url": "http://arxiv.org/abs/2102.08247v2",
    "pdf_url": "http://arxiv.org/pdf/2102.08247v2",
    "published_date": "2021-02-16",
    "categories": [
      "cs.RO",
      "cs.AR",
      "eess.IV",
      "B.7; I.2.9"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual Analysis of Large Multivariate Scattered Data using Clustering and Probabilistic Summaries",
    "authors": [
      "Tobias Rapp",
      "Christoph Peters",
      "Carsten Dachsbacher"
    ],
    "abstract": "Rapidly growing data sizes of scientific simulations pose significant challenges for interactive visualization and analysis techniques. In this work, we propose a compact probabilistic representation to interactively visualize large scattered datasets. In contrast to previous approaches that represent blocks of volumetric data using probability distributions, we model clusters of arbitrarily structured multivariate data. In detail, we discuss how to efficiently represent and store a high-dimensional distribution for each cluster. We observe that it suffices to consider low-dimensional marginal distributions for two or three data dimensions at a time to employ common visual analysis techniques. Based on this observation, we represent high-dimensional distributions by combinations of low-dimensional Gaussian mixture models. We discuss the application of common interactive visual analysis techniques to this representation. In particular, we investigate several frequency-based views, such as density plots in 1D and 2D, density-based parallel coordinates, and a time histogram. We visualize the uncertainty introduced by the representation, discuss a level-of-detail mechanism, and explicitly visualize outliers. Furthermore, we propose a spatial visualization by splatting anisotropic 3D Gaussians for which we derive a closed-form solution. Lastly, we describe the application of brushing and linking to this clustered representation. Our evaluation on several large, real-world datasets demonstrates the scaling of our approach.",
    "arxiv_url": "http://arxiv.org/abs/2008.09544v2",
    "pdf_url": "http://arxiv.org/pdf/2008.09544v2",
    "published_date": "2020-08-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Algebraic 3D Graphic Statics: reciprocal constructions",
    "authors": [
      "M√°rton Hablicsek",
      "Masoud Akbarzadeh",
      "Yi Guo"
    ],
    "abstract": "The recently developed 3D graphic statics (3DGS) lacks a rigorous mathematical definition relating the geometrical and topological properties of the reciprocal polyhedral diagrams as well as a precise method for the geometric construction of these diagrams. This paper provides a fundamental algebraic formulation for 3DGS by developing equilibrium equations around the edges of the primal diagram and satisfying the equations by the closeness of the polygons constructed by the edges of the corresponding faces in the dual/reciprocal diagram. The research provides multiple numerical methods for solving the equilibrium equations and explains the advantage of using each technique. The approach of this paper can be used for compression-and-tension combined form-finding and analysis as it allows constructing both the form and force diagram based on the interpretation of the input diagram. Besides, the paper expands on the geometric/static degrees of (in)determinacies of the diagrams using the algebraic formulation and shows how these properties can be used for the constrained manipulation of the polyhedrons in an interactive environment without breaking the reciprocity between the two.",
    "arxiv_url": "http://arxiv.org/abs/2007.15720v1",
    "pdf_url": "http://arxiv.org/pdf/2007.15720v1",
    "published_date": "2020-07-30",
    "categories": [
      "cs.CG",
      "J.6; J.2"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "compression",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Algebraic 3D Graphic Statics: Constrained Areas",
    "authors": [
      "Masoud Akbarzadeh",
      "Marton Hablicsek"
    ],
    "abstract": "This research provides algorithms and numerical methods to geometrically control the magnitude of the internal and external forces in the reciprocal diagrams of 3D/Polyhedral Graphic statics (3DGS). In 3DGS, the form of the structure and its equilibrium of forces is represented by two polyhedral diagrams that are geometrically and topologically related. The areas of the faces of the force diagram represent the magnitude of the internal and external forces in the system. For the first time, the methods of this research allow the user to control and constrain the areas and edge lengths of the faces of general polyhedrons that can be convex, self-intersecting, or concave. As a result, a designer can explicitly control the force magnitudes in the force diagram and explore the equilibrium of a variety of compression and tension-combined funicular structural forms. In this method, a quadratic formulation is used to compute the area of a single face based on its edge lengths. The approach is applied to manipulating the face geometry with a predefined area and the edge lengths. Subsequently, the geometry of the polyhedron is updated with newly changed faces. This approach is a multi-step algorithm where each step includes computing the geometry of a single face and updating the polyhedral geometry. One of the unique results of this framework is the construction of the zero-area, self-intersecting faces, where the sum of the signed areas of a self-intersecting face is zero, representing a member with zero force in the form diagram. The methodology of this research can clarify the equilibrium of some systems that could not be previously justified using reciprocal polyhedral diagrams. Therefore, it generalizes the principle of the equilibrium of polyhedral frames and opens a completely new horizon in the design of highly-sophisticated funicular polyhedral structures beyond compression-only systems.",
    "arxiv_url": "http://arxiv.org/abs/2007.15133v1",
    "pdf_url": "http://arxiv.org/pdf/2007.15133v1",
    "published_date": "2020-07-29",
    "categories": [
      "cs.CG",
      "physics.app-ph",
      "J.6; J.2"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "face",
      "compression",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-GMNet: Single-View 3D Shape Recovery as A Gaussian Mixture",
    "authors": [
      "Kohei Yamashita",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "abstract": "In this paper, we introduce 3D-GMNet, a deep neural network for 3D object shape reconstruction from a single image. As the name suggests, 3D-GMNet recovers 3D shape as a Gaussian mixture. In contrast to voxels, point clouds, or meshes, a Gaussian mixture representation provides an analytical expression with a small memory footprint while accurately representing the target 3D shape. At the same time, it offers a number of additional advantages including instant pose estimation and controllable level-of-detail reconstruction, while also enabling interpretation as a point cloud, volume, and a mesh model. We train 3D-GMNet end-to-end with single input images and corresponding 3D models by introducing two novel loss functions, a 3D Gaussian mixture loss and a 2D multi-view loss, which collectively enable accurate shape reconstruction as kernel density estimation. We thoroughly evaluate the effectiveness of 3D-GMNet with synthetic and real images of objects. The results show accurate reconstruction with a compact representation that also realizes novel applications of single-image 3D reconstruction.",
    "arxiv_url": "http://arxiv.org/abs/1912.04663v2",
    "pdf_url": "http://arxiv.org/pdf/1912.04663v2",
    "published_date": "2019-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shape reconstruction",
      "3d gaussian",
      "3d reconstruction",
      "ar",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Correcting the formalism governing Bloch Surface Waves excited by 3D Gaussian beams",
    "authors": [
      "Fadi Issam Baida",
      "Maria-Pilar Bernal"
    ],
    "abstract": "Due to the growing number of publications and applications based on the exploitation of Bloch surface waves and the gross errors and approximations that are regularly used to evaluate the properties of this type of wave, we judge seriously important for successful interpretation and understanding of experiments to implement adapted formalism allowing to extract the relevant information. Through a comprehensive calculation supported by an analytical development, we establish a generalized formula for the propagation length which is different from what is usually employed in the literature. We also demonstrate that the Goos-H\\\"anchen shift becomes an extrinsic property that depends on the beam dimension with an asymptotic behavior limiting its value to that of the propagation length. The proposed theoretical scheme allows predicting some new and unforeseen results such as the effect due to a slight deviation of the angle of incidence or of the beam-waist position with respect to the structure. This formalism can be used to describe any polarization-dependent resonant structure illuminated by a polarized Gaussian beam.",
    "arxiv_url": "http://arxiv.org/abs/1907.03476v1",
    "pdf_url": "http://arxiv.org/pdf/1907.03476v1",
    "published_date": "2019-07-08",
    "categories": [
      "physics.optics",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Power-spectrum simulations of radial redshift distributions",
    "authors": [
      "Andrei Ryabinkov",
      "Aleksandr Kaminker"
    ],
    "abstract": "On the base of the simplest model of a modulation of 3D Gaussian field in $k$-space we produce a set of simulations to bring out the effects of a modulating function $f_{\\rm mod} (k)=f_1 (k) + f_2 (k)$ on power spectra of radial (shell-like) distributions of cosmological objects, where a model function $f_1 (k)$ reproduces the smoothed power spectrum of underlying 3D density fluctuations, while $f_2 (k)$ is a wiggling function imitating the baryon acoustic oscillations (BAO). It is shown that some excess of realizations of simulated radial distributions actually displays quasi-periodical components with periods about a characteristic scale $2\\pi/k \\sim 100~h^{-1}$~Mpc detected as power-spectrum peaks in vicinity of the first maximum of the modulation function $f_2 (k)$. We revised our previous estimations of the significance of such peaks and found that they were largely overestimated. Thereby quasi-periodical components appearing in some radial distributions of matter are likely to be stochastic (rather than determinative), while the amplitudes of the respective spectral peaks can be quite noticeable. They are partly enhanced by smooth part of the modulating function $f_1(k)$ and, to a far lesser extent, by effects of the BAO (i.e. $f_2(k)$). The results of the simulations match quite well with statistical properties of the radial distributions of the brightest cluster galaxies (BCGs).",
    "arxiv_url": "http://arxiv.org/abs/1905.06283v1",
    "pdf_url": "http://arxiv.org/pdf/1905.06283v1",
    "published_date": "2019-05-15",
    "categories": [
      "astro-ph.CO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deep AutoEncoder-based Lossy Geometry Compression for Point Clouds",
    "authors": [
      "Wei Yan",
      "Yiting shao",
      "Shan Liu",
      "Thomas H Li",
      "Zhu Li",
      "Ge Li"
    ],
    "abstract": "Point cloud is a fundamental 3D representation which is widely used in real world applications such as autonomous driving. As a newly-developed media format which is characterized by complexity and irregularity, point cloud creates a need for compression algorithms which are more flexible than existing codecs. Recently, autoencoders(AEs) have shown their effectiveness in many visual analysis tasks as well as image compression, which inspires us to employ it in point cloud compression. In this paper, we propose a general autoencoder-based architecture for lossy geometry point cloud compression. To the best of our knowledge, it is the first autoencoder-based geometry compression codec that directly takes point clouds as input rather than voxel grids or collections of images. Compared with handcrafted codecs, this approach adapts much more quickly to previously unseen media contents and media formats, meanwhile achieving competitive performance. Our architecture consists of a pointnet-based encoder, a uniform quantizer, an entropy estimation block and a nonlinear synthesis transformation module. In lossy geometry compression of point cloud, results show that the proposed method outperforms the test model for categories 1 and 3 (TMC13) published by MPEG-3DG group on the 125th meeting, and on average a 73.15\\% BD-rate gain is achieved.",
    "arxiv_url": "http://arxiv.org/abs/1905.03691v1",
    "pdf_url": "http://arxiv.org/pdf/1905.03691v1",
    "published_date": "2019-04-18",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "autonomous driving",
      "geometry",
      "compression",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D cosmic shear: numerical challenges, 3D lensing random fields generation and Minkowski Functionals for cosmological inference",
    "authors": [
      "A. Spurio Mancini",
      "P. L. Taylor",
      "R. Reischke",
      "T. Kitching",
      "V. Pettorino",
      "B. M. Sch√§fer",
      "B. Zieser",
      "Ph. M. Merkel"
    ],
    "abstract": "Cosmic shear - the weak gravitational lensing effect generated by fluctuations of the gravitational tidal fields of the large-scale structure - is one of the most promising tools for current and future cosmological analyses. The spherical-Bessel decomposition of the cosmic shear field (\"3D cosmic shear\") is one way to maximise the amount of redshift information in a lensing analysis and therefore provides a powerful tool to investigate in particular the growth of cosmic structure that is crucial for dark energy studies. However, the computation of simulated 3D cosmic shear covariance matrices presents numerical difficulties, due to the required integrations over highly oscillatory functions. We present and compare two numerical methods and relative implementations to perform these integrations. We then show how to generate 3D Gaussian random fields on the sky in spherical coordinates, starting from the 3D cosmic shear covariances. To validate our field-generation procedure, we calculate the Minkowski functionals associated with our random fields, compare them with the known expectation values for the Gaussian case and demonstrate parameter inference from Minkowski functionals from a cosmic shear survey. This is a first step towards producing fully 3D Minkowski functionals for a lognormal field in 3D to extract Gaussian and non-Gaussian information from the cosmic shear field, as well as towards the use of Minkowski functionals as a probe of cosmology beyond the commonly used two-point statistics.",
    "arxiv_url": "http://arxiv.org/abs/1807.11461v3",
    "pdf_url": "http://arxiv.org/pdf/1807.11461v3",
    "published_date": "2018-07-30",
    "categories": [
      "astro-ph.CO"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Point Cloud Attribute Compression Using Slice-based Layered Structure and Block-based Intra Prediction",
    "authors": [
      "Yiting Shao",
      "Qi Zhang",
      "Ge Li",
      "Zhu Li"
    ],
    "abstract": "Point cloud compression is a key enabler for the emerging applications of immersive visual communication, autonomous driving and smart cities, etc. In this paper, we propose a hybrid point cloud attribute compression scheme built on an original layered data structure. First, a slice-partition scheme and geometry-adaptive k dimensional-tree (kd-tree) method are devised to generate the four-layer structure. Second, we introduce an efficient block-based intra prediction scheme containing a DC prediction mode and several angular modes, in order to exploit the spatial correlation between adjacent points. Third, an adaptive transform scheme based on Graph Fourier Transform (GFT) is Lagrangian optimized to achieve better transform efficiency. The Lagrange multiplier is off-line derived based on the statistics of color attribute coding. Last but not least, multiple reordering scan modes are dedicated to improve coding efficiency for entropy coding. In intra-frame compression of point cloud color attributes, results demonstrate that our method performs better than the state-of-the-art region-adaptive hierarchical transform (RAHT) system, and on average a 29.37$\\%$ BD-rate gain is achieved. Comparing with the test model for category 1 (TMC1) anchor's coding results, which were recently published by MPEG-3DG group on 121st meeting, a 16.37$\\%$ BD-rate gain is obtained.",
    "arxiv_url": "http://arxiv.org/abs/1804.10783v1",
    "pdf_url": "http://arxiv.org/pdf/1804.10783v1",
    "published_date": "2018-04-28",
    "categories": [
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "autonomous driving",
      "geometry",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Generic Phase between Disordered Weyl Semimetal and Diffusive Metal",
    "authors": [
      "Ying Su",
      "X. S. Wang",
      "X. R. Wang"
    ],
    "abstract": "Quantum phase transitions of three-dimensional (3D) Weyl semimetals (WSMs) subject to uncorrelated on-site disorder are investigated through quantum conductance calculations and finite-size scaling of localization length. Contrary to previous claims that a direct transition from a WSM to a diffusive metal (DM) occurs, an intermediate phase of Chern insulator (CI) between the two distinct metallic phases should exist due to internode scattering that is comparable to intranode scattering. The critical exponent of localization length is $\\nu\\simeq 1.3$ for both the WSM-CI and CI-DM transitions, in the same universality class of 3D Gaussian unitary ensemble of the Anderson localization transition. The CI phase is confirmed by quantized nonzero Hall conductances in the bulk insulating phase established by localization length calculations. The disorder-induced various plateau-plateau transitions in both the WSM and CI phases are observed and explained by the self-consistent Born approximation. Furthermore, we clarify that the occurrence of zero density of states at Weyl nodes is not a good criterion for the disordered WSM, and there is no fundamental principle to support the hypothesis of divergence of localization length at the WSM-DM transition.",
    "arxiv_url": "http://arxiv.org/abs/1701.00905v2",
    "pdf_url": "http://arxiv.org/pdf/1701.00905v2",
    "published_date": "2017-01-04",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.mes-hall"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures",
    "authors": [
      "Jonathan Vacher",
      "Andrew Isaac Meso",
      "Laurent U. Perrinet",
      "Gabriel Peyr√©"
    ],
    "abstract": "A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a generative model intended to probe visual motion perception with a dynamic texture model. It is first derived in a set of axiomatic steps constrained by biological plausibility. We extend previous contributions by detailing three equivalent formulations of this texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as 3D Gaussian fields. Secondly, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly texture synthesis using time-discretized auto-regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are essential for the construction of a Bayesian inference framework. We use the dynamic texture model to psychophysically probe speed perception in humans using zoom-like changes in the spatial frequency content of the stimulus. The human data replicates previous findings showing perceived speed to be positively biased by spatial frequency increments. A Bayesian observer who combines a Gaussian likelihood centered at the true speed and a spatial frequency dependent width with a \"slow speed prior\" successfully accounts for the perceptual bias. More precisely, the bias arises from a decrease in the observer's likelihood width estimated from the experiments as the spatial frequency increases. Such a trend is compatible with the trend of the dynamic texture likelihood width.",
    "arxiv_url": "http://arxiv.org/abs/1611.01390v2",
    "pdf_url": "http://arxiv.org/pdf/1611.01390v2",
    "published_date": "2016-11-02",
    "categories": [
      "q-bio.NC",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d gaussian",
      "human",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Model-based Outdoor Performance Capture",
    "authors": [
      "Nadia Robertini",
      "Dan Casas",
      "Helge Rhodin",
      "Hans-Peter Seidel",
      "Christian Theobalt"
    ],
    "abstract": "We propose a new model-based method to accurately reconstruct human performances captured outdoors in a multi-camera setup. Starting from a template of the actor model, we introduce a new unified implicit representation for both, articulated skeleton tracking and nonrigid surface shape refinement. Our method fits the template to unsegmented video frames in two stages - first, the coarse skeletal pose is estimated, and subsequently non-rigid surface shape and body pose are jointly refined. Particularly for surface shape refinement we propose a new combination of 3D Gaussians designed to align the projected model with likely silhouette contours without explicit segmentation or edge detection. We obtain reconstructions of much higher quality in outdoor settings than existing methods, and show that we are on par with state-of-the-art methods on indoor scenes for which they were designed",
    "arxiv_url": "http://arxiv.org/abs/1610.06740v1",
    "pdf_url": "http://arxiv.org/pdf/1610.06740v1",
    "published_date": "2016-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "outdoor",
      "face",
      "body",
      "3d gaussian",
      "human",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stability of 3D Gaussian vortices in an unbounded, rotating, vertically-stratified, Boussinesq flow: Linear analysis",
    "authors": [
      "Mani Mahdinia",
      "Pedram Hassanzadeh",
      "Philip S. Marcus",
      "Chung-Hsiang Jiang"
    ],
    "abstract": "The linear stability of three-dimensional (3D) vortices in rotating, stratified flows has been studied by analyzing the non-hydrostatic inviscid Boussinesq equations. We have focused on a widely-used model of geophysical and astrophysical vortices, which assumes an axisymmetric Gaussian structure for pressure anomalies in the horizontal and vertical directions. For a range of Rossby number ($-0.5 < Ro < 0.5$) and Burger number ($0.02 < Bu < 2.3$) relevant to observed long-lived vortices, the growth rate and spatial structure of the most unstable eigenmodes have been numerically calculated and presented as a function of $Ro-Bu$. We have found neutrally-stable vortices only over a small region of the $Ro-Bu$ parameter space: cyclones with $Ro \\sim 0.02-0.05$ and $Bu \\sim 0.85-0.95$. However, we have also found that anticyclones in general have slower growth rates compared to cyclones. In particular, the growth rate of the most unstable eigenmode for anticyclones in a large region of the parameter space (e.g., $Ro<0$ and $0.5 \\lesssim Bu \\lesssim 1.3$) is slower than $50$ turn-around times of the vortex (which often corresponds to several years for ocean eddies). For cyclones, the region with such slow growth rates is confined to $0<Ro<0.1$ and $0.5 \\lesssim Bu \\lesssim 1.3$. While most calculations have been done for $f/\\bar{N}=0.1$ (where $f$ and $\\bar{N}$ are the Coriolis and background Brunt-V\\\"ais\\\"al\\\"a frequencies), we have numerically verified and explained analytically, using non-dimensionalized equations, the insensitivity of the results to reducing $f/\\bar{N}$ to the more ocean-relevant value of $0.01$. The results of this paper provide a steppingstone to study the more complicated problems of the stability of geophysical (e.g., those in the atmospheres of giant planets) and astrophysical vortices (in accretion disks).",
    "arxiv_url": "http://arxiv.org/abs/1605.06859v3",
    "pdf_url": "http://arxiv.org/pdf/1605.06859v3",
    "published_date": "2016-05-22",
    "categories": [
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Multi-view Performance Capture of Fine-Scale Surface Detail",
    "authors": [
      "Nadia Robertini",
      "Edilson De Aguiar",
      "Thomas Helten",
      "Christian Theobalt"
    ],
    "abstract": "We present a new effective way for performance capture of deforming meshes with fine-scale time-varying surface detail from multi-view video. Our method builds up on coarse 4D surface reconstructions, as obtained with commonly used template-based methods. As they only capture models of coarse-to-medium scale detail, fine scale deformation detail is often done in a second pass by using stereo constraints, features, or shading-based refinement. In this paper, we propose a new effective and stable solution to this second step. Our framework creates an implicit representation of the deformable mesh using a dense collection of 3D Gaussian functions on the surface, and a set of 2D Gaussians for the images. The fine scale deformation of all mesh vertices that maximizes photo-consistency can be efficiently found by densely optimizing a new model-to-image consistency energy on all vertex positions. A principal advantage is that our problem formulation yields a smooth closed form energy with implicit occlusion handling and analytic derivatives. Error-prone correspondence finding, or discrete sampling of surface displacement values are also not needed. We show several reconstructions of human subjects wearing loose clothing, and we qualitatively and quantitatively show that we robustly capture more detail than related methods.",
    "arxiv_url": "http://arxiv.org/abs/1602.02023v1",
    "pdf_url": "http://arxiv.org/pdf/1602.02023v1",
    "published_date": "2016-02-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "deformation",
      "3d gaussian",
      "human",
      "4d",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation",
    "authors": [
      "Holger R. Roth",
      "Le Lu",
      "Amal Farag",
      "Hoo-Chang Shin",
      "Jiamin Liu",
      "Evrim Turkbey",
      "Ronald M. Summers"
    ],
    "abstract": "Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-to-fine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via $P{-}\\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional ConvNet ($R_1{-}\\mathrm{ConvNet}$) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a \"zoom-out\" fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked $R_2{-}\\mathrm{ConvNet}$ leveraging the joint space of CT intensities and the $P{-}\\mathrm{ConvNet}$ dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6$\\pm$6.3% in training and 71.8$\\pm$10.7% in testing.",
    "arxiv_url": "http://arxiv.org/abs/1506.06448v1",
    "pdf_url": "http://arxiv.org/pdf/1506.06448v1",
    "published_date": "2015-06-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "medical",
      "3d gaussian",
      "ar",
      "segmentation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uniform Triangles with Equality Constraints",
    "authors": [
      "Steven R. Finch"
    ],
    "abstract": "The equality constraint a+b+c=1 for random triangle sides corresponds to breaking a stick in two places. An analog a^2+b^2+c^2=1 has a remarkable feature: the bivariate density for angles coincides with that for 3D Gaussian triangles. Interesting complications also arise for a+b=1 and for a^2+b^2=1, with the understanding that the angle gamma opposite side c is Uniform[0,pi]. Closed-form expressions for several side moments remain open.",
    "arxiv_url": "http://arxiv.org/abs/1411.5216v2",
    "pdf_url": "http://arxiv.org/pdf/1411.5216v2",
    "published_date": "2014-11-19",
    "categories": [
      "math.PR",
      "math.MG",
      "60D05 (Primary) 51M04, 51M25, 62H10, 62E15, 52A22, 52A38 (Secondary)"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Superconducting properties in tantalum decorated three-dimensional graphene and carbon structures",
    "authors": [
      "Cayetano S. F. Cobaleda",
      "Xiaoyin Xiao",
      "D. Bruce Burckel",
      "Ronen Polsky",
      "Duanni Huang",
      "Enrique Diez",
      "W. Pan"
    ],
    "abstract": "We present here the results on superconducting properties in tantalum thin films (100nm thick) deposited on three-dimensional graphene (3DG) and carbon structures. A superconducting transition is observed in both composite thin films with a superconducting transition temperature of 1.2K and 1.0K, respectively. We have further measured the magnetoresistance at various temperatures and differential resistance dV/dI at different magnetic fields in these two composite thin films. In both samples, a much large critical magnetic field (~ 2 Tesla) is observed and this critical magnetic field shows linear temperature dependence. Finally, an anomalously large cooling effect was observed in the differential resistance measurements in our 3DG-tantalum device when the sample turns superconducting. Our results may have important implications in flexible superconducting electronic device applications.",
    "arxiv_url": "http://arxiv.org/abs/1408.5679v1",
    "pdf_url": "http://arxiv.org/pdf/1408.5679v1",
    "published_date": "2014-08-25",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "What are we missing in elliptical galaxies ?",
    "authors": [
      "Jeremy Mould"
    ],
    "abstract": "The scaling relation for early type galaxies in the 6dF galaxy survey does not have the velocity dispersion dependence expected from standard stellar population models. As noted in recent work with SDSS, there seems to be an additional dependence of mass to light ratio with velocity dispersion, possibly due to a bottom heavy initial mass function. Here we offer a new understanding of the 6dF galaxy survey 3D gaussian Fundamental Plane in terms of a parameterized Jeans equation, but leave mass dependence of M/L and mass dependence of structure still degenerate with just the present constraints. Hybrid models have been proposed recently. Our new analysis brings into focus promising lines of enquiry which could be pursued to lift this degeneracy, including stellar atmospheres computation, kinematic probes of ellipticals at large radius, and a large sample of one micron spectra.",
    "arxiv_url": "http://arxiv.org/abs/1403.1623v1",
    "pdf_url": "http://arxiv.org/pdf/1403.1623v1",
    "published_date": "2014-03-06",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "survey",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Local softening of information geometric indicators of chaos in statistical modeling in the presence of quantum-like considerations",
    "authors": [
      "Adom Giffin",
      "S. A. Ali",
      "Carlo Cafaro"
    ],
    "abstract": "In a previous paper (C. Cafaro et al., 2012), we compared an uncorrelated 3D Gaussian statistical model to an uncorrelated 2D Gaussian statistical model obtained from the former model by introducing a constraint that resembles the quantum mechanical canonical minimum uncertainty relation. Analysis was completed by way of the information geometry and the entropic dynamics of each system. This analysis revealed that the chaoticity of the 2D Gaussian statistical model, quantified by means of the Information Geometric Entropy (IGE), is softened or weakened with respect to the chaoticity of the 3D Gaussian statistical model due to the accessibility of more information. In this companion work, we further constrain the system in the context of a correlation constraint among the system's micro-variables and show that the chaoticity is further weakened, but only locally. Finally, the physicality of the constraints is briefly discussed, particularly in the context of quantum entanglement.",
    "arxiv_url": "http://arxiv.org/abs/1308.4679v2",
    "pdf_url": "http://arxiv.org/pdf/1308.4679v2",
    "published_date": "2013-08-21",
    "categories": [
      "nlin.CD",
      "quant-ph"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d gaussian",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Softening the Complexity of Entropic Motion on Curved Statistical Manifolds",
    "authors": [
      "Carlo Cafaro",
      "Adom Giffin",
      "Cosmo Lupo",
      "Stefano Mancini"
    ],
    "abstract": "We study the information geometry and the entropic dynamics of a 3D Gaussian statistical model. We then compare our analysis to that of a 2D Gaussian statistical model obtained from the higher-dimensional model via introduction of an additional information constraint that resembles the quantum mechanical canonical minimum uncertainty relation. We show that the chaoticity (temporal complexity) of the 2D Gaussian statistical model, quantified by means of the Information Geometric Entropy (IGE) and the Jacobi vector field intensity, is softened with respect to the chaoticity of the 3D Gaussian statistical model.",
    "arxiv_url": "http://arxiv.org/abs/1110.6714v1",
    "pdf_url": "http://arxiv.org/pdf/1110.6714v1",
    "published_date": "2011-10-31",
    "categories": [
      "math-ph",
      "math.MP",
      "quant-ph"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "geometry",
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Direct evidence of dust growth in L183 from MIR light scattering",
    "authors": [
      "J. Steinacker",
      "L. Pagani",
      "A. Bacmann",
      "S. Guieu"
    ],
    "abstract": "Theoretical arguments suggest that dust grains should grow in the dense cold parts of molecular clouds. Evidence of larger grains has so far been gathered in near/mid infrared extinction and millimeter observations. Interpreting the data is, however, aggravated by the complex interplay of density and dust properties (as well as temperature for thermal emission). We present new Spitzer data of L183 in bands that are sensitive and insensitive to PAHs. The visual extinction AV map derived in a former paper was fitted by a series of 3D Gaussian distributions. For different dust models, we calculate the scattered MIR radiation images of structures that agree agree with the AV map and compare them to the Spitzer data. The Spitzer data of L183 show emission in the 3.6 and 4.5 micron bands, while the 5.8 micron band shows slight absorption. The emission layer of stochastically heated particles should coincide with the layer of strongest scattering of optical interstellar radiation, which is seen as an outer surface on I band images different from the emission region seen in the Spitzer images. Moreover, PAH emission is expected to strongly increase from 4.5 to 5.8 micron, which is not seen. Hence, we interpret this emission to be MIR cloudshine. Scattered light modeling when assuming interstellar medium dust grains without growth does not reproduce flux measurable by Spitzer. In contrast, models with grains growing with density yield images with a flux and pattern comparable to the Spitzer images in the bands 3.6, 4.5, and 8.0 micron.",
    "arxiv_url": "http://arxiv.org/abs/0912.0145v2",
    "pdf_url": "http://arxiv.org/pdf/0912.0145v2",
    "published_date": "2009-12-01",
    "categories": [
      "astro-ph.GA"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recent HBT results in Au+Au and p+p collisions from PHENIX",
    "authors": [
      "A. M. Glenn"
    ],
    "abstract": "We present Hanbury-Brown Twiss measurements from the PHENIX experiment at RHIC for final results for charged kaon pairs from sqrt{s_{NN}} = 200 GeV Au+Au collisions and preliminary results for charged pion pairs from sqrt{s} = 200 GeV p+p collisions. We find that for kaon pairs from Au+Au, each traditional 3D Gaussian radius shows approximately the same linear increase as a function of N^{1/3}_{part}. An imaging analysis reveals a significant non-Gaussian tail for r \\gtrsim 10 fm. The presence of a tail for kaon pairs demonstrates that similar non-Gaussian tails observed in earlier pion measurements cannot be fully explained by decays of long-lived resonances. The preliminary analysis of pions from sqrt{s} = 200 GeV p+p minimum biased collisions show correlations which are well suited to traditional 3D HBT radii extraction via the Bowler-Sinyukov method, and we present R_out, R_side, and R_long as a function of mean transverse pair mass.",
    "arxiv_url": "http://arxiv.org/abs/0907.5002v2",
    "pdf_url": "http://arxiv.org/pdf/0907.5002v2",
    "published_date": "2009-07-28",
    "categories": [
      "nucl-ex"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The Local Theory of the Cosmic Skeleton",
    "authors": [
      "D. Pogosyan",
      "C. Pichon",
      "C. Gay",
      "S. Prunet",
      "J. F. Cardoso",
      "T. Sousbie",
      "S. Colombi"
    ],
    "abstract": "The local theory of the critical lines of 2D and 3D Gaussian fields that underline the cosmic structures is presented. In the context of cosmological matter distribution the subset of critical lines of the 3D density field serves to delineate the skeleton of the observed filamentary structure at large scales. A stiff approximation used to quantitatively describe the filamentary skeleton shows that the flux of the skeleton lines is related to the average Gaussian curvature of the 1D (2D) sections of the field, much in the same way as the density of the peaks. The distribution of the length of the critical lines with threshold is analyzed in detail, while the extended descriptors of the skeleton - its curvature and its singular points, are introduced and briefly described. Theoretical predictions are compared to measurements of the skeleton in realizations of Gaussian random fields in 2D and 3D. It is found that the stiff approximation predicts accurately the shape of the differential length, allows for analytical insight, and explicit closed form solutions. Finally, it provides a simple classification of the singular points of the critical lines: i) critical points; ii) bifurcation points; iii) slopping plateaux.",
    "arxiv_url": "http://arxiv.org/abs/0811.1530v1",
    "pdf_url": "http://arxiv.org/pdf/0811.1530v1",
    "published_date": "2008-11-10",
    "categories": [
      "astro-ph"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Non-Perturbative U(1) Gauge Theory at Finite Temperature",
    "authors": [
      "Bernd A. Berg",
      "Alexei Bazavov"
    ],
    "abstract": "For compact U(1) lattice gauge theory (LGT) we have performed a finite size scaling analysis on $N_{\\tau} N_s^3$ lattices for $N_{\\tau}$ fixed by extrapolating spatial volumes of size $N_s\\le 18$ to $N_s\\to\\infty$. Within the numerical accuracy of the thus obtained fits we find for $N_{\\tau}=4$, 5 and~6 second order critical exponents, which exhibit no obvious $N_{\\tau}$ dependence. The exponents are consistent with 3d Gaussian values, but not with either first order transitions or the universality class of the 3d XY model. As the 3d Gaussian fixed point is known to be unstable, the scenario of a yet unidentified non-trivial fixed point close to the 3d Gaussian emerges as one of the possible explanations.",
    "arxiv_url": "http://arxiv.org/abs/hep-lat/0605019v3",
    "pdf_url": "http://arxiv.org/pdf/hep-lat/0605019v3",
    "published_date": "2006-05-29",
    "categories": [
      "hep-lat",
      "cond-mat.stat-mech"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Statistical properties of pinning fields in the 3d-Gaussian RFIM",
    "authors": [
      "Xavier Illa",
      "Eduard Vives"
    ],
    "abstract": "We have defined pinning fields as those random fields that keep some of the magnetic moments unreversed in the region of negative external applied field during the demagnetizing process. An analysis of the statistical properties of such pinning fields is presented within the context of the Gaussian Random Field Ising Model (RFIM). We show that the average of the pinning fields exhibits a drastic increase close to the coercive field and that such an increase is discontinuous for low degrees of disorder. This behaviour can be described with standard finite size scaling (FSS) assumptions. Furthermore, we also show that the pinning fields corresponding to states close to coercivity exhibit strong statistical correlations.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/0605340v1",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/0605340v1",
    "published_date": "2006-05-12",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Factorization Properties in the 3D Edwards-Anderson Model",
    "authors": [
      "Pierluigi Contucci",
      "Cristian Giardina"
    ],
    "abstract": "Starting from the study of a linear combination of multi-overlaps which can be rigorously shown to vanish for large systems we numerically analyze the factorization properties of the link-overlaps multi-distribution for the 3D Gaussian Edward-Anderson spin-glass model. We find evidence of a pure factorization law for the multi-correlation functions. For instance the quantity [<Q_{12}^2> - <Q_{12}Q_{34}>]/<Q_{12}^2> tends to zero at increasing volumes. We also perform the same analysis for the standard overlap for which instead the lack of factorization persists increasing the size of the system. The necessity of a better understanding of the mutual relation between the two overlaps is pointed out.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/0503155v2",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/0503155v2",
    "published_date": "2005-03-07",
    "categories": [
      "cond-mat.dis-nn",
      "math-ph",
      "math.MP"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Continuum radiative transfer in complex dust configurations around young stellar objects and active nuclei II. 3D Structure of the dense molecular cloud core Rho Oph D",
    "authors": [
      "J. Steinacker",
      "A. Bacmann",
      "Th. Henning",
      "R. Klessen",
      "M. Stickel"
    ],
    "abstract": "Constraints on the density and thermal 3D structure of the dense molecular cloud core Rho Oph D are derived from a detailed 3D radiative transfer modeling. Two ISOCAM images at 7 and 15 micron are fitted simultaneously by representing the dust distribution in the core with a series of 3D Gaussian density profiles. Size, total density, and position of the Gaussians are optimized by simulated annealing to obtain a 2D column density map. The projected core density has a complex elongated pattern with two peaks. We propose a new method to calculate an approximate temperature in an externally illuminated complex 3D structure from a mean optical depth. This T(tau)-method is applied to a 1.3 mm map obtained with the IRAM 30m telescope to find the approximate 3D density and temperature distribution of the core Rho Oph D. The spatial 3D distribution deviates strongly from spherical symmetry. The elongated structure is in general agreement with recent gravo-turbulent collapse calculations for molecular clouds. We discuss possible ambiguities of the background determination procedure, errors of the maps, the accuracy of the T(tau)-method, and the influence of the assumed dust particle sizes and properties.",
    "arxiv_url": "http://arxiv.org/abs/astro-ph/0410635v2",
    "pdf_url": "http://arxiv.org/pdf/astro-ph/0410635v2",
    "published_date": "2004-10-26",
    "categories": [
      "astro-ph"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spanning avalanches in the three-dimensional Gaussian Random Field Ising Model with metastable dynamics: field dependence and geometrical properties",
    "authors": [
      "Francisco-Jose Perez-Reche",
      "Eduard Vives"
    ],
    "abstract": "Spanning avalanches in the 3D Gaussian Random Field Ising Model (3D-GRFIM) with metastable dynamics at T=0 have been studied. Statistical analysis of the field values for which avalanches occur has enabled a Finite-Size Scaling (FSS) study of the avalanche density to be performed. Furthermore, direct measurement of the geometrical properties of the avalanches has confirmed an earlier hypothesis that several kinds of spanning avalanches with two different fractal dimensions coexist at the critical point. We finally compare the phase diagram of the 3D-GRFIM with metastable dynamics with the same model in equilibrium at T=0.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/0403754v1",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/0403754v1",
    "published_date": "2004-03-31",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Finite Size Scaling analysis of the avalanches in the 3d Gaussian Random Field Ising Model with metastable dynamics",
    "authors": [
      "F. J. Perez-Reche",
      "Eduard Vives"
    ],
    "abstract": "A numerical study is presented of the 3d Gaussian Random Field Ising Model at T=0 driven by an external field. Standard synchronous relaxation dynamics is employed to obtain the magnetization versus field hysteresis loops. The focus is on the analysis of the number and size distribution of the magnetization avalanches. They are classified as being non-spanning, 1d-spanning, 2d-spanning or 3d-spanning depending on whether or not they span the whole lattice in the different space directions. Moreover, finite-size scaling analysis enables identification of two different types of non-spanning avalanches (critical and supercritical) and two different types of 3d-spanning avalanches (critical and subcritical), whose numbers increase with L as a power-law with different exponents. We conclude by giving a scenario for the avalanches behaviour in the thermodynamic limit.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/0206075v3",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/0206075v3",
    "published_date": "2002-06-06",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "The three-dimensional random field Ising magnet: interfaces, scaling, and the nature of states",
    "authors": [
      "A. Alan Middleton",
      "Daniel S. Fisher"
    ],
    "abstract": "The nature of the zero temperature ordering transition in the 3D Gaussian random field Ising magnet is studied numerically, aided by scaling analyses. In the ferromagnetic phase the scaling of the roughness of the domain walls, $w\\sim L^\\zeta$, is consistent with the theoretical prediction $\\zeta = 2/3$. As the randomness is increased through the transition, the probability distribution of the interfacial tension of domain walls scales as for a single second order transition. At the critical point, the fractal dimensions of domain walls and the fractal dimension of the outer surface of spin clusters are investigated: there are at least two distinct physically important fractal dimensions. These dimensions are argued to be related to combinations of the energy scaling exponent, $\\theta$, which determines the violation of hyperscaling, the correlation length exponent $\\nu$, and the magnetization exponent $\\beta$. The value $\\beta = 0.017\\pm 0.005$ is derived from the magnetization: this estimate is supported by the study of the spin cluster size distribution at criticality. The variation of configurations in the interior of a sample with boundary conditions is consistent with the hypothesis that there is a single transition separating the disordered phase with one ground state from the ordered phase with two ground states. The array of results are shown to be consistent with a scaling picture and a geometric description of the influence of boundary conditions on the spins. The details of the algorithm used and its implementation are also described.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/0107489v1",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/0107489v1",
    "published_date": "2001-07-24",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tunneling of a Massless Field through a 3D Gaussian Barrier",
    "authors": [
      "G. Modanese"
    ],
    "abstract": "We propose a method for the approximate computation of the Green function of a scalar massless field Phi subjected to potential barriers of given size and shape in spacetime. This technique is applied to the case of a 3D gaussian ellipsoid-like barrier, placed on the axis between two pointlike sources of the field. Instead of the Green function we compute its temporal integral, that gives the static potential energy of the interaction of the two sources. Such interaction takes place in part by tunneling of the quanta of Phi across the barrier. We evaluate numerically the correction to the potential in dependence on the size of the barrier and on the barrier-sources distance.",
    "arxiv_url": "http://arxiv.org/abs/hep-th/9808009v2",
    "pdf_url": "http://arxiv.org/pdf/hep-th/9808009v2",
    "published_date": "1998-08-03",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Equilibrium and off-equilibrium simulations of the 4d Gaussian spin glass",
    "authors": [
      "Giorgio Parisi",
      "Federico Ricci-Tersenghi",
      "Juan J. Ruiz-Lorenzo"
    ],
    "abstract": "In this paper we study the on and off-equilibrium properties of the four dimensional Gaussian spin glass. In the static case we determine with more precision that in previous simulations both the critical temperature as well as the critical exponents. In the off-equilibrium case we settle the general form of the autocorrelation function, and show that is possible to obtain dynamically, for the first time, a value for the order parameter.",
    "arxiv_url": "http://arxiv.org/abs/cond-mat/9606051v2",
    "pdf_url": "http://arxiv.org/pdf/cond-mat/9606051v2",
    "published_date": "1996-06-09",
    "categories": [
      "cond-mat"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]