[
  {
    "title": "MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering",
    "authors": [
      "Jaehoon Choi",
      "Yonghan Lee",
      "Hyungtae Lee",
      "Heesung Kwon",
      "Dinesh Manocha"
    ],
    "abstract": "Recently, 3D Gaussian splatting has gained attention for its capability to generate high-fidelity rendering results. At the same time, most applications such as games, animation, and AR/VR use mesh-based representations to represent and render 3D scenes. We propose a novel approach that integrates mesh representation with 3D Gaussian splats to perform high-quality rendering of reconstructed real-world scenes. In particular, we introduce a distance-based Gaussian splatting technique to align the Gaussian splats with the mesh surface and remove redundant Gaussian splats that do not contribute to the rendering. We consider the distance between each Gaussian splat and the mesh surface to distinguish between tightly-bound and loosely-bound Gaussian splats. The tightly-bound splats are flattened and aligned well with the mesh geometry. The loosely-bound Gaussian splats are used to account for the artifacts in reconstructed 3D meshes in terms of rendering. We present a training strategy of binding Gaussian splats to the mesh geometry, and take into account both types of splats. In this context, we introduce several regularization techniques aimed at precisely aligning tightly-bound Gaussian splats with the mesh surface during the training process. We validate the effectiveness of our method on large and unbounded scene from mip-NeRF 360 and Deep Blending datasets. Our method surpasses recent mesh-based neural rendering techniques by achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset, demonstrating better rendering quality. We provide analyses for each type of Gaussian splat and achieve a reduction in the number of Gaussian splats by 30% compared to the original 3D Gaussian splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "neural rendering",
      "high-fidelity",
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
    "authors": [
      "Ola Shorinwa",
      "Jiankai Sun",
      "Mac Schwager"
    ],
    "abstract": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "localization",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffGS: Functional Gaussian Splatting Diffusion",
    "authors": [
      "Junsheng Zhou",
      "Weiqi Zhang",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A New Split Algorithm for 3D Gaussian Splatting",
    "authors": [
      "Qiyuan Feng",
      "Gengchen Cao",
      "Haoxiang Chen",
      "Tai-Jiang Mu",
      "Ralph R. Martin",
      "Shi-Min Hu"
    ],
    "abstract": "3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-14",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
    "authors": [
      "Raphael Du Sablon",
      "David Hart"
    ],
    "abstract": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "fast",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate Radiance Fields",
    "authors": [
      "Jungeon Kim",
      "Geonsoo Park",
      "Seungyong Lee"
    ],
    "abstract": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STGA: Selective-Training Gaussian Head Avatars",
    "authors": [
      "Hanzhi Guo",
      "Yixiao Chen",
      "Dongye Xiaonuo",
      "Zeyu Tian",
      "Dongdong Weng",
      "Le Luo"
    ],
    "abstract": "We propose selective-training Gaussian head avatars (STGA) to enhance the details of dynamic head Gaussian. The dynamic head Gaussian model is trained based on the FLAME parameterized model. Each Gaussian splat is embedded within the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before training, our selection strategy calculates the 3D Gaussian splat to be optimized in each frame. The parameters of these 3D Gaussian splats are optimized in the training of each frame, while those of the other splats are frozen. This means that the splats participating in the optimization process differ in each frame, to improve the realism of fine details. Compared with network-based methods, our method achieves better results with shorter training time. Compared with mesh-based methods, our method produces more realistic details within the same training time. Additionally, the ablation experiment confirms that our method effectively enhances the quality of details.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "head",
      "ar",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing",
    "authors": [
      "Xiaowei Song",
      "Jv Zheng",
      "Shiran Yuan",
      "Huan-ang Gao",
      "Jingwei Zhao",
      "Xiang He",
      "Weihao Gu",
      "Hao Zhao"
    ],
    "abstract": "In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at https://github.com/zsy1987/SA-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and Better Geometry Reconstruction",
    "authors": [
      "Ruihan Yu",
      "Tianyu Huang",
      "Jingwang Ling",
      "Feng Xu"
    ],
    "abstract": "2D Gaussian Splatting has recently emerged as a significant method in 3D reconstruction, enabling novel view synthesis and geometry reconstruction simultaneously. While the well-known Gaussian kernel is broadly used, its lack of anisotropy and deformation ability leads to dim and vague edges at object silhouettes, limiting the reconstruction quality of current Gaussian splatting methods. To enhance the representation power, we draw inspiration from quantum physics and propose to use the Gaussian-Hermite kernel as the new primitive in Gaussian splatting. The new kernel takes a unified mathematical form and extends the Gaussian function, which serves as the zero-rank term in the updated formulation. Our experiments demonstrate the extraordinary performance of Gaussian-Hermite kernel in both geometry reconstruction and novel-view synthesis tasks. The proposed kernel outperforms traditional Gaussian Splatting kernels, showcasing its potential for high-quality 3D reconstruction and rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSVC: Efficient Video Representation and Compression Through 2D Gaussian Splatting",
    "authors": [
      "Longan Wang",
      "Yuang Shi",
      "Wei Tsang Ooi"
    ],
    "abstract": "3D Gaussian splats have emerged as a revolutionary, effective, learned representation for static 3D scenes. In this work, we explore using 2D Gaussian splats as a new primitive for representing videos. We propose GSVC, an approach to learning a set of 2D Gaussian splats that can effectively represent and compress video frames. GSVC incorporates the following techniques: (i) To exploit temporal redundancy among adjacent frames, which can speed up training and improve the compression efficiency, we predict the Gaussian splats of a frame based on its previous frame; (ii) To control the trade-offs between file size and quality, we remove Gaussian splats with low contribution to the video quality; (iii) To capture dynamics in videos, we randomly add Gaussian splats to fit content with large motion or newly-appeared objects; (iv) To handle significant changes in the scene, we detect key frames based on loss differences during the learning process. Experiment results show that GSVC achieves good rate-distortion trade-offs, comparable to state-of-the-art video codecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080 video.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaDe-GS: Rasterizing Depth in Gaussian Splatting",
    "authors": [
      "Baowen Zhang",
      "Chuan Fang",
      "Rakesh Shrestha",
      "Yixun Liang",
      "Xiaoxiao Long",
      "Ping Tan"
    ],
    "abstract": "Gaussian Splatting (GS) has proven to be highly effective in novel view synthesis, achieving high-quality and real-time rendering. However, its potential for reconstructing detailed 3D shapes has not been fully explored. Existing methods often suffer from limited shape accuracy due to the discrete and unstructured nature of Gaussian splats, which complicates the shape extraction. While recent techniques like 2D GS have attempted to improve shape reconstruction, they often reformulate the Gaussian primitives in ways that reduce both rendering quality and computational efficiency. To address these problems, our work introduces a rasterized approach to render the depth maps and surface normal maps of general 3D Gaussian splats. Our method not only significantly enhances shape reconstruction accuracy but also maintains the computational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and maintains similar computational efficiency as the original 3D GS methods. Our method is a significant advancement in Gaussian Splatting and can be directly integrated into existing Gaussian Splatting-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "shape reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
    "authors": [
      "Haofei Xu",
      "Songyou Peng",
      "Fangjinhua Wang",
      "Hermann Blum",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "Gaussian splatting and single-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale multi-view posed datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. In addition, DepthSplat enables feed-forward reconstruction from 12 input views (512x960 resolutions) in 0.6 seconds.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction with Geometry Constrain",
    "authors": [
      "Butian Xiong",
      "Xiaoyu Ye",
      "Tze Ho Elden Tse",
      "Kai Han",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "With the emergence of Gaussian Splats, recent efforts have focused on large-scale scene geometric reconstruction. However, most of these efforts either concentrate on memory reduction or spatial space division, neglecting information in the semantic space. In this paper, we propose a novel method, named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware 3D Gaussian Splats. Specifically, we leverage prior information stored in large vision models such as SAM and DINO to generate semantic masks. We then introduce a geometric complexity measurement function to serve as soft regularization, guiding the shape of each Gaussian Splat within specific semantic areas. Additionally, we present a method that estimates the expected number of Gaussian Splats in different semantic areas, effectively providing a lower bound for Gaussian Splats in these areas. Subsequently, we extract the point cloud using a novel probability density-based extraction method, transforming Gaussian Splats into a point cloud crucial for downstream tasks. Our method also offers the potential for detailed semantic inquiries while maintaining high image-based reconstruction results. We provide extensive experiments on publicly available large-scale scene reconstruction datasets with highly accurate point clouds as ground truth and our novel dataset. Our results demonstrate the superiority of our method over current state-of-the-art Gaussian Splats reconstruction methods by a significant margin in terms of geometric-based measurement metrics. Code and additional results will soon be available on our project page.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "large scene",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features",
    "authors": [
      "Arnab Dey",
      "Cheng-You Lu",
      "Andrew I. Comport",
      "Srinath Sridhar",
      "Chin-Teng Lin",
      "Jean Martinet"
    ],
    "abstract": "Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "body",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction",
    "authors": [
      "Haoxuan Qu",
      "Yujun Cai",
      "Hossein Rahmani",
      "Ajay Kumar",
      "Junsong Yuan",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "high quality"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Recent Advances in 3D Gaussian Splatting",
    "authors": [
      "Tong Wu",
      "Yu-Jie Yuan",
      "Ling-Xiao Zhang",
      "Jie Yang",
      "Yan-Pei Cao",
      "Ling-Qi Yan",
      "Lin Gao"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis. Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique. This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "survey",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering",
    "authors": [
      "Abdullah Hamdi",
      "Luke Melas-Kyriazi",
      "Jinjie Mai",
      "Guocheng Qian",
      "Ruoshi Liu",
      "Carl Vondrick",
      "Bernard Ghanem",
      "Andrea Vedaldi"
    ],
    "abstract": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-15",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance",
    "authors": [
      "Hanlin Chen",
      "Chen Li",
      "Yunsong Wang",
      "Gim Hee Lee"
    ],
    "abstract": "Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization. However, the reconstruction results still lack fine details because of the over-smoothed depth map or sparse point cloud. In this work, we propose a neural implicit surface reconstruction pipeline with guidance from 3D Gaussian Splatting to recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is that it can generate dense point clouds with detailed structure. Nonetheless, a naive adoption of 3D Gaussian Splatting can fail since the generated points are the centers of 3D Gaussians that do not necessarily lie on the surface. We thus introduce a scale regularizer to pull the centers close to the surface by enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine the point cloud from 3D Gaussians Splatting with the normal priors from the surface predicted by neural implicit models instead of using a fixed set of points as guidance. Consequently, the quality of surface reconstruction improves from the guidance of the more accurate 3D Gaussian splatting. By jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our approach benefits from both representations and generates complete surfaces with intricate details. Experiments on Tanks and Temples verify the effectiveness of our proposed method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OmniGS: Fast Radiance Field Reconstruction using Omnidirectional Gaussian Splatting",
    "authors": [
      "Longwei Li",
      "Huajian Huang",
      "Sai-Kit Yeung",
      "Hui Cheng"
    ],
    "abstract": "Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in various domains. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. We realize differentiable optimization of the omnidirectional radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. The code will be publicly available.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes",
    "authors": [
      "Stephen Pasch",
      "Joel K. Salzman",
      "Changxi Zheng"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "mapping",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian as a New Era: A Survey",
    "authors": [
      "Ben Fei",
      "Jingyi Xu",
      "Rui Zhang",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ying He"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "robotics",
      "ar",
      "mapping",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Camera Splatting for Continuous View Optimization",
    "authors": [
      "Gahye Lee",
      "Hyomin Kim",
      "Gwangjin Ju",
      "Jooeun Son",
      "Hyejeong Yoon",
      "Seungyong Lee"
    ],
    "abstract": "We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering",
    "authors": [
      "Yonghan Lee",
      "Jaehoon Choi",
      "Dongki Jung",
      "Jaeseong Yun",
      "Soohyun Ryu",
      "Dinesh Manocha",
      "Suyong Yeon"
    ],
    "abstract": "We present a novel-view rendering algorithm, Mode-GS, for ground-robot trajectory datasets. Our approach is based on using anchored Gaussian splats, which are designed to overcome the limitations of existing 3D Gaussian splatting algorithms. Prior neural rendering methods suffer from severe splat drift due to scene complexity and insufficient multi-view observation, and can fail to fix splats on the true geometry in ground-robot datasets. Our method integrates pixel-aligned anchors from monocular depths and generates Gaussian splats around these anchors using residual-form Gaussian decoders. To address the inherent scale ambiguity of monocular depth, we parameterize anchors with per-view depth-scales and employ scale-consistent depth loss for online scale calibration. Our method results in improved rendering performance, based on PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns, and achieves state-of-the-art rendering performance on the R3LIVE odometry dataset and the Tanks and Temples dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DisC-GS: Discontinuity-aware Gaussian Splatting",
    "authors": [
      "Haoxuan Qu",
      "Zhuoling Li",
      "Hossein Rahmani",
      "Yujun Cai",
      "Jun Liu"
    ],
    "abstract": "Recently, Gaussian Splatting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis. In this paper, we highlight a fundamental limitation of Gaussian Splatting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions. To address this issue, we propose a novel framework enabling Gaussian Splatting to perform discontinuity-aware image rendering. Additionally, we introduce a Bézier-boundary gradient approximation strategy within our framework to keep the \"differentiability\" of the proposed discontinuity-aware rendering process. Extensive experiments demonstrate the efficacy of our framework.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications",
    "authors": [
      "Naruya Kondo",
      "Yuto Asano",
      "Yoichi Ochiai"
    ],
    "abstract": "We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: https://gaussian-vrm.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "dynamic",
      "ar",
      "gaussian splatting",
      "avatar",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision",
    "authors": [
      "Alberta Longhini",
      "Marcel Büsching",
      "Bardienus P. Duisterhof",
      "Jens Lundell",
      "Jeffrey Ichnowski",
      "Mårten Björkman",
      "Danica Kragic"
    ],
    "abstract": "We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering",
    "authors": [
      "Lukas Radl",
      "Michael Steiner",
      "Mathias Parger",
      "Alexander Weinrauch",
      "Bernhard Kerbl",
      "Markus Steinberger"
    ],
    "abstract": "Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "motion",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment",
    "authors": [
      "Haoran Wang",
      "Jingwei Huang",
      "Lu Yang",
      "Tianchen Deng",
      "Gaojing Zhang",
      "Mingrui Li"
    ],
    "abstract": "3D Gaussian Splatting has shown remarkable capabilities in novel view rendering tasks and exhibits significant potential for multi-view optimization.However, the original 3D Gaussian Splatting lacks color representation for inputs in low-light environments. Simply using enhanced images as inputs would lead to issues with multi-view consistency, and current single-view enhancement systems rely on pre-trained data, lacking scene generalization. These problems limit the application of 3D Gaussian Splatting in low-light conditions in the field of robotics, including high-fidelity modeling and feature matching. To address these challenges, we propose an unsupervised multi-view stereoscopic system based on Gaussian Splatting, called Low-Light Gaussian Splatting (LLGS). This system aims to enhance images in low-light environments while reconstructing the scene. Our method introduces a decomposable Gaussian representation called M-Color, which separately characterizes color information for targeted enhancement. Furthermore, we propose an unsupervised optimization method with zero-knowledge priors, using direction-based enhancement to ensure multi-view consistency. Experiments conducted on real-world datasets demonstrate that our system outperforms state-of-the-art methods in both low-light enhancement and 3D Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Laplace-Beltrami Operator for Gaussian Splatting",
    "authors": [
      "Hongyu Zhou",
      "Zorah Lähner"
    ],
    "abstract": "With the rising popularity of 3D Gaussian splatting and the expanse of applications from rendering to 3D reconstruction, there comes also a need for geometry processing applications directly on this new representation. While considering the centers of Gaussians as a point cloud or meshing them is an option that allows to apply existing algorithms, this might ignore information present in the data or be unnecessarily expensive. Additionally, Gaussian splatting tends to contain a large number of outliers which do not affect the rendering quality but need to be handled correctly in order not to produce noisy results in geometry processing applications. In this work, we propose a formulation to compute the Laplace-Beltrami operator, a widely used tool in geometry processing, directly on Gaussian splatting using the Mahalanobis distance. While conceptually similar to a point cloud Laplacian, our experiments show superior accuracy on the point clouds encoded in the Gaussian splatting centers and, additionally, the operator can be used to evaluate the quality of the output during optimization.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-24",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection Strategy",
    "authors": [
      "Letian Huang",
      "Jiayang Bai",
      "Jie Guo",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance, and robustness in sparse viewpoints, leading to various improvements. However, there has been a notable lack of attention to the fundamental problem of projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which can accommodate a variety of camera models. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification",
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ],
    "abstract": "In this study, we explore the essential challenge of fast scene optimization for Gaussian Splatting. Through a thorough analysis of the geometry modeling process, we reveal that dense point clouds can be effectively reconstructed early in optimization through Gaussian representations. This insight leads to our approach of aggressive Gaussian densification, which provides a more efficient alternative to conventional progressive densification methods. By significantly increasing the number of critical Gaussians, we enhance the model capacity to capture dense scene geometry at the early stage of optimization. This strategy is seamlessly integrated into the Mini-Splatting densification and simplification framework, enabling rapid convergence without compromising quality. Additionally, we introduce visibility culling within Gaussian Splatting, leveraging per-view Gaussian importance as precomputed visibility to accelerate the optimization process. Our Mini-Splatting2 achieves a balanced trade-off among optimization time, the number of Gaussians, and rendering quality, establishing a strong baseline for future Gaussian-Splatting-based works. Our work sets the stage for more efficient, high-quality 3D scene modeling in real-world applications, and the code will be made available no matter acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DoF Object Pose Dataset Generation",
    "authors": [
      "Lukas Meyer",
      "Floris Erich",
      "Yusuke Yoshiyasu",
      "Marc Stamminger",
      "Noriaki Ando",
      "Yukiyasu Domae"
    ],
    "abstract": "We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting.   Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. <i>PEGASUS</i> allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted.   Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Occam's LGS: An Efficient Approach for Language Gaussian Splatting",
    "authors": [
      "Jiahuan Cheng",
      "Jan-Nico Zaech",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene representation, offering efficient, high-quality reconstruction and rendering. A key reason for its success is the simplicity of representing scenes with sets of Gaussians, making it interpretable and adaptable. To enhance understanding beyond visual representation, recent approaches extend Gaussian Splatting with semantic vision-language features, enabling open-set tasks. Typically, these language features are aggregated from multiple 2D views, however, existing methods rely on cumbersome techniques, resulting in high computational costs and longer training times.   In this work, we show that the complicated pipelines for language 3D Gaussian Splatting are simply unnecessary. Instead, we follow a probabilistic formulation of Language Gaussian Splatting and apply Occam's razor to the task at hand, leading to a highly efficient weighted multi-view feature aggregation technique. Doing so offers us state-of-the-art results with a speed-up of two orders of magnitude without any compression, allowing for easy scene manipulation. Project Page: https://insait-institute.github.io/OccamLGS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Yehor Karpichev",
      "Brandon Haworth",
      "Homayoun Najjaran"
    ],
    "abstract": "Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "semantic",
      "robotics",
      "ar",
      "gaussian splatting",
      "nerf",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting",
    "authors": [
      "Yijia Guo",
      "Yuanxi Bai",
      "Liwen Hu",
      "Ziyi Guo",
      "Mianzhi Liu",
      "Yu Cai",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a real-time high-quality relighting method for Gaussian splats in low-frequency lighting environments that captures soft shadows and interreflections by precomputing 3D Gaussian splats' radiance transfer. Existing studies have demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields' efficiency for dynamic lighting scenarios. However, the current relighting method based on 3DGS still struggles to compute high-quality shadow and indirect illumination in real time for dynamic light, leading to unrealistic rendering results. We solve this problem by precomputing the expensive transport simulations required for complex transfer functions like shadowing, the resulting transfer functions are represented as dense sets of vectors or matrices for every Gaussian splat. We introduce distinct precomputing methods tailored for training and rendering stages, along with unique ray tracing and indirect lighting precomputation techniques for 3D Gaussian splats to accelerate training speed and compute accurate indirect lighting related to environment light. Experimental analyses demonstrate that our approach achieves state-of-the-art visual quality while maintaining competitive training times and allows high-quality real-time (30+ fps) relighting for dynamic light and relatively complex scenes at 1080p resolution.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "illumination",
      "ray tracing",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
    "authors": [
      "Alexander Valverde",
      "Brian Xu",
      "Yuyin Zhou",
      "Meng Xu",
      "Hongyun Wang"
    ],
    "abstract": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures",
    "authors": [
      "Sebastian Weiss",
      "Derek Bradley"
    ],
    "abstract": "Gaussian Splatting has recently emerged as the go-to representation for reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian primitives has further improved multi-view consistency and surface reconstruction accuracy. In this work we highlight the similarity between 2D Gaussian Splatting (2DGS) and billboards from traditional computer graphics. Both use flat semi-transparent 2D geometry that is positioned, oriented and scaled in 3D space. However 2DGS uses a solid color per splat and an opacity modulated by a Gaussian distribution, where billboards are more expressive, modulating the color with a uv-parameterized texture. We propose to unify these concepts by presenting Gaussian Billboards, a modification of 2DGS to add spatially-varying color achieved using per-splat texture interpolation. The result is a mixture of the two representations, which benefits from both the robust scene optimization power of 2DGS and the expressiveness of texture mapping. We show that our method can improve the sharpness and quality of the scene representation in a wide range of qualitative and quantitative evaluations compared to the original 2DGS implementation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "mapping",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution",
    "authors": [
      "Pranav Chougule"
    ],
    "abstract": "In this paper, I present a comprehensive study comparing Photogrammetry and Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I created a dataset of images from a real-world scene and constructed 3D models using both methods. To evaluate the performance, I compared the models using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and lp/mm resolution based on the USAF resolution chart. A significant contribution of this work is the development of a modified Gaussian Splatting repository, which I forked and enhanced to enable rendering images from novel camera poses generated in the Blender environment. This innovation allows for the synthesis of high-quality novel views, showcasing the flexibility and potential of Gaussian Splatting. My investigation extends to an augmented dataset that includes both original ground images and novel views synthesized via Gaussian Splatting. This augmented dataset was employed to generate a new photogrammetry model, which was then compared against the original photogrammetry model created using only the original images. The results demonstrate the efficacy of using Gaussian Splatting to generate novel high-quality views and its potential to improve photogrammetry-based 3D reconstructions. The comparative analysis highlights the strengths and limitations of both approaches, providing valuable information for applications in extended reality (XR), photogrammetry, and autonomous vehicle simulations. Code is available at https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BOGausS: Better Optimized Gaussian Splatting",
    "authors": [
      "Stéphane Pateux",
      "Matthieu Gendrin",
      "Luce Morin",
      "Théo Ladune",
      "Xiaoran Jiang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaySplats: Ray Tracing based Gaussian Splatting",
    "authors": [
      "Krzysztof Byrski",
      "Marcin Mazur",
      "Jacek Tabor",
      "Tadeusz Dziarmaga",
      "Marcin Kądziołka",
      "Dawid Baran",
      "Przemysław Spurek"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a process that enables the direct creation of 3D objects from 2D images. This representation offers numerous advantages, including rapid training and rendering. However, a significant limitation of 3DGS is the challenge of incorporating light and shadow reflections, primarily due to the utilization of rasterization rather than ray tracing for rendering. This paper introduces RaySplats, a model that employs ray-tracing based Gaussian Splatting. Rather than utilizing the projection of Gaussians, our method employs a ray-tracing mechanism, operating directly on Gaussian primitives represented by confidence ellipses with RGB colors. In practice, we compute the intersection between ellipses and rays to construct ray-tracing algorithms, facilitating the incorporation of meshes with Gaussian Splatting models and the addition of lights, shadows, and other related effects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature Splatting for Better Novel View Synthesis with Low Overlap",
    "authors": [
      "T. Berriel Martins",
      "Javier Civera"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first \"splatted\" into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. Code available at https://github.com/tberriel/FeatSplat .   Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator",
    "authors": [
      "Rami Wilson"
    ],
    "abstract": "Modern autonomous vehicle simulators feature an ever-growing library of assets, including vehicles, buildings, roads, pedestrians, and more. While this level of customization proves beneficial when creating virtual urban environments, this process becomes cumbersome when intending to train within a digital twin or a duplicate of a real scene. Gaussian splatting emerged as a powerful technique in scene reconstruction and novel view synthesis, boasting high fidelity and rendering speeds. In this paper, we introduce GSAVS, an autonomous vehicle simulator that supports the creation and development of autonomous vehicle models. Every asset within the simulator is a 3D Gaussian splat, including the vehicles and the environment. However, the simulator runs within a classical 3D engine, rendering 3D Gaussian splats in real-time. This allows the simulator to utilize the photorealism that 3D Gaussian splatting boasts while providing the customization and ease of use of a classical 3D engine.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions",
    "authors": [
      "AmirHossein Naghi Razlighi",
      "Elaheh Badali Golezani",
      "Shohreh Kasaei"
    ],
    "abstract": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment",
    "authors": [
      "Mangyu Kong",
      "Jaewon Lee",
      "Seongwon Lee",
      "Euntai Kim"
    ],
    "abstract": "We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic SLAM framework built on the foundation of Gaussian Splatting. While recent advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene representation, most approaches assume a static environment, making them vulnerable to photometric and geometric inconsistencies caused by dynamic objects. To address these challenges, we integrate Gaussian Splatting SLAM with a robust filtering process to handle dynamic objects throughout the entire pipeline, including Gaussian insertion and keyframe selection. Within this framework, to further improve the accuracy of dynamic object removal, we introduce a robust mask generation method that enforces photometric consistency across keyframes, reducing noise from inaccurate segmentation and artifacts such as shadows. Additionally, we propose the loop-aware window selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops between the current and past frames, facilitating joint optimization of the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art performance in both camera tracking and novel view synthesis on various dynamic SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "segmentation",
      "tracking",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NegGS: Negative Gaussian Splatting",
    "authors": [
      "Artur Kasymov",
      "Bartosz Czekaj",
      "Marcin Mazur",
      "Jacek Tabor",
      "Przemysław Spurek"
    ],
    "abstract": "One of the key advantages of 3D rendering is its ability to simulate intricate scenes accurately. One of the most widely used methods for this purpose is Gaussian Splatting, a novel approach that is known for its rapid training and inference capabilities. In essence, Gaussian Splatting involves incorporating data about the 3D objects of interest into a series of Gaussian distributions, each of which can then be depicted in 3D in a manner analogous to traditional meshes. It is regrettable that the use of Gaussians in Gaussian Splatting is currently somewhat restrictive due to their perceived linear nature. In practice, 3D objects are often composed of complex curves and highly nonlinear structures. This issue can to some extent be alleviated by employing a multitude of Gaussian components to reflect the complex, nonlinear structures accurately. However, this approach results in a considerable increase in time complexity. This paper introduces the concept of negative Gaussians, which are interpreted as items with negative colors. The rationale behind this approach is based on the density distribution created by dividing the probability density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian. Such a distribution can be used to approximate structures such as donut and moon-shaped datasets. Experimental findings indicate that the application of these techniques enhances the modeling of high-frequency elements with rapid color transitions. Additionally, it improves the representation of shadows. To the best of our knowledge, this is the first paper to extend the simple elipsoid shapes of Gaussian Splatting to more complex nonlinear structures.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-28",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gabor Splatting: Reconstruction of High-frequency Surface Texture using Gabor Noise",
    "authors": [
      "Haato Watanabe",
      "Kenji Tojo",
      "Nobuyuki Umetani"
    ],
    "abstract": "3D Gaussian splatting has experienced explosive popularity in the past few years in the field of novel view synthesis. The lightweight and differentiable representation of the radiance field using the Gaussian enables rapid and high-quality reconstruction and fast rendering. However, reconstructing objects with high-frequency surface textures (e.g., fine stripes) requires many skinny Gaussian kernels because each Gaussian represents only one color if viewed from one direction. Thus, reconstructing the stripes pattern, for example, requires Gaussians for at least the number of stripes. We present 3D Gabor splatting, which augments the Gaussian kernel to represent spatially high-frequency signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term and spatially fluctuating wave functions, making it suitable for representing spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can reconstruct various high-frequency textures on the objects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lightweight",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Gaussian Splatting SLAM by Leveraging Loop Closure",
    "authors": [
      "Zunjie Zhu",
      "Youxu Fang",
      "Xin Li",
      "Chengang Yan",
      "Feng Xu",
      "Chau Yuen",
      "Yanyan Li"
    ],
    "abstract": "3D Gaussian Splatting algorithms excel in novel view rendering applications and have been adapted to extend the capabilities of traditional SLAM systems. However, current Gaussian Splatting SLAM methods, designed mainly for hand-held RGB or RGB-D sensors, struggle with tracking drifts when used with rotating RGB-D camera setups. In this paper, we propose a robust Gaussian Splatting SLAM architecture that utilizes inputs from rotating multiple RGB-D cameras to achieve accurate localization and photorealistic rendering performance. The carefully designed Gaussian Splatting Loop Closure module effectively addresses the issue of accumulated tracking and mapping errors found in conventional Gaussian Splatting SLAM systems. First, each Gaussian is associated with an anchor frame and categorized as historical or novel based on its timestamp. By rendering different types of Gaussians at the same viewpoint, the proposed loop detection strategy considers both co-visibility relationships and distinct rendering outcomes. Furthermore, a loop closure optimization approach is proposed to remove camera pose drift and maintain the high quality of 3D Gaussian models. The approach uses a lightweight pose graph optimization algorithm to correct pose drift and updates Gaussians based on the optimized poses. Additionally, a bundle adjustment scheme further refines camera poses using photometric and geometric constraints, ultimately enhancing the global consistency of scenarios. Quantitative and qualitative evaluations on both synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art methods in camera pose estimation and novel view rendering tasks. The code will be open-sourced for the community.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "lightweight",
      "localization",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Shell Texture Splatting: More Details and Fewer Primitives",
    "authors": [
      "Xin Zhang",
      "Anpei Chen",
      "Jincheng Xiong",
      "Pinxuan Dai",
      "Yujun Shen",
      "Weiwei Xu"
    ],
    "abstract": "Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Projecting Gaussian Ellipsoids While Avoiding Affine Projection Approximation",
    "authors": [
      "Han Qi",
      "Tao Cai",
      "Xiyue Han"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its real-time rendering speed and state-of-the-art rendering quality. However, during the rendering process, the use of the Jacobian of the affine approximation of the projection transformation leads to inevitable errors, resulting in blurriness, artifacts and a lack of scene consistency in the final rendered images. To address this issue, we introduce an ellipsoid-based projection method to calculate the projection of Gaussian ellipsoid onto the image plane, which is the primitive of 3D Gaussian Splatting. As our proposed ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera origins inside them or parts lying below $z=0$ plane in the camera space, we designed a pre-filtering strategy. Experiments over multiple widely adopted benchmark datasets show that our ellipsoid-based projection method can enhance the rendering quality of 3D Gaussian Splatting and its extensions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction",
    "authors": [
      "Zhuowen Shen",
      "Yuan Liu",
      "Zhang Chen",
      "Zhong Li",
      "Jiepeng Wang",
      "Yongqing Liang",
      "Zhengming Yu",
      "Jingdong Zhang",
      "Yi Xu",
      "Scott Schaefer",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "Gaussian splatting has achieved impressive improvements for both novel-view synthesis and surface reconstruction from multi-view images. However, current methods still struggle to reconstruct high-quality surfaces from only sparse view input images using Gaussian splatting. In this paper, we propose a novel method called SolidGS to address this problem. We observed that the reconstructed geometry can be severely inconsistent across multi-views, due to the property of Gaussian function in geometry rendering. This motivates us to consolidate all Gaussians by adopting a more solid kernel function, which effectively improves the surface reconstruction quality. With the additional help of geometrical regularization and monocular normal estimation, our method achieves superior performance on the sparse view surface reconstruction than all the Gaussian splatting methods and neural field methods on the widely used DTU, Tanks-and-Temples, and LLFF datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius",
    "authors": [
      "Xinzhe Wang",
      "Ran Yi",
      "Lizhuang Ma"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a recent explicit 3D representation that has achieved high-quality reconstruction and real-time rendering of complex scenes. However, the rasterization pipeline still suffers from unnecessary overhead resulting from avoidable serial Gaussian culling, and uneven load due to the distinct number of Gaussian to be rendered across pixels, which hinders wider promotion and application of 3DGS. In order to accelerate Gaussian splatting, we propose AdR-Gaussian, which moves part of serial culling in Render stage into the earlier Preprocess stage to enable parallel culling, employing adaptive radius to narrow the rendering pixel range for each Gaussian, and introduces a load balancing method to minimize thread waiting time during the pixel-parallel rendering. Our contributions are threefold, achieving a rendering speed of 310% while maintaining equivalent or even better quality than the state-of-the-art. Firstly, we propose to early cull Gaussian-Tile pairs of low splatting opacity based on an adaptive radius in the Gaussian-parallel Preprocess stage, which reduces the number of affected tile through the Gaussian bounding circle, thus reducing unnecessary overhead and achieving faster rendering speed. Secondly, we further propose early culling based on axis-aligned bounding box for Gaussian splatting, which achieves a more significant reduction in ineffective expenses by accurately calculating the Gaussian size in the 2D directions. Thirdly, we propose a balancing algorithm for pixel thread load, which compresses the information of heavy-load pixels to reduce thread waiting time, and enhance information of light-load pixels to hedge against rendering quality loss. Experiments on three datasets demonstrate that our algorithm can significantly improve the Gaussian Splatting rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "motion",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians",
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ],
    "abstract": "In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through intersection preserving and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our Mini-Splatting integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. \\href{https://github.com/fatPeter/mini-splatting}{Code is available}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat",
    "authors": [
      "Jonathan Michaux",
      "Seth Isaacson",
      "Challen Enninful Adu",
      "Adam Li",
      "Rahul Kashyap Swayampakula",
      "Parker Ewen",
      "Sean Rice",
      "Katherine A. Skinner",
      "Ram Vasudevan"
    ],
    "abstract": "Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning about collisions in radiance models and the computational complexity associated with operating in dense models. This paper addresses these challenges by proposing SPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting model. This paper first derives a method to rigorously upper-bound the probability of collision between a robot and a radiance field. Then, this paper introduces a normalized reformulation of Gaussian Splatting that enables efficient computation of this collision bound. Finally, this paper presents a method to optimize trajectories that avoid collisions in a Gaussian Splat. Experiments show that SPLANNING outperforms state-of-the-art methods in generating collision-free trajectories in cluttered environments. The proposed system is also tested on a real-world robot manipulator. A project page is available at https://roahmlab.github.io/splanning.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-25",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting",
    "authors": [
      "Joanna Waczyńska",
      "Piotr Borycki",
      "Sławomir Tadeja",
      "Jacek Tabor",
      "Przemysław Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and real-time rendering. The main drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity of conditioning several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "animation",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a fundamental challenge in computer graphics. While recent advances in Gaussian Splatting have shown promising results separately handling dynamic scenes (4DGS) and view-dependent effects (6DGS), no existing method unifies these capabilities while maintaining real-time performance. We present 7D Gaussian Splatting (7DGS), a unified framework representing scene elements as seven-dimensional Gaussians spanning position (3D), time (1D), and viewing direction (3D). Our key contribution is an efficient conditional slicing mechanism that transforms 7D Gaussians into view- and time-conditioned 3D Gaussians, maintaining compatibility with existing 3D Gaussian Splatting pipelines while enabling joint optimization. Experiments demonstrate that 7DGS outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time rendering (401 FPS) on challenging dynamic scenes with complex view-dependent effects. The project page is: https://gaozhongpai.github.io/7dgs/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM",
    "authors": [
      "Gyuhyeon Pak",
      "Euntai Kim"
    ],
    "abstract": "Recently, map representations based on radiance fields such as 3D Gaussian Splatting and NeRF, which excellent for realistic depiction, have attracted considerable attention, leading to attempts to combine them with SLAM. While these approaches can build highly realistic maps, large-scale SLAM still remains a challenge because they require a large number of Gaussian images for mapping and adjacent images as keyframes for tracking. We propose a novel 3D Gaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D and IMU sensors for large-scale indoor environments. To reduce the computational load of 3DGS-based tracking, we adopt an ICP-based tracking framework that combines IMU preintegration to provide a good initial guess for accurate pose estimation. Our proposed method is the first to propose that Gaussian Splatting-based SLAM can be effectively performed in large-scale environments by integrating IMU sensor measurements. This proposal not only enhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios but also achieves SLAM performance comparable to state-of-the-art methods in large-scale indoor environments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-23",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos",
    "authors": [
      "Yuxin Yao",
      "Yan Zhang",
      "Zhening Huang",
      "Joan Lasenby"
    ],
    "abstract": "Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-22",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming",
    "authors": [
      "Jiayang Bai",
      "Letian Huang",
      "Jie Guo",
      "Wen Gong",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-splatting: Multistage Isotropy-informed Covariance Regularization Optimization for High-Fidelity 3D Gaussian Splatting",
    "authors": [
      "Jee Won Lee",
      "Hansol Lim",
      "Sooyeun Yang",
      "Jongseong Brad Choi"
    ],
    "abstract": "High-fidelity 3D Gaussian Splatting methods excel at capturing fine textures but often overlook model compactness, resulting in massive splat counts, bloated memory, long training, and complex post-processing. We present Micro-Splatting: Two-Stage Adaptive Growth and Refinement, a unified, in-training pipeline that preserves visual detail while drastically reducing model complexity without any post-processing or auxiliary neural modules. In Stage I (Growth), we introduce a trace-based covariance regularization to maintain near-isotropic Gaussians, mitigating low-pass filtering in high-frequency regions and improving spherical-harmonic color fitting. We then apply gradient-guided adaptive densification that subdivides splats only in visually complex regions, leaving smooth areas sparse. In Stage II (Refinement), we prune low-impact splats using a simple opacity-scale importance score and merge redundant neighbors via lightweight spatial and feature thresholds, producing a lean yet detail-rich model. On four object-centric benchmarks, Micro-Splatting reduces splat count and model size by up to 60% and shortens training by 20%, while matching or surpassing state-of-the-art PSNR, SSIM, and LPIPS in real-time rendering. These results demonstrate that Micro-Splatting delivers both compactness and high fidelity in a single, efficient, end-to-end framework.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "high-fidelity",
      "compact",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G-Style: Stylized Gaussian Splatting",
    "authors": [
      "Áron Samuel Kovács",
      "Pedro Hermosilla",
      "Renata G. Raidou"
    ],
    "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting is a powerful 3D representation for novel view synthesis, as -- compared to other approaches based on Neural Radiance Fields -- it provides fast scene renderings and user control over the scene. Recent pre-prints have demonstrated that the style of Gaussian Splatting scenes can be modified using an image exemplar. However, since the scene geometry remains fixed during the stylization process, current solutions fall short of producing satisfactory results. Our algorithm aims to address these limitations by following a three-step process: In a pre-processing step, we remove undesirable Gaussians with large projection areas or highly elongated shapes. Subsequently, we combine several losses carefully designed to preserve different scales of the style in the image, while maintaining as much as possible the integrity of the original scene content. During the stylization process and following the original design of Gaussian Splatting, we split Gaussians where additional detail is necessary within our scene by tracking the gradient of the stylized color. Our experiments demonstrate that G-Style generates high-quality stylizations within just a few minutes, outperforming existing methods both qualitatively and quantitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-28",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "tracking",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting",
    "authors": [
      "Hanqing Jiang",
      "Xiaojun Xiang",
      "Han Sun",
      "Hongjie Li",
      "Liyang Zhou",
      "Xiaoyu Zhang",
      "Guofeng Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently attracted wide attentions in various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation, due to its photorealistic and efficient rendering performance. High-quality reconstrution of 3DGS relies on sufficient splats and a reasonable distribution of these splats to fit real geometric surface and texture details, which turns out to be a challenging problem. We present GeoTexDensifier, a novel geometry-texture-aware densification strategy to reconstruct high-quality Gaussian splats which better comply with the geometric structure and texture richness of the scene. Specifically, our GeoTexDensifier framework carries out an auxiliary texture-aware densification method to produce a denser distribution of splats in fully textured areas, while keeping sparsity in low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile, a geometry-aware splitting strategy takes depth and normal priors to guide the splitting sampling and filter out the noisy splats whose initial positions are far from the actual geometric surfaces they aim to fit, under a Validation of Depth Ratio Change checking. With the help of relative monocular depth prior, such geometry-aware validation can effectively reduce the influence of scattered Gaussians to the final rendering quality, especially in regions with weak textures or without sufficient training views. The texture-aware densification and geometry-aware splitting strategies are fully combined to obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier framework on various datasets and compare our Novel View Synthesis results to other state-of-the-art 3DGS approaches, with detailed quantitative and qualitative evaluations to demonstrate the effectiveness of our method in producing more photorealistic 3DGS models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
    "authors": [
      "Binbin Huang",
      "Zehao Yu",
      "Anpei Chen",
      "Andreas Geiger",
      "Shenghua Gao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-correct 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction",
    "authors": [
      "Jinguang Tong",
      "Xuesong li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ],
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting. Code is avilable at https://github.com/youngju-na/SHARE",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FOCI: Trajectory Optimization on Gaussian Splats",
    "authors": [
      "Mario Gomez Andreu",
      "Maximum Wilder-Smith",
      "Victor Klemm",
      "Vaishakh Patil",
      "Jesus Tordesillas",
      "Marco Hutter"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at https://rffr.leggedrobotics.com/works/foci/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred Shading",
    "authors": [
      "Tong Wu",
      "Jia-Mu Sun",
      "Yu-Kun Lai",
      "Yuewen Ma",
      "Leif Kobbelt",
      "Lin Gao"
    ],
    "abstract": "Reconstructing and editing 3D objects and scenes both play crucial roles in computer graphics and computer vision. Neural radiance fields (NeRFs) can achieve realistic reconstruction and editing results but suffer from inefficiency in rendering. Gaussian splatting significantly accelerates rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting utilizes a single Spherical Harmonic (SH) function to model both texture and lighting, limiting independent editing capabilities of these components. Recently, attempts have been made to decouple texture and lighting with the Gaussian splatting representation but may fail to produce plausible geometry and decomposition results on reflective scenes. Additionally, the forward shading technique they employ introduces noticeable blending artifacts during relighting, as the geometry attributes of Gaussians are optimized under the original illumination and may not be suitable for novel lighting conditions. To address these issues, we introduce DeferredGS, a method for decoupling and editing the Gaussian splatting representation using deferred shading. To achieve successful decoupling, we model the illumination with a learnable environment map and define additional attributes such as texture parameters and normal direction on Gaussians, where the normal is distilled from a jointly trained signed distance function. More importantly, we apply deferred shading, resulting in more realistic relighting effects compared to previous methods. Both qualitative and quantitative experiments demonstrate the superior performance of DeferredGS in novel view synthesis and editing tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "nerf",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled Appearance and Geometry Modeling",
    "authors": [
      "Victor Rong",
      "Jingxiang Chen",
      "Sherwin Bahmani",
      "Kiriakos N. Kutulakos",
      "David B. Lindell"
    ],
    "abstract": "Gaussian splatting has demonstrated excellent performance for view synthesis and scene reconstruction. The representation achieves photorealistic quality by optimizing the position, scale, color, and opacity of thousands to millions of 2D or 3D Gaussian primitives within a scene. However, since each Gaussian primitive encodes both appearance and geometry, these attributes are strongly coupled--thus, high-fidelity appearance modeling requires a large number of Gaussian primitives, even when the scene geometry is simple (e.g., for a textured planar surface). We propose to texture each 2D Gaussian primitive so that even a single Gaussian can be used to capture appearance details. By employing per-primitive texturing, our appearance representation is agnostic to the topology and complexity of the scene's geometry. We show that our approach, GStex, yields improved visual quality over prior work in texturing Gaussian splats. Furthermore, we demonstrate that our decoupling enables improved novel view synthesis performance compared to 2D Gaussian splatting when reducing the number of Gaussian primitives, and that GStex can be used for scene appearance editing and re-texturing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveGS: Active Scene Reconstruction Using Gaussian Splatting",
    "authors": [
      "Liren Jin",
      "Xingguang Zhong",
      "Yue Pan",
      "Jens Behley",
      "Cyrill Stachniss",
      "Marija Popović"
    ],
    "abstract": "Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an RGB-D camera on a mobile platform. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. At the core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the real-world applicability of our framework using an unmanned aerial vehicle.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSCodec Studio: A Modular Framework for Gaussian Splat Compression",
    "authors": [
      "Sicheng Li",
      "Chengzhen Wu",
      "Hao Li",
      "Xiang Gao",
      "Yiyi Liao",
      "Lu Yu"
    ],
    "abstract": "3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available at https://github.com/JasonLSC/GSCodec_Studio , to advance the research on Gaussian Splats compression.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-02",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
    "authors": [
      "Seungjun Oh",
      "Younggeun Lee",
      "Hyejin Jeon",
      "Eunbyung Park"
    ],
    "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BeSplat: Gaussian Splatting from a Single Blurry Image and Event Stream",
    "authors": [
      "Gopi Raju Matta",
      "Reddypalli Trisha",
      "Kaushik Mitra"
    ],
    "abstract": "Novel view synthesis has been greatly enhanced by the development of radiance field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively addressed key challenges, such as long training times and slow rendering speeds, typically associated with Neural Radiance Fields (NeRF), while maintaining high-quality reconstructions. In this work (BeSplat), we demonstrate the recovery of sharp radiance field (Gaussian splats) from a single motion-blurred image and its corresponding event stream. Our method jointly learns the scene representation via Gaussian Splatting and recovers the camera motion through Bezier SE(3) formulation effectively, minimizing discrepancies between synthesized and real-world measurements of both blurry image and corresponding event stream. We evaluate our approach on both synthetic and real datasets, showcasing its ability to render view-consistent, sharp images from the learned radiance field and the estimated camera trajectory. To the best of our knowledge, ours is the first work to address this highly challenging ill-posed problem in a Gaussian Splatting framework with the effective incorporation of temporal information captured using the event stream.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
    "authors": [
      "Yuhang Cao",
      "Haojun Yan",
      "Danya Yao"
    ],
    "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis, and most methods reconstruct surfaces via post-hoc mesh extraction. However, existing methods suffer from two limitations: (i) inaccurate geometry in texture-less indoor regions, and (ii) the decoupling of mesh extraction from optimization, thereby missing the opportunity to leverage mesh geometry to guide splat optimization. In this paper, we present OMeGa, an end-to-end framework that jointly optimizes an explicit triangle mesh and 2D Gaussian splats via a flexible binding strategy, where spatial attributes of Gaussian Splats are expressed in the mesh frame and texture attributes are retained on splats. To further improve reconstruction accuracy, we integrate mesh constraints and monocular normal supervision into the optimization, thereby regularizing geometry learning. In addition, we propose a heuristic, iterative mesh-refinement strategy that splits high-error faces and prunes unreliable ones to further improve the detail and accuracy of the reconstructed mesh. OMeGa achieves state-of-the-art performance on challenging indoor reconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS baseline while maintaining competitive novel-view rendering quality. The experimental results demonstrate that OMeGa effectively addresses prior limitations in indoor texture-less reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Variational Bayes Gaussian Splatting",
    "authors": [
      "Toon Van de Maele",
      "Ozan Catal",
      "Alexander Tschantz",
      "Christopher L. Buckley",
      "Tim Verbelen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has emerged as a promising approach for modeling 3D scenes using mixtures of Gaussians. The predominant optimization method for these models relies on backpropagating gradients through a differentiable rendering pipeline, which struggles with catastrophic forgetting when dealing with continuous streams of data. To address this limitation, we propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that frames training a Gaussian splat as variational inference over model parameters. By leveraging the conjugacy properties of multivariate Gaussians, we derive a closed-form variational update rule, allowing efficient updates from partial, sequential observations without the need for replay buffers. Our experiments show that VBGS not only matches state-of-the-art performance on static datasets, but also enables continual learning from sequentially streamed 2D and 3D data, drastically improving performance in this setting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ],
    "abstract": "We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "geometry",
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering",
    "authors": [
      "Zhiwen Yan",
      "Weng Fei Low",
      "Yu Chen",
      "Gim Hee Lee"
    ],
    "abstract": "3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\\%-66\\% PSNR and 160\\%-2400\\% rendering speed improvement at 4$\\times$-128$\\times$ scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splitting. Our code and more results are available on our project website https://jokeryan.github.io/projects/ms-gs/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting",
    "authors": [
      "Wanshui Gan",
      "Fang Liu",
      "Hongbin Xu",
      "Ningkai Mo",
      "Naoto Yokoya"
    ],
    "abstract": "We introduce GaussianOcc, a systematic method that investigates the two usages of Gaussian splatting for fully self-supervised and efficient 3D occupancy estimation in surround views. First, traditional methods for self-supervised 3D occupancy estimation still require ground truth 6D poses from sensors during training. To address this limitation, we propose Gaussian Splatting for Projection (GSP) module to provide accurate scale information for fully self-supervised training from adjacent view projection. Additionally, existing methods rely on volume rendering for final 3D voxel representation learning using 2D signals (depth maps, semantic maps), which is both time-consuming and less effective. We propose Gaussian Splatting from Voxel space (GSV) to leverage the fast rendering properties of Gaussian splatting. As a result, the proposed GaussianOcc method enables fully self-supervised (no ground truth pose) 3D occupancy estimation in competitive performance with low computational cost (2.7 times faster in training and 5 times faster in rendering). The relevant code is available in https://github.com/GANWANSHUI/GaussianOcc.git.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
    "authors": [
      "Guanxing Lu",
      "Shiyi Zhang",
      "Ziwei Wang",
      "Changliu Liu",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\\% in average success rate. Project page: https://guanxinglu.github.io/ManiGaussian/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Sanghyun Son",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "nerf",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise",
    "authors": [
      "Xinhai Li",
      "Huaibin Wang",
      "Kuo-Kun Tseng"
    ],
    "abstract": "Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain. However, the pixel-wise rendering of NeRF and its ray marching light sampling constrain the rendering speed, impacting its utility in downstream industrial applications. Gaussian Splatting has recently shown a trend of replacing the traditional pointwise sampling technique commonly used in NeRF-based methodologies, and it is changing various aspects of 3D reconstruction. This paper introduces a novel text to 3D content generation framework, Gaussian Diffusion, based on Gaussian Splatting and produces more realistic renderings. The challenge of achieving multi-view consistency in 3D generation significantly impedes modeling complexity and accuracy. Taking inspiration from SJC, we explore employing multi-view noise distributions to perturb images generated by 3D Gaussian Splatting, aiming to rectify inconsistencies in multi-view geometry. We ingeniously devise an efficient method to generate noise that produces Gaussian noise from diverse viewpoints, all originating from a shared noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap models in local minima, causing artifacts like floaters, burrs, or proliferative elements. To mitigate these issues, we propose the variational Gaussian Splatting technique to enhance the quality and stability of 3D appearance. To our knowledge, our approach represents the first comprehensive utilization of Gaussian Diffusion across the entire spectrum of 3D content generation processes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deblurring 3D Gaussian Splatting",
    "authors": [
      "Byeonghyeon Lee",
      "Howoong Lee",
      "Xiangyu Sun",
      "Usman Ali",
      "Eunbyung Park"
    ],
    "abstract": "Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, Deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While Deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring. Qualitative results are available at https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation",
    "authors": [
      "Quankai Gao",
      "Qiangeng Xu",
      "Zhe Cao",
      "Ben Mildenhall",
      "Wenchao Ma",
      "Le Chen",
      "Danhang Tang",
      "Ulrich Neumann"
    ],
    "abstract": "Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method's effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: https://zerg-overmind.github.io/GaussianFlow.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer",
    "authors": [
      "Da Li",
      "Donggang Jia",
      "Yousef Rajeh",
      "Dominik Engel",
      "Ivan Viola"
    ],
    "abstract": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM",
    "authors": [
      "Dapeng Feng",
      "Zhiqiang Chen",
      "Yizhen Yin",
      "Shipeng Zhong",
      "Yuhua Qi",
      "Hongbo Chen"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with photorealistic scene reconstruction emerging as a key challenge. To address this, we introduce Computational Alignment for Real-Time Gaussian Splatting SLAM (CaRtGS), a novel method enhancing the efficiency and quality of photorealistic scene reconstruction in real-time environments. Leveraging 3D Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and processing speed, which is crucial for scene photorealistic reconstruction. Our approach tackles computational misalignment in Gaussian Splatting SLAM (GS-SLAM) through an adaptive strategy that enhances optimization iterations, addresses long-tail optimization, and refines densification. Experiments on Replica, TUM-RGBD, and VECtor datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity rendering with fewer Gaussian primitives. This work propels SLAM towards real-time, photorealistic dense rendering, significantly advancing photorealistic scene representation. For the benefit of the research community, we release the code and accompanying videos on our project website: https://dapengfeng.github.io/cartgs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "localization",
      "slam",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mathematical Supplement for the $\\texttt{gsplat}$ Library",
    "authors": [
      "Vickie Ye",
      "Angjoo Kanazawa"
    ],
    "abstract": "This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-04",
    "categories": [
      "cs.MS",
      "cs.CV",
      "cs.GR",
      "math.NA"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints",
    "authors": [
      "Elias De Smijter",
      "Renaud Detry",
      "Christophe De Vleeschouwer"
    ],
    "abstract": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "compact",
      "lighting",
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap",
    "authors": [
      "Jingwei Huang",
      "Mingrui Li",
      "Lei Sun",
      "Aaron Xuxiang Tian",
      "Tianchen Deng",
      "Hongyu Wang"
    ],
    "abstract": "SLAM systems based on Gaussian Splatting have garnered attention due to their capabilities for rapid real-time rendering and high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure detection. To address these issues, we introduce NGM-SLAM, the first 3DGS based SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting. We utilize neural radiance field submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps. Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate hole filling and high-quality scene expression, supporting monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "large scene",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection",
    "authors": [
      "Hongru Yan",
      "Yu Zheng",
      "Yueqi Duan"
    ],
    "abstract": "Skins wrapping around our bodies, leathers covering over the sofa, sheet metal coating the car - it suggests that objects are enclosed by a series of continuous surfaces, which provides us with informative geometry prior for objectness deduction. In this paper, we propose Gaussian-Det which leverages Gaussian Splatting as surface representation for multi-view based 3D object detection. Unlike existing monocular or NeRF-based methods which depict the objects via discrete positional data, Gaussian-Det models the objects in a continuous manner by formulating the input Gaussians as feature descriptors on a mass of partial surfaces. Furthermore, to address the numerous outliers inherently introduced by Gaussian splatting, we accordingly devise a Closure Inferring Module (CIM) for the comprehensive surface-based objectness deduction. CIM firstly estimates the probabilistic feature residuals for partial surfaces given the underdetermined nature of Gaussian Splatting, which are then coalesced into a holistic representation on the overall surface closure of the object proposal. In this way, the surface information Gaussian-Det exploits serves as the prior on the quality and reliability of objectness and the information basis of proposal refinement. Experiments on both synthetic and real-world datasets demonstrate that Gaussian-Det outperforms various existing approaches, in terms of both average precision and recall.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting",
    "authors": [
      "Raja Kumar",
      "Vanshika Vats"
    ],
    "abstract": "3D Gaussian splatting has surpassed neural radiance field methods in novel view synthesis by achieving lower computational costs and real-time high-quality rendering. Although it produces a high-quality rendering with a lot of input views, its performance drops significantly when only a few views are available. In this work, we address this by proposing a depth-aware Gaussian splatting method for few-shot novel view synthesis. We use monocular depth prediction as a prior, along with a scale-invariant depth loss, to constrain the 3D shape under just a few input views. We also model color using lower-order spherical harmonics to avoid overfitting. Further, we observe that removing splats with lower opacity periodically, as performed in the original work, leads to a very sparse point cloud and, hence, a lower-quality rendering. To mitigate this, we retain all the splats, leading to a better reconstruction in a few view settings. Experimental results show that our method outperforms the traditional 3D Gaussian splatting methods by achieving improvements of 10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and 14.1% in perceptual similarity, thereby validating the effectiveness of our approach. The code will be made available at: https://github.com/raja-kumar/depth-aware-3DGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Pushing Rendering Boundaries: Hard Gaussian Splatting",
    "authors": [
      "Qingshan Xu",
      "Jiequan Cui",
      "Xuanyu Yi",
      "Yuxuan Wang",
      "Yuan Zhou",
      "Yew-Soon Ong",
      "Hanwang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive Novel View Synthesis (NVS) results in a real-time rendering manner. During training, it relies heavily on the average magnitude of view-space positional gradients to grow Gaussians to reduce rendering loss. However, this average operation smooths the positional gradients from different viewpoints and rendering errors from different pixels, hindering the growth and optimization of many defective Gaussians. This leads to strong spurious artifacts in some areas. To address this problem, we propose Hard Gaussian Splatting, dubbed HGS, which considers multi-view significant positional gradients and rendering errors to grow hard Gaussians that fill the gaps of classical Gaussian Splatting on 3D scenes, thus achieving superior NVS results. In detail, we present positional gradient driven HGS, which leverages multi-view significant positional gradients to uncover hard Gaussians. Moreover, we propose rendering error guided HGS, which identifies noticeable pixel rendering errors and potentially over-large Gaussians to jointly mine hard Gaussians. By growing and optimizing these hard Gaussians, our method helps to resolve blurring and needle-like artifacts. Experiments on various datasets demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity",
    "authors": [
      "Zhentao Huang",
      "Minglun Gong"
    ],
    "abstract": "In this paper, we introduce Textured-GS, an innovative method for rendering Gaussian splatting that incorporates spatially defined color and opacity variations using Spherical Harmonics (SH). This approach enables each Gaussian to exhibit a richer representation by accommodating varying colors and opacities across its surface, significantly enhancing rendering quality compared to traditional methods. To demonstrate the merits of our approach, we have adapted the Mini-Splatting architecture to integrate textured Gaussians without increasing the number of Gaussians. Our experiments across multiple real-world datasets show that Textured-GS consistently outperforms both the baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The results highlight the potential of Textured-GS to advance Gaussian-based rendering technologies, promising more efficient and high-quality scene reconstructions. Our implementation is available at https://github.com/ZhentaoHuang/Textured-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks",
    "authors": [
      "Florian Barthel",
      "Arian Beckmann",
      "Wieland Morgenstern",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes. Project page: florian-barthel.github.io/gaussian_decoder",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SmileSplat: Generalizable Gaussian Splats for Unconstrained Sparse Images",
    "authors": [
      "Yanyan Li",
      "Yixin Fang",
      "Federico Tombari",
      "Gim Hee Lee"
    ],
    "abstract": "Sparse Multi-view Images can be Learned to predict explicit radiance fields via Generalizable Gaussian Splatting approaches, which can achieve wider application prospects in real-life when ground-truth camera parameters are not required as inputs. In this paper, a novel generalizable Gaussian Splatting method, SmileSplat, is proposed to reconstruct pixel-aligned Gaussian surfels for diverse scenarios only requiring unconstrained sparse multi-view images. First, Gaussian surfels are predicted based on the multi-head Gaussian regression decoder, which can are represented with less degree-of-freedom but have better multi-view consistency. Furthermore, the normal vectors of Gaussian surfel are enhanced based on high-quality of normal priors. Second, the Gaussians and camera parameters (both extrinsic and intrinsic) are optimized to obtain high-quality Gaussian radiance fields for novel view synthesis tasks based on the proposed Bundle-Adjusting Gaussian Splatting module. Extensive experiments on novel view rendering and depth map prediction tasks are conducted on public datasets, demonstrating that the proposed method achieves state-of-the-art performance in various 3D vision tasks. More information can be found on our project page (https://yanyan-li.github.io/project/gs/smilesplat)",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
    "authors": [
      "In-Hwan Jin",
      "Hyeongju Mun",
      "Joonsoo Kim",
      "Kugjin Yun",
      "Kyeongbo Kong"
    ],
    "abstract": "Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency. Video demonstrations are available at https://anonymous.4open.science/w/MoE-GS-68BA/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images",
    "authors": [
      "Jaeyoung Chung",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ],
    "abstract": "In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting. Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality. However, it tends to overfit the training views when only a small number of images are available. To address this issue, we introduce a dense depth map as a geometry guide to mitigate overfitting. We obtained the depth map using a pre-trained monocular depth estimation model and aligning the scale and offset using sparse COLMAP feature points. The adjusted depth aids in the color-based optimization of 3D Gaussian splatting, mitigating floating artifacts, and ensuring adherence to geometric constraints. We verify the proposed method on the NeRF-LLFF dataset with varying numbers of few images. Our approach demonstrates robust geometry compared to the original method that relies solely on images. Project page: robot0321.github.io/DepthRegGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting",
    "authors": [
      "Xiangrui Liu",
      "Xinju Wu",
      "Pingping Zhang",
      "Shiqi Wang",
      "Zhu Li",
      "Sam Kwong"
    ],
    "abstract": "Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Unified Representation of 3D Gaussian Splatting",
    "authors": [
      "Yuelin Xin",
      "Yuheng Liu",
      "Xiaohui Xie",
      "Xinke Li"
    ],
    "abstract": "A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering",
    "authors": [
      "Antoine Guédon",
      "Vincent Lepetit"
    ],
    "abstract": "We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lighting",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events",
    "authors": [
      "Siyu Chen",
      "Shenghai Yuan",
      "Thien-Minh Nguyen",
      "Zhuyu Huang",
      "Chenyang Shi",
      "Jin Jing",
      "Lihua Xie"
    ],
    "abstract": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over traditional SLAM methods, enabling photorealistic 3D reconstruction that conventional approaches often struggle to achieve. However, existing GS-SLAM systems perform poorly under persistent and severe motion blur commonly encountered in real-world scenarios, leading to significantly degraded tracking accuracy and compromised 3D reconstruction quality. To address this limitation, we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D inputs to simultaneously reduce motion blur in images and compensate for the sparse and discrete nature of event streams, enabling robust tracking and high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system explicitly models the camera's continuous trajectory during exposure, supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian Splatting scene. Furthermore, we introduce a learnable camera response function to align the dynamic ranges of events and images, along with a no-event loss to suppress ringing artifacts during reconstruction. We validate our approach on a new dataset comprising synthetic and real-world sequences with significant motion blur. Extensive experimental results demonstrate that EGS-SLAM consistently outperforms existing GS-SLAM systems in both trajectory accuracy and photorealistic 3D Gaussian Splatting reconstruction. The source code will be available at https://github.com/Chensiyu00/EGS-SLAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-09",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "mapping",
      "ar",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization",
    "authors": [
      "Pengcheng Zhu",
      "Yaoming Zhuang",
      "Baoquan Chen",
      "Li Li",
      "Chengdong Wu",
      "Zhanlin Liu"
    ],
    "abstract": "This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM based on Gaussian Splatting has shown promising results. However, in monocular scenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit weaker tracking capability. To address these limitations, we jointly optimize sparse visual odometry tracking and 3D Gaussian Splatting scene representation for the first time. We obtain depth maps on visual odometry keyframe windows using a fast Multi-View Stereo (MVS) network for the geometric supervision of Gaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense Adjustment Ring (SDAR) to reduce the negative effect of estimated depth maps and preserve the consistency in scale between the visual odometry and Gaussian maps. We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art. Additionally, it outperforms previous monocular methods in terms of novel view synthesis and geometric reconstruction fidelities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "localization",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions",
    "authors": [
      "Jinwei Lin"
    ],
    "abstract": "3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain an implicit neural learning rendering result than the traditional neural rendering technology but keep the more high-definition fast rendering speed. But it is still difficult to achieve a fast enough efficiency on 3D Gaussian Splatting for the practical applications. To Address this issue, we propose the I3DS, a synthetic model performance improvement evaluation solution and experiments test. From multiple and important levels or dimensions of the original 3D Gaussian Splatting, we made more than two thousand various kinds of experiments to test how the selected different items and components can make an impact on the training efficiency of the 3D Gaussian Splatting model. In this paper, we will share abundant and meaningful experiences and methods about how to improve the training, performance and the impacts caused by different items of the model. A special but normal Integer compression in base 95 and a floating-point compression in base 94 with ASCII encoding and decoding mechanism is presented. Many real and effective experiments and test results or phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can gain excellent performance improvements than the previous one. The project code is available as open source.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "neural rendering",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting",
    "authors": [
      "Chun-Tin Wu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "compact",
      "dynamic",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photometric Stereo using Gaussian Splatting and inverse rendering",
    "authors": [
      "Matéo Ducastel",
      "David Tschumperlé",
      "Yvain Quéau"
    ],
    "abstract": "Recent state-of-the-art algorithms in photometric stereo rely on neural networks and operate either through prior learning or inverse rendering optimization. Here, we revisit the problem of calibrated photometric stereo by leveraging recent advances in 3D inverse rendering using the Gaussian Splatting formalism. This allows us to parameterize the 3D scene to be reconstructed and optimize it in a more interpretable manner. Our approach incorporates a simplified model for light representation and demonstrates the potential of the Gaussian Splatting rendering engine for the photometric stereo problem.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-09",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency",
    "authors": [
      "Florian Hahlbohm",
      "Fabian Friederichs",
      "Tim Weyrich",
      "Linus Franke",
      "Moritz Kappel",
      "Susana Castillo",
      "Marc Stamminger",
      "Martin Eisemann",
      "Marcus Magnor"
    ],
    "abstract": "3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both for inverse rendering as well as real-time exploration of scenes. In these applications, coherence across camera frames and multiple views is crucial, be it for robust convergence of a scene reconstruction or for artifact-free fly-throughs. Recent work started mitigating artifacts that break multi-view coherence, including popping artifacts due to inconsistent transparency sorting and perspective-correct outlines of (2D) splats. At the same time, real-time requirements forced such implementations to accept compromises in how transparency of large assemblies of 3D Gaussians is resolved, in turn breaking coherence in other ways. In our work, we aim at achieving maximum coherence, by rendering fully perspective-correct 3D Gaussians while using a high-quality approximation of accurate blending, hybrid transparency, on a per-pixel level, in order to retain real-time frame rates. Our fast and perspectively accurate approach for evaluation of 3D Gaussians does not require matrix inversions, thereby ensuring numerical stability and eliminating the need for special handling of degenerate splats, and the hybrid transparency formulation for blending maintains similar quality as fully resolved per-pixel transparencies at a fraction of the rendering costs. We further show that each of these two components can be independently integrated into Gaussian splatting systems. In combination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster optimization, and equal or better image quality with fewer rendering artifacts compared to traditional 3DGS on common benchmarks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-10",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Cheng Zhao",
      "Su Sun",
      "Ruoyu Wang",
      "Yuliang Guo",
      "Jun-Jun Wan",
      "Zhou Huang",
      "Xinyu Huang",
      "Yingjie Victor Chen",
      "Liu Ren"
    ],
    "abstract": "Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "urban scene",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReSplat: Learning Recurrent Gaussian Splats",
    "authors": [
      "Haofei Xu",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ],
    "abstract": "While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \\times 256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
    "authors": [
      "Junyi Wu",
      "Jiaming Xu",
      "Jinhao Li",
      "Yongkang Zhou",
      "Jiayi Pan",
      "Xingyang Li",
      "Guohao Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.   To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.   Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks",
    "authors": [
      "Joji Joseph",
      "Bharadwaj Amrutur",
      "Shalabh Bhatnagar"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful 3D scene representation technique, capturing fine details with high efficiency. In this paper, we introduce a novel voting-based method that extends 2D segmentation models to 3D Gaussian splats. Our approach leverages masked gradients, where gradients are filtered by input 2D masks, and these gradients are used as votes to achieve accurate segmentation. As a byproduct, we discovered that inference-time gradients can also be used to prune Gaussians, resulting in up to 21% compression. Additionally, we explore few-shot affordance transfer, allowing annotations from 2D images to be effectively transferred onto 3D Gaussian splats. The robust yet straightforward mathematical formulation underlying this approach makes it a highly effective tool for numerous downstream applications, such as augmented reality (AR), object editing, and robotics. The project code and additional resources are available at https://jojijoseph.github.io/3dgs-segmentation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "compression",
      "few-shot",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Luqi Cheng",
      "Zhangshuo Qi",
      "Zijie Zhou",
      "Chao Lu",
      "Guangming Xiong"
    ],
    "abstract": "Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at https://github.com/ChengLuqi/LT-gaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions",
    "authors": [
      "Vishagar Arunan",
      "Saeedha Nazar",
      "Hashiru Pramuditha",
      "Vinasirajan Viruthshaan",
      "Sameera Ramasinghe",
      "Simon Lucey",
      "Ranga Rodrigo"
    ],
    "abstract": "Splatting-based 3D reconstruction methods have gained popularity with the advent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel views. These methods commonly resort to using exponential family functions, such as the Gaussian function, as reconstruction kernels due to their anisotropic nature, ease of projection, and differentiability in rasterization. However, the field remains restricted to variations within the exponential family, leaving generalized reconstruction kernels largely underexplored, partly due to the lack of easy integrability in 3D to 2D projections. In this light, we show that a class of decaying anisotropic radial basis functions (DARBFs), which are non-negative functions of the Mahalanobis distance, supports splatting by approximating the Gaussian function's closed-form integration advantage. With this fresh perspective, we demonstrate up to 34% faster convergence during training and a 45% reduction in memory consumption across various DARB reconstruction kernels, while maintaining comparable PSNR, SSIM, and LPIPS results. We will make the code available.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid Gaussian Splatting",
    "authors": [
      "Zhenzhong Cao",
      "Chenyang Zhao",
      "Qianyi Zhang",
      "Jinzheng Guang",
      "Yinuo Song Jingtai Liu"
    ],
    "abstract": "High-quality reconstruction is crucial for dense SLAM. Recent popular approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and semantic reconstruction of scenes. However, these methods often overlook issues of detail and consistency in different parts of the scene. To address this, we propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level pyramid gaussian splatting, which enables high-quality dense reconstruction of scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level pyramid gaussian splatting method that restores scene details by extracting multi-level image pyramids for gaussian splatting training, ensuring consistency in RGB, depth, and semantic reconstructions. Additionally, we design a tightly-coupled multi-features reconstruction optimization mechanism, allowing the reconstruction accuracy of RGB, depth, and semantic maps to mutually enhance each other during the rendering optimization process. Extensive quantitative, qualitative, and ablation experiments on the Replica and ScanNet public datasets demonstrate that our proposed method outperforms current state-of-the-art methods. The open-source code will be available at: https://github.com/zhenzhongcao/RGBDS-SLAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "slam",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable Beta Splatting",
    "authors": [
      "Rong Liu",
      "Dylan Sun",
      "Meida Chen",
      "Yue Wang",
      "Andrew Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by enabling real-time rendering. However, its reliance on Gaussian kernels for geometry and low-order Spherical Harmonics (SH) for color encoding limits its ability to capture complex geometries and diverse colors. We introduce Deformable Beta Splatting (DBS), a deformable and compact approach that enhances both geometry and color representation. DBS replaces Gaussian kernels with deformable Beta Kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. In addition, we extended the Beta Kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to SH-based methods. Furthermore, Unlike prior densification techniques that depend on Gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of the splatting kernel type. Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of DBS for real-time radiance field rendering. Interactive demonstrations and source code are available on our project website: https://rongliu-leo.github.io/beta-splatting/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "compact",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
    "authors": [
      "Hongyu Li",
      "Chaofeng Chen",
      "Xiaoming Li",
      "Guangming Lu"
    ],
    "abstract": "Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting",
    "authors": [
      "Yangming Li",
      "Chaoyu Liu",
      "Lihao Liu",
      "Simon Masnou",
      "Carola-Bibiane Schönlieb"
    ],
    "abstract": "A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they might also be unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework, GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning",
    "authors": [
      "Ye Yuan",
      "Xueting Li",
      "Yangyi Huang",
      "Shalini De Mello",
      "Koki Nagano",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-18",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "animation",
      "geometry",
      "body",
      "ar",
      "gaussian splatting",
      "avatar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for Geometrically Consistent Rendering and Reconstruction",
    "authors": [
      "Jianheng Liu",
      "Yunfei Wan",
      "Bowen Wang",
      "Chunran Zheng",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "abstract": "Digital twins are fundamental to the development of autonomous driving and embodied artificial intelligence. However, achieving high-granularity surface reconstruction and high-fidelity rendering remains a challenge. Gaussian splatting offers efficient photorealistic rendering but struggles with geometric inconsistencies due to fragmented primitives and sparse observational data in robotics applications. Existing regularization methods, which rely on render-derived constraints, often fail in complex environments. Moreover, effectively integrating sparse LiDAR data with Gaussian splatting remains challenging. We propose a unified LiDAR-visual system that synergizes Gaussian splatting with a neural signed distance field. The accurate LiDAR point clouds enable a trained neural signed distance field to offer a manifold geometry field. This motivates us to offer an SDF-based Gaussian initialization for physically grounded primitive placement and a comprehensive geometric regularization for geometrically consistent rendering and reconstruction. Experiments demonstrate superior reconstruction accuracy and rendering quality across diverse trajectories. To benefit the community, the codes are released at https://github.com/hku-mars/GS-SDF.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-13",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "robotics",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Universal Beta Splatting",
    "authors": [
      "Rong Liu",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meida Chen",
      "Van Nguyen Nguyen",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Yue Wang",
      "Andrew Feng",
      "Ziyan Wu"
    ],
    "abstract": "We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at https://rongliu-leo.github.io/universal-beta-splatting/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review (Updated Post-Gaussian Splatting)",
    "authors": [
      "Kyle Gao",
      "Yina Gao",
      "Hongjie He",
      "Dening Lu",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "In March 2020, Neural Radiance Field (NeRF) revolutionized Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. In August 2023, Gaussian Splatting, a direct competitor to the NeRF-based framework, was proposed, gaining tremendous momentum and overtaking NeRF-based research in terms of interest as the dominant framework for novel view synthesis. We present a comprehensive survey of NeRF papers from the past five years (2020-2025). These include papers from the pre-Gaussian Splatting era, where NeRF dominated the field for novel view synthesis and 3D implicit and hybrid representation neural field learning. We also include works from the post-Gaussian Splatting era where NeRF and implicit/hybrid neural fields found more niche applications.   Our survey is organized into architecture and application-based taxonomies in the pre-Gaussian Splatting era, as well as a categorization of active research areas for NeRF, neural field, and implicit/hybrid neural representation methods. We provide an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of classical NeRF, implicit and hybrid neural representation, and neural field models, and an overview of key datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2022-10-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "robotics",
      "ar",
      "mapping",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting",
    "authors": [
      "Shakiba Kheradmand",
      "Delio Vicini",
      "George Kopanas",
      "Dmitry Lagun",
      "Kwang Moo Yi",
      "Mark Matthews",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a popular radiance field method, with many application-specific extensions. Most variants rely on the same core algorithm: depth-sorting of Gaussian splats then rasterizing in primitive order. This ensures correct alpha compositing, but can cause rendering artifacts due to built-in approximations. Moreover, for a fixed representation, sorted rendering offers little control over render cost and visual fidelity. For example, and counter-intuitively, rendering a lower-resolution image is not necessarily faster. In this work, we address the above limitations by combining 3D Gaussian splatting with stochastic rasterization. Concretely, we leverage an unbiased Monte Carlo estimator of the volume rendering equation. This removes the need for sorting, and allows for accurate 3D blending of overlapping Gaussians. The number of Monte Carlo samples further imbues 3DGS with a way to trade off computation time and quality. We implement our method using OpenGL shaders, enabling efficient rendering on modern GPU hardware. At a reasonable visual quality, our method renders more than four times faster than sorted rasterization.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "authors": [
      "Fengzhi Guo",
      "Chih-Chuan Hsu",
      "Sihao Ding",
      "Cheng Zhang"
    ],
    "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface",
    "authors": [
      "Jiahao Luo",
      "Jing Liu",
      "James Davis"
    ],
    "abstract": "We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields",
    "authors": [
      "Mihnea-Bogdan Jurca",
      "Remco Royen",
      "Ion Giosan",
      "Adrian Munteanu"
    ],
    "abstract": "Gaussian Splatting has revolutionized the world of novel view synthesis by achieving high rendering performance in real-time. Recently, studies have focused on enriching these 3D representations with semantic information for downstream tasks. In this paper, we introduce RT-GS2, the first generalizable semantic segmentation method employing Gaussian Splatting. While existing Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2 demonstrates the ability to generalize to unseen scenes. Our method adopts a new approach by first extracting view-independent 3D Gaussian features in a self-supervised manner, followed by a novel View-Dependent / View-Independent (VDVI) feature fusion to enhance semantic consistency over different views. Extensive experimentation on three different datasets showcases RT-GS2's superiority over the state-of-the-art methods in semantic segmentation quality, exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our method achieves real-time performance of 27.03 FPS, marking an astonishing 901 times speedup compared to existing approaches. This work represents a significant advancement in the field by introducing, to the best of our knowledge, the first real-time generalizable semantic segmentation method for 3D Gaussian representations of radiance fields.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGGS: Edge Guided Gaussian Splatting for Radiance Fields",
    "authors": [
      "Yuanhao Gong"
    ],
    "abstract": "The Gaussian splatting methods are getting popular. However, their loss function only contains the $\\ell_1$ norm and the structural similarity between the rendered and input images, without considering the edges in these images. It is well-known that the edges in an image provide important information. Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS) method that leverages the edges in the input images. More specifically, we give the edge region a higher weight than the flat region. With such edge guidance, the resulting Gaussian particles focus more on the edges instead of the flat regions. Moreover, such edge guidance does not crease the computation cost during the training and rendering stage. The experiments confirm that such simple edge-weighted loss function indeed improves about $1\\sim2$ dB on several difference data sets. With simply plugging in the edge guidance, the proposed method can improve all Gaussian splatting methods in different scenarios, such as human head modeling, building 3D reconstruction, etc.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "head",
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction under Complex Dynamic Scenes",
    "authors": [
      "Shuo Wang",
      "Binbin Huang",
      "Ruoyu Wang",
      "Shenghua Gao"
    ],
    "abstract": "Previous surface reconstruction methods either suffer from low geometric accuracy or lengthy training times when dealing with real-world complex dynamic scenes involving multi-person activities, and human-object interactions. To tackle the dynamic contents and the occlusions in complex scenes, we present a space-time 2D Gaussian Splatting approach. Specifically, to improve geometric quality in dynamic scenes, we learn canonical 2D Gaussian splats and deform these 2D Gaussian splats while enforcing the disks of the Gaussian located on the surface of the objects by introducing depth and normal regularizers. Further, to tackle the occlusion issues in complex scenes, we introduce a compositional opacity deformation strategy, which further reduces the surface recovery of those occluded areas. Experiments on real-world sparse-view video datasets and monocular dynamic datasets demonstrate that our reconstructions outperform state-of-the-art methods, especially for the surface of the details. The project page and more visualizations can be found at: https://tb2-sy.github.io/st-2dgs/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "sparse-view",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatMesh: Interactive 3D Segmentation and Editing Using Mesh-Based Gaussian Splatting",
    "authors": [
      "Kaichen Zhou",
      "Lanqing Hong",
      "Xinhai Chang",
      "Yingji Zhong",
      "Enze Xie",
      "Hao Dong",
      "Zhihao Li",
      "Yongxin Yang",
      "Zhenguo Li",
      "Wei Zhang"
    ],
    "abstract": "A key challenge in fine-grained 3D-based interactive editing is the absence of an efficient representation that balances diverse modifications with high-quality view synthesis under a given memory constraint. While 3D meshes provide robustness for various modifications, they often yield lower-quality view synthesis compared to 3D Gaussian Splatting, which, in turn, suffers from instability during extensive editing. A straightforward combination of these two representations results in suboptimal performance and fails to meet memory constraints. In this paper, we introduce SplatMesh, a novel fine-grained interactive 3D segmentation and editing algorithm that integrates 3D Gaussian Splat with a precomputed mesh and could adjust the memory request based on the requirement. Specifically, given a mesh, \\method simplifies it while considering both color and shape, ensuring it meets memory constraints. Then, SplatMesh aligns Gaussian splats with the simplified mesh by treating each triangle as a new reference point. By segmenting and editing the simplified mesh, we can effectively edit the Gaussian splats as well, which will lead to extensive experiments on real and synthetic datasets, coupled with illustrative visual examples, highlighting the superiority of our approach in terms of representation quality and editing performance. Code of our paper can be found here: https://github.com/kaichen-z/SplatMesh.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing",
    "authors": [
      "Matias Turkulainen",
      "Xuqian Ren",
      "Iaroslav Melekhov",
      "Otto Seiskari",
      "Esa Rahtu",
      "Juho Kannala"
    ],
    "abstract": "High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and AR applications. 3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. In this work, we explore the use of readily accessible geometric cues to enhance Gaussian splatting optimization in challenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use off-the-shelf monocular networks to achieve better alignment with the true scene geometry. We propose an adaptive depth loss based on the gradient of color images, improving depth estimation and novel view synthesis results over various baselines. Our simple yet effective regularization technique enables direct mesh extraction from the Gaussian representation, yielding more physically accurate reconstructions of indoor scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "authors": [
      "Xiaoyuan Wang",
      "Yizhou Zhao",
      "Botao Ye",
      "Xiaojun Shan",
      "Weijie Lyu",
      "Lu Qi",
      "Kelvin C. K. Chan",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \\ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting",
    "authors": [
      "Jiaze Li",
      "Zhengyu Wen",
      "Luo Zhang",
      "Jiangbei Hu",
      "Fei Hou",
      "Zhebin Zhang",
      "Ying He"
    ],
    "abstract": "The 3D Gaussian Splatting technique has significantly advanced the construction of radiance fields from multi-view images, enabling real-time rendering. While point-based rasterization effectively reduces computational demands for rendering, it often struggles to accurately reconstruct the geometry of the target object, especially under strong lighting. To address this challenge, we introduce a novel approach that combines octree-based implicit surface representations with Gaussian splatting. Our method consists of four stages. Initially, it reconstructs a signed distance field (SDF) and a radiance field through volume rendering, encoding them in a low-resolution octree. The initial SDF represents the coarse geometry of the target object. Subsequently, it introduces 3D Gaussians as additional degrees of freedom, which are guided by the SDF. In the third stage, the optimized Gaussians further improve the accuracy of the SDF, allowing it to recover finer geometric details compared to the initial SDF obtained in the first stage. Finally, it adopts the refined SDF to further optimize the 3D Gaussians via splatting, eliminating those that contribute little to visual appearance. Experimental results show that our method, which leverages the distribution of 3D Gaussians with SDFs, reconstructs more accurate geometry, particularly in images with specular highlights caused by strong lighting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis",
    "authors": [
      "Simon Niedermayr",
      "Josef Stumpfegger",
      "Rüdiger Westermann"
    ],
    "abstract": "Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to $31\\times$ on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to $4\\times$ higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "lightweight",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization",
    "authors": [
      "Peng Jiang",
      "Gaurav Pandey",
      "Srikanth Saripalli"
    ],
    "abstract": "This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment. By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate. To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree. This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC). Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique. The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "localization",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Video Compression using 2D Gaussian Splatting",
    "authors": [
      "Lakshya Gupta",
      "Imran N. Junejo"
    ],
    "abstract": "The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spectrally Pruned Gaussian Fields with Neural Compensation",
    "authors": [
      "Runyi Yang",
      "Zhenxin Zhu",
      "Zhou Jiang",
      "Baijun Ye",
      "Xiaoxue Chen",
      "Yifei Zhang",
      "Yuantao Chen",
      "Jian Zhao",
      "Hao Zhao"
    ],
    "abstract": "Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup",
    "authors": [
      "Joanna Waczyńska",
      "Piotr Borycki",
      "Joanna Kaleta",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to represent the scene's structure and the neural network to model dynamics. Such approaches allow fast rendering and extracting each element of such a dynamic scene. However, modifying such objects over time is challenging. SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue. However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing. Moreover, this task poses additional difficulties regarding the re-productivity of such editing. To address this, we propose Dynamic Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS. Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh. Consequently, we can separately construct new trajectories for the 3D objects composing the scene. Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting",
    "authors": [
      "Mengtian Li",
      "Shengxiang Yao",
      "Zhifeng Xie",
      "Keyu Chen"
    ],
    "abstract": "In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "geometry",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating Alternatives to SFM Point Cloud Initialization for Gaussian Splatting",
    "authors": [
      "Yalda Foroutan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian Splatting has recently been embraced as a versatile and effective method for scene reconstruction and novel view synthesis, owing to its high-quality results and compatibility with hardware rasterization. Despite its advantages, Gaussian Splatting's reliance on high-quality point cloud initialization by Structure-from-Motion (SFM) algorithms is a significant limitation to be overcome. To this end, we investigate various initialization strategies for Gaussian Splatting and delve into how volumetric reconstructions from Neural Radiance Fields (NeRF) can be utilized to bypass the dependency on SFM data. Our findings demonstrate that random initialization can perform much better if carefully designed and that by employing a combination of improved initialization strategies and structure distillation from low-cost NeRF models, it is possible to achieve equivalent results, or at times even superior, to those obtained from SFM initialization. Source code is available at https://theialab.github.io/nerf-3dgs .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhanced 3D Urban Scene Reconstruction and Point Cloud Densification using Gaussian Splatting and Google Earth Imagery",
    "authors": [
      "Kyle Gao",
      "Dening Lu",
      "Hongjie He",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "3D urban scene reconstruction and modelling is a crucial research area in remote sensing with numerous applications in academia, commerce, industry, and administration. Recent advancements in view synthesis models have facilitated photorealistic 3D reconstruction solely from 2D images. Leveraging Google Earth imagery, we construct a 3D Gaussian Splatting model of the Waterloo region centered on the University of Waterloo and are able to achieve view-synthesis results far exceeding previous 3D view-synthesis results based on neural radiance fields which we demonstrate in our benchmark. Additionally, we retrieved the 3D geometry of the scene using the 3D point cloud extracted from the 3D Gaussian Splatting model which we benchmarked against our Multi- View-Stereo dense reconstruction of the scene, thereby reconstructing both the 3D geometry and photorealistic lighting of the large-scale urban scene through 3D Gaussian Splatting",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "urban scene",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Anti-Aliased 2D Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an anti-aliased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world-space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object-space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "mapping",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives",
    "authors": [
      "Xilong Zhou",
      "Bao-Huy Nguyen",
      "Loïc Magne",
      "Vladislav Golyanik",
      "Thomas Leimkühler",
      "Christian Theobalt"
    ],
    "abstract": "Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\\times$ fewer primitives and $6\\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization",
    "authors": [
      "Shuo Sun",
      "Malcolm Mielle",
      "Achim J. Lilienthal",
      "Martin Magnusson"
    ],
    "abstract": "We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-19",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians",
    "authors": [
      "Shenhan Qian",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Davide Davoli",
      "Simon Giebenhain",
      "Matthias Nießner"
    ],
    "abstract": "We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "head",
      "ar",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning human-to-robot handovers through 3D scene reconstruction",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "gaussian splatting",
      "sparse-view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis",
    "authors": [
      "Shunyuan Zheng",
      "Boyao Zhou",
      "Ruizhi Shao",
      "Boning Liu",
      "Shengping Zhang",
      "Liqiang Nie",
      "Yebin Liu"
    ],
    "abstract": "We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "authors": [
      "Zuo-Liang Zhu",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "relightable",
      "efficient rendering",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "In this work, we argue that Gaussian splatting is a suitable unified representation for autonomous robot navigation in large-scale unstructured outdoor environments. Such environments require representations that can capture complex structures while remaining computationally tractable for real-time navigation. We demonstrate that the dense geometric and photometric information provided by a Gaussian splatting representation is useful for navigation in unstructured environments. Additionally, semantic information can be embedded in the Gaussian map to enable large-scale task-driven navigation. From the lessons learned through our experiments, we highlight several challenges and opportunities arising from the use of such a representation for robot autonomy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "gaussian splatting",
      "outdoor",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique for real-time, photorealistic rendering by optimizing anisotropic Gaussian primitives from view-dependent images. While 3D-GS has been extended to scientific visualization, prior work remains limited to single-GPU settings, restricting scalability for large datasets on high-performance computing (HPC) systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. To eliminate artifacts, we add ghost cells at partition boundaries and apply background masks to remove irrelevant pixels. Benchmarks on the Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup across 8 nodes on Polaris while preserving image quality. These results demonstrate that distributed 3D-GS enables scalable visualization of large-scale scientific data and provide a foundation for future in situ applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-15",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction",
    "authors": [
      "Suyi Chen",
      "Haibin Ling"
    ],
    "abstract": "As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. In parallel, differentiable rendering techniques such as Gaussian splatting have demonstrated remarkable scalability and efficiency for volumetric representations, suggesting a natural fit for GMM-based cryo-EM reconstruction. However, off-the-shelf Gaussian splatting methods are designed for photorealistic view synthesis and remain incompatible with cryo-EM due to mismatches in the image formation physics, reconstruction objectives, and coordinate systems. Addressing these issues, we propose cryoSplat, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a view-dependent normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. These innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoSplat over representative baselines. The code will be released upon publication.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-06",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes",
    "authors": [
      "Yuanxing Duan",
      "Fangyin Wei",
      "Qiyu Dai",
      "Yuhang He",
      "Wenzheng Chen",
      "Baoquan Chen"
    ],
    "abstract": "We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details--especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "acceleration",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Wave Splatting for Computer-Generated Holography",
    "authors": [
      "Suyeon Choi",
      "Brian Chao",
      "Jacqueline Yang",
      "Manu Gopakumar",
      "Gordon Wetzstein"
    ],
    "abstract": "State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computer-generated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-10",
    "categories": [
      "cs.GR",
      "physics.comp-ph",
      "physics.optics"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians",
    "authors": [
      "Yiwen Wang",
      "Siyuan Chen",
      "Ran Yi"
    ],
    "abstract": "3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive Quantile-Based Geometric Consistency Densification",
    "authors": [
      "Zhuoxiao Li",
      "Shanliang Yao",
      "Yijie Chu",
      "Angel F. Garcia-Fernandez",
      "Yong Yue",
      "Eng Gee Lim",
      "Xiaohui Zhu"
    ],
    "abstract": "In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting (3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements. Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to effectively enhance mesh extraction quality, this compression can potentially lead to a decrease in rendering quality. Additionally, unreliable densification processes and the calculation of depth through the accumulation of opacity can compromise the detail of mesh extraction. To address this issue, we introduce MVG-Splatting, a solution guided by Multi-View considerations. Specifically, we integrate an optimized method for calculating normals, which, combined with image gradients, helps rectify inconsistencies in the original depth computations. Additionally, utilizing projection strategies akin to those in Multi-View Stereo (MVS), we propose an adaptive quantile-based method that dynamically determines the level of additional densification guided by depth maps, from coarse to fine detail. Experimental evidence demonstrates that our method not only resolves the issues of rendering quality degradation caused by depth discrepancies but also facilitates direct mesh extraction from dense Gaussian point clouds using the Marching Cubes algorithm. This approach significantly enhances the overall fidelity and accuracy of the 3D reconstruction process, ensuring that both the geometric details and visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs",
    "authors": [
      "Brandon Smart",
      "Chuanxia Zheng",
      "Iro Laina",
      "Victor Adrian Prisacariu"
    ],
    "abstract": "In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-25",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction",
    "authors": [
      "Jialei Chen",
      "Xin Zhang",
      "Mobarakol Islam",
      "Francisco Vasconcelos",
      "Danail Stoyanov",
      "Daniel S. Elson",
      "Baoru Huang"
    ],
    "abstract": "Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting methods have shown promise in achieving high-quality reconstructions with fast rendering speeds, their use of inverse depth loss functions compresses depth variations. This can lead to a loss of fine geometric details, limiting their ability to capture precise 3D geometry and effectiveness in intraoperative application. To address these challenges, we present SurgicalGS, a dynamic 3D Gaussian Splatting framework specifically designed for surgical scene reconstruction with improved geometric accuracy. Our approach first initialises a Gaussian point cloud using depth priors, employing binary motion masks to identify pixels with significant depth variations and fusing point clouds from depth maps across frames for initialisation. We use the Flexible Deformation Model to represent dynamic scene and introduce a normalised depth regularisation loss along with an unsupervised depth smoothness constraint to ensure more accurate geometric reconstruction. Extensive experiments on two real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art reconstruction quality, especially in terms of accurate geometry, advancing the usability of 3D Gaussian Splatting in robotic-assisted surgery.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting",
    "authors": [
      "Jiaqi Lin",
      "Zhihao Li",
      "Binxiao Huang",
      "Xiao Tang",
      "Jianzhuang Liu",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Fenglong Song",
      "Wenming Yang"
    ],
    "abstract": "Gaussian Splatting has emerged as a prominent 3D representation in novel view synthesis, but it still suffers from appearance variations, which are caused by various factors, such as modern camera ISPs, different time of day, weather conditions, and local light changes. These variations can lead to floaters and color distortions in the rendered images/videos. Recent appearance modeling approaches in Gaussian Splatting are either tightly coupled with the rendering process, hindering real-time rendering, or they only account for mild global variations, performing poorly in scenes with local light changes. In this paper, we propose DAVIGS, a method that decouples appearance variations in a plug-and-play and efficient manner. By transforming the rendering results at the image level instead of the Gaussian level, our approach can model appearance variations with minimal optimization time and memory overhead. Furthermore, our method gathers appearance-related information in 3D space to transform the rendered images, thus building 3D consistency across views implicitly. We validate our method on several appearance-variant scenes, and demonstrate that it achieves state-of-the-art rendering quality with minimal training time and memory usage, without compromising rendering speeds. Additionally, it provides performance improvements for different Gaussian Splatting baselines in a plug-and-play manner.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting",
    "authors": [
      "Ola Shorinwa",
      "Johnathan Tucker",
      "Aliyah Smith",
      "Aiden Swann",
      "Timothy Chen",
      "Roya Firoozi",
      "Monroe Kennedy",
      "Mac Schwager"
    ],
    "abstract": "We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) ASK-Splat, a GSplat representation that distills semantic and grasp affordance features into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical in many robotics tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a \"digital twin\" of the evolving environment throughout the manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses ASK-Splat and SEE-Splat to propose affordance-aligned candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks and in four multi-stage manipulation tasks, using the edited scene to reflect changes due to prior manipulation stages, which is not possible with existing baselines. Video demonstrations and the code for the project are available at https://splatmover.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "robotics",
      "motion",
      "ar",
      "gaussian splatting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content",
    "authors": [
      "Jiayu Yang",
      "Weijian Su",
      "Songqian Zhang",
      "Yuqi Han",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \\M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \\M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "head",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting as Markov Chain Monte Carlo",
    "authors": [
      "Shakiba Kheradmand",
      "Daniel Rebain",
      "Gopal Sharma",
      "Weiwei Sun",
      "Jeff Tseng",
      "Hossam Isack",
      "Abhishek Kar",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi"
    ],
    "abstract": "While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which can lead to poor-quality renderings, and reliance on a good initialization. In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene-in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply introducing noise. We then rewrite the densification and pruning strategies in 3D Gaussian Splatting as simply a deterministic state transition of MCMC samples, removing these heuristics from the framework. To do so, we revise the 'cloning' of Gaussians into a relocalization scheme that approximately preserves sample probability. To encourage efficient use of Gaussians, we introduce a regularizer that promotes the removal of unused Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "localization",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular Dynamic Gaussian Splatting: Fast, Brittle, and Scene Complexity Rules",
    "authors": [
      "Yiqing Liang",
      "Mikhail Okunev",
      "Mikaela Angelina Uy",
      "Runfeng Li",
      "Leonidas Guibas",
      "James Tompkin",
      "Adam W. Harley"
    ],
    "abstract": "Gaussian splatting methods are emerging as a popular approach for converting multi-view image data into scene representations that allow view synthesis. In particular, there is interest in enabling view synthesis for dynamic scenes using only monocular input data -- an ill-posed and challenging problem. The fast pace of work in this area has produced multiple simultaneous papers that claim to work best, which cannot all be true. In this work, we organize, benchmark, and analyze many Gaussian-splatting-based methods, providing apples-to-apples comparisons that prior works have lacked. We use multiple existing datasets and a new instructive synthetic dataset designed to isolate factors that affect reconstruction quality. We systematically categorize Gaussian splatting methods into specific motion representation types and quantify how their differences impact performance. Empirically, we find that their rank order is well-defined in synthetic data, but the complexity of real-world data currently overwhelms the differences. Furthermore, the fast rendering speed of all Gaussian-based methods comes at the cost of brittleness in optimization. We summarize our experiments into a list of findings that can help to further progress in this lively problem setting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Student Splatting and Scooping",
    "authors": [
      "Jialin Zhu",
      "Jiangbei Yue",
      "Feixiang He",
      "He Wang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting has recently been introduced as a potent alternative, achieving both high-fidelity visual results and accelerated rendering performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with challenges. Specifically, large-scale scenes grapples with the integration of objects across multiple scales and disparate viewpoints, which often leads to compromised efficacy as the Gaussians need to balance between detail levels. Furthermore, the generation of initialization points via COLMAP from large-scale dataset is both computationally demanding and prone to incomplete reconstructions. To address these challenges, we present Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization. Our approach represent the scene with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The top level of the pyramid is composed of a few large Gaussians, while each subsequent layer accommodates a denser collection of smaller Gaussians. We effectively initialize these pyramidal Gaussians through sampling a rapidly trained grid-based NeRF at various frequencies. We group these pyramidal Gaussians into clusters and use a compact weighting network to dynamically determine the influence of each pyramid level of each cluster considering camera viewpoint during rendering. Our method achieves a significant performance leap across multiple large-scale datasets and attains a rendering time that is over 400 times faster than current state-of-the-art approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training",
    "authors": [
      "Ruihong Yin",
      "Vladimir Yugay",
      "Yue Li",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "abstract": "The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction",
    "authors": [
      "Zuo-Liang Zhu",
      "Beibei Wang",
      "Jian Yang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets and reconstructing faithful geometry with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Volumetric signed distance field (SDF) methods provide robust geometry reconstruction, while the expensive ray marching hinders its real-time application and slows the training. Besides, these methods struggle to capture sharp geometric details. To this end, we propose to guide 3DGS and SDF bidirectionally in a complementary manner, including an SDF-aided Gaussian splatting for efficient optimization of the relighting model and a GS-guided SDF enhancement for high-quality geometry reconstruction. At the core of our SDF-aided Gaussian splatting is the mutual supervision of the depth and normal between blended Gaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks to this mutual supervision, the learned blended Gaussians are well-constrained with a minimal time cost. As the Gaussians are rendered in a deferred shading mode, the alpha-blended Gaussians are smooth, while individual Gaussians may still be outliers, yielding floater artifacts. Therefore, we introduce an SDF-aware pruning strategy to remove Gaussian outliers located distant from the surface defined by SDF, avoiding floater issue. This way, our GS framework provides reasonable normal and achieves realistic relighting, while the mesh from depth is still problematic. Therefore, we design a GS-guided SDF refinement, which utilizes the blended normal from Gaussians to finetune SDF. With this enhancement, our method can further provide high-quality meshes for reflective objects at the cost of 17% extra training time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray marching",
      "relightable",
      "efficient rendering",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality",
    "authors": [
      "Chiara Schiavo",
      "Elena Camuffo",
      "Leonardo Badia",
      "Simone Milani"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has significantly improved the efficiency and realism of three-dimensional scene visualization in several applications, ranging from robotics to eXtended Reality (XR). This work presents SAGE (Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel framework designed to enhance the user experience by dynamically adapting the Level of Detail (LOD) of different 3DGS objects identified via a semantic segmentation. Experimental results demonstrate how SAGE effectively reduces memory and computational overhead while keeping a desired target visual quality, thus providing a powerful optimization for interactive XR applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-20",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "robotics",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians",
    "authors": [
      "Sen Peng",
      "Weixing Xie",
      "Zilong Wang",
      "Xiaohu Guo",
      "Zhonggui Chen",
      "Baorong Yang",
      "Xiao Dong"
    ],
    "abstract": "We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "geometry",
      "motion",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction",
    "authors": [
      "Christopher Wewer",
      "Kevin Raj",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ],
    "abstract": "We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not scale to large scenes and resolutions, or are limited to interpolation of close input views. latentSplat combines the strengths of regression-based and generative approaches while being trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient splatting and a fast, generative decoder. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "large scene",
      "semantic",
      "ar",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene representation, visualization and analysis",
    "authors": [
      "Saptarshi Neil Sinha",
      "Holger Graf",
      "Michael Weinmann"
    ],
    "abstract": "We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and semantically meaningful splats from registered multi-view spectrum and segmentation maps. This extension enhances the representation of scenes with multiple spectra, providing insights into the underlying materials and segmentation. We introduce an improved physically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accuracy and realism. In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our approach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as well as other non-spectral state-of-the-art learning-based approaches. Our work also demonstrates the potential of spectral scene understanding for precise scene editing techniques like style transfer, inpainting, and removal. Thereby, our contributions address challenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "nerf",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis",
    "authors": [
      "Liang Han",
      "Junsheng Zhou",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Novel view synthesis from sparse inputs is a vital yet challenging task in 3D computer vision. Previous methods explore 3D Gaussian Splatting with neural priors (e.g. depth priors) as an additional supervision, demonstrating promising quality and efficiency compared to the NeRF based methods. However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields. In this paper, We propose a novel method for synthesizing novel views from sparse views with Gaussian Splatting that does not require external prior as supervision. Our key idea lies in exploring the self-supervisions inherent in the binocular stereo consistency between each pair of binocular images constructed with disparity-guided image warping. To this end, we additionally introduce a Gaussian opacity constraint which regularizes the Gaussian locations and avoids Gaussian redundancy for improving the robustness and efficiency of inferring 3D Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and Blender datasets demonstrate that our method significantly outperforms the state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond",
    "authors": [
      "Chongjie Ye",
      "Yinyu Nie",
      "Jiahao Chang",
      "Yuantao Chen",
      "Yihao Zhi",
      "Xiaoguang Han"
    ],
    "abstract": "We present GauStudio, a novel modular framework for modeling 3D Gaussian Splatting (3DGS) to provide standardized, plug-and-play components for users to easily customize and implement a 3DGS pipeline. Supported by our framework, we propose a hybrid Gaussian representation with foreground and skyball background models. Experiments demonstrate this representation reduces artifacts in unbounded outdoor scenes and improves novel view synthesis. Finally, we propose Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse approach for high-fidelity mesh reconstruction from 3DGS inputs without fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS approach enhance 3DGS modeling and rendering capabilities, enabling higher-quality novel view synthesis and surface reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting",
    "authors": [
      "Yuchen Weng",
      "Zhengwen Shen",
      "Ruofan Chen",
      "Qi Wang",
      "Jun Wang"
    ],
    "abstract": "3D deblurring reconstruction techniques have recently seen significant advancements with the development of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these techniques can recover relatively clear 3D reconstructions from blurry image inputs, they still face limitations in handling severe blurring and complex camera motion. To address these issues, we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which integrates event camera data to enhance the robustness of 3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and using novel loss functions, EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating performance comparable to state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting",
    "authors": [
      "Wijayathunga W. M. R. D. B"
    ],
    "abstract": "We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "animation",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft HoloLens 2",
    "authors": [
      "Miriam Jäger",
      "Theodor Kapler",
      "Michael Feßenbecker",
      "Felix Birkelbach",
      "Markus Hillemann",
      "Boris Jutzi"
    ],
    "abstract": "In the fields of photogrammetry, computer vision and computer graphics, the task of neural 3D scene reconstruction has led to the exploration of various techniques. Among these, 3D Gaussian Splatting stands out for its explicit representation of scenes using 3D Gaussians, making it appealing for tasks like 3D point cloud extraction and surface reconstruction. Motivated by its potential, we address the domain of 3D scene reconstruction, aiming to leverage the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting. We present HoloGS, a novel workflow utilizing HoloLens sensor data, which bypasses the need for pre-processing steps like Structure from Motion by instantly accessing the required input data i.e. the images, camera poses and the point cloud from depth sensing. We provide comprehensive investigations, including the training process and the rendering quality, assessed through the Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate our approach on two self-captured scenes: An outdoor scene of a cultural heritage statue and an indoor scene of a fine-structured plant. Our results show that the HoloLens data, including RGB images, corresponding camera poses, and depth sensing based point clouds to initialize the Gaussians, are suitable as input for 3D Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "motion",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization",
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Muyu Xu",
      "Shijian Lu",
      "Eric Xing"
    ],
    "abstract": "3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications",
    "authors": [
      "Vitor Pereira Matias",
      "Daniel Perazzo",
      "Vinicius Silva",
      "Alberto Raposo",
      "Luiz Velho",
      "Afonso Paiva",
      "Tiago Novello"
    ],
    "abstract": "The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "animation",
      "efficient rendering",
      "survey",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "avatar",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
    "authors": [
      "Ziwei Liao",
      "Mohamed Sayed",
      "Steven L. Waslander",
      "Sara Vicente",
      "Daniyar Turmukhambetov",
      "Michael Firman"
    ],
    "abstract": "Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
    "authors": [
      "Xiaohao Xu",
      "Feng Xue",
      "Shibo Zhao",
      "Yike Pan",
      "Sebastian Scherer",
      "Xiaonan Huang"
    ],
    "abstract": "Real-time multi-agent collaboration for ego-motion estimation and high-fidelity 3D reconstruction is vital for scalable spatial intelligence. However, traditional methods produce sparse, low-detail maps, while recent dense mapping approaches struggle with high latency. To overcome these challenges, we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. Through Intra-Agent Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian splats within an agent. For global alignment, parallelized Inter-Agent Gaussian Consensus, which asynchronously aligns and optimizes local maps by regularizing multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D rendering, enabling rapid inter-agent Gaussian association and alignment. MAC-Ego3D bridges local precision and global coherence, delivering higher efficiency, largely reducing localization error, and improving mapping fidelity. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15x increase in inference speed, order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 dB. Our code will be made publicly available at https://github.com/Xiaohao-Xu/MAC-Ego3D .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "localization",
      "motion",
      "ar",
      "mapping",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes",
    "authors": [
      "Saqib Javed",
      "Ahmad Jarrar Khan",
      "Corentin Dumery",
      "Chen Zhao",
      "Mathieu Salzmann"
    ],
    "abstract": "Recent advancements in high-fidelity dynamic scene reconstruction have leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene representation. However, to make these methods viable for real-time applications such as AR/VR, gaming, and rendering on low-power devices, substantial reductions in memory usage and improvements in rendering efficiency are required. While many state-of-the-art methods prioritize lightweight implementations, they struggle in handling scenes with complex motions or long sequences. In this work, we introduce Temporally Compressed 3D Gaussian Splatting (TC3DGS), a novel technique designed specifically to effectively compress dynamic 3D Gaussian representations. TC3DGS selectively prunes Gaussians based on their temporal relevance and employs gradient-aware mixed-precision quantization to dynamically compress Gaussian parameters. It additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a post-processing step to further reduce storage by interpolating Gaussian trajectories across frames. Our experiments across multiple datasets demonstrate that TC3DGS achieves up to 67$\\times$ compression with minimal or no degradation in visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "lightweight",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "vr",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Faster and Better 3D Splatting via Group Training",
    "authors": [
      "Chengbo Wang",
      "Guozheng Ma",
      "Yifei Xue",
      "Yizhen Lao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration",
    "authors": [
      "Zhihao Liang",
      "Qi Zhang",
      "Wenbo Hu",
      "Ying Feng",
      "Lei Zhu",
      "Kui Jia"
    ],
    "abstract": "The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering. However, 3DGS is not alias-free, and its rendering at varying resolutions could produce severe blurring or jaggies. This is because 3DGS treats each pixel as an isolated, single point rather than as an area, causing insensitivity to changes in the footprints of pixels. Consequently, this discrete sampling scheme inevitably results in aliasing, owing to the restricted sampling bandwidth. In this paper, we derive an analytical solution to address this issue. More specifically, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral by subtracting the CDFs. We then introduce this approximation in the two-dimensional pixel shading, and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel. Moreover, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions. Experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
    "authors": [
      "Pramish Paudel",
      "Anubhav Khanal",
      "Ajad Chhatkuli",
      "Danda Pani Paudel",
      "Jyoti Tandukar"
    ],
    "abstract": "Personalized 3D avatars require an animatable representation of digital humans. Doing so instantly from monocular videos offers scalability to broad class of users and wide-scale applications. In this paper, we present a fast, simple, yet effective method for creating animatable 3D digital humans from monocular videos. Our method utilizes the efficiency of Gaussian splatting to model both 3D geometry and appearance. However, we observed that naively optimizing Gaussian splats results in inaccurate geometry, thereby leading to poor animations. This work achieves and illustrates the need of accurate 3D mesh-type modelling of the human body for animatable digitization through Gaussian splats. This is achieved by developing a novel pipeline that benefits from three key aspects: (a) implicit modelling of surface's displacements and the color's spherical harmonics; (b) binding of 3D Gaussians to the respective triangular faces of the body template; (c) a novel technique to render normals followed by their auxiliary supervision. Our exhaustive experiments on three different benchmark datasets demonstrates the state-of-the-art results of our method, in limited time settings. In fact, our method is faster by an order of magnitude (in terms of training time) than its closest competitor. At the same time, we achieve superior rendering and 3D reconstruction performance under the change of poses.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "animation",
      "geometry",
      "body",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting",
    "authors": [
      "Lianjun Liao",
      "Chunhui Zhang",
      "Tong Wu",
      "Henglei Lv",
      "Bailin Deng",
      "Lin Gao"
    ],
    "abstract": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "efficient rendering",
      "illumination",
      "geometry",
      "outdoor",
      "lighting",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Seamless 3D Maps Using Radiance Fields",
    "authors": [
      "Sai Tarun Sathyan",
      "Thomas B. Kinsman"
    ],
    "abstract": "It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning. The traditional methods of creating 3D maps, (such as photogrammetry), require a large number of images and odometry. Additionally, traditional methods have difficulty with reflective surfaces and specular reflections; windows and chrome in the scene can be problematic. Google Road View is a familiar application, which uses traditional methods to fuse a collection of 2D input images into the illusion of a 3D map. However, Google Road View does not create an actual 3D object model, only a collection of views. The objective of this work is to create an actual 3D object model using updated techniques. Neural Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the capability to produce more precise and intricate 3D maps. Gaussian Splatting[4] is another contemporary technique. This investigation compares Neural Radiance Fields to Gaussian Splatting, and describes some of their inner workings. Our primary contribution is a method for improving the results of the 3D reconstructed models. Our results indicate that Gaussian Splatting was superior to the NeRF technique.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting",
    "authors": [
      "Weixing Zhang",
      "Zongrui Li",
      "De Ma",
      "Huajin Tang",
      "Xudong Jiang",
      "Qian Zheng",
      "Gang Pan"
    ],
    "abstract": "3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes. Despite recent advances in improving surface reconstruction accuracy, the reconstructed results still exhibit bias and suffer from inefficiency in storage and training. This paper provides a different observation on the cause of the inefficiency and the reconstruction bias, which is attributed to the integration of the low-opacity parts (LOPs) of the generated Gaussians. We show that LOPs consist of Gaussians with overall low-opacity (LOGs) and the low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two types of LOPs by integrating spiking neurons into the Gaussian Splatting pipeline. Specifically, we introduce global and local full-precision integrate-and-fire spiking neurons to the opacity and representation function of flattened 3D Gaussians, respectively. Furthermore, we enhance the density control strategy with spiking neurons' thresholds and a new criterion on the scale of Gaussians. Our method can represent more accurate reconstructed surfaces at a lower cost. The supplementary material and code are available at https://github.com/zju-bmi-lab/SpikingGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based ray tracing with view-dependent effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D spatial-angular representation to better incorporate view-dependent effects, but the Gaussian representation and control scheme are sub-optimal. In this paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. Experiments demonstrate that 6DGS significantly outperforms 3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction of 66.5% Gaussian points compared to 3DGS. The project page is: https://gaozhongpai.github.io/6dgs/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction",
    "authors": [
      "Qilin Zhang",
      "Olaf Wysocki",
      "Boris Jutzi"
    ],
    "abstract": "Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "compact",
      "semantic",
      "urban scene",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation",
    "authors": [
      "Bram Vanherle",
      "Brent Zoomers",
      "Jeroen Put",
      "Frank Van Reeth",
      "Nick Michiels"
    ],
    "abstract": "Generating synthetic images is a useful method for cheaply obtaining labeled data for training computer vision models. However, obtaining accurate 3D models of relevant objects is necessary, and the resulting images often have a gap in realism due to challenges in simulating lighting effects and camera artifacts. We propose using the novel view synthesis method called Gaussian Splatting to address these challenges. We have developed a synthetic data pipeline for generating high-quality context-aware instance segmentation training data for specific objects. This process is fully automated, requiring only a video of the target object. We train a Gaussian Splatting model of the target object and automatically extract the object from the video. Leveraging Gaussian Splatting, we then render the object on a random background image, and monocular depth estimation is employed to place the object in a believable pose. We introduce a novel dataset to validate our approach and show superior performance over other data generation approaches, such as Cut-and-Paste and Diffusion model-based generation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis",
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ],
    "abstract": "We present billboard Splatting (BBSplat) - a novel approach for novel view synthesis based on textured geometric primitives. BBSplat represents the scene as a set of optimizable textured planar primitives with learnable RGB textures and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The proposed primitives close the rendering quality gap between 2D and 3D Gaussian Splatting (GS), enabling the accurate extraction of 3D mesh as in the 2DGS framework. Additionally, the explicit nature of planar primitives enables the use of the ray-tracing effects in rasterization. Our novel regularization term encourages textures to have a sparser structure, enabling an efficient compression that leads to a reduction in the storage space of the model up to x17 times compared to 3DGS. Our experiments show the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU, and Mip-NeRF-360. Namely, we achieve a state-of-the-art PSNR of 29.72 for DTU at Full HD resolution.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "outdoor",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAFER-Splat: A Control Barrier Function for Safe Navigation with Online Gaussian Splatting Maps",
    "authors": [
      "Timothy Chen",
      "Aiden Swann",
      "Javier Yu",
      "Ola Shorinwa",
      "Riku Murai",
      "Monroe Kennedy",
      "Mac Schwager"
    ],
    "abstract": "SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is a real-time, scalable, and minimally invasive action filter, based on control barrier functions, for safe robotic navigation in a detailed map constructed at runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier Function (CBF) that not only induces safety with respect to all Gaussian primitives in the scene, but when synthesized into a controller, is capable of processing hundreds of thousands of Gaussians while maintaining a minimal memory footprint and operating at 15 Hz during online Splat training. Of the total compute time, a small fraction of it consumes GPU resources, enabling uninterrupted training. The safety layer is minimally invasive, correcting robot actions only when they are unsafe. To showcase the safety filter, we also introduce SplatBridge, an open-source software package built with ROS for real-time GSplat mapping for robots. We demonstrate the safety and robustness of our pipeline first in simulation, where our method is 20-50x faster, safer, and less conservative than competing methods based on neural radiance fields. Further, we demonstrate simultaneous GSplat mapping and safety filtering on a drone hardware platform using only on-board perception. We verify that under teleoperation a human pilot cannot invoke a collision. Our videos and codebase can be found at https://chengine.github.io/safer-splat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-15",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tortho-Gaussian: Splatting True Digital Orthophoto Maps",
    "authors": [
      "Xin Wang",
      "Wendi Zhang",
      "Hong Xie",
      "Haibin Ai",
      "Qiangqiang Yuan",
      "Zongqian Zhan"
    ],
    "abstract": "True Digital Orthophoto Maps (TDOMs) are essential products for digital twins and Geographic Information Systems (GIS). Traditionally, TDOM generation involves a complex set of traditional photogrammetric process, which may deteriorate due to various challenges, including inaccurate Digital Surface Model (DSM), degenerated occlusion detections, and visual artifacts in weak texture regions and reflective surfaces, etc. To address these challenges, we introduce TOrtho-Gaussian, a novel method inspired by 3D Gaussian Splatting (3DGS) that generates TDOMs through orthogonal splatting of optimized anisotropic Gaussian kernel. More specifically, we first simplify the orthophoto generation by orthographically splatting the Gaussian kernels onto 2D image planes, formulating a geometrically elegant solution that avoids the need for explicit DSM and occlusion detection. Second, to produce TDOM of large-scale area, a divide-and-conquer strategy is adopted to optimize memory usage and time efficiency of training and rendering for 3DGS. Lastly, we design a fully anisotropic Gaussian kernel that adapts to the varying characteristics of different regions, particularly improving the rendering quality of reflective surfaces and slender structures. Extensive experimental evaluations demonstrate that our method outperforms existing commercial software in several aspects, including the accuracy of building boundaries, the visual quality of low-texture regions and building facades. These results underscore the potential of our approach for large-scale urban scene reconstruction, offering a robust alternative for enhancing TDOM quality and scalability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "urban scene",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Abdullah Hamdi",
      "Adrien Deliege",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "Recent advances in radiance field reconstruction, such as 3D Gaussian Splatting (3DGS), have achieved high-quality novel view synthesis and fast rendering by representing scenes with compositions of Gaussian primitives. However, 3D Gaussians present several limitations for scene reconstruction. Accurately capturing hard edges is challenging without significantly increasing the number of Gaussians, creating a large memory footprint. Moreover, they struggle to represent flat surfaces, as they are diffused in space. Without hand-crafted regularizers, they tend to disperse irregularly around the actual surface. To circumvent these issues, we introduce a novel method, named 3D Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for modeling geometrically-meaningful radiance fields from multi-view images. Smooth convex shapes offer greater flexibility than Gaussians, allowing for a better representation of 3D scenes with hard edges and dense volumes using fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and Temples, and Deep Blending. Specifically, our method attains an improvement of up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high rendering speeds and reducing the number of required primitives. Our results highlight the potential of 3D Convex Splatting to become the new standard for high-quality scene reconstruction and novel view synthesis. Project page: convexsplatting.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization",
    "authors": [
      "Jan Ackermann",
      "Jonas Kulhanek",
      "Shengqu Cai",
      "Haofei Xu",
      "Marc Pollefeys",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Songyou Peng"
    ],
    "abstract": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "robotics",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering",
    "authors": [
      "Haokai Pang",
      "Heming Zhu",
      "Adam Kortylewski",
      "Christian Theobalt",
      "Marc Habermann"
    ],
    "abstract": "Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
    "authors": [
      "Zhihao Guo",
      "Jingxuan Su",
      "Shenglin Wang",
      "Jinlong Fan",
      "Jing Zhang",
      "Wei Zhou",
      "Hadi Amirpour",
      "Yunlong Zhao",
      "Liangxiu Han",
      "Peng Wang"
    ],
    "abstract": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction",
    "authors": [
      "He Huang",
      "Qi Yang",
      "Mufan Liu",
      "Yiling Xu",
      "Zhu Li"
    ],
    "abstract": "Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: Tri-plane based compression for 3D Gaussian Splatting",
    "authors": [
      "Taorui Wang",
      "Zitong Yu",
      "Yong Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a prominent framework for novel view synthesis, providing high fidelity and rapid rendering speed. However, the substantial data volume of 3DGS and its attributes impede its practical utility, requiring compression techniques for reducing memory cost. Nevertheless, the unorganized shape of 3DGS leads to difficulties in compression. To formulate unstructured attributes into normative distribution, we propose a well-structured tri-plane to encode Gaussian attributes, leveraging the distribution of attributes for compression. To exploit the correlations among adjacent Gaussians, K-Nearest Neighbors (KNN) is used when decoding Gaussian distribution from the Tri-plane. We also introduce Gaussian position information as a prior of the position-sensitive decoder. Additionally, we incorporate an adaptive wavelet loss, aiming to focus on the high-frequency details as iterations increase. Our approach has achieved results that are comparable to or surpass that of SOTA 3D Gaussians Splatting compression work in extensive experiments across multiple datasets. The codes are released at https://github.com/timwang2001/TC-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting",
    "authors": [
      "Holly Dinkel",
      "Marcel Büsching",
      "Alberta Longhini",
      "Brian Coltin",
      "Trey Smith",
      "Danica Kragic",
      "Mårten Björkman",
      "Timothy Bretl"
    ],
    "abstract": "This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting",
    "authors": [
      "Hongbi Zhou",
      "Zhangkai Ni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a perceptual sensitivity-adaptive distribution to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting",
    "authors": [
      "Zhijing Shao",
      "Zhaolong Wang",
      "Zhuang Li",
      "Duotun Wang",
      "Xiangru Lin",
      "Yu Zhang",
      "Mingming Fan",
      "Zeyu Wang"
    ],
    "abstract": "We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-08",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "animation",
      "geometry",
      "body",
      "motion",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting",
    "authors": [
      "Junha Hyung",
      "Susung Hong",
      "Sungwon Hwang",
      "Jaeseong Lee",
      "Jaegul Choo",
      "Jin-Hwa Kim"
    ],
    "abstract": "3D reconstruction from multi-view images is one of the fundamental challenges in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising technique capable of real-time rendering with high-quality 3D reconstruction. This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying. Despite its potential, 3DGS encounters challenges such as needle-like artifacts, suboptimal geometries, and inaccurate normals caused by the Gaussians converging into anisotropic shapes with one dominant variance. We propose using the effective rank analysis to examine the shape statistics of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like shapes with the effective rank 1. To address this, we introduce the effective rank as a regularization, which constrains the structure of the Gaussians. Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts. The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity. The project page is available at https://junhahyung.github.io/erankgs.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. Bühler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "lighting",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "reflection",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSDeformer: Direct, Real-time and Extensible Cage-based Deformation for 3D Gaussian Splatting",
    "authors": [
      "Jiajun Huang",
      "Shuolin Xu",
      "Hongchuan Yu",
      "Tong-Yee Lee"
    ],
    "abstract": "We present GSDeformer, a method that enables cage-based deformation on 3D Gaussian Splatting (3DGS). Our approach bridges cage-based deformation and 3DGS by using a proxy point-cloud representation. This point cloud is generated from 3D Gaussians, and deformations applied to the point cloud are translated into transformations on the 3D Gaussians. To handle potential bending caused by deformation, we incorporate a splitting process to approximate it. Our method does not modify or extend the core architecture of 3D Gaussian Splatting, making it compatible with any trained vanilla 3DGS or its variants. Additionally, we automate cage construction for 3DGS and its variants using a render-and-reconstruct approach. Experiments demonstrate that GSDeformer delivers superior deformation results compared to existing methods, is robust under extreme deformations, requires no retraining for editing, runs in real-time, and can be extended to other 3DGS variants. Project Page: https://jhuangbu.github.io/gsdeformer/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting",
    "authors": [
      "Weihang Liu",
      "Yuke Li",
      "Yuxuan Li",
      "Jingyi Yu",
      "Xin Lou"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "ar",
      "head",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAR-GS: Gaussian Splatting based SAR Images Rendering and Target Reconstruction",
    "authors": [
      "Aobo Li",
      "Zhengxin Lei",
      "Jiangtao Wei",
      "Feng Xu"
    ],
    "abstract": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-25",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting",
    "authors": [
      "Zhicong Wu",
      "Hongbin Xu",
      "Gang Xu",
      "Ping Nie",
      "Zhixin Yan",
      "Jinkai Zheng",
      "Liangqiong Qu",
      "Ming Li",
      "Liqiang Nie"
    ],
    "abstract": "Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework. The code will be publicly available.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian splatting holography",
    "authors": [
      "Shuhe Zhang",
      "Liangcai Cao"
    ],
    "abstract": "In-line holography offers high space-bandwidth product imaging with a simplified lens-free optical system. However, in-line holographic reconstruction is troubled by twin images arising from the Hermitian symmetry of complex fields. Twin images disrupt the reconstruction in solving the ill-posed phase retrieval problem. The known parameters are less than the unknown parameters, causing phase ambiguities. State-of-the-art deep-learning or non-learning methods face challenges in balancing data fidelity with twin-image disturbance. We propose the Gaussian splatting holography (GSH) for twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting for optical field representation and compresses the number of unknown parameters by a maximum of 15 folds, transforming the original ill-posed phase retrieval into a well-posed one with reduced phase ambiguities. Additionally, the Gaussian splatting tends to form sharp patterns rather than those with noisy twin-image backgrounds as each Gaussian has a spatially slow-varying profile. Experiments show that GSH achieves constraint-free recovery for in-line holography with accuracy comparable to state-of-the-art constraint-based methods, with an average peak signal-to-noise ratio equal to 26 dB, and structure similarity equal to 0.8. Combined with total variation, GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB, and a high compression ability of up to 15 folds.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-25",
    "categories": [
      "physics.optics",
      "math.OC",
      "physics.comp-ph"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "compression",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction",
    "authors": [
      "Chenhao Zhang",
      "Yuanping Cao",
      "Lei Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian primitives for high-quality scene representation and reconstruction. While existing 3DGS methods perform well in scenes with minor view variation, large view changes from cross-view data pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction based on multi-branch construction and fusion. Our method independently reconstructs models from different sets of views as multiple independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of multi-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction",
    "authors": [
      "Yikuang Yuluo",
      "Yue Ma",
      "Kuan Shen",
      "Tongtong Jin",
      "Wang Liao",
      "Yangpu Ma",
      "Fuquan Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT reconstruction. However, existing methods rely on the average gradient magnitude of points within the view, often leading to severe needle-like artifacts under sparse-view conditions. To address this challenge, we propose GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses needle-like artifacts and improves reconstruction accuracy under sparse-view conditions. Our framework introduces two key innovations: (1) a Denoised Point Cloud Initialization Strategy that reduces initialization errors and accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that refines gradient computation using graph-based density differences, improving splitting accuracy and density representation. Experiments on X-3D and real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These results highlight the applicability of GR-Gaussian for accurate CT reconstruction under challenging sparse-view conditions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Explicit Representation for Ultra-Realistic Head Avatars",
    "authors": [
      "Hongrui Cai",
      "Yuting Xiao",
      "Xuan Wang",
      "Jiafei Li",
      "Yudong Guo",
      "Yanbo Fan",
      "Shenghua Gao",
      "Juyong Zhang"
    ],
    "abstract": "We introduce a novel approach to creating ultra-realistic head avatars and rendering them in real-time (>30fps at $2048 \\times 1334$ resolution). First, we propose a hybrid explicit representation that combines the advantages of two primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized to capture sharp and rich textures on smooth surfaces, while 3D Gaussian Splatting is employed to represent complex geometric structures. In the pipeline of modeling an avatar, after tracking parametric models based on captured multi-view RGB videos, our goal is to simultaneously optimize the texture and opacity map of mesh, as well as a set of 3D Gaussian splats localized and rigged onto the mesh facets. Specifically, we perform $α$-blending on the color and opacity values based on the merged and re-ordered z-buffer from the rasterization results of mesh and 3DGS. This process involves the mesh and 3DGS adaptively fitting the captured visual information to outline a high-fidelity digital avatar. To avoid artifacts caused by Gaussian splats crossing the mesh facets, we design a stable hybrid depth sorting strategy. Experiments illustrate that our modeled results exceed those of state-of-the-art approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "high-fidelity",
      "tracking",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility",
    "authors": [
      "Thomas Gottwald",
      "Edgar Heinert",
      "Matthias Rottmann"
    ],
    "abstract": "In this work, we present a novel method for uncertainty estimation (UE) in Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical applications such as robotics and medicine. Previous methods typically estimate the variance of Gaussian primitives and use the rendering process to obtain pixel-wise uncertainties. Our method establishes primitive representations of error and visibility of trainings views, which carries meaningful uncertainty information. This representation is obtained by projection of training error and visibility onto the primitives. Uncertainties of novel views are obtained by rendering the primitive representations of uncertainty for those novel views, yielding uncertainty feature maps. To aggregate these uncertainty feature maps of novel views, we perform a pixel-wise regression on holdout data. In our experiments, we analyze the different components of our method, investigating various combinations of uncertainty feature maps and regression models. Furthermore, we considered the effect of separating splatting into foreground and background. Our UEs show high correlations to true errors, outperforming state-of-the-art methods, especially on foreground objects. The trained regression models show generalization capabilities to new scenes, allowing uncertainty estimation without the need for holdout data.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment then Splat: Unified 3D Open-Vocabulary Segmentation via Gaussian Splatting",
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Yiran Qiao",
      "Chaoda Song",
      "Tuo Liang",
      "Jing Ma",
      "Huan Wang",
      "Yu Yin"
    ],
    "abstract": "Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentation approach for both static and dynamic scenes based on Gaussian Splatting. Segment then Splat reverses the long established approach of \"segmentation after reconstruction\" by dividing Gaussians into distinct object sets before reconstruction. Once reconstruction is complete, the scene is naturally segmented into individual objects, achieving true 3D segmentation. This design eliminates both geometric and semantic ambiguities, as well as Gaussian-object misalignment issues in dynamic scenes. It also accelerates the optimization process, as it eliminates the need for learning a separate language field. After optimization, a CLIP embedding is assigned to each object to enable open-vocabulary querying. Extensive experiments one various datasets demonstrate the effectiveness of our proposed method in both static and dynamic scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "robotics",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "gsplat: An Open-Source Library for Gaussian Splatting",
    "authors": [
      "Vickie Ye",
      "Ruilong Li",
      "Justin Kerr",
      "Matias Turkulainen",
      "Brent Yi",
      "Zhuoyang Pan",
      "Otto Seiskari",
      "Jianbo Ye",
      "Jeffrey Hu",
      "Matthew Tancik",
      "Angjoo Kanazawa"
    ],
    "abstract": "gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeomGS: LiDAR-Guided Geometry-Aware Gaussian Splatting for Robot Localization",
    "authors": [
      "Jaewon Lee",
      "Mangyu Kong",
      "Minseong Park",
      "Euntai Kim"
    ],
    "abstract": "Mapping and localization are crucial problems in robotics and autonomous driving. Recent advances in 3D Gaussian Splatting (3DGS) have enabled precise 3D mapping and scene understanding by rendering photo-realistic images. However, existing 3DGS methods often struggle to accurately reconstruct a 3D map that reflects the actual scale and geometry of the real world, which degrades localization performance. To address these limitations, we propose a novel 3DGS method called Geometry-Aware Gaussian Splatting (GeomGS). This method fully integrates LiDAR data into 3D Gaussian primitives via a probabilistic approach, as opposed to approaches that only use LiDAR as initial points or introduce simple constraints for Gaussian points. To this end, we introduce a Geometric Confidence Score (GCS), which identifies the structural reliability of each Gaussian point. The GCS is optimized simultaneously with Gaussians under probabilistic distance constraints to construct a precise structure. Furthermore, we propose a novel localization method that fully utilizes both the geometric and photometric properties of GeomGS. Our GeomGS demonstrates state-of-the-art geometric and localization performance across several benchmarks, while also improving photometric performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-23",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "localization",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "autonomous driving",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians",
    "authors": [
      "Siyun Liang",
      "Sen Wang",
      "Kunyi Li",
      "Michael Niemeyer",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "localization",
      "semantic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiant Foam: Real-Time Differentiable Ray Tracing",
    "authors": [
      "Shrisudhan Govindarajan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "light transport",
      "ray tracing",
      "acceleration",
      "ar",
      "gaussian splatting",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
    "authors": [
      "Yupeng Zhang",
      "Dezhi Zheng",
      "Ping Lu",
      "Han Zhang",
      "Lei Wang",
      "Liping xiang",
      "Cheng Luo",
      "Kaijun Deng",
      "Xiaowen Fu",
      "Linlin Shen",
      "Jinbao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "efficient rendering",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting for Efficient Satellite Image Photogrammetry",
    "authors": [
      "Luca Savant Aira",
      "Gabriele Facciolo",
      "Thibaud Ehret"
    ],
    "abstract": "Recently, Gaussian splatting has emerged as a strong alternative to NeRF, demonstrating impressive 3D modeling capabilities while requiring only a fraction of the training and rendering time. In this paper, we show how the standard Gaussian splatting framework can be adapted for remote sensing, retaining its high efficiency. This enables us to achieve state-of-the-art performance in just a few minutes, compared to the day-long optimization required by the best-performing NeRF-based Earth observation methods. The proposed framework incorporates remote-sensing improvements from EO-NeRF, such as radiometric correction and shadow modeling, while introducing novel components, including sparsity, view consistency, and opacity regularizations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "shadow",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
    "authors": [
      "Youjia Zhang",
      "Anpei Chen",
      "Yumin Wan",
      "Zikai Song",
      "Junqing Yu",
      "Yawei Luo",
      "Wei Yang"
    ],
    "abstract": "In this paper, we introduce Ref-GS, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. Ref-GS builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. Next, we introduce a spherical Mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading. Additionally, we propose a simple yet efficient geometry-lighting factorization that connects geometry and lighting via the vector outer product, significantly reducing renderer overhead when integrating volumetric attributes. Our method achieves superior photorealistic rendering for a range of open-world scenes while also accurately recovering geometry.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "lighting",
      "head",
      "gaussian splatting",
      "ar",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling",
    "authors": [
      "Brian Chao",
      "Hung-Yu Tseng",
      "Lorenzo Porzi",
      "Chen Gao",
      "Tuotuo Li",
      "Qinbo Li",
      "Ayush Saraf",
      "Jia-Bin Huang",
      "Johannes Kopf",
      "Gordon Wetzstein",
      "Changil Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian",
    "authors": [
      "Bang Du",
      "Runfa Blark Li",
      "Chen Du",
      "Truong Nguyen"
    ],
    "abstract": "The reconstruction of 3D objects from calibrated photographs represents a fundamental yet intricate challenge in the domains of computer graphics and vision. Although neural reconstruction approaches based on Neural Radiance Fields (NeRF) have shown remarkable capabilities, their processing costs remain substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely improves the training efficiency and facilitates to generate realistic rendering in real-time. However, due to the limited ability of Spherical Harmonics (SH) to represent high-frequency information, 3D-GS falls short in reconstructing glossy objects. Researchers have turned to enhance the specular expressiveness of 3D-GS through inverse rendering. Yet these methods often struggle to maintain the training and rendering efficiency, undermining the benefits of Gaussian Splatting techniques. In this paper, we introduce GlossGau, an efficient inverse rendering framework that reconstructs scenes with glossy surfaces while maintaining training and rendering speeds comparable to vanilla 3D-GS. Specifically, we explicitly model the surface normals, Bidirectional Reflectance Distribution Function (BRDF) parameters, as well as incident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the per-Gaussian Normal Distribution Function under the microfacet model. We utilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply regularization to significantly alleviate the normal estimation challenge encountered in related works. Experiments demonstrate that GlossGau achieves competitive or superior reconstruction on datasets with glossy surfaces. Compared with previous GS-based works that address the specular surface, our optimization time is considerably less.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
    "authors": [
      "Kim Jun-Seong",
      "GeonU Kim",
      "Kim Yu-Ji",
      "Yu-Chiang Frank Wang",
      "Jaesung Choe",
      "Tae-Hyun Oh"
    ],
    "abstract": "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large-scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. For video results, please visit : https://drsplat.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "localization",
      "semantic",
      "compact",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting",
    "authors": [
      "Xiaoyu Zhou",
      "Jingqi Wang",
      "Yongtao Wang",
      "Yufei Wei",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, a vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic 3D occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation",
    "authors": [
      "Jungmin Lee",
      "Seonghyuk Hong",
      "Juyong Lee",
      "Jaeyoon Lee",
      "Jongwon Choi"
    ],
    "abstract": "We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Point Encoders",
    "authors": [
      "Jim James",
      "Ben Wilson",
      "Simon Lucey",
      "James Hays"
    ],
    "abstract": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "geometry",
      "recognition",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM",
    "authors": [
      "Mostafa Mansour",
      "Ahmed Abdelsalam",
      "Ari Happonen",
      "Jari Porras",
      "Esa Rahtu"
    ],
    "abstract": "Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM. This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction",
    "authors": [
      "Yangsen Chen",
      "Hao Wang"
    ],
    "abstract": "The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "medical",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGSST: Scaling Gaussian Splatting StyleTransfer",
    "authors": [
      "Bruno Galerne",
      "Jianling Wang",
      "Lara Raad",
      "Jean-Michel Morel"
    ],
    "abstract": "Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering. 3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality. This work introduces SGSST: Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes. We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
    "authors": [
      "Shoukang Hu",
      "Ziwei Liu"
    ],
    "abstract": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting",
    "authors": [
      "Sheng Ye",
      "Zhen-Hui Dong",
      "Yubin Hu",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF Fusion",
    "authors": [
      "Jiaxin Wei",
      "Stefan Leutenegger"
    ],
    "abstract": "Traditional volumetric fusion algorithms preserve the spatial structure of 3D scenes, which is beneficial for many tasks in computer vision and robotics. However, they often lack realism in terms of visualization. Emerging 3D Gaussian splatting bridges this gap, but existing Gaussian-based reconstruction methods often suffer from artifacts and inconsistencies with the underlying 3D structure, and struggle with real-time optimization, unable to provide users with immediate feedback in high quality. One of the bottlenecks arises from the massive amount of Gaussian parameters that need to be updated during optimization. Instead of using 3D Gaussian as a standalone map representation, we incorporate it into a volumetric mapping system to take advantage of geometric information and propose to use a quadtree data structure on images to drastically reduce the number of splats initialized. In this way, we simultaneously generate a compact 3D Gaussian map with fewer artifacts and a volumetric map on the fly. Our method, GSFusion, significantly enhances computational efficiency without sacrificing rendering quality, as demonstrated on both synthetic and real datasets. Code will be available at https://github.com/goldoak/GSFusion.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "compact",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction",
    "authors": [
      "Mustafa Khan",
      "Hamidreza Fazlali",
      "Dhruv Sharma",
      "Tongtong Cao",
      "Dongfeng Bai",
      "Yuan Ren",
      "Bingbing Liu"
    ],
    "abstract": "Realistic scene reconstruction and view synthesis are essential for advancing autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian Splatting excels in real-time rendering and static scene reconstructions but struggles with modeling driving scenarios due to complex backgrounds, dynamic objects, and sparse views. We propose AutoSplat, a framework employing Gaussian splatting to achieve highly realistic reconstructions of autonomous driving scenes. By imposing geometric constraints on Gaussians representing the road and sky regions, our method enables multi-view consistent simulation of challenging scenarios including lane changes. Leveraging 3D templates, we introduce a reflected Gaussian consistency constraint to supervise both the visible and unseen side of foreground objects. Moreover, to model the dynamic appearance of foreground objects, we estimate residual spherical harmonics for each foreground Gaussian. Extensive experiments on Pandaset and KITTI demonstrate that AutoSplat outperforms state-of-the-art methods in scene reconstruction and novel view synthesis across diverse driving scenarios. Visit our project page at https://autosplat.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "autonomous driving",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting",
    "authors": [
      "Zhe Jun Tang",
      "Tat-Jen Cham"
    ],
    "abstract": "The use of 3D Gaussians as representation of radiance fields has enabled high quality novel view synthesis at real-time rendering speed. However, the choice of optimising the outgoing radiance of each Gaussian independently as spherical harmonics results in unsatisfactory view dependent effects. In response to these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering quality. Instead of optimising a single outgoing radiance parameter, 3iGS enhances 3DGS view-dependent effects by expressing the outgoing radiance as a function of a local illumination field and Bidirectional Reflectance Distribution Function (BRDF) features. We optimise a continuous incident illumination field through a Tensorial Factorisation representation, while separately fine-tuning the BRDF features of each 3D Gaussian relative to this illumination field. Our methodology significantly enhances the rendering quality of specular view-dependent effects of 3DGS, while maintaining rapid training and rendering speeds.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "illumination",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtual Memory for 3D Gaussian Splatting",
    "authors": [
      "Jonathan Haberl",
      "Philipp Fleck",
      "Clemens Arth"
    ],
    "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration",
    "authors": [
      "Rendong Zhang",
      "Alexandra Watkins",
      "Nilanjan Sarkar"
    ],
    "abstract": "Photorealistic avatars have become essential for immersive applications in virtual reality (VR) and augmented reality (AR), enabling lifelike interactions in areas such as training simulations, telemedicine, and virtual collaboration. These avatars bridge the gap between the physical and digital worlds, improving the user experience through realistic human representation. However, existing avatar creation techniques face significant challenges, including high costs, long creation times, and limited utility in virtual applications. Manual methods, such as MetaHuman, require extensive time and expertise, while automatic approaches, such as NeRF-based pipelines often lack efficiency, detailed facial expression fidelity, and are unable to be rendered at a speed sufficent for real-time applications. By involving several cutting-edge modern techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar creation pipeline that leverages monocular video input to create a scalable and efficient photorealistic avatar directly compatible with the Unity game engine. Our pipeline incorporates a novel Gaussian splatting technique with customized preprocessing that enables the user of \"in the wild\" monocular video capture, detailed facial expression reconstruction and embedding within a fully rigged avatar model. Additionally, we present a Unity-integrated Gaussian Splatting Avatar Editor, offering a user-friendly environment for VR/AR application development. Experimental results validate the effectiveness of our preprocessing pipeline in standardizing custom data for 3DGS training and demonstrate the versatility of Gaussian avatars in Unity, highlighting the scalability and practicality of our approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "lighting",
      "ar",
      "gaussian splatting",
      "avatar",
      "nerf",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting",
    "authors": [
      "Boying Li",
      "Vuong Chi Hao",
      "Peter J. Stuckey",
      "Ian Reid",
      "Hamid Rezatofighi"
    ],
    "abstract": "We propose Hier-SLAM++, a comprehensive Neuro-Symbolic semantic 3D Gaussian Splatting SLAM method with both RGB-D and monocular input featuring an advanced hierarchical categorical representation, which enables accurate pose estimation as well as global 3D semantic mapping. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making scene understanding particularly challenging and costly. To address this problem, we introduce a novel hierarchical representation that encodes both semantic and geometric information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs) as well as the 3D generative model. By utilizing the proposed hierarchical tree structure, semantic information is symbolically represented and learned in an end-to-end manner. We further introduce an advanced semantic loss designed to optimize hierarchical semantic information through both Intra-level and Inter-level optimizations. Additionally, we propose an improved SLAM system to support both RGB-D and monocular inputs using a feed-forward model. To the best of our knowledge, this is the first semantic monocular Gaussian Splatting SLAM system, significantly reducing sensor requirements for 3D semantic understanding and broadening the applicability of semantic Gaussian SLAM system. We conduct experiments on both synthetic and real-world datasets, demonstrating superior or on-par performance with state-of-the-art methods, while significantly reducing storage and training time requirements. Our project page is available at: https://hierslampp.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-20",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
    "authors": [
      "Mingqiao Ye",
      "Martin Danelljan",
      "Fisher Yu",
      "Lei Ke"
    ],
    "abstract": "The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by Segment Anything Model (SAM), along with introduced 3D spatial consistency regularization. Compared to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization, style transfer and scene recomposition. Our code and models are at https://github.com/lkeab/gaussian-grouping.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting",
    "authors": [
      "Advaith V. Sethuraman",
      "Max Rucker",
      "Onur Bagoren",
      "Pou-Chun Kung",
      "Nibarkavi N. B. Amutha",
      "Katherine A. Skinner"
    ],
    "abstract": "In this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (77% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting",
    "authors": [
      "Jiajun Dong",
      "Chengkun Wang",
      "Wenzhao Zheng",
      "Lei Chen",
      "Jiwen Lu",
      "Yansong Tang"
    ],
    "abstract": "Effective image tokenization is crucial for both multi-modal understanding and generation tasks due to the necessity of the alignment with discrete text data. To this end, existing approaches utilize vector quantization (VQ) to project pixels onto a discrete codebook and reconstruct images from the discrete representation. However, compared with the continuous latent space, the limited discrete codebook space significantly restrict the representational ability of these image tokenizers. In this paper, we propose GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting as a solution. We first represent the encoded samples as multiple flexible featured 2D Gaussians characterized by positions, rotation angles, scaling factors, and feature coefficients. We adopt the standard quantization for the Gaussian features and then concatenate the quantization results with the other intrinsic Gaussian parameters before the corresponding splatting operation and the subsequent decoding module. In general, GaussianToken integrates the local influence of 2D Gaussian distribution into the discrete space and thus enhances the representation capability of the image tokenizer. Competitive reconstruction performances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the effectiveness of our framework. Our code is available at: https://github.com/ChrisDong-THU/GaussianToken.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GI-SLAM: Gaussian-Inertial SLAM",
    "authors": [
      "Xulang Liu",
      "Ning Tan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful representation of geometry and appearance for dense Simultaneous Localization and Mapping (SLAM). Through rapid, differentiable rasterization of 3D Gaussians, many 3DGS SLAM methods achieve near real-time rendering and accelerated training. However, these methods largely overlook inertial data, witch is a critical piece of information collected from the inertial measurement unit (IMU). In this paper, we present GI-SLAM, a novel gaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking module and a realistic 3D Gaussian-based scene representation for mapping. Our method introduces an IMU loss that seamlessly integrates into the deep learning framework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the accuracy, robustness and efficiency of camera tracking. Moreover, our SLAM system supports a wide range of sensor configurations, including monocular, stereo, and RGBD cameras, both with and without IMU integration. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the EuRoC and TUM-RGBD datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-24",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized Minimal 4D Gaussian Splatting",
    "authors": [
      "Minseo Lee",
      "Byeonghyeon Lee",
      "Lucas Yunkyu Lee",
      "Eunsoo Lee",
      "Sangmin Kim",
      "Seunghyeon Song",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Jaesik Park",
      "Eunbyung Park"
    ],
    "abstract": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "high-fidelity",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "head",
      "ar",
      "face",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction",
    "authors": [
      "Yufei Han",
      "Bowen Tie",
      "Heng Guo",
      "Youwei Lyu",
      "Si Li",
      "Boxin Shi",
      "Yunpeng Jia",
      "Zhanyu Ma"
    ],
    "abstract": "Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "shape reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering",
    "authors": [
      "Siddharth Tourani",
      "Jayaram Reddy",
      "Akash Kumbar",
      "Satyajit Tourani",
      "Nishant Goyal",
      "Madhava Krishna",
      "N. Dinesh Reddy",
      "Muhammad Haris Khan"
    ],
    "abstract": "Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "tracking",
      "urban scene",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control",
    "authors": [
      "Xingjun Wang",
      "Lianlei Shan"
    ],
    "abstract": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with Expansive Lidar Image Dataset Using 3DGS and NeRF",
    "authors": [
      "Butian Xiong",
      "Nanjun Zheng",
      "Junhua Liu",
      "Zhen Li"
    ],
    "abstract": "We introduce a novel, multimodal large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields (NeRF). Our expansive U-Scene dataset surpasses any previously existing real large-scale outdoor LiDAR and image dataset in both area and point count. GauU-Scene encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. Additionally, we are the first to propose a LiDAR and image alignment method for a drone-based dataset. Our assessment of GauU-Scene includes a detailed analysis across various novel viewpoints, employing image-based metrics such as SSIM, LPIPS, and PSNR on NeRF and Gaussian Splatting based methods. This analysis reveals contradictory results when applying geometric-based metrics like Chamfer distance. The experimental results on our multimodal dataset highlight the unreliability of current image-based metrics and reveal significant drawbacks in geometric reconstruction using the current Gaussian Splatting-based method, further illustrating the necessity of our dataset for assessing geometry reconstruction tasks. We also provide detailed supplementary information on data collection protocols and make the dataset available on the following anonymous project page",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion",
    "authors": [
      "Ziyuan Qu",
      "Omkar Vengurlekar",
      "Mohamad Qadri",
      "Kevin Zhang",
      "Michael Kaess",
      "Christopher Metzler",
      "Suren Jayasuriya",
      "Adithya Pediredla"
    ],
    "abstract": "Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent technique in computer vision and graphics for reconstructing 3D scenes. GS represents a scene as a set of 3D Gaussians with varying opacities and employs a computationally efficient splatting operation along with analytical derivatives to compute the 3D Gaussian parameters given scene images captured from various viewpoints. Unfortunately, capturing surround view ($360^{\\circ}$ viewpoint) images is impossible or impractical in many real-world imaging scenarios, including underwater imaging, rooms inside a building, and autonomous navigation. In these restricted baseline imaging scenarios, the GS algorithm suffers from a well-known 'missing cone' problem, which results in poor reconstruction along the depth axis. In this manuscript, we demonstrate that using transient data (from sonars) allows us to address the missing cone problem by sampling high-frequency data along the depth axis. We extend the Gaussian splatting algorithms for two commonly used sonars and propose fusion algorithms that simultaneously utilize RGB camera data and sonar data. Through simulations, emulations, and hardware experiments across various imaging scenarios, we show that the proposed fusion algorithms lead to significantly better novel view synthesis (5 dB improvement in PSNR) and 3D geometry reconstruction (60% lower Chamfer distance).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-06",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel View Synthesis",
    "authors": [
      "Hugo Blanc",
      "Jean-Emmanuel Deschaud",
      "Alexis Paljic"
    ],
    "abstract": "Differentiable volumetric rendering-based methods made significant progress in novel view synthesis. On one hand, innovative methods have replaced the Neural Radiance Fields (NeRF) network with locally parameterized structures, enabling high-quality renderings in a reasonable time. On the other hand, approaches have used differentiable splatting instead of NeRF's ray casting to optimize radiance fields rapidly using Gaussian kernels, allowing for fine adaptation to the scene. However, differentiable ray casting of irregularly spaced kernels has been scarcely explored, while splatting, despite enabling fast rendering times, is susceptible to clearly visible artifacts.   Our work closes this gap by providing a physically consistent formulation of the emitted radiance c and density σ, decomposed with Gaussian functions associated with Spherical Gaussians/Harmonics for all-frequency colorimetric representation. We also introduce a method enabling differentiable ray casting of irregularly distributed Gaussians using an algorithm that integrates radiance fields slab by slab and leverages a BVH structure. This allows our approach to finely adapt to the scene while avoiding splatting artifacts. As a result, we achieve superior rendering quality compared to the state-of-the-art while maintaining reasonable training times and achieving inference speeds of 25 FPS on the Blender dataset. Project page with videos and code: https://raygauss.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "ray casting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for Autonomous Driving",
    "authors": [
      "Zhangshuo Qi",
      "Junyi Ma",
      "Jingyi Xu",
      "Zijie Zhou",
      "Luqi Cheng",
      "Guangming Xiong"
    ],
    "abstract": "Place recognition is a crucial component that enables autonomous vehicles to obtain localization results in GPS-denied environments. In recent years, multimodal place recognition methods have gained increasing attention. They overcome the weaknesses of unimodal sensor systems by leveraging complementary information from different modalities. However, most existing methods explore cross-modality correlations through feature-level or descriptor-level fusion, suffering from a lack of interpretability. Conversely, the recently proposed 3D Gaussian Splatting provides a new perspective on multimodal fusion by harmonizing different modalities into an explicit scene representation. In this paper, we propose a 3D Gaussian Splatting-based multimodal place recognition network dubbed GSPR. It explicitly combines multi-view RGB images and LiDAR point clouds into a spatio-temporally unified scene representation with the proposed Multimodal Gaussian Splatting. A network composed of 3D graph convolution and transformer is designed to extract spatio-temporal features and global descriptors from the Gaussian scenes for place recognition. Extensive evaluations on three datasets demonstrate that our method can effectively leverage complementary strengths of both multi-view cameras and LiDAR, achieving SOTA place recognition performance while maintaining solid generalization ability. Our open-source code will be released at https://github.com/QiZS-BIT/GSPR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "recognition",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflective Gaussian Splatting",
    "authors": [
      "Yuxuan Yao",
      "Zixuan Zeng",
      "Chun Gu",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (Ref-Gaussian) framework characterized with two components: (I) Physically based deferred rendering that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) Gaussian-grounded inter-reflection that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "nerf",
      "reflection",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting with NeRF-based Color and Opacity",
    "authors": [
      "Dawid Malarz",
      "Weronika Smolak",
      "Jacek Tabor",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects. By encoding the shape and color information within neural network weights, NeRFs excel at producing strikingly sharp novel views of 3D objects. Recently, numerous generalizations of NeRFs utilizing generative models have emerged, expanding its versatility. In contrast, Gaussian Splatting (GS) offers a similar render quality with faster training and inference as it does not need neural networks to work. It encodes information about the 3D objects in the set of Gaussian distributions that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are difficult to condition since they usually require circa hundred thousand Gaussian components. To mitigate the caveats of both models, we propose a hybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS representation of the 3D object's shape and NeRF-based encoding of color and opacity. Our model uses Gaussian distributions with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and a neural network that takes Gaussian parameters and viewing direction to produce changes in the said color and opacity. As a result, our model better describes shadows, light reflections, and the transparency of 3D objects without adding additional texture and light components.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "shadow",
      "ar",
      "gaussian splatting",
      "nerf",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis",
    "authors": [
      "Rui Peng",
      "Wangze Xu",
      "Luyang Tang",
      "Liwei Liao",
      "Jianbo Jiao",
      "Ronggang Wang"
    ],
    "abstract": "Despite the substantial progress of novel view synthesis, existing methods, either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian Splatting (3DGS), suffer significant degradation when the input becomes sparse. Numerous efforts have been introduced to alleviate this problem, but they still struggle to synthesize satisfactory results efficiently, especially in the large scene. In this paper, we propose SCGaussian, a Structure Consistent Gaussian Splatting method using matching priors to learn 3D consistent scene structure. Considering the high interdependence of Gaussian attributes, we optimize the scene structure in two folds: rendering geometry and, more importantly, the position of Gaussian primitives, which is hard to be directly constrained in the vanilla 3DGS due to the non-structure property. To achieve this, we present a hybrid Gaussian representation. Besides the ordinary non-structure Gaussian primitives, our model also consists of ray-based Gaussian primitives that are bound to matching rays and whose optimization of their positions is restricted along the ray. Thus, we can utilize the matching correspondence to directly enforce the position of these Gaussian primitives to converge to the surface points where rays intersect. Extensive experiments on forward-facing, surrounding, and complex large scenes show the effectiveness of our approach with state-of-the-art performance and high efficiency. Code is available at https://github.com/prstrive/SCGaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "few-shot",
      "geometry",
      "large scene",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian Radiance Fields",
    "authors": [
      "Yuru Xiao",
      "Deming Zhai",
      "Wenbo Zhao",
      "Kui Jiang",
      "Junjun Jiang",
      "Xianming Liu"
    ],
    "abstract": "Radiance fields represented by 3D Gaussians excel at synthesizing novel views, offering both high training efficiency and fast rendering. However, with sparse input views, the lack of multi-view consistency constraints results in poorly initialized Gaussians and unreliable heuristics for optimization, leading to suboptimal performance. Existing methods often incorporate depth priors from dense estimation networks but overlook the inherent multi-view consistency in input images. Additionally, they rely on dense initialization, which limits the efficiency of scene representation. To overcome these challenges, we propose a view synthesis framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic scene reconstruction from sparse views. The key innovations of MCGS in enhancing multi-view consistency are as follows: i) We leverage matching priors from a sparse matcher to initialize Gaussians primarily on textured regions, while low-texture areas are populated with randomly distributed Gaussians. This yields a compact yet sufficient set of initial Gaussians. ii) We propose a multi-view consistency-guided progressive pruning strategy to dynamically eliminate inconsistent Gaussians. This approach confines their optimization to a consistency-constrained space, which ensures robust and coherent scene reconstruction. These strategies enhance robustness to sparse views, accelerate rendering, and reduce memory consumption, making MCGS a practical framework for 3D Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GFreeDet: Exploiting Gaussian Splatting and Foundation Models for Model-free Unseen Object Detection in the BOP Challenge 2024",
    "authors": [
      "Xingyu Liu",
      "Gu Wang",
      "Chengxi Li",
      "Yingyue Li",
      "Chenyangguang Zhang",
      "Ziqin Huang",
      "Xiangyang Ji"
    ],
    "abstract": "We present GFreeDet, an unseen object detection approach that leverages Gaussian splatting and vision Foundation models under model-free setting. Unlike existing methods that rely on predefined CAD templates, GFreeDet reconstructs objects directly from reference videos using Gaussian splatting, enabling robust detection of novel objects without prior 3D models. Evaluated on the BOP-H3 benchmark, GFreeDet achieves comparable performance to CAD-based methods, demonstrating the viability of model-free detection for mixed reality (MR) applications. Notably, GFreeDet won the best overall method and the best fast method awards in the model-free 2D detection track at BOP Challenge 2024.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians",
    "authors": [
      "Hiba Dahmani",
      "Moussab Bennehar",
      "Nathan Piasco",
      "Luis Roldao",
      "Dzmitry Tsishkou"
    ],
    "abstract": "Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes",
    "authors": [
      "Xiaohan Li",
      "Ziren Gong",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia",
      "Dong Liu",
      "Jun Wu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM applications due to its fast rendering and high-fidelity representation. However, existing 3DGS-SLAM systems have predominantly focused on indoor environments and relied on active depth sensors, leaving a gap for large-scale outdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian Splatting SLAM system designed for outdoor scenarios. Our approach uses only RGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages depth estimates from pre-trained deep stereo networks to guide 3D Gaussian optimization with a multi-loss strategy enhancing both geometric consistency and visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM achieves superior tracking accuracy and mapping performance compared to other 3DGS-based solutions in complex outdoor environments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "outdoor",
      "tracking",
      "urban scene",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering",
    "authors": [
      "Antoine Guédon",
      "Vincent Lepetit"
    ],
    "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "animation",
      "efficient rendering",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting",
    "authors": [
      "Wanlin Liang",
      "Hongbin Xu",
      "Weitao Chen",
      "Feng Xiao",
      "Wenxiong Kang"
    ],
    "abstract": "3D neural style transfer has gained significant attention for its potential to provide user-friendly stylization with spatial consistency. However, existing 3D style transfer methods often fall short in terms of inference efficiency, generalization ability, and struggle to handle dynamic scenes with temporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D style transfer framework designed to achieve real-time stylization of arbitrary style references while maintaining reasonable content affinity, multi-view consistency, and temporal coherence. Our approach leverages an embedded 4D Gaussian Splatting technique, which is trained using a reversible neural network for reducing content loss in the feature distillation process. Utilizing the 4D embedded Gaussians, we predict a 4D style transformation matrix that facilitates spatially and temporally consistent style transfer with Gaussian Splatting. Experiments demonstrate that our method can achieve high-quality and zero-shot stylization for 4D scenarios with enhanced efficiency and spatial-temporal consistency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGAN: Real-Time Photorealistic controllable Human Avatars",
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Richard Bowden"
    ],
    "abstract": "Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "segmentation",
      "neural rendering",
      "semantic",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "abstract": "State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "body",
      "motion",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting with Enhanced Mesh Reconstruction",
    "authors": [
      "Jianing Zhang",
      "Yuchao Zheng",
      "Ziwei Li",
      "Qionghai Dai",
      "Xiaoyun Yuan"
    ],
    "abstract": "Gaussian splatting has gained attention for its efficient representation and rendering of 3D scenes using continuous Gaussian primitives. However, it struggles with sparse-view inputs due to limited geometric and photometric information, causing ambiguities in depth, shape, and texture.   we propose GBR: Generative Bundle Refinement, a method for high-fidelity Gaussian splatting and meshing using only 4-6 input views. GBR integrates a neural bundle adjustment module to enhance geometry accuracy and a generative depth refinement module to improve geometry fidelity. More specifically, the neural bundle adjustment module integrates a foundation network to produce initial 3D point maps and point matches from unposed images, followed by bundle adjustment optimization to improve multiview consistency and point cloud accuracy. The generative depth refinement module employs a diffusion-based strategy to enhance geometric details and fidelity while preserving the scale. Finally, for Gaussian splatting optimization, we propose a multimodal loss function incorporating depth and normal consistency, geometric regularization, and pseudo-view supervision, providing robust guidance under sparse-view conditions. Experiments on widely used datasets show that GBR significantly outperforms existing methods under sparse-view inputs. Additionally, GBR demonstrates the ability to reconstruct and render large-scale real-world scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable details using only 6 views.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather Effects",
    "authors": [
      "Gal Fiebelman",
      "Hadar Averbuch-Elor",
      "Sagie Benaim"
    ],
    "abstract": "3D Gaussian Splatting has recently enabled fast and photorealistic reconstruction of static 3D scenes. However, introducing dynamic elements that interact naturally with such static scenes remains challenging. Accordingly, we present a novel hybrid framework that combines Gaussian-particle representations for incorporating physically-based global weather effects into static 3D Gaussian Splatting scenes, correctly handling the interactions of dynamic elements with the static scene. We follow a three-stage process: we first map static 3D Gaussians to a particle-based representation. We then introduce dynamic particles and simulate their motion using the Material Point Method (MPM). Finally, we map the simulated particles back to the Gaussian domain while introducing appearance parameters tailored for specific effects. To correctly handle the interactions of dynamic elements with the static scene, we introduce specialized collision handling techniques. Our approach supports a variety of weather effects, including snowfall, rainfall, fog, and sandstorms, and can also support falling objects, all with physically plausible motion and appearance. Experiments demonstrate that our method significantly outperforms existing approaches in both visual quality and physical realism.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling",
    "authors": [
      "Cheng Peng",
      "Yutao Tang",
      "Yifan Zhou",
      "Nengyu Wang",
      "Xijun Liu",
      "Deming Li",
      "Rama Chellappa"
    ],
    "abstract": "Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated benchmarks; however, images captured in real life are often blurry. In this work, we analyze the robustness of Gaussian-Splatting-based methods against various image blur, such as motion blur, defocus blur, downscaling blur, \\etc. Under these degradations, Gaussian-Splatting-based methods tend to overfit and produce worse results than Neural-Radiance-Field-based methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling capacities such that a 3D-consistent and high quality scene can be reconstructed despite image-wise blur. Specifically, we model blur by estimating per-pixel convolution kernels from a Blur Proposal Network (BPN). BPN is designed to consider spatial, color, and depth variations of the scene to maximize modeling capacity. Additionally, BPN also proposes a quality-assessing mask, which indicates regions where blur occur. Finally, we introduce a coarse-to-fine kernel optimization scheme; this optimization scheme is fast and avoids sub-optimal solutions due to a sparse point cloud initialization, which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate that BAGS achieves photorealistic renderings under various challenging blur conditions and imaging geometry, while significantly improving upon existing approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "geometry",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting",
    "authors": [
      "Yufeng Jin",
      "Vignesh Prasad",
      "Snehal Jauhri",
      "Mathias Franzius",
      "Georgia Chalvatzaki"
    ],
    "abstract": "Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \\& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "tracking",
      "robotics",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting",
    "authors": [
      "Hyeongmin Lee",
      "Kyungjune Baek"
    ],
    "abstract": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91$\\times$ compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments. The source code is available at https://github.com/HyeongminLEE/RD4DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with Gaussian Splatting",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jin Wu",
      "Jun Ma"
    ],
    "abstract": "In this paper, we introduce GS-LIVM, a real-time photo-realistic LiDAR-Inertial-Visual mapping framework with Gaussian Splatting tailored for outdoor scenes. Compared to existing methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), our approach enables real-time photo-realistic mapping while ensuring high-quality image rendering in large-scale unbounded outdoor environments. In this work, Gaussian Process Regression (GPR) is employed to mitigate the issues resulting from sparse and unevenly distributed LiDAR observations. The voxel-based 3D Gaussians map representation facilitates real-time dense mapping in large outdoor environments with acceleration governed by custom CUDA kernels. Moreover, the overall framework is designed in a covariance-centered manner, where the estimated covariance is used to initialize the scale and rotation of 3D Gaussians, as well as update the parameters of the GPR. We evaluate our algorithm on several outdoor datasets, and the results demonstrate that our method achieves state-of-the-art performance in terms of mapping efficiency and rendering quality. The source code is available on GitHub.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-18",
    "categories": [
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "acceleration",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VeGaS: Video Gaussian Splatting",
    "authors": [
      "Weronika Smolak-Dyżewska",
      "Dawid Malarz",
      "Kornel Howil",
      "Jan Kaczmarczyk",
      "Marcin Mazur",
      "Przemysław Spurek"
    ],
    "abstract": "Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering",
    "authors": [
      "Michael Steiner",
      "Thomas Köhler",
      "Lukas Radl",
      "Felix Windisch",
      "Dieter Schmalstieg",
      "Markus Steinberger"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction",
    "authors": [
      "Diwen Wan",
      "Ruijie Lu",
      "Gang Zeng"
    ],
    "abstract": "Rendering novel view images in dynamic scenes is a crucial yet challenging task. Current methods mainly utilize NeRF-based methods to represent the static scene and an additional time-variant MLP to model scene deformations, resulting in relatively low rendering quality as well as slow inference speed. To tackle these challenges, we propose a novel framework named Superpoint Gaussian Splatting (SP-GS). Specifically, our framework first employs explicit 3D Gaussians to reconstruct the scene and then clusters Gaussians with similar properties (e.g., rotation, translation, and location) into superpoints. Empowered by these superpoints, our method manages to extend 3D Gaussian splatting to dynamic scenes with only a slight increase in computational expense. Apart from achieving state-of-the-art visual quality and real-time rendering under high resolutions, the superpoint representation provides a stronger manipulation capability. Extensive experiments demonstrate the practicality and effectiveness of our approach on both synthetic and real-world datasets. Please see our project page at https://dnvtmf.github.io/SP_GS.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes",
    "authors": [
      "Feng Xiao",
      "Hongbin Xu",
      "Wanlin Liang",
      "Wenxiong Kang"
    ],
    "abstract": "The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting",
    "authors": [
      "Shuojue Yang",
      "Zijian Wu",
      "Mingxuan Hong",
      "Qian Li",
      "Daiyun Shen",
      "Septimiu E. Salcudean",
      "Yueming Jin"
    ],
    "abstract": "Real2Sim is becoming increasingly important with the rapid development of surgical artificial intelligence (AI) and autonomy. In this work, we propose a novel Real2Sim methodology, Instrument-Splatting, that leverages 3D Gaussian Splatting to provide fully controllable 3D reconstruction of surgical instruments from monocular surgical videos. To maintain both high visual fidelity and manipulability, we introduce a geometry pre-training to bind Gaussian point clouds on part mesh with accurate geometric priors and define a forward kinematics to control the Gaussians as flexible as real instruments. Afterward, to handle unposed videos, we design a novel instrument pose tracking method leveraging semantics-embedded Gaussians to robustly refine per-frame instrument poses and joint states in a render-and-compare manner, which allows our instrument Gaussian to accurately learn textures and reach photorealistic rendering. We validated our method on 2 publicly released surgical videos and 4 videos collected on ex vivo tissues and green screens. Quantitative and qualitative evaluations demonstrate the effectiveness and superiority of the proposed method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "tracking",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes",
    "authors": [
      "Allen Tu",
      "Haiyang Ying",
      "Alex Hanson",
      "Yonghan Lee",
      "Tom Goldstein",
      "Matthias Zwicker"
    ],
    "abstract": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve high-quality novel view synthesis by using neural networks to predict the time-varying deformation of each Gaussian. However, performing per-Gaussian neural inference at every frame poses a significant bottleneck, limiting rendering speed and increasing memory and compute requirements. In this paper, we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS representations by reducing neural inference through two complementary techniques. First, we propose a temporal sensitivity pruning score that identifies and removes Gaussians with low contribution to the dynamic scene reconstruction. We also introduce an annealing smooth pruning mechanism that improves pruning robustness in real-world scenes with imprecise camera poses. Second, we propose GroupFlow, a motion analysis technique that clusters Gaussians by trajectory similarity and predicts a single rigid transformation per group instead of separate deformations for each Gaussian. Together, our techniques accelerate rendering by $10.37\\times$, reduce model size by $7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset. SpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be integrated into any deformable 3DGS or 4DGS framework.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "nerf",
      "vr",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading",
    "authors": [
      "Donghyun Lee",
      "Dawoon Jeong",
      "Jae W. Lee",
      "Hongil Yoon"
    ],
    "abstract": "The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS",
    "authors": [
      "Wanhua Li",
      "Yujie Zhao",
      "Minghan Qin",
      "Yang Liu",
      "Yuanhao Cai",
      "Chuang Gan",
      "Hanspeter Pfister"
    ],
    "abstract": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\\times$ speedup and a 47 $\\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Chao Yang",
      "Tao Guan",
      "Lili Ju"
    ],
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance",
    "authors": [
      "Jingqiu Zhou",
      "Lue Fan",
      "Xuesong Chen",
      "Linjiang Huang",
      "Si Liu",
      "Hongsheng Li"
    ],
    "abstract": "In this paper, we present GaussianPainter, the first method to paint a point cloud into 3D Gaussians given a reference image. GaussianPainter introduces an innovative feed-forward approach to overcome the limitations of time-consuming test-time optimization in 3D Gaussian splatting. Our method addresses a critical challenge in the field: the non-uniqueness problem inherent in the large parameter space of 3D Gaussian splatting. This space, encompassing rotation, anisotropic scales, and spherical harmonic coefficients, introduces the challenge of rendering similar images from substantially different Gaussian fields. As a result, feed-forward networks face instability when attempting to directly predict high-quality Gaussian fields, struggling to converge on consistent parameters for a given output. To address this issue, we propose to estimate a surface normal for each point to determine its Gaussian rotation. This strategy enables the network to effectively predict the remaining Gaussian parameters in the constrained space. We further enhance our approach with an appearance injection module, incorporating reference image appearance into Gaussian fields via a multiscale triplane representation. Our method successfully balances efficiency and fidelity in 3D Gaussian generation, achieving high-quality, diverse, and robust 3D content creation from point clouds in a single forward pass.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian Splatting",
    "authors": [
      "Lizhi Bai",
      "Chunqi Tian",
      "Jun Yang",
      "Siyu Zhang",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a promising technique for high-quality 3D rendering, leading to increasing interest in integrating 3DGS into realism SLAM systems. However, existing methods face challenges such as Gaussian primitives redundancy, forgetting problem during continuous optimization, and difficulty in initializing primitives in monocular case due to lack of depth information. In order to achieve efficient and photorealistic mapping, we propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian primitives optimization and consists of three key components. Firstly, we propose an efficient incremental mapping approach to achieve a compact and accurate representation of the scene through adaptive sampling and Gaussian primitives filtering. Secondly, a dynamic window optimization method is proposed to mitigate the forgetting problem and improve map consistency. Finally, for the monocular case, a monocular keyframe initialization method based on sparse point cloud is proposed to improve the initialization accuracy of Gaussian primitives, which provides a geometric basis for subsequent optimization. The results of numerous experiments demonstrate that RP-SLAM achieves state-of-the-art map rendering accuracy while ensuring real-time performance and model compactness.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-13",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
    "authors": [
      "Dejia Xu",
      "Ye Yuan",
      "Morteza Mardani",
      "Sifei Liu",
      "Jiaming Song",
      "Zhangyang Wang",
      "Arash Vahdat"
    ],
    "abstract": "Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: https://ir1d.github.io/AGG/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images",
    "authors": [
      "Jungho Lee",
      "Suhwan Cho",
      "Taeoh Kim",
      "Ho-Deok Jang",
      "Minhyeok Lee",
      "Geonho Cha",
      "Dongyoon Wee",
      "Dogyoon Lee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting with Deferred Reflection",
    "authors": [
      "Keyang Ye",
      "Qiming Hou",
      "Kun Zhou"
    ],
    "abstract": "The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior",
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ],
    "abstract": "We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "animation",
      "body",
      "ar",
      "gaussian splatting",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections",
    "authors": [
      "Jiayue Liu",
      "Xiao Tang",
      "Freeman Cheng",
      "Roy Yang",
      "Zhihao Li",
      "Jianzhuang Liu",
      "Yi Huang",
      "Jiaqi Lin",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Chun Yuan"
    ],
    "abstract": "3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVideo: Efficient Video Representation and Compression by Gaussian Splatting",
    "authors": [
      "Inseo Lee",
      "Youngyoon Choi",
      "Joonseok Lee"
    ],
    "abstract": "Implicit Neural Representation for Videos (NeRV) has introduced a novel paradigm for video representation and compression, outperforming traditional codecs. As model size grows, however, slow encoding and decoding speed and high memory consumption hinder its application in practice. To address these limitations, we propose a new video representation and compression method based on 2D Gaussian Splatting to efficiently handle video data. Our proposed deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D Gaussians at each frame, significantly reducing memory cost. Equipped with a multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts changes in color, coordinates, and shape of initialized Gaussians, given the time step. By leveraging temporal gradients, our model effectively captures temporal redundancy at negligible cost, significantly enhancing video representation efficiency. Our method reduces GPU memory usage by up to 78.4%, and significantly expedites video processing, achieving 5.5x faster training and 12.5x faster decoding compared to the state-of-the-art NeRV methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "lightweight",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "abstract": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LGS: A Light-weight 4D Gaussian Splatting for Efficient Surgical Scene Reconstruction",
    "authors": [
      "Hengyu Liu",
      "Yifan Liu",
      "Chenxin Li",
      "Wuyang Li",
      "Yixuan Yuan"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3D-GS) techniques and their dynamic scene modeling variants, 4D-GS, offers promising prospects for real-time rendering of dynamic surgical scenarios. However, the prerequisite for modeling dynamic scenes by a large number of Gaussian units, the high-dimensional Gaussian attributes and the high-resolution deformation fields, all lead to serve storage issues that hinder real-time rendering in resource-limited surgical equipment. To surmount these limitations, we introduce a Lightweight 4D Gaussian Splatting framework (LGS) that can liberate the efficiency bottlenecks of both rendering and storage for dynamic endoscopic reconstruction. Specifically, to minimize the redundancy of Gaussian quantities, we propose Deformation-Aware Pruning by gauging the impact of each Gaussian on deformation. Concurrently, to reduce the redundancy of Gaussian attributes, we simplify the representation of textures and lighting in non-crucial areas by pruning the dimensions of Gaussian attributes. We further resolve the feature field redundancy caused by the high resolution of 4D neural spatiotemporal encoder for modeling dynamic scenes via a 4D feature field condensation. Experiments on public benchmarks demonstrate efficacy of LGS in terms of a compression rate exceeding 9 times while maintaining the pleasing visual quality and real-time rendering efficiency. LGS confirms a substantial step towards its application in robotic surgical services.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "lightweight",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HUGS: Human Gaussian Splats",
    "authors": [
      "Muhammed Kocabas",
      "Jen-Hao Rick Chang",
      "James Gabriel",
      "Oncel Tuzel",
      "Anurag Ranjan"
    ],
    "abstract": "Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "animation",
      "neural rendering",
      "body",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Joo Chan Lee",
      "Daniel Rho",
      "Jong Hwan Ko",
      "Usman Ali",
      "Eunbyung Park"
    ],
    "abstract": "The neural radiance field (NeRF) has made significant strides in representing 3D scenes and synthesizing novel views. Despite its advancements, the high computational costs of NeRF have posed challenges for its deployment in resource-constrained environments and real-time applications. As an alternative to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers rapid rendering speeds while maintaining excellent image quality. However, as it represents objects and scenes using a myriad of Gaussians, it requires substantial storage to achieve high-quality representation. To mitigate the storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality. Inspired by classical matrix and tensor factorization techniques, our method represents and approximates dense clusters of Gaussians with significantly fewer Gaussians through efficient factorization. We aim to efficiently represent dense 3D Gaussians by approximating them with a limited amount of information for each axis and their combinations. This method allows us to encode a substantially large number of Gaussians along with their essential attributes -- such as color, scale, and rotation -- necessary for rendering using a relatively small number of elements. Extensive experimental results demonstrate that F-3DGS achieves a significant reduction in storage costs while maintaining comparable quality in rendered images.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel View Synthesis",
    "authors": [
      "Zhongche Qu",
      "Zhi Zhang",
      "Cong Liu",
      "Jianhua Yin"
    ],
    "abstract": "Conventional geometry-based SLAM systems lack dense 3D reconstruction capabilities since their data association usually relies on feature correspondences. Additionally, learning-based SLAM systems often fall short in terms of real-time performance and accuracy. Balancing real-time performance with dense 3D reconstruction capabilities is a challenging problem. In this paper, we propose a real-time RGB-D SLAM system that incorporates a novel view synthesis technique, 3D Gaussian Splatting, for 3D scene representation and pose estimation. This technique leverages the real-time rendering performance of 3D Gaussian Splatting with rasterization and allows for differentiable optimization in real time through CUDA implementation. We also enable mesh reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To estimate accurate camera poses, we utilize a rotation-translation decoupled strategy with inverse optimization. This involves iteratively updating both in several iterations through gradient-based optimization. This process includes differentiably rendering RGB, depth, and silhouette maps and updating the camera parameters to minimize a combined loss of photometric loss, depth geometry loss, and visibility loss, given the existing 3D Gaussian map. However, 3D Gaussian Splatting (3DGS) struggles to accurately represent surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to reduced accuracy in both camera pose estimation and scene reconstruction. To address this, we utilize depth priors as additional regularization to enforce geometric constraints, thereby improving the accuracy of both pose estimation and 3D reconstruction. We also provide extensive experimental results on public benchmark datasets to demonstrate the effectiveness of our proposed methods in terms of pose accuracy, geometric accuracy, and rendering performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-10",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting",
    "authors": [
      "Mohammad Nomaan Qureshi",
      "Sparsh Garg",
      "Francisco Yandun",
      "David Held",
      "George Kantor",
      "Abhisesh Silwal"
    ],
    "abstract": "Sim2Real transfer, particularly for manipulation policies relying on RGB images, remains a critical challenge in robotics due to the significant domain shift between synthetic and real-world visual data. In this paper, we propose SplatSim, a novel framework that leverages Gaussian Splatting as the primary rendering primitive to reduce the Sim2Real gap for RGB-based manipulation policies. By replacing traditional mesh representations with Gaussian Splats in simulators, SplatSim produces highly photorealistic synthetic data while maintaining the scalability and cost-efficiency of simulation. We demonstrate the effectiveness of our framework by training manipulation policies within SplatSim and deploying them in the real world in a zero-shot manner, achieving an average success rate of 86.25%, compared to 97.5% for policies trained on real-world data. Videos can be found on our project page: https://splatsim.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-16",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction",
    "authors": [
      "Yuxuan Mu",
      "Xinxin Zuo",
      "Chuan Guo",
      "Yilin Wang",
      "Juwei Lu",
      "Xiaofeng Wu",
      "Songcen Xu",
      "Peng Dai",
      "Youliang Yan",
      "Li Cheng"
    ],
    "abstract": "We present GSD, a diffusion model approach based on Gaussian Splatting (GS) representation for 3D object reconstruction from a single view. Prior works suffer from inconsistent 3D geometry or mediocre rendering quality due to improper representations. We take a step towards resolving these shortcomings by utilizing the recent state-of-the-art 3D explicit representation, Gaussian Splatting, and an unconditional diffusion model. This model learns to generate 3D objects represented by sets of GS ellipsoids. With these strong generative 3D priors, though learning unconditionally, the diffusion model is ready for view-guided reconstruction without further model fine-tuning. This is achieved by propagating fine-grained 2D features through the efficient yet flexible splatting function and the guided denoising sampling process. In addition, a 2D diffusion model is further employed to enhance rendering fidelity, and improve reconstructed GS quality by polishing and re-using the rendered images. The final reconstructed objects explicitly come with high-quality 3D structure and texture, and can be efficiently rendered in arbitrary views. Experiments on the challenging real-world CO3D dataset demonstrate the superiority of our approach. Project page: https://yxmu.foo/GSD/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review",
    "authors": [
      "Anurag Dalal",
      "Daniel Hagen",
      "Kjell G. Robbersmyr",
      "Kristian Muri Knausgård"
    ],
    "abstract": "Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting",
    "authors": [
      "Kyle Gao",
      "Liangzhi Li",
      "Hongjie He",
      "Dening Lu",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "abstract": "Recently released open-source pre-trained foundational image segmentation and object detection models (SAM2+GroundingDINO) allow for geometrically consistent segmentation of objects of interest in multi-view 2D images. Users can use text-based or click-based prompts to segment objects of interest without requiring labeled training datasets. Gaussian Splatting allows for the learning of the 3D representation of a scene's geometry and radiance based on 2D images. Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and our improvements in mask refinement based on morphological operations and contour simplification, we created a pipeline to extract the 3D mesh of any building based on its name, address, or geographic coordinates.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation",
    "authors": [
      "Mingyu Jeong",
      "Eunsung Kim",
      "Sehun Park",
      "Andrew Jaeyong Choi"
    ],
    "abstract": "We present NVSim, a framework that automatically constructs large-scale, navigable indoor simulators from only common image sequences, overcoming the cost and scalability limitations of traditional 3D scanning. Our approach adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed floors a common issue in robotic traversal data. We introduce Floor-Aware Gaussian Splatting to ensure a clean, navigable ground plane, and a novel mesh-free traversability checking algorithm that constructs a topological graph by directly analyzing rendered views. We demonstrate our system's ability to generate valid, large-scale navigation graphs from real-world data. A video demonstration is avilable at https://youtu.be/tTiIQt6nXC8",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A new dataset and comparison for multi-camera frame synthesis",
    "authors": [
      "Conall Daly",
      "Anil Kokaram"
    ],
    "abstract": "Many methods exist for frame synthesis in image sequences but can be broadly categorised into frame interpolation and view synthesis techniques. Fundamentally, both frame interpolation and view synthesis tackle the same task, interpolating a frame given surrounding frames in time or space. However, most frame interpolation datasets focus on temporal aspects with single cameras moving through time and space, while view synthesis datasets are typically biased toward stereoscopic depth estimation use cases. This makes direct comparison between view synthesis and frame interpolation methods challenging. In this paper, we develop a novel multi-camera dataset using a custom-built dense linear camera array to enable fair comparison between these approaches. We evaluate classical and deep learning frame interpolators against a view synthesis method (3D Gaussian Splatting) for the task of view in-betweening. Our results reveal that deep learning methods do not significantly outperform classical methods on real image data, with 3D Gaussian Splatting actually underperforming frame interpolators by as much as 3.5 dB PSNR. However, in synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-12",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering",
    "authors": [
      "Jiayi Song",
      "Zihan Ye",
      "Qingyuan Zhou",
      "Weidong Yang",
      "Ben Fei",
      "Jingyi Xu",
      "Ying He",
      "Wanli Ouyang"
    ],
    "abstract": "Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of Distractors",
    "authors": [
      "Paul Ungermann",
      "Armin Ettenhofer",
      "Matthias Nießner",
      "Barbara Roessle"
    ],
    "abstract": "3D Gaussian Splatting has shown impressive novel view synthesis results; nonetheless, it is vulnerable to dynamic objects polluting the input data of an otherwise static scene, so called distractors. Distractors have severe impact on the rendering quality as they get represented as view-dependent effects or result in floating artifacts. Our goal is to identify and ignore such distractors during the 3D Gaussian optimization to obtain a clean reconstruction. To this end, we take a self-supervised approach that looks at the image residuals during the optimization to determine areas that have likely been falsified by a distractor. In addition, we leverage a pretrained segmentation network to provide object awareness, enabling more accurate exclusion of distractors. This way, we obtain segmentation masks of distractors to effectively ignore them in the loss formulation. We demonstrate that our approach is robust to various distractors and strongly improves rendering quality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting",
    "authors": [
      "Yumeng He",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce MetaGS to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the Blinn-Phong reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-31",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views",
    "authors": [
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Hanzhang Tu",
      "Ruizhi Shao",
      "Boning Liu",
      "Shengping Zhang",
      "Liqiang Nie",
      "Yebin Liu"
    ],
    "abstract": "Differentiable rendering techniques have recently shown promising results for free-viewpoint video synthesis of characters. However, such methods, either Gaussian Splatting or neural implicit rendering, typically necessitate per-subject optimization which does not meet the requirement of real-time rendering in an interactive application. We propose a generalizable Gaussian Splatting approach for high-resolution image rendering under a sparse-view camera setting. To this end, we introduce Gaussian parameter maps defined on the source views and directly regress Gaussian properties for instant novel view synthesis without any fine-tuning or optimization. We train our Gaussian parameter regression module on human-only data or human-scene data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable with both depth and rendering supervision or with only rendering supervision. We further introduce a regularization term and an epipolar attention mechanism to preserve geometry consistency between two source views, especially when neglecting depth supervision. Experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction",
    "authors": [
      "Xinyu Zhang",
      "Haonan Chang",
      "Yuhan Liu",
      "Abdeslam Boularias"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "robotics",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering",
    "authors": [
      "Yutao Feng",
      "Xiang Feng",
      "Yintong Shang",
      "Ying Jiang",
      "Chang Yu",
      "Zeshun Zong",
      "Tianjia Shao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ],
    "abstract": "We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \\url{https://gaussiansplashing.github.io/}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-27",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "face",
      "reflection",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian Splatting",
    "authors": [
      "Zhenyuan Liu",
      "Yu Guo",
      "Xinyuan Li",
      "Bernd Bickel",
      "Ran Zhang"
    ],
    "abstract": "We present Bidirectional Gaussian Primitives, an image-based novel view synthesis technique designed to represent and render 3D objects with surface and volumetric materials under dynamic illumination. Our approach integrates light intrinsic decomposition into the Gaussian splatting framework, enabling real-time relighting of 3D objects. To unify surface and volumetric material within a cohesive appearance model, we adopt a light- and view-dependent scattering representation via bidirectional spherical harmonics. Our model does not use a specific surface normal-related reflectance function, making it more compatible with volumetric representations like Gaussian splatting, where the normals are undefined. We demonstrate our method by reconstructing and rendering objects with complex materials. Using One-Light-At-a-Time (OLAT) data as input, we can reproduce photorealistic appearances under novel lighting conditions in real time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "illumination",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
    "authors": [
      "Wanting Zhang",
      "Haodong Xiang",
      "Zhichao Liao",
      "Xiansong Lai",
      "Xinghui Li",
      "Long Zeng"
    ],
    "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose",
    "authors": [
      "Kaiwen Jiang",
      "Yang Fu",
      "Mukund Varma T",
      "Yash Belhe",
      "Xiaolong Wang",
      "Hao Su",
      "Ravi Ramamoorthi"
    ],
    "abstract": "Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset. Project page: https://raymondjiangkw.github.io/cogs.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera",
    "authors": [
      "Jian Huang",
      "Chengrui Dong",
      "Xuanhua Chen",
      "Peidong Liu"
    ],
    "abstract": "Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for novel view synthesis have achieved remarkable progress with frame-based camera (e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel type of bio-inspired visual sensor, i.e. event camera, has demonstrated advantages in high temporal resolution, high dynamic range, low power consumption and low latency. Due to its unique asynchronous and irregular data capturing process, limited work has been proposed to apply neural representation or 3D Gaussian splatting for an event camera. In this work, we present IncEventGS, an incremental 3D Gaussian Splatting reconstruction algorithm with a single event camera. To recover the 3D scene representation incrementally, we exploit the tracking and mapping paradigm of conventional SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker firstly estimates an initial camera motion based on prior reconstructed 3D-GS scene representation. The mapper then jointly refines both the 3D scene representation and camera motion based on the previously estimated motion trajectory from the tracker. The experimental results demonstrate that IncEventGS delivers superior performance compared to prior NeRF-based methods and other related baselines, even we do not have the ground-truth camera poses. Furthermore, our method can also deliver better performance compared to state-of-the-art event visual odometry methods in terms of camera motion estimation. Code is publicly available at: https://github.com/wu-cvgl/IncEventGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "mapping",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable Radial Kernel Splatting",
    "authors": [
      "Yi-Hua Huang",
      "Ming-Xian Lin",
      "Yang-Tian Sun",
      "Ziyi Yang",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ],
    "abstract": "Recently, Gaussian splatting has emerged as a robust technique for representing 3D scenes, enabling real-time rasterization and high-fidelity rendering. However, Gaussians' inherent radial symmetry and smoothness constraints limit their ability to represent complex shapes, often requiring thousands of primitives to approximate detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature. iven DRK's planar nature, we further develop accurate ray-primitive intersection computation for depth sorting and introduce efficient kernel culling strategies for improved rasterization efficiency. Extensive experiments demonstrate that DRK outperforms existing methods in both representation efficiency and rendering quality, achieving state-of-the-art performance while dramatically reducing primitive count.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels",
    "authors": [
      "Haodong Chen",
      "Runnan Chen",
      "Qiang Qu",
      "Zhaoqing Wang",
      "Tongliang Liu",
      "Xiaoming Chen",
      "Yuk Ying Chung"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training images, improving rendering consistency, and optimizing density control, yet the role of kernel design remains underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of the causes of these artifacts, limiting detail capture in high-frequency regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which replaces Gaussian kernels with linear kernels to achieve sharper and more precise results, particularly in high-frequency regions. Through evaluations on three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along with a 30% FPS improvement over baseline 3DGS. The implementation will be made publicly available upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis",
    "authors": [
      "Zhan Li",
      "Zhang Chen",
      "Zhong Li",
      "Yi Xu"
    ],
    "abstract": "Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "motion",
      "ar",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS^3: Efficient Relighting with Triple Gaussian Splatting",
    "authors": [
      "Zoubin Bi",
      "Yixin Zeng",
      "Chong Zeng",
      "Fan Pei",
      "Xiang Feng",
      "Kun Zhou",
      "Hongzhi Wu"
    ],
    "abstract": "We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "shadow",
      "illumination",
      "geometry",
      "lighting",
      "global illumination",
      "gaussian splatting",
      "ar",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings",
    "authors": [
      "Abdalla Arafa",
      "Didier Stricker"
    ],
    "abstract": "Novel view synthesis has seen significant advancements with 3D Gaussian Splatting (3DGS), enabling real-time photorealistic rendering. However, the inherent fuzziness of Gaussian Splatting presents challenges for 3D scene understanding, restricting its broader applications in AR/VR and robotics. While recent works attempt to learn semantics via 2D foundation model distillation, they inherit fundamental limitations: alpha blending averages semantics across objects, making 3D-level understanding impossible. We propose a paradigm-shifting alternative that bypasses differentiable rendering for semantics entirely. Our key insight is to leverage predecomposed object-level Gaussians and represent each object through multiview CLIP feature aggregation, creating comprehensive \"bags of embeddings\" that holistically describe objects. This allows: (1) accurate open-vocabulary object retrieval by comparing text queries to object-level (not Gaussian-level) embeddings, and (2) seamless task adaptation: propagating object IDs to pixels for 2D segmentation or to Gaussians for 3D extraction. Experiments demonstrate that our method effectively overcomes the challenges of 3D open-vocabulary object extraction while remaining comparable to state-of-the-art performance in 2D open-vocabulary segmentation, ensuring minimal compromise.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bézier Splatting for Fast and Differentiable Vector Graphics Rendering",
    "authors": [
      "Xi Liu",
      "Chaoyi Zhou",
      "Nanxuan Zhao",
      "Siyu Huang"
    ],
    "abstract": "Differentiable vector graphics (VGs) are widely used in image vectorization and vector synthesis, while existing representations are costly to optimize and struggle to achieve high-quality rendering results for high-resolution images. This work introduces a new differentiable VG representation, dubbed Bézier Splatting, that enables fast yet high-fidelity VG rasterization. Bézier Splatting samples 2D Gaussians along Bézier curves, which naturally provide positional gradients at object boundaries. Thanks to the efficient splatting-based differentiable rasterizer, Bézier Splatting achieves 30x and 150x faster per forward and backward rasterization step for open curves compared to DiffVG. Additionally, we introduce an adaptive pruning and densification strategy that dynamically adjusts the spatial distribution of curves to escape local minima, further improving VG quality. Furthermore, our new VG representation supports conversion to standard XML-based SVG format, enhancing interoperability with existing VG tools and pipelines. Experimental results show that Bézier Splatting significantly outperforms existing methods with better visual fidelity and significant optimization speedup.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-20",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction",
    "authors": [
      "Yuchen Gao",
      "Xiao Xu",
      "Eckehard Steinbach",
      "Daniel E. Lucani",
      "Qi Zhang"
    ],
    "abstract": "This paper presents a multimodal framework that integrates touch signals (contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our approach enhances scene reconstruction, particularly under challenging conditions like low lighting, limited camera viewpoints, and occlusions. Different from the visual-only method, the proposed approach incorporates spatially selective touch measurements to refine both the geometry and appearance of the 3D Gaussian representation. To guide the touch exploration, we introduce a two-stage sampling scheme that initially probes sparse regions and then concentrates on high-uncertainty boundaries identified from the reconstructed mesh. A geometric loss is proposed to ensure surface smoothness, resulting in improved geometry. Experimental results across diverse scenarios show consistent improvements in geometric accuracy. In the most challenging case with severe occlusion, the Chamfer Distance is reduced by over 15x, demonstrating the effectiveness of integrating touch cues into 3D Gaussian Splatting. Furthermore, our approach maintains a fully online pipeline, underscoring its feasibility in visually degraded environments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-11",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fast Feedforward 3D Gaussian Splatting Compression",
    "authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Mengyao Li",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity rendering for novel view synthesis, storage requirements pose challenges for their widespread adoption. Although various compression techniques have been proposed, previous art suffers from a common limitation: for any existing 3DGS, per-scene optimization is needed to achieve compression, making the compression sluggish and slow. To address this issue, we introduce Fast Compression of 3D Gaussian Splatting (FCGS), an optimization-free model that can compress 3DGS representations rapidly in a single feed-forward pass, which significantly reduces compression time from minutes to seconds. To enhance compression efficiency, we propose a multi-path entropy module that assigns Gaussian attributes to different entropy constraint paths for balance between size and fidelity. We also carefully design both inter- and intra-Gaussian context models to remove redundancies among the unstructured Gaussian blobs. Overall, FCGS achieves a compression ratio of over 20X while maintaining fidelity, surpassing most per-scene SOTA optimization-based methods. Our code is available at: https://github.com/YihangChen-ee/FCGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting",
    "authors": [
      "Zijian Wang",
      "Beizhen Zhao",
      "Hao Wang"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable capabilities in real-time and photorealistic novel view synthesis. However, traditional 3DGS representations often struggle with large-scale scene management and efficient storage, particularly when dealing with complex environments or limited computational resources. To address these limitations, we introduce a novel perceive-sample-compress framework for 3D Gaussian Splatting. Specifically, we propose a scene perception compensation algorithm that intelligently refines Gaussian parameters at each level. This algorithm intelligently prioritizes visual importance for higher fidelity rendering in critical areas, while optimizing resource usage and improving overall visible quality. Furthermore, we propose a pyramid sampling representation to manage Gaussian primitives across hierarchical levels. Finally, to facilitate efficient storage of proposed hierarchical pyramid representations, we develop a Generalized Gaussian Mixed model compression algorithm to achieve significant compression ratios without sacrificing visual fidelity. The extensive experiments demonstrate that our method significantly improves memory efficiency and high visual quality while maintaining real-time rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "You Need a Transition Plane: Bridging Continuous Panoramic 3D Reconstruction with Perspective Gaussian Splatting",
    "authors": [
      "Zhijie Shen",
      "Chunyu Lin",
      "Shujuan Huang",
      "Lang Nie",
      "Kang Liao",
      "Yao Zhao"
    ],
    "abstract": "Recently, reconstructing scenes from a single panoramic image using advanced 3D Gaussian Splatting (3DGS) techniques has attracted growing interest. Panoramic images offer a 360$\\times$ 180 field of view (FoV), capturing the entire scene in a single shot. However, panoramic images introduce severe distortion, making it challenging to render 3D Gaussians into 2D distorted equirectangular space directly. Converting equirectangular images to cubemap projections partially alleviates this problem but introduces new challenges, such as projection distortion and discontinuities across cube-face boundaries. To address these limitations, we present a novel framework, named TPGS, to bridge continuous panoramic 3D scene reconstruction with perspective Gaussian splatting. Firstly, we introduce a Transition Plane between adjacent cube faces to enable smoother transitions in splatting directions and mitigate optimization ambiguity in the boundary region. Moreover, an intra-to-inter face optimization strategy is proposed to enhance local details and restore visual consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians within individual cube faces and then fine-tune them in the stitched panoramic space. Additionally, we introduce a spherical sampling technique to eliminate visible stitching seams. Extensive experiments on indoor and outdoor, egocentric, and roaming benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods. Code and models will be available at https://github.com/zhijieshen-bjtu/TPGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EDGS: Eliminating Densification for Efficient Convergence of 3DGS",
    "authors": [
      "Dmytro Kotovenko",
      "Olga Grebenkova",
      "Björn Ommer"
    ],
    "abstract": "3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a lengthy optimization path. Moreover, this incremental approach often leads to suboptimal renderings, particularly in high-frequency regions where detail is critical.   We propose a fundamentally different approach: we eliminate densification process with a one-step approximation of scene geometry using triangulated pixels from dense image correspondences. This dense initialization allows us to estimate rough geometry of the scene while preserving rich details from input RGB images, providing each Gaussian with well-informed colors, scales, and positions. As a result, we dramatically shorten the optimization path and remove the need for densification. Unlike traditional methods that rely on sparse keypoints, our dense initialization ensures uniform detail across the scene, even in high-frequency regions where 3DGS and other methods struggle. Moreover, since all splats are initialized in parallel at the start of optimization, we eliminate the need to wait for densification to adjust new Gaussians.   Our method not only outperforms speed-optimized models in training efficiency but also achieves higher rendering quality than state-of-the-art approaches, all while using only half the splats of standard 3DGS. It is fully compatible with other 3DGS acceleration techniques, making it a versatile and efficient solution that can be integrated with existing approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-15",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "acceleration",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
    "authors": [
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Hanzhi Chang",
      "Jiacheng Deng",
      "Jiahao Lu",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ],
    "abstract": "Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes",
    "authors": [
      "Chensheng Peng",
      "Chengwei Zhang",
      "Yixiao Wang",
      "Chenfeng Xu",
      "Yichen Xie",
      "Wenzhao Zheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "abstract": "We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective static-dynamic decomposition and high-fidelity surface reconstruction in complex driving scenarios. Our approach employs a two-stage optimization pipeline of dynamic street Gaussians. In the first stage, we extract 2D motion masks based on the observation that 3D Gaussian Splatting inherently can reconstruct only the static regions in dynamic environments. These extracted 2D motion priors are then mapped into the Gaussian space in a differentiable manner, leveraging an efficient formulation of dynamic Gaussians in the second stage. Combined with the introduced geometric regularizations, our method are able to address the over-fitting issues caused by data sparsity in autonomous driving, reconstructing physically plausible Gaussians that align with object surfaces rather than floating in air. Furthermore, we introduce temporal cross-view consistency to ensure coherence across time and viewpoints, resulting in high-quality surface reconstruction. Comprehensive experiments demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior self-supervised arts and achieving accuracy comparable to methods relying on external 3D bounding box annotations. Code is available at https://github.com/chengweialan/DeSiRe-GS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "face",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GASP: Gaussian Avatars with Synthetic Priors",
    "authors": [
      "Jack Saunders",
      "Charlie Hewitt",
      "Yanan Jian",
      "Marek Kowalski",
      "Tadas Baltrusaitis",
      "Yiye Chen",
      "Darren Cosker",
      "Virginia Estellers",
      "Nicholas Gyde",
      "Vinay P. Namboodiri",
      "Benjamin E Lundell"
    ],
    "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://microsoft.github.io/GASP/) for results.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "ar",
      "gaussian splatting",
      "high quality"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fully Explicit Dynamic Gaussian Splatting",
    "authors": [
      "Junoh Lee",
      "Chang-Yeon Won",
      "Hyunjun Jung",
      "Inhwan Bae",
      "Hae-Gon Jeon"
    ],
    "abstract": "3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design a Explicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "tracking",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting",
    "authors": [
      "Lintao Xiang",
      "Hongpei Zheng",
      "Yating Huang",
      "Qijun Yang",
      "Hujun Yin"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "lightweight",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation",
    "authors": [
      "Pengzhi Li",
      "Chengshuai Tang",
      "Qinxuan Huang",
      "Zhiheng Li"
    ],
    "abstract": "In this paper, we explore the existing challenges in 3D artistic scene generation by introducing ART3D, a novel framework that combines diffusion models and 3D Gaussian splatting techniques. Our method effectively bridges the gap between artistic and realistic images through an innovative image semantic transfer algorithm. By leveraging depth information and an initial artistic image, we generate a point cloud map, addressing domain differences. Additionally, we propose a depth consistency module to enhance 3D scene consistency. Finally, the 3D scene serves as initial points for optimizing Gaussian splats. Experimental results demonstrate ART3D's superior performance in both content and structural consistency metrics when compared to existing methods. ART3D significantly advances the field of AI in art creation by providing an innovative solution for generating high-quality 3D artistic scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting",
    "authors": [
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction",
    "authors": [
      "Shengji Tang",
      "Weicai Ye",
      "Peng Ye",
      "Weihao Lin",
      "Yang Zhou",
      "Tao Chen",
      "Wanli Ouyang"
    ],
    "abstract": "Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians",
    "authors": [
      "Yufei Liu",
      "Junshu Tang",
      "Chu Zheng",
      "Shijie Zhang",
      "Jinkun Hao",
      "Junwei Zhu",
      "Dongjin Huang"
    ],
    "abstract": "High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "animation",
      "high-fidelity",
      "body",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Immersive Teleoperation Framework for Locomanipulation Tasks",
    "authors": [
      "Takuya Boehringer",
      "Jonathan Embley-Riches",
      "Karim Hammoud",
      "Valerio Modugno",
      "Dimitrios Kanoulas"
    ],
    "abstract": "Recent advancements in robotic loco-manipulation have leveraged Virtual Reality (VR) to enhance the precision and immersiveness of teleoperation systems, significantly outperforming traditional methods reliant on 2D camera feeds and joystick controls. Despite these advancements, challenges remain, particularly concerning user experience across different setups. This paper introduces a novel VR-based teleoperation framework designed for a robotic manipulator integrated onto a mobile platform. Central to our approach is the application of Gaussian splatting, a technique that abstracts the manipulable scene into a VR environment, thereby enabling more intuitive and immersive interactions. Users can navigate and manipulate within the virtual scene as if interacting with a real robot, enhancing both the engagement and efficacy of teleoperation tasks. An extensive user study validates our approach, demonstrating significant usability and efficiency improvements. Two-thirds (66%) of participants completed tasks faster, achieving an average time reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface overall, with unanimous (100%) recommendations for future use, highlighting improvements in precision, responsiveness, and situational awareness. Finally, we demonstrate the effectiveness of our framework through real-world experiments in two distinct application scenarios, showcasing the practical capabilities and versatility of the Splat-based VR interface.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting",
    "authors": [
      "Yi-Hsin Li",
      "Thomas Sikora",
      "Sebastian Knorr",
      "Måarten Sjöström"
    ],
    "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "sparse-view",
      "geometry",
      "motion",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting",
    "authors": [
      "Jianchao Wang",
      "Peng Zhou",
      "Cen Li",
      "Rong Quan",
      "Jie Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. Project Website: https://jcwang-gh.github.io/EFA-GS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling",
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Models with Anisotropic Gaussian Splatting for Image Inpainting",
    "authors": [
      "Jacob Fein-Ashley",
      "Benjamin Fein-Ashley"
    ],
    "abstract": "Image inpainting is a fundamental task in computer vision, aiming to restore missing or corrupted regions in images realistically. While recent deep learning approaches have significantly advanced the state-of-the-art, challenges remain in maintaining structural continuity and generating coherent textures, particularly in large missing areas. Diffusion models have shown promise in generating high-fidelity images but often lack the structural guidance necessary for realistic inpainting. We propose a novel inpainting method that combines diffusion models with anisotropic Gaussian splatting to capture both local structures and global context effectively. By modeling missing regions using anisotropic Gaussian functions that adapt to local image gradients, our approach provides structural guidance to the diffusion-based inpainting network. The Gaussian splat maps are integrated into the diffusion process, enhancing the model's ability to generate high-fidelity and structurally coherent inpainting results. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques, producing visually plausible results with enhanced structural integrity and texture realism.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "high-fidelity"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2D Triangle Splatting for Direct Differentiable Mesh Training",
    "authors": [
      "Kaifeng Sheng",
      "Zheng Zhou",
      "Yingliang Peng",
      "Qianwei Wang"
    ],
    "abstract": "Differentiable rendering with 3D Gaussian primitives has emerged as a powerful method for reconstructing high-fidelity 3D scenes from multi-view images. While it offers improvements over NeRF-based methods, this representation still encounters challenges with rendering speed and advanced rendering effects, such as relighting and shadow rendering, compared to mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a novel method that replaces 3D Gaussian primitives with 2D triangle facelets. This representation naturally forms a discrete mesh-like structure while retaining the benefits of continuous volumetric modeling. By incorporating a compactness parameter into the triangle primitives, we enable direct training of photorealistic meshes. Our experimental results demonstrate that our triangle-based method, in its vanilla version (without compactness tuning), achieves higher fidelity compared to state-of-the-art Gaussian-based methods. Furthermore, our approach produces reconstructed meshes with superior visual quality compared to existing mesh reconstruction methods. Please visit our project page at https://gaoderender.github.io/triangle-splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "high-fidelity",
      "compact",
      "lighting",
      "ar",
      "nerf",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Lucas-Kanade",
    "authors": [
      "Liuyue Xie",
      "Joel Julin",
      "Koichiro Niinuma",
      "Laszlo A. Jeni"
    ],
    "abstract": "Gaussian Splatting and its dynamic extensions are effective for reconstructing 3D scenes from 2D images when there is significant camera movement to facilitate motion parallax and when scene objects remain relatively static. However, in many real-world scenarios, these conditions are not met. As a consequence, data-driven semantic and geometric priors have been favored as regularizers, despite their bias toward training data and their neglect of broader movement dynamics.   Departing from this practice, we propose a novel analytical approach that adapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By leveraging the intrinsic properties of the forward warp field network, we derive an analytical velocity field that, through time integration, facilitates accurate scene flow computation. This enables the precise enforcement of motion constraints on warp fields, thus constraining both 2D motion and 3D positions of the Gaussians. Our method excels in reconstructing highly dynamic scenes with minimal camera movement, as demonstrated through experiments on both synthetic and real-world scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Shuangyu Xie",
      "Chung Min Kim",
      "Justin Kerr",
      "Bart M. van Marrewijk",
      "Mieke van Vlaardingen",
      "Tim van Daalen",
      "E. N. van Loo",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Rick van de Zedde",
      "Ken Goldberg"
    ],
    "abstract": "Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-16",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis",
    "authors": [
      "Zhicheng Lu",
      "Xiang Guo",
      "Le Hui",
      "Tianrui Chen",
      "Min Yang",
      "Xiao Tang",
      "Feng Zhu",
      "Yuchao Dai"
    ],
    "abstract": "In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at https://npucvr.github.io/GaGS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "vr",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SEGS-SLAM: Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding",
    "authors": [
      "Tianci Wen",
      "Zhiang Liu",
      "Yongchun Fang"
    ],
    "abstract": "3D Gaussian splatting (3D-GS) has recently revolutionized novel view synthesis in the simultaneous localization and mapping (SLAM) problem. However, most existing algorithms fail to fully capture the underlying structure, resulting in structural inconsistency. Additionally, they struggle with abrupt appearance variations, leading to inconsistent visual quality. To address these problems, we propose SEGS-SLAM, a structure-enhanced 3D Gaussian Splatting SLAM, which achieves high-quality photorealistic mapping. Our main contributions are two-fold. First, we propose a structure-enhanced photorealistic mapping (SEPM) framework that, for the first time, leverages highly structured point cloud to initialize structured 3D Gaussians, leading to significant improvements in rendering quality. Second, we propose Appearance-from-Motion embedding (AfME), enabling 3D Gaussians to better model image appearance variations across different camera poses. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that SEGS-SLAM significantly outperforms state-of-the-art (SOTA) methods in photorealistic mapping quality, e.g., an improvement of $19.86\\%$ in PSNR over MonoGS on the TUM RGB-D dataset for monocular cameras. The project page is available at https://segs-slam.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "slam",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LI-GS: Gaussian Splatting with LiDAR Incorporated for Accurate Large-Scale Reconstruction",
    "authors": [
      "Changjian Jiang",
      "Ruilan Gao",
      "Kele Shao",
      "Yue Wang",
      "Rong Xiong",
      "Yu Zhang"
    ],
    "abstract": "Large-scale 3D reconstruction is critical in the field of robotics, and the potential of 3D Gaussian Splatting (3DGS) for achieving accurate object-level reconstruction has been demonstrated. However, ensuring geometric accuracy in outdoor and unbounded scenes remains a significant challenge. This study introduces LI-GS, a reconstruction system that incorporates LiDAR and Gaussian Splatting to enhance geometric accuracy in large-scale scenes. 2D Gaussain surfels are employed as the map representation to enhance surface alignment. Additionally, a novel modeling method is proposed to convert LiDAR point clouds to plane-constrained multimodal Gaussian Mixture Models (GMMs). The GMMs are utilized during both initialization and optimization stages to ensure sufficient and continuous supervision over the entire scene while mitigating the risk of over-fitting. Furthermore, GMMs are employed in mesh extraction to eliminate artifacts and improve the overall geometric quality. Experiments demonstrate that our method outperforms state-of-the-art methods in large-scale 3D reconstruction, achieving higher accuracy compared to both LiDAR-based methods and Gaussian-based methods with improvements of 52.6% and 68.7%, respectively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure",
    "authors": [
      "Ziheng Xu",
      "Qingfeng Li",
      "Chen Chen",
      "Xuefeng Liu",
      "Jianwei Niu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its application in dense Simultaneous Localization and Mapping (SLAM), enabling real-time rendering and high-fidelity mapping. However, existing 3DGS-based SLAM methods often suffer from accumulated tracking errors and map drift, particularly in large-scale environments. To address these issues, we introduce GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization of camera poses and scene models. Our approach employs frame-to-model tracking and triggers hierarchical loop closure using a global-to-local strategy to minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we facilitate efficient map updates following loop corrections in large scenes. Additionally, our uncertainty-minimized keyframe selection strategy prioritizes keyframes observing more valuable 3D Gaussians to enhance submap optimization. Experimental results on various datasets demonstrate that GLC-SLAM achieves superior or competitive tracking and mapping performance compared to state-of-the-art dense RGB-D SLAM systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "large scene",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting",
    "authors": [
      "Mateusz Nowak",
      "Wojciech Jarosz",
      "Peter Chin"
    ],
    "abstract": "Reconstructing a 3D scene from images is challenging due to the different ways light interacts with surfaces depending on the viewer's position and the surface's material. In classical computer graphics, materials can be classified as diffuse or specular, interacting with light differently. The standard 3D Gaussian Splatting model struggles to represent view-dependent content, since it cannot differentiate an object within the scene from the light interacting with its specular surfaces, which produce highlights or reflections. In this paper, we propose to extend the 3D Gaussian Splatting model by introducing an additional symmetric matrix to enhance the opacity representation of each 3D Gaussian. This improvement allows certain Gaussians to be suppressed based on the viewer's perspective, resulting in a more accurate representation of view-dependent reflections and specular highlights without compromising the scene's integrity. By allowing the opacity to be view dependent, our enhanced model achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep Blending, and Nerf-Synthetic datasets without a significant loss in rendering speed, achieving >60FPS, and only incurring a minimal increase in memory used.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy",
    "authors": [
      "Letian Huang",
      "Jie Guo",
      "Jialin Dan",
      "Ruoyu Fu",
      "Shujie Wang",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has achieved impressive results in novel view synthesis, demonstrating high fidelity and efficiency. However, it easily exhibits needle-like artifacts, especially when increasing the sampling rate. Mip-Splatting tries to remove these artifacts with a 3D smoothing filter for frequency constraints and a 2D Mip filter for approximated supersampling. Unfortunately, it tends to produce over-blurred results, and sometimes needle-like Gaussians still persist. Our spectral analysis of the covariance matrix during optimization and densification reveals that current 3D-GS lacks shape awareness, relying instead on spectral radius and view positional gradients to determine splitting. As a result, needle-like Gaussians with small positional gradients and low spectral entropy fail to split and overfit high-frequency details. Furthermore, both the filters used in 3D-GS and Mip-Splatting reduce the spectral entropy and increase the condition number during zooming in to synthesize novel view, causing view inconsistencies and more pronounced artifacts. Our Spectral-GS, based on spectral analysis, introduces 3D shape-aware splitting and 2D view-consistent filtering strategies, effectively addressing these issues, enhancing 3D-GS's capability to represent high-frequency details without noticeable artifacts, and achieving high-quality photorealistic rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
    "authors": [
      "Ye Tao",
      "Jiawei Zhang",
      "Yahao Shi",
      "Dongqing Zou",
      "Bin Zhou"
    ],
    "abstract": "Image-based 3D generation has vast applications in robotics and gaming, where high-quality, diverse outputs and consistent 3D representations are crucial. However, existing methods have limitations: 3D diffusion models are limited by dataset scarcity and the absence of strong pre-trained priors, while 2D diffusion-based approaches struggle with geometric consistency. We propose a method that leverages 2D diffusion models' implicit 3D reasoning ability while ensuring 3D consistency via Gaussian-splatting-based geometric distillation. Specifically, the proposed Gaussian Splatting Decoder enforces 3D consistency by transforming SV3D latent outputs into an explicit 3D representation. Unlike SV3D, which only relies on implicit 2D representations for video generation, Gaussian Splatting explicitly encodes spatial and appearance attributes, enabling multi-view consistency through geometric constraints. These constraints correct view inconsistencies, ensuring robust geometric consistency. As a result, our approach simultaneously generates high-quality, multi-view-consistent images and accurate 3D models, providing a scalable solution for single-image-based 3D generation and bridging the gap between 2D Diffusion diversity and 3D structural coherence. Experimental results demonstrate state-of-the-art multi-view consistency and strong generalization across diverse datasets. The code will be made publicly available upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models",
    "authors": [
      "Zhibin Liu",
      "Haoye Dong",
      "Aviral Chharia",
      "Hefeng Wu"
    ],
    "abstract": "Generating lifelike 3D humans from a single RGB image remains a challenging task in computer vision, as it requires accurate modeling of geometry, high-quality texture, and plausible unseen parts. Existing methods typically use multi-view diffusion models for 3D generation, but they often face inconsistent view issues, which hinder high-quality 3D human generation. To address this, we propose Human-VDM, a novel method for generating 3D human from a single RGB image using Video Diffusion Models. Human-VDM provides temporally consistent views for 3D human generation using Gaussian Splatting. It consists of three modules: a view-consistent human video diffusion module, a video augmentation module, and a Gaussian Splatting module. First, a single image is fed into a human video diffusion module to generate a coherent human video. Next, the video augmentation module applies super-resolution and video interpolation to enhance the textures and geometric smoothness of the generated video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans under the guidance of these high-resolution and view-consistent images. Experiments demonstrate that Human-VDM achieves high-quality 3D human from a single image, outperforming state-of-the-art methods in both generation quality and quantity. Project page: https://human-vdm.github.io/Human-VDM/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-04",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields",
    "authors": [
      "Yuanhao Gong"
    ],
    "abstract": "The 3D Gaussian splatting methods are getting popular. However, they work directly on the signal, leading to a dense representation of the signal. Even with some techniques such as pruning or distillation, the results are still dense. In this paper, we propose to model the gradient of the original signal. The gradients are much sparser than the original signal. Therefore, the gradients use much less Gaussian splats, leading to the more efficient storage and thus higher computational performance during both training and rendering. Thanks to the sparsity, during the view synthesis, only a small mount of pixels are needed, leading to much higher computational performance ($100\\sim 1000\\times$ faster). And the 2D image can be recovered from the gradients via solving a Poisson equation with linear computation complexity. Several experiments are performed to confirm the sparseness of the gradients and the computation performance of the proposed method. The method can be applied various applications, such as human body modeling and indoor environment modeling.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "human",
      "body",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction",
    "authors": [
      "Haoran Wang",
      "Nantheera Anantrasirichai",
      "Fan Zhang",
      "David Bull"
    ],
    "abstract": "3D Gaussian splatting (3DGS) offers the capability to achieve real-time high quality 3D scene rendering. However, 3DGS assumes that the scene is in a clear medium environment and struggles to generate satisfactory representations in underwater scenes, where light absorption and scattering are prevalent and moving objects are involved. To overcome these, we introduce a novel Gaussian Splatting-based method, UW-GS, designed specifically for underwater applications. It introduces a color appearance that models distance-dependent color variation, employs a new physics-based density control strategy to enhance clarity for distant objects, and uses a binary motion mask to handle dynamic content. Optimized with a well-designed loss function supporting for scattering media and strengthened by pseudo-depth maps, UW-GS outperforms existing methods with PSNR gains up to 1.26dB. To fully verify the effectiveness of the model, we also developed a new underwater dataset, S-UW, with dynamic object masks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction",
    "authors": [
      "Xuesong Li",
      "Jinguang Tong",
      "Jie Hong",
      "Vivien Rolland",
      "Lars Petersson"
    ],
    "abstract": "Dynamic scene reconstruction from monocular video is essential for real-world applications. We introduce DGNS, a hybrid framework integrating \\underline{D}eformable \\underline{G}aussian Splatting and Dynamic \\underline{N}eural \\underline{S}urfaces, effectively addressing dynamic novel-view synthesis and 3D geometry reconstruction simultaneously. During training, depth maps generated by the deformable Gaussian splatting module guide the ray sampling for faster processing and provide depth supervision within the dynamic neural surface module to improve geometry reconstruction. Conversely, the dynamic neural surface directs the distribution of Gaussian primitives around the surface, enhancing rendering quality. In addition, we propose a depth-filtering approach to further refine depth supervision. Extensive experiments conducted on public datasets demonstrate that DGNS achieves state-of-the-art performance in 3D reconstruction, along with competitive results in novel-view synthesis.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR",
    "authors": [
      "Christophe Bolduc",
      "Yannick Hold-Geoffroy",
      "Zhixin Shu",
      "Jean-François Lalonde"
    ],
    "abstract": "We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: https://lvsn.github.io/gaslight/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Volumetrically Consistent 3D Gaussian Rasterization",
    "authors": [
      "Chinmay Talegaonkar",
      "Yash Belhe",
      "Ravi Ramamoorthi",
      "Nicholas Antipa"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds. However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy. We show that the core approximations in splatting are unnecessary, even within a rasterizer; We instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically. We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS. This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points. Our code is publicly available at: https://github.com/chinmay0301ucsd/Vol3DGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image",
    "authors": [
      "Chen Guo",
      "Zhuo Su",
      "Jian Wang",
      "Shuang Li",
      "Xu Chang",
      "Zhaohu Li",
      "Yang Zhao",
      "Guidong Wang",
      "Ruqi Huang"
    ],
    "abstract": "Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-19",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "animation",
      "neural rendering",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time Sparse-View 3D Rendering",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Yu Lin",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leverage monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information to regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world application.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition",
    "authors": [
      "Aggelina Chatziagapi",
      "Grigorios G. Chrysos",
      "Dimitris Samaras"
    ],
    "abstract": "We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that learns a single neural representation for multiple identities, using only monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human avatars require per-identity optimization. However, learning a multi-identity representation presents advantages in robustly animating humans under arbitrary poses. We propose to construct a high-order tensor that combines all the learnable 3DGS parameters for all the training identities. By assuming a low-rank structure and factorizing the tensor, we model the complex rigid and non-rigid deformations of multiple subjects in a unified network, significantly reducing the total number of parameters. Our proposed approach leverages information from all the training identities and enables robust animation under challenging unseen poses, outperforming existing approaches. It can also be extended to learn unseen identities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "animation",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
    "authors": [
      "Yuxuan Xie",
      "Xuan Yu",
      "Changjian Jiang",
      "Sitong Mao",
      "Shunbo Zhou",
      "Rui Fan",
      "Rong Xiong",
      "Yue Wang"
    ],
    "abstract": "Open-vocabulary panoptic reconstruction is a challenging task for simultaneous scene reconstruction and understanding. Recently, methods have been proposed for 3D scene understanding based on Gaussian splatting. However, these methods are multi-staged, suffering from the accumulated errors and the dependence of hand-designed components. To streamline the pipeline and achieve global optimization, we propose PanopticSplatting, an end-to-end system for open-vocabulary panoptic reconstruction. Our method introduces query-guided Gaussian segmentation with local cross attention, lifting 2D instance masks without cross-frame association in an end-to-end way. The local cross attention within view frustum effectively reduces the training memory, making our model more accessible to large scenes with more Gaussians and objects. In addition, to address the challenge of noisy labels in 2D pseudo masks, we propose label blending to promote consistent 3D segmentation with less noisy floaters, as well as label warping on 2D predictions which enhances multi-view coherence and segmentation accuracy. Our method demonstrates strong performances in 3D scene panoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with both NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover, PanopticSplatting can be easily generalized to numerous variants of Gaussian splatting, and we demonstrate its robustness on different Gaussian base models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "large scene",
      "ar",
      "gaussian splatting",
      "nerf",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving",
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Zhuoyang Sun",
      "Yingjie Victor Chen",
      "Mei Chen"
    ],
    "abstract": "Most existing Dynamic Gaussian Splatting methods for complex dynamic urban scenarios rely on accurate object-level supervision from expensive manual labeling, limiting their scalability in real-world applications. In this paper, we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without requiring tracked 3D bounding boxes, enabling accurate dynamic scene reconstruction and novel view RGB/depth/flow synthesis. SplatFlow designs a unified framework to seamlessly integrate time-dependent 4D Gaussian representation within NMFF, where NMFF is a set of implicit functions to model temporal motions of both LiDAR points and Gaussians as continuous motion flow fields. Leveraging NMFF, SplatFlow effectively decomposes static background and dynamic objects, representing them with 3D and 4D Gaussian primitives, respectively. NMFF also models the correspondences of each 4D Gaussian across time, which aggregates temporal features to enhance cross-view consistency of dynamic components. SplatFlow further improves dynamic object identification by distilling features from 2D foundation models into 4D space-time representation. Comprehensive evaluations conducted on the Waymo and KITTI Datasets validate SplatFlow's state-of-the-art (SOTA) performance for both image reconstruction and novel view synthesis in dynamic urban scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Hanxiao Sun",
      "YuPeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ],
    "abstract": "Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameters and normal, alongside the absence of physical constraints for indirect lighting. In this paper, we present a novel framework called Spatially-vayring Gaussian Inverse Rendering (SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we propose a new representation-Spatially-varying Gaussian (SVG)-that allows per-Gaussian spatially varying parameters. This enhanced representation is complemented by a SVG splatting scheme akin to vertex/fragment shading in traditional graphics pipelines. Furthermore, we integrate a physically-based indirect lighting model, enabling more realistic relighting. The proposed SVG-IR framework significantly improves rendering quality, outperforming state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio (PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in relighting tasks, all while maintaining a real-time rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction",
    "authors": [
      "Bo Jia",
      "Yanan Guo",
      "Ying Chang",
      "Benkui Zhang",
      "Ying Xie",
      "Kangning Du",
      "Lin Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS. Our code will be made publicly available at (https://github.com/Bistu3DV/MND-GS/).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction",
    "authors": [
      "Weihao Yu",
      "Yuanhao Cai",
      "Ruyi Zha",
      "Zhiwen Fan",
      "Chenxin Li",
      "Yixuan Yuan"
    ],
    "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Code is publicly available at: https://x2-gaussian.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "face",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining",
    "authors": [
      "Qi Ma",
      "Yue Li",
      "Bin Ren",
      "Nicu Sebe",
      "Ender Konukoglu",
      "Theo Gevers",
      "Luc Van Gool",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build ShapeSplat, a large-scale dataset of 3DGS using the commonly used ShapeNet, ModelNet and Objaverse datasets. Our dataset ShapeSplat consists of 206K objects spanning over 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 3.8 GPU years on a TITAN XP GPU.   We utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce Gaussian-MAE, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Proxy-GS: Efficient 3D Gaussian Splatting via Proxy Mesh",
    "authors": [
      "Yuanyuan Gao",
      "Yuning Gong",
      "Yifei Liu",
      "Li Jingfeng",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Yanci Zhang",
      "Dan Xu",
      "Xiao Sun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives",
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Geng Lin",
      "Vasu Singla",
      "Matthias Zwicker",
      "Tom Goldstein"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS to substantially improve rendering speed. These improvements also yield the ancillary benefits of reduced model size and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\\mathit{6.71\\times}$ across scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting",
    "authors": [
      "Lingzhe Zhao",
      "Peng Wang",
      "Peidong Liu"
    ],
    "abstract": "While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at https://lingzhezhao.github.io/BAD-Gaussians/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set",
    "authors": [
      "Wenyuan Zhang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "It is vital to infer a signed distance function (SDF) in multi-view based surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel perspective for volume rendering, and shows advantages in rendering efficiency and quality. Although 3DGS provides a promising neural rendering option, it is still hard to infer SDFs for surface reconstruction with 3DGS due to the discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To resolve these issues, we propose a method that seamlessly merge 3DGS with the learning of neural SDFs. Our key idea is to more effectively constrain the SDF inference with the multi-view consistency. To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF using neural pulling, and then render the aligned 3D Gaussians through the differentiable rasterization. Meanwhile, we update the neural SDF by pulling neighboring space to the pulled 3D Gaussians, which progressively refine the signed distance field near the surface. With both differentiable pulling and splatting, we jointly optimize 3D Gaussians and the neural SDF with both RGB and geometry constraints, which recovers more accurate, smooth, and complete surfaces with more geometry details. Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Juan Zhang",
      "Tongfei Chen",
      "Shuo Yang",
      "Shuwei Shao",
      "Wenhao Dong",
      "Baochang Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting",
    "authors": [
      "Francesco Palandra",
      "Andrea Sanchietti",
      "Daniele Baieri",
      "Emanuele Rodolà"
    ],
    "abstract": "We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model. The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-08",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation",
    "authors": [
      "Sitian Shen",
      "Jing Xu",
      "Yuheng Yuan",
      "Xingyi Yang",
      "Qiuhong Shen",
      "Xinchao Wang"
    ],
    "abstract": "User-friendly 3D object editing is a challenging task that has attracted significant attention recently. The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing. While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing. In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks. Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input. This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing. Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50$\\times$ faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
    "authors": [
      "Yiwei Xu",
      "Yifei Yu",
      "Wentian Gan",
      "Tengfei Wang",
      "Zongqian Zhan",
      "Hao Cheng",
      "Xin Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast real-time performance, but existing methods rely on offline training after full Structure-from-Motion (SfM) processing. In contrast, this work introduces Gaussian on-the-fly Splatting (abbreviated as On-the-Fly GS), a progressive framework enabling near real-time 3DGS optimization during image capture. As each image arrives, its pose and sparse points are updated via On-the-Fly SfM, and newly optimized Gaussians are immediately integrated into the 3DGS field. To achieve this, we propose a progressive Local & Semi-Global optimization to prioritize the new image and its neighbors by their corresponding overlapping relationship, allowing the new image and its overlapping images to get more training. To further stabilize training across previous and new images, an adaptive learning rate schedule balances the iterations and the learning rate. Extensive experiments on multiple benchmarks show that our On-the-Fly GS reduces training time significantly, optimizing each new image in seconds with minimal rendering loss, offering one of the first practical steps toward rapid, progressive 3DGS reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting",
    "authors": [
      "Hyunwoo Park",
      "Gun Ryu",
      "Wonjun Kim"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in the field of novel view synthesis due to its fast performance while yielding the excellent image quality. However, 3DGS in sparse-view settings (e.g., three-view inputs) often faces with the problem of overfitting to training views, which significantly drops the visual quality of novel view images. Many existing approaches have tackled this issue by using strong priors, such as 2D generative contextual information and external depth signals. In contrast, this paper introduces a prior-free method, so-called DropGaussian, with simple changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians during the training process in a similar way of dropout, which allows non-excluded Gaussians to have larger gradients while improving their visibility. This makes the remaining Gaussians to contribute more to the optimization process for rendering with sparse input views. Such simple operation effectively alleviates the overfitting problem and enhances the quality of novel view synthesis. By simply applying DropGaussian to the original 3DGS framework, we can achieve the competitive performance with existing prior-based 3DGS methods in sparse-view settings of benchmark datasets without any additional complexity. The code and model are publicly available at: https://github.com/DCVL-3D/DropGaussian release.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing",
    "authors": [
      "Zhenya Yang",
      "Bingchen Gong",
      "Kai Chen"
    ],
    "abstract": "Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction",
    "authors": [
      "Stanislaw Szymanowicz",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "We introduce the \\method, an ultra-efficient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, maps the input image to one 3D Gaussian per pixel. The resulting set of Gaussians thus has the form an image, the Splatter Image. We further extend the method take several images as input via cross-view attention. Owning to the speed of the renderer (588 FPS), we use a single GPU for training while generating entire images at each iteration to optimize perceptual metrics like LPIPS. On several synthetic, real, multi-category and large-scale benchmark datasets, we achieve better results in terms of PSNR, LPIPS, and other metrics while training and evaluating much faster than prior works. Code, models, demo and more results are available at https://szymanowiczs.github.io/splatter-image.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PCGS: Progressive Compression of 3D Gaussian Splatting",
    "authors": [
      "Yihang Chen",
      "Mengyao Li",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves impressive rendering fidelity and speed for novel view synthesis. However, its substantial data size poses a significant challenge for practical applications. While many compression techniques have been proposed, they fail to efficiently utilize existing bitstreams in on-demand applications due to their lack of progressivity, leading to a waste of resource. To address this issue, we propose PCGS (Progressive Compression of 3D Gaussian Splatting), which adaptively controls both the quantity and quality of Gaussians (or anchors) to enable effective progressivity for on-demand applications. Specifically, for quantity, we introduce a progressive masking strategy that incrementally incorporates new anchors while refining existing ones to enhance fidelity. For quality, we propose a progressive quantization approach that gradually reduces quantization step sizes to achieve finer modeling of Gaussian attributes. Furthermore, to compact the incremental bitstreams, we leverage existing quantization results to refine probability prediction, improving entropy coding efficiency across progressive levels. Overall, PCGS achieves progressivity while maintaining compression performance comparable to SoTA non-progressive methods. Code available at: github.com/YihangChen-ee/PCGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeblurGS: Gaussian Splatting for Camera Motion Blur",
    "authors": [
      "Jeongtaek Oh",
      "Jaeyoung Chung",
      "Dongwoo Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Inverse Rendering with Approximated Global Illumination",
    "authors": [
      "Zirui Wu",
      "Jianteng Chen",
      "Laijian Li",
      "Shaoteng Wu",
      "Zhikai Zhu",
      "Kang Xu",
      "Martin R. Oswald",
      "Jie Song"
    ],
    "abstract": "3D Gaussian Splatting shows great potential in reconstructing photo-realistic 3D scenes. However, these methods typically bake illumination into their representations, limiting their use for physically-based rendering and scene editing. Although recent inverse rendering approaches aim to decompose scenes into material and lighting components, they often rely on simplifying assumptions that fail when editing. We present a novel approach that enables efficient global illumination for 3D Gaussians Splatting through screen-space ray tracing. Our key insight is that a substantial amount of indirect light can be traced back to surfaces visible within the current view frustum. Leveraging this observation, we augment the direct shading computed by 3D Gaussians with Monte-Carlo screen-space ray-tracing to capture one-bounce indirect illumination. In this way, our method enables realistic global illumination without sacrificing the computational efficiency and editability benefits of 3D Gaussians. Through experiments, we show that the screen-space approximation we utilize allows for indirect illumination and supports real-time rendering and editing. Code, data, and models will be made available at our project page: https://wuzirui.github.io/gs-ssr.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "illumination",
      "lighting",
      "global illumination",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
    "authors": [
      "Wending Liu",
      "Siyun Liang",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "abstract": "We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-15",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Triangle Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Adrien Deliege",
      "Abdullah Hamdi",
      "Silvio Giancola",
      "Anthony Cioppa",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Andrea Tagliasacchi",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for a triangle comeback. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible with standard graphics stacks and GPU hardware, and highly efficient: for the \\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks. The project page is https://trianglesplatting.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Masked Autoencoders",
    "authors": [
      "Jathushan Rajasegaran",
      "Xinlei Chen",
      "Rulilong Li",
      "Christoph Feichtenhofer",
      "Jitendra Malik",
      "Shiry Ginosar"
    ],
    "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation",
    "authors": [
      "Xiaoliang Ju",
      "Hongsheng Li"
    ],
    "abstract": "We present DirectTriGS, a novel framework designed for 3D object generation with Gaussian Splatting (GS). GS-based rendering for 3D content has gained considerable attention recently. However, there has been limited exploration in directly generating 3D Gaussians compared to traditional generative modeling approaches. The main challenge lies in the complex data structure of GS represented by discrete point clouds with multiple channels. To overcome this challenge, we propose employing the triplane representation, which allows us to represent Gaussian Splatting as an image-like continuous field. This representation effectively encodes both the geometry and texture information, enabling smooth transformation back to Gaussian point clouds and rendering into images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is fully differentiable, so that the rendering loss can supervise both texture and geometry encoding. Furthermore, the triplane representation can be compressed using a Variational Autoencoder (VAE), which can subsequently be utilized in latent diffusion to generate 3D objects. The experiments demonstrate that the proposed generation framework can produce high-quality 3D object geometry and rendering results in the text-to-3D task.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tetrahedron Splatting for 3D Generation",
    "authors": [
      "Chun Gu",
      "Zeyu Yang",
      "Zijie Pan",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "abstract": "3D representation is essential to the significant advance of 3D generation with 2D diffusion priors. As a flexible representation, NeRF has been first adopted for 3D representation. With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges. Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction. In this work, we introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering simultaneously. This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve generation quality and stability. Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge. Our TeT-Splatting can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization. Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PUGS: Zero-shot Physical Understanding with Gaussian Splatting",
    "authors": [
      "Yinghao Shuai",
      "Ran Yu",
      "Yuantao Chen",
      "Zijian Jiang",
      "Xiaowei Song",
      "Nan Wang",
      "Jv Zheng",
      "Jianzhu Ma",
      "Meng Yang",
      "Zhicheng Wang",
      "Wenbo Ding",
      "Hao Zhao"
    ],
    "abstract": "Current robotic systems can understand the categories and poses of objects well. But understanding physical properties like mass, friction, and hardness, in the wild, remains challenging. We propose a new method that reconstructs 3D objects using the Gaussian splatting representation and predicts various physical properties in a zero-shot manner. We propose two techniques during the reconstruction phase: a geometry-aware regularization loss function to improve the shape quality and a region-aware feature contrastive loss function to promote region affinity. Two other new techniques are designed during inference: a feature-based property propagation module and a volume integration module tailored for the Gaussian representation. Our framework is named as zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS achieves new state-of-the-art results on the standard benchmark of ABO-500 mass prediction. We provide extensive quantitative ablations and qualitative visualization to demonstrate the mechanism of our designs. We show the proposed methodology can help address challenging real-world grasping tasks. Our codes, data, and models are available at https://github.com/EverNorif/PUGS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos",
    "authors": [
      "Junyi Wu",
      "Jiachen Tao",
      "Haoxuan Wang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ],
    "abstract": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework",
    "authors": [
      "Yi-Ting Chen",
      "Ting-Hsuan Liao",
      "Pengsheng Guo",
      "Alexander Schwing",
      "Jia-Bin Huang"
    ],
    "abstract": "We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based super-resolution framework that leverages off-the-shelf diffusion-based 2D super-resolution models. 3DSR encourages 3D consistency across views via the use of an explicit 3D Gaussian-splatting-based scene representation. This makes the proposed 3DSR different from prior work, such as image upsampling or the use of video super-resolution, which either don't consider 3D consistency or aim to incorporate 3D consistency implicitly. Notably, our method enhances visual quality without additional fine-tuning, ensuring spatial coherence within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data, demonstrating that it produces high-resolution results that are visually compelling, while maintaining structural consistency in 3D reconstructions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d gaussian",
      "3d reconstruction",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction",
    "authors": [
      "Kai Xu",
      "Tze Ho Elden Tse",
      "Jizong Peng",
      "Angela Yao"
    ],
    "abstract": "We propose a novel framework for scene decomposition and static background reconstruction from everyday videos. By integrating the trained motion masks and modeling the static scene as Gaussian splats with dynamics-aware optimization, our method achieves more accurate background reconstruction results than previous works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods, DAS3R is more robust in complex motion scenarios, capable of handling videos where dynamic objects occupy a significant portion of the scene, and does not require camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates enhanced performance and robustness with a margin of more than 2 dB in PSNR. The project's webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting",
    "authors": [
      "Karly Hou",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Recently, Gaussian Splatting methods have emerged as a desirable substitute for prior Radiance Field methods for novel-view synthesis of scenes captured with multi-view images or videos. In this work, we propose a novel extension to 4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual learning, we hierarchically decompose the dynamic scene into a \"video-segment-frame\" structure, with segments dynamically adjusted by optical flow. Then, instead of directly predicting the time-dependent signals, we model the signal as the sum of video-constant values, segment-constant values, and frame-specific residuals, as inspired by the success of residual learning. This approach allows more flexible models that adapt to highly variable scenes. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets, with the greatest improvements on complex scenes with large movements, occlusions, and fine details, where current methods degrade most.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation",
    "authors": [
      "Xu Tang",
      "Junan Jia",
      "Yijing Wang",
      "Jingjing Ma",
      "Xiangrong Zhang"
    ],
    "abstract": "In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS), traditional methods struggle to address semantic ambiguity caused by scale variations and structural occlusions in aerial images. This limits their segmentation accuracy and consistency. To tackle these challenges, we propose a novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian point drop module, which integrates semantic confidence estimation with a learnable sparsity mechanism based on the Hard Concrete distribution. This module effectively eliminates redundant and semantically ambiguous Gaussian points, enhancing both segmentation performance and representation compactness. Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation pipeline. It leverages 2D foundation models to enhance supervision when ground-truth labels are limited, thereby further improving segmentation accuracy. To advance research in this domain, we introduce a challenging benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse real-world aerial scenes with sparse annotations. Experimental results demonstrate that SAD-Splat achieves an excellent balance between segmentation accuracy and representation compactness. It offers an efficient and scalable solution for 3D aerial scene understanding.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "semantic",
      "compact",
      "ar",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting",
    "authors": [
      "Xiaobao Wei",
      "Qingpo Wuwu",
      "Zhongyu Zhao",
      "Zhuangzhe Wu",
      "Nan Huang",
      "Ming Lu",
      "Ningning MA",
      "Shanghang Zhang"
    ],
    "abstract": "Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed plug-and-play EMD module compensates for the lack of motion modeling in self-supervised street Gaussian splatting methods. We also introduce tailored training strategies to extend EMD to supervised approaches. Comprehensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art novel view synthesis performance in self-supervised settings. The code is available at: https://qingpowuwu.github.io/emd.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments",
    "authors": [
      "Guile Wu",
      "Dongfeng Bai",
      "Bingbing Liu"
    ],
    "abstract": "This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "urban scene",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust LiDAR-Camera Calibration with 2D Gaussian Splatting",
    "authors": [
      "Shuyi Zhou",
      "Shuxiang Xie",
      "Ryoichi Ishikawa",
      "Takeshi Oishi"
    ],
    "abstract": "LiDAR-camera systems have become increasingly popular in robotics recently. A critical and initial step in integrating the LiDAR and camera data is the calibration of the LiDAR-camera system. Most existing calibration methods rely on auxiliary target objects, which often involve complex manual operations, whereas targetless methods have yet to achieve practical effectiveness. Recognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric information from camera image sequences, we propose a calibration method that estimates LiDAR-camera extrinsic parameters using geometric constraints. The proposed method begins by reconstructing colorless 2DGS using LiDAR point clouds. Subsequently, we update the colors of the Gaussian splats by minimizing the photometric loss. The extrinsic parameters are optimized during this process. Additionally, we address the limitations of the photometric loss by incorporating the reprojection and triangulation losses, thereby enhancing the calibration robustness and accuracy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-01",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration",
    "authors": [
      "Quentin Herau",
      "Moussab Bennehar",
      "Arthur Moreau",
      "Nathan Piasco",
      "Luis Roldao",
      "Dzmitry Tsishkou",
      "Cyrille Migniot",
      "Pascal Vasseur",
      "Cédric Demonceaux"
    ],
    "abstract": "Reliable multimodal sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-EVT: Cross-Modal Event Camera Tracking based on Gaussian Splatting",
    "authors": [
      "Tao Liu",
      "Runze Yuan",
      "Yi'ang Ju",
      "Xun Xu",
      "Jiaqi Yang",
      "Xiangting Meng",
      "Xavier Lagorce",
      "Laurent Kneip"
    ],
    "abstract": "Reliable self-localization is a foundational skill for many intelligent mobile platforms. This paper explores the use of event cameras for motion tracking thereby providing a solution with inherent robustness under difficult dynamics and illumination. In order to circumvent the challenge of event camera-based mapping, the solution is framed in a cross-modal way. It tracks a map representation that comes directly from frame-based cameras. Specifically, the proposed method operates on top of gaussian splatting, a state-of-the-art representation that permits highly efficient and realistic novel view synthesis. The key of our approach consists of a novel pose parametrization that uses a reference pose plus first order dynamics for local differential image rendering. The latter is then compared against images of integrated events in a staggered coarse-to-fine optimization scheme. As demonstrated by our results, the realistic view rendering ability of gaussian splatting leads to stable and accurate tracking across a variety of both publicly available and newly recorded data sequences.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "localization",
      "tracking",
      "dynamic",
      "motion",
      "gaussian splatting",
      "mapping",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sort-free Gaussian Splatting via Weighted Sum Rendering",
    "authors": [
      "Qiqi Hou",
      "Randall Rauwendaal",
      "Zifeng Li",
      "Hoang Le",
      "Farzad Farhadzadeh",
      "Fatih Porikli",
      "Alexei Bourd",
      "Amir Said"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering performance is constrained by its dependence on costly non-commutative alpha-blending operations. These operations mandate complex view dependent sorting operations that introduce computational overhead, especially on the resource-constrained platforms such as mobile phones. In this paper, we propose Weighted Sum Rendering, which approximates alpha blending with weighted sums, thereby removing the need for sorting. This simplifies implementation, delivers superior performance, and eliminates the \"popping\" artifacts caused by sorting. Experimental results show that optimizing a generalized Gaussian splatting formulation to the new differentiable rendering yields competitive image quality. The method was implemented and tested in a mobile device GPU, achieving on average $1.23\\times$ faster rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
    "authors": [
      "Jing Yang",
      "Yufeng Yang"
    ],
    "abstract": "Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Adaptive Density Control for 3D Gaussian Splatting",
    "authors": [
      "Glenn Grubert",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become one of the most influential works in the past year. Due to its efficient and high-quality novel view synthesis capabilities, it has been widely adopted in many research fields and applications. Nevertheless, 3DGS still faces challenges to properly manage the number of Gaussian primitives that are used during scene reconstruction. Following the adaptive density control (ADC) mechanism of 3D Gaussian Splatting, new Gaussians in under-reconstructed regions are created, while Gaussians that do not contribute to the rendering quality are pruned. We observe that those criteria for densifying and pruning Gaussians can sometimes lead to worse rendering by introducing artifacts. We especially observe under-reconstructed background or overfitted foreground regions. To encounter both problems, we propose three new improvements to the adaptive density control mechanism. Those include a correction for the scene extent calculation that does not only rely on camera positions, an exponentially ascending gradient threshold to improve training convergence, and significance-aware pruning strategy to avoid background artifacts. With these adaptions, we show that the rendering quality improves while using the same number of Gaussians primitives. Furthermore, with our improvements, the training converges considerably faster, allowing for more than twice as fast training times while yielding better quality than 3DGS. Finally, our contributions are easily compatible with most existing derivative works of 3DGS making them relevant for future works.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Model for Animation and Texturing",
    "authors": [
      "Xiangzhi Eric Wang",
      "Zackary P. T. Sin"
    ],
    "abstract": "3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance. Despite this achievement, however, it is not readily applicable to developing interactive applications. Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model. We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer. By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications. It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering. Our work, on the other hand, only requires an approximated geometry for rendering an object in high fidelity. Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians. We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-29",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps",
    "authors": [
      "Timothy Chen",
      "Ola Shorinwa",
      "Joseph Bruno",
      "Aiden Swann",
      "Javier Yu",
      "Weijia Zeng",
      "Keiko Nagami",
      "Philip Dames",
      "Mac Schwager"
    ],
    "abstract": "We present Splat-Nav, a real-time robot navigation pipeline for Gaussian Splatting (GSplat) scenes, a powerful new 3D scene representation. Splat-Nav consists of two components: 1) Splat-Plan, a safe planning module, and 2) Splat-Loc, a robust vision-based pose estimation module. Splat-Plan builds a safe-by-construction polytope corridor through the map based on mathematically rigorous collision constraints and then constructs a Bézier curve trajectory through this corridor. Splat-Loc provides real-time recursive state estimates given only an RGB feed from an on-board camera, leveraging the point-cloud representation inherent in GSplat scenes. Working together, these modules give robots the ability to recursively re-plan smooth and safe trajectories to goal locations. Goals can be specified with position coordinates, or with language commands by using a semantic GSplat. We demonstrate improved safety compared to point cloud-based methods in extensive simulation experiments. In a total of 126 hardware flights, we demonstrate equivalent safety and speed compared to motion capture and visual odometry, but without a manual frame alignment required by those methods. We show online re-planning at more than 2 Hz and pose estimation at about 25 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation. We provide experiment videos on our project page at https://chengine.github.io/splatnav/. Our codebase and ROS nodes can be found at https://github.com/chengine/splatnav.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "motion",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS",
    "authors": [
      "Zhiwen Fan",
      "Kevin Wang",
      "Kairun Wen",
      "Zehao Zhu",
      "Dejia Xu",
      "Zhangyang Wang"
    ],
    "abstract": "Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce LightGaussian, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussian's global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average 15x compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "neural rendering",
      "compact",
      "motion",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids",
    "authors": [
      "Wieland Morgenstern",
      "Florian Barthel",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a highly promising technique for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it utilizes efficient rasterization allowing for very fast rendering at high-quality. However, the storage size is significantly higher, which hinders practical deployment, e.g. on resource constrained devices. In this paper, we introduce a compact scene representation organizing the parameters of 3D Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a drastic reduction in storage requirements without compromising visual quality during rendering. Central to our idea is the explicit exploitation of perceptual redundancies present in natural scenes. In essence, the inherent nature of a scene allows for numerous permutations of Gaussian parameters to equivalently represent it. To this end, we propose a novel highly parallel algorithm that regularly arranges the high-dimensional Gaussian parameters into a 2D grid while preserving their neighborhood structure. During training, we further enforce local smoothness between the sorted parameters in the grid. The uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless integration with established renderers. Our method achieves a reduction factor of 17x to 42x in size for complex scenes with no increase in training time, marking a substantial leap forward in the domain of 3D scene distribution and consumption. Additional information can be found on our project page: https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
    "authors": [
      "Xuechang Tu",
      "Lukas Radl",
      "Michael Steiner",
      "Markus Steinberger",
      "Bernhard Kerbl",
      "Fernando de la Torre"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for novel-view synthesis, providing exceptional performance through efficient software-based GPU rasterization. Its versatility enables real-time applications, including on mobile and lower-powered devices. However, 3DGS faces key challenges in virtual reality (VR): (1) temporal artifacts, such as popping during head movements, (2) projection-based distortions that result in disturbing and view-inconsistent floaters, and (3) reduced framerates when rendering large numbers of Gaussians, falling below the critical threshold for VR. Compared to desktop environments, these issues are drastically amplified by large field-of-view, constant head movements, and high resolution of head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine and extend several recent advancements in 3DGS to address challenges of VR holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal Projection can complement each other, by modifying the individual techniques and core 3DGS rasterizer. Additionally, we propose an efficient foveated rasterizer that handles focus and peripheral areas in a single GPU launch, avoiding redundant computations and improving GPU utilization. Our method also incorporates a fine-tuning step that optimizes Gaussian parameters based on StopThePop depth evaluations and Optimal Projection. We validate our method through a controlled user study with 25 participants, showing a strong preference for VRSplat over other configurations of Mini-Splatting. VRSplat is the first, systematically evaluated 3DGS approach capable of supporting modern VR applications, achieving 72+ FPS while eliminating popping and stereo-disrupting floaters.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-15",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
    "authors": [
      "Giacomo D'Amicantonio",
      "Snehashis Majhi",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "François Bremond",
      "Egor Bondarev"
    ],
    "abstract": "Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction",
    "authors": [
      "Danpeng Chen",
      "Hai Li",
      "Weicai Ye",
      "Yifan Wang",
      "Weijian Xie",
      "Shangjin Zhai",
      "Nan Wang",
      "Haomin Liu",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due to its high-quality rendering, and ultra-fast training and rendering speed. However, due to the unstructured and irregular nature of Gaussian point clouds, it is difficult to guarantee geometric reconstruction accuracy and multi-view consistency simply by relying on image reconstruction loss. Although many studies on surface reconstruction based on 3DGS have emerged recently, the quality of their meshes is generally unsatisfactory. To address this problem, we propose a fast planar-based Gaussian splatting reconstruction representation (PGSR) to achieve high-fidelity surface reconstruction while ensuring high-quality rendering. Specifically, we first introduce an unbiased depth rendering method, which directly renders the distance from the camera origin to the Gaussian plane and the corresponding normal map based on the Gaussian distribution of the point cloud, and divides the two to obtain the unbiased depth. We then introduce single-view geometric, multi-view photometric, and geometric regularization to preserve global geometric accuracy. We also propose a camera exposure compensation model to cope with scenes with large illumination variations. Experiments on indoor and outdoor scenes show that our method achieves fast training and rendering while maintaining high-fidelity rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "illumination",
      "high-fidelity",
      "outdoor",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UrbanGS: Semantic-Guided Gaussian Splatting for Urban Scene Reconstruction",
    "authors": [
      "Ziwen Li",
      "Jiaxin Huang",
      "Runnan Chen",
      "Yunlong Che",
      "Yandong Guo",
      "Tongliang Liu",
      "Fakhri Karray",
      "Mingming Gong"
    ],
    "abstract": "Reconstructing urban scenes is challenging due to their complex geometries and the presence of potentially dynamic objects. 3D Gaussian Splatting (3DGS)-based methods have shown strong performance, but existing approaches often incorporate manual 3D annotations to improve dynamic object modeling, which is impractical due to high labeling costs. Some methods leverage 4D Gaussian Splatting (4DGS) to represent the entire scene, but they treat static and dynamic objects uniformly, leading to unnecessary updates for static elements and ultimately degrading reconstruction quality. To address these issues, we propose UrbanGS, which leverages 2D semantic maps and an existing dynamic Gaussian approach to distinguish static objects from the scene, enabling separate processing of definite static and potentially dynamic elements. Specifically, for definite static regions, we enforce global consistency to prevent unintended changes in dynamic Gaussian and introduce a K-nearest neighbor (KNN)-based regularization to improve local coherence on low-textured ground surfaces. Notably, for potentially dynamic objects, we aggregate temporal information using learnable time embeddings, allowing each Gaussian to model deformations over time. Extensive experiments on real-world datasets demonstrate that our approach outperforms state-of-the-art methods in reconstruction quality and efficiency, accurately preserving static content while capturing dynamic elements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "urban scene",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting",
    "authors": [
      "Dario Tscholl",
      "Yashwanth Nakka",
      "Brian Gunter"
    ],
    "abstract": "We present a perception-driven safety filter that converts each 3D Gaussian Splat (3DGS) into a closed-form forward collision cone, which in turn yields a first-order control barrier function (CBF) embedded within a quadratic program (QP). By exploiting the analytic geometry of splats, our formulation provides a continuous, closed-form representation of collision constraints that is both simple and computationally efficient. Unlike distance-based CBFs, which tend to activate reactively only when an obstacle is already close, our collision-cone CBF activates proactively, allowing the robot to adjust earlier and thereby produce smoother and safer avoidance maneuvers at lower computational cost. We validate the method on a large synthetic scene with approximately 170k splats, where our filter reduces planning time by a factor of 3 and significantly decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while maintaining the same level of safety. The approach is entirely analytic, requires no high-order CBF extensions (HOCBFs), and generalizes naturally to robots with physical extent through a principled Minkowski-sum inflation of the splats. These properties make the method broadly applicable to real-time navigation in cluttered, perception-derived extreme environments, including space robotics and satellite systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes",
    "authors": [
      "Xijie Yang",
      "Linning Xu",
      "Lihan Jiang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion",
    "authors": [
      "Yuanze Lin",
      "Ronald Clark",
      "Philip Torr"
    ],
    "abstract": "We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting for Large-scale Surface Reconstruction from Aerial Images",
    "authors": [
      "YuanZheng Wu",
      "Jin Liu",
      "Shunping Ji"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent ability in small-scale 3D surface reconstruction. However, extending 3DGS to large-scale scenes remains a significant challenge. To address this gap, we propose a novel 3DGS-based method for large-scale surface reconstruction using aerial multi-view stereo (MVS) images, named Aerial Gaussian Splatting (AGS). First, we introduce a data chunking method tailored for large-scale aerial images, making 3DGS feasible for surface reconstruction over extensive scenes. Second, we integrate the Ray-Gaussian Intersection method into 3DGS to obtain depth and normal information. Finally, we implement multi-view geometric consistency constraints to enhance the geometric consistency across different views. Our experiments on multiple datasets demonstrate, for the first time, the 3DGS-based method can match conventional aerial MVS methods on geometric accuracy in aerial large-scale surface reconstruction, and our method also beats state-of-the-art GS-based methods both on geometry and rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation",
    "authors": [
      "Anthony Opipari",
      "Aravindhan K Krishnan",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Remembering where object segments were predicted in the past is useful for improving the accuracy and consistency of class-agnostic video segmentation algorithms. Existing video segmentation algorithms typically use either no object-level memory (e.g. FastSAM) or they use implicit memories in the form of recurrent neural network features (e.g. SAM2). In this paper, we augment both types of segmentation models using an explicit 3D memory and show that the resulting models have more accurate and consistent predictions. For this, we develop an online 3D Gaussian Splatting (3DGS) technique to store predicted object-level segments generated throughout the duration of a video. Based on this 3DGS representation, a set of fusion techniques are developed, named FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve their respective foundation models' predictions. Ablation experiments are used to validate the proposed techniques' design and hyperparameter settings. Results from both real-world and simulated benchmarking experiments show that models which use explicit 3D memories result in more accurate and consistent predictions than those which use no memory or only implicit neural network memories. Project Page: https://topipari.com/projects/FastSAM-Splat/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-27",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting",
    "authors": [
      "Joji Joseph",
      "Bharadwaj Amrutur",
      "Shalabh Bhatnagar"
    ],
    "abstract": "We introduce a training-free method for feature field rendering in Gaussian splatting. Our approach back-projects 2D features into pre-trained 3D Gaussians, using a weighted sum based on each Gaussian's influence in the final rendering. While most training-based feature field rendering methods excel at 2D segmentation but perform poorly at 3D segmentation without post-processing, our method achieves high-quality results in both 2D and 3D segmentation. Experimental results demonstrate that our approach is fast, scalable, and offers performance comparable to training-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoGS: Controllable Gaussian Splatting",
    "authors": [
      "Heng Yu",
      "Joel Julin",
      "Zoltán Á. Milacski",
      "Koichiro Niinuma",
      "László A. Jeni"
    ],
    "abstract": "Capturing and re-animating the 3D structure of articulated objects present significant barriers. On one hand, methods requiring extensively calibrated multi-view setups are prohibitively complex and resource-intensive, limiting their practical applicability. On the other hand, while single-camera Neural Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive training and rendering costs. 3D Gaussian Splatting would be a suitable alternative but for two reasons. Firstly, existing methods for 3D dynamic Gaussians require synchronized multi-view cameras, and secondly, the lack of controllability in dynamic scenarios. We present CoGS, a method for Controllable Gaussian Splatting, that enables the direct manipulation of scene elements, offering real-time control of dynamic scenes without the prerequisite of pre-computing control signals. We evaluated CoGS using both synthetic and real-world datasets that include dynamic objects that differ in degree of difficulty. In our evaluations, CoGS consistently outperformed existing dynamic and controllable neural representations in terms of visual fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gassidy: Gaussian Splatting SLAM in Dynamic Environments",
    "authors": [
      "Long Wen",
      "Shixin Li",
      "Yu Zhang",
      "Yuhong Huang",
      "Jianjie Lin",
      "Fengjunjie Pan",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) allows flexible adjustments to scene representation, enabling continuous optimization of scene quality during dense visual simultaneous localization and mapping (SLAM) in static environments. However, 3DGS faces challenges in handling environmental disturbances from dynamic objects with irregular movement, leading to degradation in both camera tracking accuracy and map reconstruction quality. To address this challenge, we develop an RGB-D dense SLAM which is called Gaussian Splatting SLAM in Dynamic Environments (Gassidy). This approach calculates Gaussians to generate rendering loss flows for each environmental component based on a designed photometric-geometric loss function. To distinguish and filter environmental disturbances, we iteratively analyze rendering loss flows to detect features characterized by changes in loss values between dynamic objects and static components. This process ensures a clean environment for accurate scene reconstruction. Compared to state-of-the-art SLAM methods, experimental results on open datasets show that Gassidy improves camera tracking precision by up to 97.9% and enhances map quality by up to 6%.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-23",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "tracking",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond",
    "authors": [
      "Anna-Maria Halacheva",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-01",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compact",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting",
    "authors": [
      "Zheng Zhou",
      "Yu-Jie Xiong",
      "Chun-Ming Xia",
      "Jia-Chen Zhang",
      "Hong-Jian Zhan"
    ],
    "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware adaptive density control framework to address these challenges. Our key innovations: the gradient coherence ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50\\% reduced memory consumption through optimized Gaussians utilization.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-Aware Human-Object Rendering from Sparse Views via 3D Gaussian Splatting",
    "authors": [
      "Weiquan Wang",
      "Jun Xiao",
      "Yueting Zhuang",
      "Long Chen"
    ],
    "abstract": "Rendering realistic human-object interactions (HOIs) from sparse-view inputs is challenging due to occlusions and incomplete observations, yet crucial for various real-world applications. Existing methods always struggle with either low rendering qualities (\\eg, visual fidelity and physically plausible HOIs) or high computational costs. To address these limitations, we propose HOGS (Human-Object Rendering via 3D Gaussian Splatting), a novel framework for efficient and physically plausible HOI rendering from sparse views. Specifically, HOGS combines 3D Gaussian Splatting with a physics-aware optimization process. It incorporates a Human Pose Refinement module for accurate pose estimation and a Sparse-View Human-Object Contact Prediction module for efficient contact region identification. This combination enables coherent joint rendering of human and object Gaussians while enforcing physically plausible interactions. Extensive experiments on the HODome dataset demonstrate that HOGS achieves superior rendering quality, efficiency, and physical plausibility compared to existing methods. We further show its extensibility to hand-object grasp rendering tasks, presenting its broader applicability to articulated object interactions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-12",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians",
    "authors": [
      "Erik Sandström",
      "Keisuke Tateno",
      "Michael Oechsle",
      "Michael Niemeyer",
      "Luc Van Gool",
      "Martin R. Oswald",
      "Federico Tombari"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful representation of geometry and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM), as it provides a compact dense map representation while enabling efficient and high-quality map rendering. However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth. In response, we propose the first RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction. Our experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing RGB-only SLAM methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes. The source code is available at https://github.com/eriksandstroem/Splat-SLAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "compact",
      "tracking",
      "localization",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video",
    "authors": [
      "Hongsheng Wang",
      "Xiang Cai",
      "Xi Sun",
      "Jinhong Yue",
      "Zhanyun Tang",
      "Shengyu Zhang",
      "Feng Lin",
      "Fei Wu"
    ],
    "abstract": "Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clo}thed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively. Codes are available at https://wanghongsheng01.github.io/MOSS/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "body",
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Xiangyu Sun",
      "Runnan Chen",
      "Mingming Gong",
      "Dong Xu",
      "Tongliang Liu"
    ],
    "abstract": "Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery",
    "authors": [
      "Yi-Zhen Tsai",
      "Xuechen Zhang",
      "Zheng Li",
      "Jiasi Chen"
    ],
    "abstract": "Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "high quality",
      "head",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping",
    "authors": [
      "Zhihao Cao",
      "Hanyu Wu",
      "Li Wa Tang",
      "Zizhou Luo",
      "Zihan Zhu",
      "Wei Zhang",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage. We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines poses and depths via dense photometric and geometric residuals, while a scale consistency module enforces metric alignment across views using low-rank priors. The system supports RGB input and maintains real-time performance at large scale. Experiments on synthetic and real-world datasets show that MCGS-SLAM consistently yields accurate trajectories and photorealistic reconstructions, usually outperforming monocular baselines. Notably, the wide field of view from multi-camera input enables reconstruction of side-view regions that monocular setups miss, critical for safe autonomous operation. These results highlight the promise of multi-camera Gaussian Splatting SLAM for high-fidelity mapping in robotics and autonomous driving.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-17",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "slam",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting",
    "authors": [
      "Qifeng Chen",
      "Sheng Yang",
      "Sicong Du",
      "Tao Tang",
      "Rengan Xie",
      "Peng Chen",
      "Yuchi Huo"
    ],
    "abstract": "We present LiDAR-GS, a Gaussian Splatting (GS) method for real-time, high-fidelity re-simulation of LiDAR scans in public urban road scenes. Recent GS methods proposed for cameras have achieved significant advancements in real-time rendering beyond Neural Radiance Fields (NeRF). However, applying GS representation to LiDAR, an active 3D sensor type, poses several challenges that must be addressed to preserve high accuracy and unique characteristics. Specifically, LiDAR-GS designs a differentiable laser beam splatting, using range-view representation for precise surface splatting by projecting lasers onto micro cross-sections, effectively eliminating artifacts associated with local affine approximations. Furthermore, LiDAR-GS leverages Neural Gaussian Representation, which further integrate view-dependent clues, to represent key LiDAR properties that are influenced by the incident direction and external factors. Combining these practices with some essential adaptations, e.g., dynamic instances decomposition, LiDAR-GS succeeds in simultaneously re-simulating depth, intensity, and ray-drop channels, achieving state-of-the-art results in both rendering frame rate and quality on publically available large scene datasets when compared with the methods using explicit mesh or implicit NeRF. Our source code is publicly available at https://www.github.com/cqf7419/LiDAR-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "large scene",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Second-order Optimization of Gaussian Splats with Importance Sampling",
    "authors": [
      "Hamza Pehlivan",
      "Andrea Boscolo Camiletto",
      "Lin Geng Foo",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to its high rendering quality and fast inference time. However, 3DGS predominantly relies on first-order optimizers such as Adam, which leads to long training times. To address this limitation, we propose a novel second-order optimization strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which we specifically tailor towards Gaussian Splatting. Our key insight is that the Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only a limited number of pixels. We exploit this sparsity by proposing a matrix-free and GPU-parallelized LM optimization. To further improve its efficiency, we propose sampling strategies for both the camera views and loss function and, consequently, the normal equation, significantly reducing the computational complexity. In addition, we increase the convergence rate of the second-order approximation by introducing an effective heuristic to determine the learning rate that avoids the expensive computation cost of line search methods. As a result, our method achieves a $3\\times$ speedup over standard LM and outperforms Adam by $~6\\times$ when the Gaussian count is low while remaining competitive for moderate counts. Project Page: https://vcai.mpi-inf.mpg.de/projects/LM-IS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding",
    "authors": [
      "Dianyi Yang",
      "Yu Gao",
      "Xihan Wang",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: https://young-bit.github.io/opengs-github.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "semantic",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generalizable Human Gaussians for Sparse View Synthesis",
    "authors": [
      "Youngjoong Kwon",
      "Baole Fang",
      "Yixing Lu",
      "Haoye Dong",
      "Cheng Zhang",
      "Francisco Vicente Carrasco",
      "Albert Mosella-Montoro",
      "Jianjin Xu",
      "Shingo Takagi",
      "Daeil Kim",
      "Aayush Prakash",
      "Fernando De la Torre"
    ],
    "abstract": "Recent progress in neural rendering has brought forth pioneering methods, such as NeRF and Gaussian Splatting, which revolutionize view rendering across various domains like AR/VR, gaming, and content creation. While these methods excel at interpolating {\\em within the training data}, the challenge of generalizing to new scenes and objects from very sparse views persists. Specifically, modeling 3D humans from sparse views presents formidable hurdles due to the inherent complexity of human geometry, resulting in inaccurate reconstructions of geometry and textures. To tackle this challenge, this paper leverages recent advancements in Gaussian Splatting and introduces a new method to learn generalizable human Gaussians that allows photorealistic and accurate view-rendering of a new human subject from a limited set of sparse views in a feed-forward manner. A pivotal innovation of our approach involves reformulating the learning of 3D Gaussian parameters into a regression process defined on the 2D UV space of a human template, which allows leveraging the strong geometry prior and the advantages of 2D convolutions. In addition, a multi-scaffold is proposed to effectively represent the offset details. Our method outperforms recent methods on both within-dataset generalization as well as cross-dataset generalization settings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "vr",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
    "authors": [
      "Wenyuan Zhang",
      "Jimin Tang",
      "Weiqi Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Zipeng Qi",
      "Wenjun Lu",
      "David Bull",
      "Nantheera Anantrasirichai"
    ],
    "abstract": "Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "illumination",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SRGS: Super-Resolution 3D Gaussian Splatting",
    "authors": [
      "Xiang Feng",
      "Yongbo He",
      "Yubo Wang",
      "Yan Yang",
      "Wen Li",
      "Yifei Chen",
      "Zhenzhong Kuang",
      "Jiajun ding",
      "Jianping Fan",
      "Yu Jun"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes will be released upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
    "authors": [
      "Wangze Xu",
      "Huachen Gao",
      "Shihe Shen",
      "Rui Peng",
      "Jianbo Jiao",
      "Ronggang Wang"
    ],
    "abstract": "Recently, the Neural Radiance Field (NeRF) advancement has facilitated few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D vision applications. Despite numerous attempts to reduce the dense input requirement in NeRF, it still suffers from time-consumed training and rendering processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time high-quality rendering with an explicit point-based representation. However, similar to NeRF, it tends to overfit the train views for lack of constraints. In this paper, we propose \\textbf{MVPGS}, a few-shot NVS method that excavates the multi-view priors based on 3D Gaussian Splatting. We leverage the recent learning-based Multi-view Stereo (MVS) to enhance the quality of geometric initialization for 3DGS. To mitigate overfitting, we propose a forward-warping method for additional appearance constraints conforming to scenes based on the computed geometry. Furthermore, we introduce a view-consistent geometry constraint for Gaussian parameters to facilitate proper optimization convergence and utilize a monocular depth regularization as compensation. Experiments show that the proposed method achieves state-of-the-art performance with real-time rendering speed. Project page: https://zezeaaa.github.io/projects/MVPGS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian Supervision",
    "authors": [
      "Baixin Xu",
      "Jiangbei Hu",
      "Jiaze Li",
      "Ying He"
    ],
    "abstract": "Surface reconstruction from multi-view images is a core challenge in 3D vision. Recent studies have explored signed distance fields (SDF) within Neural Radiance Fields (NeRF) to achieve high-fidelity surface reconstructions. However, these approaches often suffer from slow training and rendering speeds compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques attempt to fuse depth information to extract geometry from 3DGS, but frequently result in incomplete reconstructions and fragmented surfaces. In this paper, we introduce GSurf, a novel end-to-end method for learning a signed distance field directly from Gaussian primitives. The continuous and smooth nature of SDF addresses common issues in the 3DGS family, such as holes resulting from noisy or missing depth data. By using Gaussian splatting for rendering, GSurf avoids the redundant volume rendering typically required in other GS and SDF integrations. Consequently, GSurf achieves faster training and rendering speeds while delivering 3D reconstruction quality comparable to neural implicit surface methods, such as VolSDF and NeuS. Experimental results across various benchmark datasets demonstrate the effectiveness of our method in producing high-fidelity 3D reconstructions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation",
    "authors": [
      "Xuyi Meng",
      "Chen Wang",
      "Jiahui Lei",
      "Kostas Daniilidis",
      "Jiatao Gu",
      "Lingjie Liu"
    ],
    "abstract": "Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "M3: 3D-Spatial MultiModal Memory",
    "authors": [
      "Xueyan Zou",
      "Yuchen Song",
      "Ri-Zhao Qiu",
      "Xuanbin Peng",
      "Jianglong Ye",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Material Gaussian Model for Relightable 3D Generation",
    "authors": [
      "Jingrui Ye",
      "Lingting Zhu",
      "Runze Zhang",
      "Zeyu Hu",
      "Yingda Yin",
      "Lanjiong Li",
      "Lequan Yu",
      "Qingmin Liao"
    ],
    "abstract": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "relightable",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian",
    "authors": [
      "Hao Tian",
      "Rui Liu",
      "Wen Shen",
      "Yilong Hu",
      "Zhihao Zheng",
      "Xiaolin Qin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "human",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation",
    "authors": [
      "Zipeng Qi",
      "Hao Chen",
      "Haotian Zhang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "In this paper, we propose a novel semantic splatting approach based on Gaussian Splatting to achieve efficient and low-latency. Our method projects the RGB attributes and semantic features of point clouds onto the image plane, simultaneously rendering RGB images and semantic segmentation results. Leveraging the explicit structure of point clouds and a one-time rendering strategy, our approach significantly enhances efficiency during optimization and rendering. Additionally, we employ SAM2 to generate pseudo-labels for boundary regions, which often lack sufficient supervision, and introduce two-level aggregation losses at the 2D feature map and 3D spatial levels to improve the view-consistent and spatial continuity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting",
    "authors": [
      "Butian Xiong",
      "Zhuo Li",
      "Zhen Li"
    ],
    "abstract": "We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "gaussian splatting",
      "3d reconstruction",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting",
    "authors": [
      "Wugang Meng",
      "Tianfu Wu",
      "Huan Yin",
      "Fumin Zhang"
    ],
    "abstract": "Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot's sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-16",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-based View Selection in Gaussian Splatting Reconstruction",
    "authors": [
      "Monica M. Q. Li",
      "Pierre-Yves Lajoie",
      "Giovanni Beltrame"
    ],
    "abstract": "Three-dimensional reconstruction is a fundamental problem in robotics perception. We examine the problem of active view selection to perform 3D Gaussian Splatting reconstructions with as few input images as possible. Although 3D Gaussian Splatting has made significant progress in image rendering and 3D reconstruction, the quality of the reconstruction is strongly impacted by the selection of 2D images and the estimation of camera poses through Structure-from-Motion (SfM) algorithms. Current methods to select views that rely on uncertainties from occlusions, depth ambiguities, or neural network predictions directly are insufficient to handle the issue and struggle to generalize to new scenes. By ranking the potential views in the frequency domain, we are able to effectively estimate the potential information gain of new viewpoints without ground truth data. By overcoming current constraints on model architecture and efficacy, our method achieves state-of-the-art results in view selection, demonstrating its potential for efficient image-based 3D reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "robotics",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Modeling uncertainty for Gaussian Splatting",
    "authors": [
      "Luca Savant",
      "Diego Valsesia",
      "Enrico Magli"
    ],
    "abstract": "We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes",
    "authors": [
      "Yunsong Wang",
      "Tianxin Huang",
      "Hanlin Chen",
      "Gim Hee Lee"
    ],
    "abstract": "Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "large scene",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-24",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "understanding",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
    "authors": [
      "Qi Wu",
      "Janick Martinez Esturo",
      "Ashkan Mirzaei",
      "Nicolas Moenne-Loccoz",
      "Zan Gojcic"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing the particles instead, but, this comes at the cost of significantly slower rendering. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation. The source code is available at: https://github.com/nv-tlabs/3dgrut.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "high-fidelity",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AbsGS: Recovering Fine Details for 3D Gaussian Splatting",
    "authors": [
      "Zongxin Ye",
      "Wenyu Li",
      "Sidun Liu",
      "Peng Qiao",
      "Yong Dou"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: https://ty424.github.io/AbsGS.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets",
    "authors": [
      "Bernhard Kerbl",
      "Andréas Meuleman",
      "Georgios Kopanas",
      "Michael Wimmer",
      "Alexandre Lanvin",
      "George Drettakis"
    ],
    "abstract": "Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels.We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour. Project Page: https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "large scene",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSEditPro: 3D Gaussian Splatting Editing with Attention-based Progressive Localization",
    "authors": [
      "Yanhao Sun",
      "RunZe Tian",
      "Xiao Han",
      "XinYao Liu",
      "Yan Zhang",
      "Kai Xu"
    ],
    "abstract": "With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D representations like Neural Radiance Fields (NeRF), many text-driven generative editing methods based on NeRF have appeared. However, the implicit encoding of geometric and textural information poses challenges in accurately locating and controlling objects during editing. Recently, significant advancements have been made in the editing methods of 3D Gaussian Splatting, a real-time rendering technology that relies on explicit representation. However, these methods still suffer from issues including inaccurate localization and limited manipulation over editing. To tackle these challenges, we propose GSEditPro, a novel 3D scene editing framework which allows users to perform various creative and precise editing using text prompts only. Leveraging the explicit nature of the 3D Gaussian distribution, we introduce an attention-based progressive localization module to add semantic labels to each Gaussian during rendering. This enables precise localization on editing areas by classifying Gaussians based on their relevance to the editing prompts derived from cross-attention layers of the T2I model. Furthermore, we present an innovative editing optimization method based on 3D Gaussian Splatting, obtaining stable and refined editing results through the guidance of Score Distillation Sampling and pseudo ground truth. We prove the efficacy of our method through extensive experiments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "semantic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "Recent developments in 3D reconstruction and neural rendering have significantly propelled the capabilities of photo-realistic 3D scene rendering across various academic and industrial fields. The 3D Gaussian Splatting technique, alongside its derivatives, integrates the advantages of primitive-based and volumetric representations to deliver top-tier rendering quality and efficiency. Despite these advancements, the method tends to generate excessive redundant noisy Gaussians overfitted to every training view, which degrades the rendering quality. Additionally, while 3D Gaussian Splatting excels in small-scale and object-centric scenes, its application to larger scenes is hindered by constraints such as limited video memory, excessive optimization duration, and variable appearance across views. To address these challenges, we introduce GaussianFocus, an innovative approach that incorporates a patch attention algorithm to refine rendering quality and implements a Gaussian constraints strategy to minimize redundancy. Moreover, we propose a subdivision reconstruction strategy for large-scale scenes, dividing them into smaller, manageable blocks for individual training. Our results indicate that GaussianFocus significantly reduces unnecessary Gaussians and enhances rendering quality, surpassing existing State-of-The-Art (SoTA) methods. Furthermore, we demonstrate the capability of our approach to effectively manage and render large scenes, such as urban environments, whilst maintaining high fidelity in the visual output.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "large scene",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene",
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Luxin Yan",
      "Gim Hee Lee"
    ],
    "abstract": "High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes",
    "authors": [
      "Ke Wu",
      "Zicheng Zhang",
      "Muer Tie",
      "Ziqing Ai",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer, Score Manager, and Pose Refinement, which collectively improve mapping speed and localization accuracy. This enables the SLAM system to handle large-scale urban environments with up to 50 million Gaussian ellipsoids. To ensure global consistency in large-scale scenes, we design a Loop Closure module, which innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian Splatting for loop closure detection and correction of the Gaussian map. Additionally, we propose a Dynamic Eraser to address the inevitable presence of dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor and outdoor environments demonstrate that our approach achieves localization performance on par with Visual-Inertial Odometry while surpassing recent GS/NeRF SLAM methods. It also significantly outperforms all existing methods in terms of mapping and rendering quality. Furthermore, we developed a mobile app and verified that our framework can generate high-quality Gaussian maps in real time using only a smartphone camera and a low-frequency IMU sensor. To the best of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method capable of operating in outdoor environments and supporting kilometer-scale large scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-14",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "large scene",
      "outdoor",
      "localization",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Linus Franke",
      "Darius Rückert",
      "Laura Fink",
      "Marc Stamminger"
    ],
    "abstract": "Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [Rückert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage.   The project page is located at: https://lfranke.github.io/trips/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "E2GS: Event Enhanced Gaussian Splatting",
    "authors": [
      "Hiroyuki Deguchi",
      "Mana Masuda",
      "Takuya Nakabayashi",
      "Hideo Saito"
    ],
    "abstract": "Event cameras, known for their high dynamic range, absence of motion blur, and low energy usage, have recently found a wide range of applications thanks to these attributes. In the past few years, the field of event-based 3D reconstruction saw remarkable progress, with the Neural Radiance Field (NeRF) based approach demonstrating photorealistic view synthesis results. However, the volume rendering paradigm of NeRF necessitates extensive training and rendering times. In this paper, we introduce Event Enhanced Gaussian Splatting (E2GS), a novel method that incorporates event data into Gaussian Splatting, which has recently made significant advances in the field of novel view synthesis. Our E2GS effectively utilizes both blurry images and event data, significantly improving image deblurring and producing high-quality novel view synthesis. Our comprehensive experiments on both synthetic and real-world datasets demonstrate our E2GS can generate visually appealing renderings while offering faster training and rendering speed (140 FPS). Our code is available at https://github.com/deguchihiroyuki/E2GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving",
    "authors": [
      "Huasong Han",
      "Kaixuan Zhou",
      "Xiaoxiao Long",
      "Yusen Wang",
      "Chunxia Xiao"
    ],
    "abstract": "We propose GGS, a Generalizable Gaussian Splatting method for Autonomous Driving which can achieve realistic rendering under large viewpoint changes. Previous generalizable 3D gaussian splatting methods are limited to rendering novel views that are very close to the original pair of images, which cannot handle large differences in viewpoint. Especially in autonomous driving scenarios, images are typically collected from a single lane. The limited training perspective makes rendering images of a different lane very challenging. To further improve the rendering capability of GGS under large viewpoint changes, we introduces a novel virtual lane generation module into GSS method to enables high-quality lane switching even without a multi-lane dataset. Besides, we design a diffusion loss to supervise the generation of virtual lane image to further address the problem of lack of data in the virtual lanes. Finally, we also propose a depth refinement module to optimize depth estimation in the GSS model. Extensive validation of our method, compared to existing approaches, demonstrates state-of-the-art performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IG-SLAM: Instant Gaussian SLAM",
    "authors": [
      "F. Aykut Sarikamis",
      "A. Aydin Alatan"
    ],
    "abstract": "3D Gaussian Splatting has recently shown promising results as an alternative scene representation in SLAM systems to neural implicit representations. However, current methods either lack dense depth maps to supervise the mapping process or detailed training designs that consider the scale of the environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only SLAM system that employs robust Dense-SLAM methods for tracking and combines them with Gaussian Splatting. A 3D map of the environment is constructed using accurate pose and dense depth provided by tracking. Additionally, we utilize depth uncertainty in map optimization to improve 3D reconstruction. Our decay strategy in map optimization enhances convergence and allows the system to run at 10 fps in a single process. We demonstrate competitive performance with state-of-the-art RGB-only SLAM systems while achieving faster operation speeds. We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC datasets. The system achieves photo-realistic 3D reconstruction in large-scale sequences, particularly in the EuRoC dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-02",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats",
    "authors": [
      "Mingyang Xie",
      "Haoming Cai",
      "Sachin Shah",
      "Yiran Xu",
      "Brandon Y. Feng",
      "Jia-Bin Huang",
      "Christopher A. Metzler"
    ],
    "abstract": "We introduce a simple yet effective approach for separating transmitted and reflected light. Our key insight is that the powerful novel view synthesis capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian splatting) allow one to perform flash/no-flash reflection separation using unpaired measurements -- this relaxation dramatically simplifies image acquisition over conventional paired flash/no-flash reflection separation methods. Through extensive real-world experiments, we demonstrate our method, Flash-Splat, accurately reconstructs both transmitted and reflected scenes in 3D. Our method outperforms existing 3D reflection separation methods, which do not leverage illumination control, by a large margin. Our project webpage is at https://flash-splat.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-03",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human Gaussian Splatting: Real-time Rendering of Animatable Avatars",
    "authors": [
      "Arthur Moreau",
      "Jifei Song",
      "Helisa Dhamo",
      "Richard Shaw",
      "Yiren Zhou",
      "Eduardo Pérez-Pellitero"
    ],
    "abstract": "This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose an animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. The body is represented by a set of gaussian primitives in a canonical space which is deformed with a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (HuGS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4 dataset while being able to render in real-time (80 fps for 512x512 resolution).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "body",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction",
    "authors": [
      "Jiaqi Lin",
      "Zhihao Li",
      "Xiao Tang",
      "Jianzhuang Liu",
      "Shiyong Liu",
      "Jiayue Liu",
      "Yangdi Lu",
      "Xiaofei Wu",
      "Songcen Xu",
      "Youliang Yan",
      "Wenming Yang"
    ],
    "abstract": "Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "large scene",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
    "authors": [
      "Yuan Shen",
      "Duygu Ceylan",
      "Paul Guerrero",
      "Zexiang Xu",
      "Niloy J. Mitra",
      "Shenlong Wang",
      "Anna Frühstück"
    ],
    "abstract": "We present a simple, modular, and generic method that upsamples coarse 3D models by adding geometric and appearance details. While generative 3D models now exist, they do not yet match the quality of their counterparts in image and video domains. We demonstrate that it is possible to directly repurpose existing (pretrained) video models for 3D super-resolution and thus sidestep the problem of the shortage of large repositories of high-quality 3D training models. We describe how to repurpose video upsampling models, which are not 3D consistent, and combine them with 3D consolidation to produce 3D-consistent results. As output, we produce high quality Gaussian Splat models, which are object centric and effective. Our method is category agnostic and can be easily incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian on a variety of 3D inputs, which are diverse both in terms of complexity and representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our simple method significantly improves the fidelity of the final 3D models. Check our project website for details: supergaussian.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "high quality",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
    "authors": [
      "An Zhao",
      "Piaopiao Yu",
      "Zhe Zhu",
      "Mingqiang Wei"
    ],
    "abstract": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction. However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur. To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images. BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions. To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages. Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.Our source code is available at https://github.com/wsxujm/bsgs",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel Reconstruction from Sparse-View Dynamic DSA Images",
    "authors": [
      "Zhentao Liu",
      "Ruyi Zha",
      "Huangxuan Zhao",
      "Hongdong Li",
      "Zhiming Cui"
    ],
    "abstract": "Reconstructing 3D vessel structures from sparse-view dynamic digital subtraction angiography (DSA) images enables accurate medical assessment while reducing radiation exposure. Existing methods often produce suboptimal results or require excessive computation time. In this work, we propose 4D radiative Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently. In detail, we represent the vessels with 4D radiative Gaussian kernels. Each kernel has time-invariant geometry parameters, including position, rotation, and scale, to model static vessel structures. The time-dependent central attenuation of each kernel is predicted from a compact neural network to capture the temporal varying response of contrast agent flow. We splat these Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize the model with real captured ones. The final 3D vessel volume is voxelized from the well-trained kernels. Moreover, we introduce accumulated attenuation pruning and bounded scaling activation to improve reconstruction quality. Extensive experiments on real-world patient data demonstrate that 4DRGS achieves impressive results in 5 minutes training, which is 32x faster than the state-of-the-art method. This underscores the potential of 4DRGS for real-world clinics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-17",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "medical",
      "sparse-view",
      "geometry",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring the Versal AI Engine for 3D Gaussian Splatting",
    "authors": [
      "Kotaro Shimamura",
      "Ayumi Ohno",
      "Shinya Takamaeda-Yamazaki"
    ],
    "abstract": "Dataflow-oriented spatial architectures are the emerging paradigm for higher computation performance and efficiency.   AMD Versal AI Engine is a commercial spatial architecture consisting of tiles of VLIW processors supporting SIMD operations arranged in a two-dimensional mesh.   The architecture requires the explicit design of task assignments and dataflow configurations for each tile to maximize performance, demanding advanced techniques and meticulous design.   However, a few works revealed the performance characteristics of the Versal AI Engine through practical workloads.   In this work, we provide the comprehensive performance evaluation of the Versal AI Engine using Gaussian feature computation in 3D Gaussian splatting as a practical workload, and we then propose a novel dedicated algorithm to fully exploit the hardware architecture.   The computations of 3D Gaussian splatting include matrix multiplications and color computations utilizing high-dimensional spherical harmonic coefficients.   These tasks are processed efficiently by leveraging the SIMD capabilities and their instruction-level parallelism.   Additionally, pipelined processing is achieved by assigning different tasks to individual cores, thereby fully exploiting the spatial parallelism of AI Engines.   The proposed method demonstrated a 226-fold throughput increase in simulation-based evaluation, outperforming a naive approach.   These findings provide valuable insights for application development that effectively harnesses the spatial and architectural advantages of AI Engines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-17",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large-Scale Gaussian Splatting SLAM",
    "authors": [
      "Zhe Xin",
      "Chenyang Wu",
      "Penghui Huang",
      "Yanyong Zhang",
      "Yinian Mao",
      "Guoquan Huang"
    ],
    "abstract": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown encouraging and impressive results for visual SLAM. However, most representative methods require RGBD sensors and are only available for indoor environments. The robustness of reconstruction in large-scale outdoor scenarios remains unexplored. This paper introduces a large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses under large view changes. In tracking, we introduce feature-alignment warping constraints to alleviate the adverse effects of appearance similarity in rendering losses. For the scalability of large-scale scenarios, we introduce continuous Gaussian Splatting submaps to tackle unbounded scenes with limited memory. Loops are detected between GS submaps by place recognition and the relative pose between looped keyframes is optimized utilizing rendering and feature warping losses. After the global optimization of camera poses and Gaussian points, a structure refinement module enhances the reconstruction quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM achieves superior performance over existing Neural, 3DGS-based, and even traditional approaches. Project page: https://lsg-slam.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "recognition",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface Reconstruction from Sparse Image Sequences",
    "authors": [
      "Zhen Tan",
      "Xieyuanli Chen",
      "Jinpu Zhang",
      "Lei Feng",
      "Dewen Hu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in novel view synthesis. However, its efficacy diminishes considerably in sparse image sequences, where inherent data sparsity amplifies geometric uncertainty during optimization. This often leads to convergence at suboptimal local minima, resulting in noticeable structural artifacts in the reconstructed scenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided Gaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial Uncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS pipeline. UNG-GS enables high-fidelity rendering and achieves high-precision reconstruction without relying on priors. Specifically, we first integrate Gaussian-based probabilistic modeling into the training of 3DGS to optimize the SUF, providing the model with adaptive error tolerance. An uncertainty-aware depth rendering strategy is then employed to weight depth contributions based on the SUF, effectively reducing noise while preserving fine details. Furthermore, an uncertainty-guided normal refinement method adjusts the influence of neighboring depth values in normal estimation, promoting robust results. Extensive experiments demonstrate that UNG-GS significantly outperforms state-of-the-art methods in both sparse and dense sequences. The code will be open-source.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction",
    "authors": [
      "Yuhan Xie",
      "Yixi Cai",
      "Yinqiang Zhang",
      "Lei Yang",
      "Jia Pan"
    ],
    "abstract": "This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting",
    "authors": [
      "Yiwen Chen",
      "Zilong Chen",
      "Chi Zhang",
      "Feng Wang",
      "Xiaofeng Yang",
      "Yikai Wang",
      "Zhongang Cai",
      "Lei Yang",
      "Huaping Liu",
      "Guosheng Lin"
    ],
    "abstract": "3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS",
    "authors": [
      "Zhisheng Huang",
      "Peng Wang",
      "Jingdong Zhang",
      "Yuan Liu",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its efficiency and quality, but like many novel view synthesis methods, it heavily depends on accurate camera poses from Structure-from-Motion (SfM) systems. Although recent SfM pipelines have made impressive progress, questions remain about how to further improve both their robust performance in challenging conditions (e.g., textureless scenes) and the precision of camera parameter estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework that bridges this gap by jointly optimizing 3D Gaussians and camera parameters from large reconstruction priors MASt3R-SfM. We note that naively performing joint 3D Gaussian and camera optimization faces two challenges: the sensitivity to the quality of SfM initialization, and its limited capacity for global optimization, leading to suboptimal reconstruction results. Our 3R-GS, overcomes these issues by incorporating optimized practices, enabling robust scene reconstruction even with imperfect camera registration. Extensive experiments demonstrate that 3R-GS delivers high-quality novel view synthesis and precise camera pose estimation while remaining computationally efficient. Project page: https://zsh523.github.io/3R-GS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting",
    "authors": [
      "Runze Chen",
      "Mingyu Xiao",
      "Haiyong Luo",
      "Fang Zhao",
      "Fan Wu",
      "Hao Xiong",
      "Qi Liu",
      "Meng Song"
    ],
    "abstract": "We introduce Crowd-Sourced Splatting (CSS), a novel 3D Gaussian Splatting (3DGS) pipeline designed to overcome the challenges of pose-free scene reconstruction using crowd-sourced imagery. The dream of reconstructing historically significant but inaccessible scenes from collections of photographs has long captivated researchers. However, traditional 3D techniques struggle with missing camera poses, limited viewpoints, and inconsistent lighting. CSS addresses these challenges through robust geometric priors and advanced illumination modeling, enabling high-quality novel view synthesis under complex, real-world conditions. Our method demonstrates clear improvements over existing approaches, paving the way for more accurate and flexible applications in AR, VR, and large-scale 3D reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction",
    "authors": [
      "Antoine Guédon",
      "Diego Gomez",
      "Nissim Maruani",
      "Bingchen Gong",
      "George Drettakis",
      "Maks Ovsjanikov"
    ],
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "animation",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PRoGS: Progressive Rendering of Gaussian Splats",
    "authors": [
      "Brent Zoomers",
      "Maarten Wijnants",
      "Ivan Molenaers",
      "Joni Vanherck",
      "Jeroen Put",
      "Lode Jorissen",
      "Nick Michiels"
    ],
    "abstract": "Over the past year, 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However, it can require a substantial amount of storage since each splat's individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint, they still necessitate retrieving the entire scene before any part of it can be rendered. In this work, we introduce a novel approach for progressively rendering such scenes, aiming to display visible content that closely approximates the final scene as early as possible without loading the entire scene into memory. This approach benefits both on-device rendering applications limited by memory constraints and streaming applications where minimal bandwidth usage is preferred. To achieve this, we approximate the contribution of each Gaussian to the final scene and construct an order of prioritization on their inclusion in the rendering process. Additionally, we demonstrate that our approach can be combined with existing compression methods to progressively render (and stream) 3DGS scenes, optimizing bandwidth usage by focusing on the most important splats within a scene. Overall, our work establishes a foundation for making remotely hosted 3DGS content more quickly accessible to end-users in over-the-top consumption scenarios, with our results showing significant improvements in quality across all metrics compared to existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-03",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting",
    "authors": [
      "Chenyu Han",
      "Corentin Dumery"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene reconstruction for its high visual accuracy. However, uncertainty estimation of 3DGS scenes remains underexplored and is crucial to downstream tasks such as asset extraction and scene completion. Since the appearance of 3D gaussians is view-dependent, the color of a gaussian can thus be certain from an angle and uncertain from another. We thus propose to model uncertainty in 3DGS as an additional view-dependent per-gaussian feature that can be modeled with spherical harmonics. This simple yet effective modeling is easily interpretable and can be integrated into the traditional 3DGS pipeline. It is also significantly faster than ensemble methods while maintaining high accuracy, as demonstrated in our experiments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction",
    "authors": [
      "Han Jiao",
      "Jiakai Sun",
      "Yexing Xu",
      "Lei Zhao",
      "Wei Xing",
      "Huaizhong Lin"
    ],
    "abstract": "3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting",
    "authors": [
      "Jaewoo Jung",
      "Jisang Han",
      "Honggyu An",
      "Jiwon Kang",
      "Seonghoon Park",
      "Seungryong Kim"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When the quality of the initial point cloud deteriorates, such as in the presence of noise or when using randomly initialized point cloud, 3DGS often undergoes large performance drops. To address this limitation, we propose a novel optimization strategy dubbed RAIN-GS (Relaing Accurate Initialization Constraint for 3D Gaussian Splatting). Our approach is based on an in-depth analysis of the original 3DGS optimization scheme and the analysis of the SfM initialization in the frequency domain. Leveraging simple modifications based on our analyses, RAIN-GS successfully trains 3D Gaussians from sub-optimal point cloud (e.g., randomly initialized point cloud), effectively relaxing the need for accurate initialization. We demonstrate the efficacy of our strategy through quantitative and qualitative comparisons on multiple datasets, where RAIN-GS trained with random point cloud achieves performance on-par with or even better than 3DGS trained with accurate SfM point cloud. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightweight Predictive 3D Gaussian Splats",
    "authors": [
      "Junli Cao",
      "Vidit Goel",
      "Chaoyang Wang",
      "Anil Kag",
      "Ju Hu",
      "Sergei Korolev",
      "Chenfanfu Jiang",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "Recent approaches representing 3D objects and scenes using Gaussian splats show increased rendering speed across a variety of platforms and devices. While rendering such representations is indeed extremely efficient, storing and transmitting them is often prohibitively expensive. To represent large-scale scenes, one often needs to store millions of 3D Gaussians, occupying gigabytes of disk space. This poses a very practical limitation, prohibiting widespread adoption.Several solutions have been proposed to strike a balance between disk size and rendering quality, noticeably reducing the visual quality. In this work, we propose a new representation that dramatically reduces the hard drive footprint while featuring similar or improved quality when compared to the standard 3D Gaussian splats. When compared to other compact solutions, ours offers higher quality renderings with significantly reduced storage, being able to efficiently run on a mobile device in real-time. Our key observation is that nearby points in the scene can share similar representations. Hence, only a small ratio of 3D points needs to be stored. We introduce an approach to identify such points which are called parent points. The discarded points called children points along with attributes can be efficiently predicted by tiny MLPs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-27",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "compact",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane Representation",
    "authors": [
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Recent advancements in photo-realistic novel view synthesis have been significantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit nature of 3DGS data entails considerable storage requirements, highlighting a pressing need for more efficient data representations. To address this, we present Implicit Gaussian Splatting (IGS), an innovative hybrid model that integrates explicit point clouds with implicit feature embeddings through a multi-level tri-plane architecture. This architecture features 2D feature grids at various resolutions across different levels, facilitating continuous spatial domain representation and enhancing spatial correlations among Gaussian primitives. Building upon this foundation, we introduce a level-based progressive training scheme, which incorporates explicit spatial regularization. This method capitalizes on spatial correlations to enhance both the rendering quality and the compactness of the IGS representation. Furthermore, we propose a novel compression pipeline tailored for both point clouds and 2D feature grids, considering the entropy variations across different levels. Extensive experimental evaluations demonstrate that our algorithm can deliver high-quality rendering using only a few MBs, effectively balancing storage efficiency and rendering fidelity, and yielding results that are competitive with the state-of-the-art.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "compact",
      "lighting",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo",
    "authors": [
      "Yixiao Chen",
      "Bin Liang",
      "Hanzhi Guo",
      "Yongqing Cheng",
      "Jiayi Zhao",
      "Dongdong Weng"
    ],
    "abstract": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency Data Synthesis",
    "authors": [
      "Kang Yang",
      "Gaofeng Dong",
      "Sijie Ji",
      "Wan Du",
      "Mani Srivastava"
    ],
    "abstract": "Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier-Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray-Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-03",
    "categories": [
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "high-fidelity",
      "localization",
      "ar",
      "gaussian splatting",
      "nerf",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction",
    "authors": [
      "Marko Mihajlovic",
      "Sergey Prokudin",
      "Siyu Tang",
      "Robert Maier",
      "Federica Bogo",
      "Tony Tung",
      "Edmond Boyer"
    ],
    "abstract": "Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
    "authors": [
      "Shubhendu Jena",
      "Amine Ouasfi",
      "Mae Younes",
      "Adnane Boukhayma"
    ],
    "abstract": "We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "shape reconstruction",
      "face",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D-HGS: 3D Half-Gaussian Splatting",
    "authors": [
      "Haolin Li",
      "Jinyang Liu",
      "Mario Sznaier",
      "Octavia Camps"
    ],
    "abstract": "Photo-realistic image rendering from 3D scene reconstruction has advanced significantly with neural rendering techniques. Among these, 3D Gaussian Splatting (3D-GS) outperforms Neural Radiance Fields (NeRFs) in quality and speed but struggles with shape and color discontinuities. We propose 3D Half-Gaussian (3D-HGS) kernels as a plug-and-play solution to address these limitations. Our experiments show that 3D-HGS enhances existing 3D-GS methods, achieving state-of-the-art rendering quality without compromising speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-04",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping",
    "authors": [
      "Tengfei Wang",
      "Xin Wang",
      "Yongmao Hou",
      "Yiwei Xu",
      "Wendi Zhang",
      "Zongqian Zhan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-scale urban areas without downsampling the original image resolution. First, the Cross-modal model - Language Segment Anything is leveraged to segment building masks. Then, the segmented building regions is grouped into sub-regions according to the visibility check across registered images. The Gaussian kernels for these sub-regions are optimized in parallel with masked pixels. In addition, the normal loss is re-formulated for the detected edges of masks to alleviate the ambiguities in normal vectors on edges. Finally, to improve the optimization of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts for the complexity of the corresponding scenes, effectively minimizing the thread waiting time in the pixel-parallel rendering stage as well as the reconstruction lost. Extensive experiments are tested on various urban datasets, the results demonstrated the superior performance of our PG-SAG on building surface reconstruction, compared to several state-of-the-art 3DGS-based methods. Project Web:https://github.com/TFWang-9527/PG-SAG.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "authors": [
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Perez-Pellitero"
    ],
    "abstract": "Novel view synthesis has shown rapid progress recently, with methods capable of producing increasingly photorealistic results. 3D Gaussian Splatting has emerged as a promising method, producing high-quality renderings of scenes and enabling interactive viewing at real-time frame rates. However, it is limited to static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct dynamic scenes. We model a scene's dynamics using dynamic MLPs, learning deformations from temporally-local canonical representations to per-frame 3D Gaussians. To disentangle static and dynamic regions, tuneable parameters weigh each Gaussian's respective MLP parameters, improving the dynamics modelling of imbalanced scenes. We introduce a sliding window training strategy that partitions the sequence into smaller manageable windows to handle arbitrary length scenes while maintaining high rendering quality. We propose an adaptive sampling strategy to determine appropriate window size hyperparameters based on the scene's motion, balancing training overhead with visual quality. Training a separate dynamic 3D Gaussian model for each sliding window allows the canonical representation to change, enabling the reconstruction of scenes with significant geometric changes. Temporal consistency is enforced using a fine-tuning step with self-supervising consistency loss on randomly sampled novel views. As a result, our method produces high-quality renderings of general dynamic scenes with competitive quantitative performance, which can be viewed in real-time in our dynamic interactive viewer.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views",
    "authors": [
      "Yaniv Wolf",
      "Amit Bracha",
      "Ron Kimmel"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for accurately representing scenes. However, despite its superior novel view synthesis capabilities, extracting the geometry of the scene directly from the Gaussian properties remains a challenge, as those are optimized based on a photometric loss. While some concurrent models have tried adding geometric constraints during the Gaussian optimization process, they still produce noisy, unrealistic surfaces.   We propose a novel approach for bridging the gap between the noisy 3DGS representation and the smooth 3D mesh representation, by injecting real-world knowledge into the depth extraction process. Instead of extracting the geometry of the scene directly from the Gaussian properties, we instead extract the geometry through a pre-trained stereo-matching model. We render stereo-aligned pairs of images corresponding to the original training poses, feed the pairs into a stereo model to get a depth profile, and finally fuse all of the profiles together to get a single mesh.   The resulting reconstruction is smoother, more accurate and shows more intricate details compared to other methods for surface reconstruction from Gaussian Splatting, while only requiring a small overhead on top of the fairly short 3DGS optimization process.   We performed extensive testing of the proposed method on in-the-wild scenes, obtained using a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the method on the Tanks and Temples and DTU benchmarks, achieving state-of-the-art results.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Go-SLAM: Grounded Object Segmentation and Localization with Gaussian Splatting SLAM",
    "authors": [
      "Phu Pham",
      "Dipam Patel",
      "Damon Conover",
      "Aniket Bera"
    ],
    "abstract": "We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting SLAM to reconstruct dynamic environments while embedding object-level information within the scene representations. This framework employs advanced object segmentation techniques, assigning a unique identifier to each Gaussian splat that corresponds to the object it represents. Consequently, our system facilitates open-vocabulary querying, allowing users to locate objects using natural language descriptions. Furthermore, the framework features an optimal path generation module that calculates efficient navigation paths for robots toward queried objects, considering obstacles and environmental uncertainties. Comprehensive evaluations in various scene settings demonstrate the effectiveness of our approach in delivering high-fidelity scene reconstructions, precise object segmentation, flexible object querying, and efficient robot path planning. This work represents an additional step forward in bridging the gap between 3D scene reconstruction, semantic object understanding, and real-time environment interactions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-25",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "high-fidelity",
      "semantic",
      "localization",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation",
    "authors": [
      "Yuchen Li",
      "Chaoran Feng",
      "Zhenyu Tang",
      "Kaiyuan Deng",
      "Wangbo Yu",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic event dataset for high-fidelity event vision tasks, captured from real-world sparse multi-view RGB images. Existing event datasets are often synthesized from dense RGB videos, which typically lack viewpoint diversity and geometric consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E overcomes these limitations by first reconstructing photorealistic static scenes using 3D Gaussian Splatting, and subsequently employing a novel, physically-informed event simulation pipeline. This pipeline generally integrates adaptive trajectory interpolation with physically-consistent event contrast threshold modeling. Such an approach yields temporally dense and geometrically consistent event streams under diverse motion and lighting conditions, while ensuring strong alignment with underlying scene structures. Experimental results on event-based 3D reconstruction demonstrate GS2E's superior generalization capabilities and its practical value as a benchmark for advancing event vision research.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relightable",
      "neural rendering",
      "illumination",
      "geometry",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "reflection",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Michael Bi Mi",
      "Xinchao Wang"
    ],
    "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GT",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model",
    "authors": [
      "Hongliang Zhong",
      "Can Wang",
      "Jingbo Zhang",
      "Jing Liao"
    ],
    "abstract": "Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Surface Priors for Editable Gaussian Splatting",
    "authors": [
      "Jakub Szymkowiak",
      "Weronika Jakubowska",
      "Dawid Malarz",
      "Weronika Smolak-Dyżewska",
      "Maciej Zięba",
      "Przemyslaw Musialski",
      "Wojtek Pałubicki",
      "Przemysław Spurek"
    ],
    "abstract": "In computer graphics and vision, recovering easily modifiable scene appearance from image data is crucial for applications such as content creation. We introduce a novel method that integrates 3D Gaussian Splatting with an implicit surface representation, enabling intuitive editing of recovered scenes through mesh manipulation. Starting with a set of input images and camera poses, our approach reconstructs the scene surface using a neural signed distance field. This neural surface acts as a geometric prior guiding the training of Gaussian Splatting components, ensuring their alignment with the scene geometry. To facilitate editing, we encode the visual and geometric information into a lightweight triangle soup proxy. Edits applied to the mesh extracted from the neural surface propagate seamlessly through this intermediate structure to update the recovered appearance. Unlike previous methods relying on the triangle soup proxy representation, our approach supports a wider range of modifications and fully leverages the mesh topology, enabling a more flexible and intuitive editing process. The complete source code for this project can be accessed at: https://github.com/WJakubowska/NeuralSurfacePriors.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting",
    "authors": [
      "Xinpeng Liu",
      "Zeyi Huang",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ],
    "abstract": "Novel view synthesis has demonstrated impressive progress recently, with 3D Gaussian splatting (3DGS) offering efficient training time and photorealistic real-time rendering. However, reliance on Cartesian coordinates limits 3DGS's performance on distant objects, which is important for reconstructing unbounded outdoor environments. We found that, despite its ultimate simplicity, using homogeneous coordinates, a concept on the projective geometry, for the 3DGS pipeline remarkably improves the rendering accuracies of distant objects. We therefore propose Homogeneous Gaussian Splatting (HoGS) incorporating homogeneous coordinates into the 3DGS framework, providing a unified representation for enhancing near and distant objects. HoGS effectively manages both expansive spatial positions and scales particularly in outdoor unbounded environments by adopting projective geometry principles. Experiments show that HoGS significantly enhances accuracy in reconstructing distant objects while maintaining high-quality rendering of nearby objects, along with fast training speed and real-time rendering capability. Our implementations are available on our project page https://kh129.github.io/hogs/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Globally Consistent RGB-D SLAM with 2D Gaussian Splatting",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Liren Jin",
      "Marija Popović",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable performance of high-fidelity 3D reconstruction. However, the lack of depth rendering consistency and efficient loop closure limits the quality of its geometric reconstructions and its ability to perform globally consistent mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using 2D Gaussian splatting as the map representation. By leveraging the depth-consistent rendering property of the 2D variant, we propose an accurate camera pose optimization method and achieve geometrically accurate 3D reconstruction. In addition, we implement efficient loop detection and camera relocalization by leveraging MASt3R, a 3D foundation model, and achieve efficient map updates by maintaining a local active map. Experiments show that our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface reconstruction quality, and more consistent global map reconstruction compared to existing rendering-based SLAM methods, while maintaining high-fidelity image rendering and improved computational efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and Mapping",
    "authors": [
      "Ruoyu Fan",
      "Yuhui Wen",
      "Jiajia Dai",
      "Tao Zhang",
      "Long Zeng",
      "Yong-jin Liu"
    ],
    "abstract": "We propose $S^3$LAM, a novel RGB-D SLAM system that leverages 2D surfel splatting to achieve highly accurate geometric representations for simultaneous tracking and mapping. Unlike existing 3DGS-based SLAM approaches that rely on 3D Gaussian ellipsoids, we utilize 2D Gaussian surfels as primitives for more efficient scene representation. By focusing on the surfaces of objects in the scene, this design enables $S^3$LAM to reconstruct high-quality geometry, benefiting both mapping and tracking. To address inherent SLAM challenges including real-time optimization under limited viewpoints, we introduce a novel adaptive surface rendering strategy that improves mapping accuracy while maintaining computational efficiency. We further derive camera pose Jacobians directly from 2D surfel splatting formulation, highlighting the importance of our geometrically accurate representation that improves tracking convergence. Extensive experiments on both synthetic and real-world datasets validate that $S^3$LAM achieves state-of-the-art performance. Code will be made publicly available.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "lighting",
      "slam",
      "ar",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting",
    "authors": [
      "Jens Piekenbrinck",
      "Christian Schmidt",
      "Alexander Hermans",
      "Narunas Vaskevicius",
      "Timm Linder",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
    "authors": [
      "Davide Di Nucci",
      "Matteo Tomei",
      "Guido Borghi",
      "Luca Ciuffreda",
      "Roberto Vezzani",
      "Rita Cucchiara"
    ],
    "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting",
    "authors": [
      "Yuwei Jia",
      "Yutang Lu",
      "Zhe Cui",
      "Fei Su"
    ],
    "abstract": "Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "recognition",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis",
    "authors": [
      "Anand Kumar",
      "Kavinder Roghit Kanthen",
      "Josna John"
    ],
    "abstract": "We can achieve fast and consistent early skin cancer detection with recent developments in computer vision and deep learning techniques. However, the existing skin lesion segmentation and classification prediction models run independently, thus missing potential efficiencies from their integrated execution. To unify skin lesion analysis, our paper presents the Gaussian Splatting - Transformer UNet (GS-TransUNet), a novel approach that synergistically combines 2D Gaussian splatting with the Transformer UNet architecture for automated skin cancer diagnosis. Our unified deep learning model efficiently delivers dual-function skin lesion classification and segmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets, our network demonstrates superior performance compared to existing state-of-the-art models across multiple metrics through 5-fold cross-validation. Our findings illustrate significant advancements in the precision of segmentation and classification. This integration sets new benchmarks in the field and highlights the potential for further research into multi-task medical image analysis methodologies, promising enhancements in automated diagnostic systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "medical",
      "segmentation",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "authors": [
      "Chenguo Lin",
      "Panwang Pan",
      "Bangbang Yang",
      "Zeming Li",
      "Yadong Mu"
    ],
    "abstract": "Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffSplat in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting",
    "authors": [
      "Qianpu Sun",
      "Changyong Shu",
      "Sifan Zhou",
      "Zichen Yu",
      "Yan Chen",
      "Dawei Yang",
      "Yuan Chun"
    ],
    "abstract": "3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering",
    "authors": [
      "Wenkai Zhu",
      "Xu Li",
      "Qimin Xu",
      "Benwu Wang",
      "Kun Wei",
      "Yiming Peng",
      "Zihang Wang"
    ],
    "abstract": "3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human chain-of-thought process for information seeking, we introduce a hierarchical collaborative representation module that facilitates mutual reinforcement for mapping optimization, effectively mitigating scale drift and enhancing reconstruction robustness. Furthermore, to effectively eliminate the influence of dynamic objects, we propose a joint dynamic modeling module that generates fine-grained dynamic masks by fusing open-world segmentation with implicit residual constraints, guided by uncertainty estimates from DINO-Depth features. Extensive evaluations on KITTI, nuScenes, and self-collected datasets demonstrate that our approach achieves state-of-the-art performance compared to existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "human",
      "high-fidelity",
      "outdoor",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Re-Nerfing: Improving Novel View Synthesis through Novel View Synthesis",
    "authors": [
      "Felix Tristram",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "Recent neural rendering and reconstruction techniques, such as NeRFs or Gaussian Splatting, have shown remarkable novel view synthesis capabilities but require hundreds of images of the scene from diverse viewpoints to render high-quality novel views. With fewer images available, these methods start to fail since they can no longer correctly triangulate the underlying 3D geometry and converge to a non-optimal solution. These failures can manifest as floaters or blurry renderings in sparsely observed areas of the scene. In this paper, we propose Re-Nerfing, a simple and general add-on approach that leverages novel view synthesis itself to tackle this problem. Using an already trained NVS method, we render novel views between existing ones and augment the training data to optimize a second model. This introduces additional multi-view constraints and allows the second model to converge to a better solution. With Re-Nerfing we achieve significant improvements upon multiple pipelines based on NeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and LLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra supervision signals, making it a flexible and practical add-on.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text",
    "authors": [
      "Gyumin Shim",
      "Sangmin Lee",
      "Jaegul Choo"
    ],
    "abstract": "In this paper, we introduce GaussianMotion, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting. Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control. In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses. By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner. Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results. Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "efficient rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer",
    "authors": [
      "Filip Slezak",
      "Magnus K. Gjerde",
      "Joakim B. Haurum",
      "Ivan Nikolov",
      "Morten S. Laursen",
      "Thomas B. Moeslund"
    ],
    "abstract": "In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for stereo dataset generation, offering an efficient alternative to Neural Radiance Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore utilizing the reconstructed geometry from the explicit 3D representations as well as depth estimates from the FoundationStereo model in an expert knowledge transfer setup. We find that when fine-tuning stereo models on 3DGS-generated datasets, we demonstrate competitive performance in zero-shot generalization benchmarks. When using the reconstructed geometry directly, we observe that it is often noisy and contains artifacts, which propagate noise to the trained model. In contrast, we find that the disparity estimates from FoundationStereo are cleaner and consequently result in a better performance on the zero-shot generalization benchmarks. Our method highlights the potential for low-cost, high-fidelity dataset creation and fast fine-tuning for deep stereo models. Moreover, we also reveal that while the latest Gaussian Splatting based methods have achieved superior performance on established benchmarks, their robustness falls short in challenging in-the-wild settings warranting further exploration.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM",
    "authors": [
      "Roman Titkov",
      "Egor Zubkov",
      "Dmitry Yudin",
      "Jaafar Mahmoud",
      "Malik Mohrat",
      "Gennady Sidorov"
    ],
    "abstract": "Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM -- a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera's ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: https://titrom025.github.io/LEG-SLAM/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "semantic",
      "slam",
      "motion",
      "robotics",
      "gaussian splatting",
      "mapping",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors",
    "authors": [
      "Rui Xu",
      "Wenyue Chen",
      "Jiepeng Wang",
      "Yuan Liu",
      "Peng Wang",
      "Lin Gao",
      "Shiqing Xin",
      "Taku Komura",
      "Xin Li",
      "Wenping Wang"
    ],
    "abstract": "Gaussian Splattings demonstrate impressive results in multi-view reconstruction based on Gaussian explicit representations. However, the current Gaussian primitives only have a single view-dependent color and an opacity to represent the appearance and geometry of the scene, resulting in a non-compact representation. In this paper, we introduce a new method called SuperGaussians that utilizes spatially varying colors and opacity in a single Gaussian primitive to improve its representation ability. We have implemented bilinear interpolation, movable kernels, and even tiny neural networks as spatially varying functions. Quantitative and qualitative experimental results demonstrate that all three functions outperform the baseline, with the best movable kernels achieving superior novel view synthesis performance on multiple datasets, highlighting the strong potential of spatially varying functions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "compact",
      "lighting",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting",
    "authors": [
      "Yufei Deng",
      "Yuanjian Wang",
      "Rong Xiao",
      "Chenwei Tang",
      "Jizhe Zhou",
      "Jiahao Fan",
      "Deng Xiong",
      "Jiancheng Lv",
      "Huajin Tang"
    ],
    "abstract": "While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view synthesis, its performance degrades with motion blur. In scenarios with rapid motion or low-light conditions, existing RGB-based deblurring methods struggle to model camera pose and radiance changes during exposure, reducing reconstruction accuracy. Event cameras, capturing continuous brightness changes during exposure, can effectively assist in modeling motion blur and improving reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D Gaussians from event streams and severely blurred images. This method jointly learns the parameters of these Gaussians while recovering camera motion trajectories during exposure time. Specifically, we first construct a blur loss function by synthesizing multiple latent sharp images during the exposure time, minimizing the difference between real and synthesized blurred images. Then we use event stream to supervise the light intensity changes between latent sharp images at any time within the exposure period, supplementing the light intensity dynamic changes lost in RGB images. Furthermore, we optimize the latent sharp images at intermediate exposure times based on the event-based double integral (EDI) prior, applying consistency constraints to enhance the details and texture information of the reconstructed images. Extensive experiments on synthetic and real-world datasets show that EBAD-Gaussian can achieve high-quality 3D scene reconstruction under the condition of blurred images and event stream inputs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays",
    "authors": [
      "Sascha Jecklin",
      "Aidana Massalimova",
      "Ruyi Zha",
      "Lilian Calvet",
      "Christoph J. Laux",
      "Mazda Farshad",
      "Philipp Fürnstahl"
    ],
    "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often supported by image-based navigation systems. Recently, supervised learning approaches have gained attention for reconstructing 3D spinal anatomy from sparse fluoroscopic data, significantly reducing reliance on radiation-intensive 3D imaging systems. However, these methods typically require large amounts of annotated training data and may struggle to generalize across varying patient anatomies or imaging conditions. Instance-learning approaches like Gaussian splatting could offer an alternative by avoiding extensive annotation requirements. While Gaussian splatting has shown promise for novel view synthesis, its application to sparse, arbitrarily posed real intraoperative X-rays has remained largely unexplored. This work addresses this limitation by extending the $R^2$-Gaussian splatting framework to reconstruct anatomically consistent 3D volumes under these challenging conditions. We introduce an anatomy-guided radiographic standardization step using style transfer, improving visual consistency across views, and enhancing reconstruction quality. Notably, our framework requires no pretraining, making it inherently adaptable to new patients and anatomies. We evaluated our approach using an ex-vivo dataset. Expert surgical evaluation confirmed the clinical utility of the 3D reconstructions for navigation, especially when using 20 to 30 views, and highlighted the standardization's benefit for anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM) confirmed performance trade-offs compared to idealized settings, but also validated the improvement gained from standardization over raw inputs. This work demonstrates the feasibility of instance-based volumetric reconstruction from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for surgical navigation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "sparse-view",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
    "authors": [
      "Sangwoon Kwak",
      "Joonsoo Kim",
      "Jun Young Jeong",
      "Won-Sik Cheong",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Editing with A Single Image",
    "authors": [
      "Guan Luo",
      "Tian-Xing Xu",
      "Ying-Tian Liu",
      "Xiao-Xiong Fan",
      "Fang-Lue Zhang",
      "Song-Hai Zhang"
    ],
    "abstract": "The modeling and manipulation of 3D scenes captured from the real world are pivotal in various applications, attracting growing research interest. While previous works on editing have achieved interesting results through manipulating 3D meshes, they often require accurately reconstructed meshes to perform editing, which limits their application in 3D content generation. To address this gap, we introduce a novel single-image-driven 3D scene editing approach based on 3D Gaussian Splatting, enabling intuitive manipulation via directly editing the content on a 2D image plane. Our method learns to optimize the 3D Gaussians to align with an edited version of the image rendered from a user-specified viewpoint of the original scene. To capture long-range object deformation, we introduce positional loss into the optimization process of 3D Gaussian Splatting and enable gradient propagation through reparameterization. To handle occluded 3D Gaussians when rendering from the specified viewpoint, we build an anchor-based structure and employ a coarse-to-fine optimization strategy capable of handling long-range deformation while maintaining structural stability. Furthermore, we design a novel masking strategy to adaptively identify non-rigid deformation regions for fine-scale modeling. Extensive experiments show the effectiveness of our method in handling geometric details, long-range, and non-rigid deformation, demonstrating superior editing flexibility and quality compared to previous approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-14",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting",
    "authors": [
      "Wei Sun",
      "Yanzhao Zhou",
      "Jianbin Jiao",
      "Yuan Li"
    ],
    "abstract": "Open-vocabulary 3D scene understanding is crucial for applications requiring natural language-driven spatial interpretation, such as robotics and augmented reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation for scene reconstruction, integrating it with open-vocabulary frameworks reveals a key challenge: cross-view granularity inconsistency. This issue, stemming from 2D segmentation methods like SAM, results in inconsistent object segmentations across views (e.g., a \"coffee set\" segmented as a single entity in one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based methods often rely on isolated per-Gaussian feature learning, neglecting the spatial context needed for cohesive object reasoning, leading to fragmented representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel framework that addresses this challenge by incorporating spatial context into 3DGS. CAGS constructs local graphs to propagate contextual features across Gaussians, reducing noise from inconsistent granularity, employs mask-centric contrastive learning to smooth SAM-derived features across views, and leverages a precomputation strategy to reduce computational cost by precomputing neighborhood relationships, enabling efficient training in large-scale scenes. By integrating spatial context, CAGS significantly improves 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet, enabling robust language-guided 3D scene understanding.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting",
    "authors": [
      "Junzhe Jiang",
      "Chun Gu",
      "Yurui Chen",
      "Li Zhang"
    ],
    "abstract": "LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV",
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "nerf",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention",
    "authors": [
      "Ziao Liu",
      "Zhenjia Li",
      "Yifeng Shi",
      "Xiangang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field",
    "authors": [
      "Haoqin Hong",
      "Ding Fan",
      "Fubin Dou",
      "Zhi-Li Zhou",
      "Haoran Sun",
      "Congcong Zhu",
      "Jingrun Chen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "efficient rendering",
      "high quality",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes",
    "authors": [
      "Yang Liu",
      "Chuanchen Luo",
      "Zhongkai Mao",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10$\\times$ compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting",
    "authors": [
      "Juncong Zhou",
      "Chao Hu",
      "Guanlin Wu",
      "Zixiang Ren",
      "Han Hu",
      "Juyong Zhang",
      "Rui Zhang",
      "Jie Xu"
    ],
    "abstract": "This paper investigates the construction of channel knowledge map (CKM) from sparse channel measurements. Dif ferent from conventional two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station configurations, we present a six-dimensional (6D) CKM framework named bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod eling wireless channels across dynamic transmitter (Tx) and receiver (Rx) positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual scatterer clusters and environmental obstacles in the wireless environment. By properly learning the bidirectional scattering patterns and complex attenuation profiles based on channel measurements, these ellipsoids inherently cap ture the electromagnetic transmission characteristics of wireless environments, thereby accurately modeling signal transmission under varying transceiver configurations. Experiment results show that BiWGS significantly outperforms classic multi-layer perception (MLP) for the construction of 6D channel power gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction accuracy comparable to the state-of-the art wireless radiation field Gaussian splatting (WRF-GS) for 3D CKM construction. This validates the capability of the proposed BiWGS in accomplishing dimensional expansion of 6D CKM construction, without compromising fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-30",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction",
    "authors": [
      "Wenhua Wu",
      "Tong Zhao",
      "Chensheng Peng",
      "Lei Yang",
      "Yintao Wei",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typically rely on multi-view image inputs and require prolonged optimization times. In this paper, we propose BEV-GS, a real-time single-frame road surface reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists of a prediction module and a rendering module. The prediction module introduces separate geometry and texture networks following Bird's-Eye-View paradigm. Geometric and texture parameters are directly estimated from a single frame, avoiding per-scene optimization. In the rendering module, we utilize grid Gaussian for road surface representation and novel view synthesis, which better aligns with road surface characteristics. Our method achieves state-of-the-art performance on the real-world dataset RSRD. The road elevation error reduces to 1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and real-time applications. The code will be available at: \\href{https://github.com/cat-wwh/BEV-GS}{\\texttt{https://github.com/cat-wwh/BEV-GS}}",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-16",
    "categories": [
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation",
    "authors": [
      "Maximilian Fehrentz",
      "Alexander Winkler",
      "Thomas Heiliger",
      "Nazim Haouchine",
      "Christian Heiliger",
      "Nassir Navab"
    ],
    "abstract": "We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at https://maxfehrentz.github.io/ct-informed-splatting/ .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis",
    "authors": [
      "Chen Zhao",
      "Xuan Wang",
      "Tong Zhang",
      "Saqib Javed",
      "Mathieu Salzmann"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A $\\mathbfΔ$-model and a $\\mathbfΣ$-model are jointly trained on the available images. The $\\mathbfΔ$-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the $\\mathbfΣ$-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the $\\mathbfΣ$-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TranSplat: Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation",
    "authors": [
      "Jeongyun Kim",
      "Jeongho Noh",
      "Dong-Guw Lee",
      "Ayoung Kim"
    ],
    "abstract": "Transparent object manipulation remains a significant challenge in robotics due to the difficulty of acquiring accurate and dense depth measurements. Conventional depth sensors often fail with transparent objects, resulting in incomplete or erroneous depth data. Existing depth completion methods struggle with interframe consistency and incorrectly model transparent objects as Lambertian surfaces, leading to poor depth reconstruction. To address these challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian Splatting method tailored for transparent objects. TranSplat uses a latent diffusion model to generate surface embeddings that provide consistent and continuous representations, making it robust to changes in viewpoint and lighting. By integrating these surface embeddings with input RGB images, TranSplat effectively captures the complexities of transparent surfaces, enhancing the splatting of 3D Gaussians and improving depth completion. Evaluations on synthetic and real-world transparent object benchmarks, as well as robot grasping tasks, show that TranSplat achieves accurate and dense depth completion, demonstrating its effectiveness in practical applications. We open-source synthetic dataset and model: https://github. com/jeongyun0609/TranSplat",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "robotics",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects",
    "authors": [
      "Qiaojun Yu",
      "Xibin Yuan",
      "Yu jiang",
      "Junting Chen",
      "Dongzhe Zheng",
      "Ce Hao",
      "Yang You",
      "Yixing Chen",
      "Yao Mu",
      "Liu Liu",
      "Cewu Lu"
    ],
    "abstract": "Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: https://sites.google.com/view/artgs/home",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-03",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "robotics",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization",
    "authors": [
      "Maxime Pietrantoni",
      "Gabriela Csurka",
      "Torsten Sattler"
    ],
    "abstract": "Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "localization",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping",
    "authors": [
      "Yuhang Zheng",
      "Xiangyu Chen",
      "Yupeng Zheng",
      "Songen Gu",
      "Runyi Yang",
      "Bu Jin",
      "Pengfei Li",
      "Chengliang Zhong",
      "Zengmao Wang",
      "Lina Liu",
      "Chao Yang",
      "Dawei Wang",
      "Zhen Chen",
      "Xiaoxiao Long",
      "Meiqing Wang"
    ],
    "abstract": "Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-14",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "geometry",
      "robotics",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ],
    "abstract": "Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "localization",
      "ar",
      "gaussian splatting",
      "avatar",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction",
    "authors": [
      "Ke Song",
      "Yunhe Wu",
      "Chunchit Siu",
      "Huiyuan Xiong"
    ],
    "abstract": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a Single Image",
    "authors": [
      "Han Yan",
      "Mingrui Zhang",
      "Yang Li",
      "Chao Ma",
      "Pan Ji"
    ],
    "abstract": "We present PhyCAGE, the first approach for physically plausible compositional 3D asset generation from a single image. Given an input image, we first generate consistent multi-view images for components of the assets. These images are then fitted with 3D Gaussian Splatting representations. To ensure that the Gaussians representing objects are physically compatible with each other, we introduce a Physical Simulation-Enhanced Score Distillation Sampling (PSE-SDS) technique to further optimize the positions of the Gaussians. It is achieved by setting the gradient of the SDS loss as the initial velocity of the physical simulation, allowing the simulator to act as a physics-guided optimizer that progressively corrects the Gaussians' positions to a physically compatible state. Experimental results demonstrate that the proposed method can generate physically plausible compositional 3D assets given a single image.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives",
    "authors": [
      "Jiadi Cui",
      "Junming Cao",
      "Fuqiang Zhao",
      "Zhipeng He",
      "Yifan Chen",
      "Yuhui Zhong",
      "Lan Xu",
      "Yujiao Shi",
      "Yingliang Zhang",
      "Jingyi Yu"
    ],
    "abstract": "Large garages are ubiquitous yet intricate scenes that present unique challenges due to their monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction often fail in these environments due to poor correspondence construction. To address these challenges, we introduce LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate data acquisition. Using this Polar device, we present the GarageWorld dataset, consisting of eight expansive garage scenes with diverse geometric structures, which will be made publicly available for further research. Our approach demonstrates that LiDAR point clouds collected by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We introduce a novel depth regularizer that effectively eliminates floating artifacts in rendered images. Additionally, we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail (LOD) rendering. This includes adapted scaling factors for individual levels and a random-resolution-level training scheme to optimize the Gaussians across different resolutions. This representation enables efficient rendering of large-scale garage scenes on lightweight devices via a web-based renderer. Experimental results on our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the superiority of our method in terms of rendering quality and resource efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "lightweight",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos",
    "authors": [
      "Wenkang Zhang",
      "Yan Zhao",
      "Qiang Wang",
      "Li Song",
      "Zhengxue Cheng"
    ],
    "abstract": "Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-08",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting",
    "authors": [
      "Lingting Zhu",
      "Zhao Wang",
      "Jiahao Cui",
      "Zhenchao Jin",
      "Guying Lin",
      "Lequan Yu"
    ],
    "abstract": "Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision with spatial-temporal weight masks to optimize 3D targets with tool occlusion from a single viewpoint, and surface-aligned regularization terms to capture the much better geometry. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at https://github.com/HKU-MedAI/EndoGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-21",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Text-to-3D using Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Feng Wang",
      "Yikai Wang",
      "Huaping Liu"
    ],
    "abstract": "Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent state-of-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and addressing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with delicate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our code is available at https://github.com/gsgen3d/gsgen",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-09-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment for Accelerated 3D Mesh Reconstruction",
    "authors": [
      "Shengchao Zhao",
      "Yundong Li"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS), which lead to high-quality novel view synthesis and accelerated rendering, have remarkably improved the quality of radiance field reconstruction. However, the extraction of mesh from a massive number of minute 3D Gaussian points remains great challenge due to the large volume of Gaussians and difficulty of representation of sharp signals caused by their inherent low-pass characteristics. To address this issue, we propose DyGASR, which utilizes generalized exponential function instead of traditional 3D Gaussian to decrease the number of particles and dynamically optimize the representation of the captured signal. In addition, it is observed that reconstructing mesh with Generalized Exponential Splatting(GES) without modifications frequently leads to failures since the generalized exponential distribution centroids may not precisely align with the scene surface. To overcome this, we adopt Sugar's approach and introduce Generalized Surface Regularization (GSR), which reduces the smallest scaling vector of each point cloud to zero and ensures normal alignment perpendicular to the surface, facilitating subsequent Poisson surface mesh reconstruction. Additionally, we propose a dynamic resolution adjustment strategy that utilizes a cosine schedule to gradually increase image resolution from low to high during the training stage, thus avoiding constant full resolution, which significantly boosts the reconstruction speed. Our approach surpasses existing 3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations on various scene datasets, demonstrating a 25\\% increase in speed, and a 30\\% reduction in memory usage.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ],
    "abstract": "Spike cameras, as an innovative neuromorphic camera that captures scenes with the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Previous spike-based 3D reconstruction approaches often employ a casecased pipeline: starting with high-quality image reconstruction from spike streams based on established spike-to-image reconstruction algorithms, then progressing to camera pose estimation and 3D reconstruction. However, this cascaded approach suffers from substantial cumulative errors, where quality limitations of initial image reconstructions negatively impact pose estimation, ultimately degrading the fidelity of the 3D reconstruction. To address these issues, we propose a synergistic optimization framework, \\textbf{USP-Gaussian}, that unifies spike-based image reconstruction, pose correction, and Gaussian splatting into an end-to-end framework. Leveraging the multi-view consistency afforded by 3DGS and the motion capture capability of the spike camera, our framework enables a joint iterative optimization that seamlessly integrates information between the spike-to-image network and 3DGS. Experiments on synthetic datasets with accurate poses demonstrate that our method surpasses previous approaches by effectively eliminating cascading errors. Moreover, we integrate pose optimization to achieve robust 3D reconstruction in real-world scenarios with inaccurate initial poses, outperforming alternative methods by effectively reducing noise and preserving fine texture details. Our code, data and trained models will be available at https://github.com/chenkang455/USP-Gaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene",
    "authors": [
      "Xi Shi",
      "Lingli Chen",
      "Peng Wei",
      "Xi Wu",
      "Tian Jiang",
      "Yonggang Luo",
      "Lecheng Xie"
    ],
    "abstract": "Existing Gaussian splatting methods often fall short in achieving satisfactory novel view synthesis in driving scenes, primarily due to the absence of crafty designs and geometric constraints for the involved elements. This paper introduces a novel neural rendering method termed Decoupled Hybrid Gaussian Splatting (DHGS), targeting at promoting the rendering quality of novel view synthesis for static driving scenes. The novelty of this work lies in the decoupled and hybrid pixel-level blender for road and non-road layers, without the conventional unified differentiable rendering logic for the entire scene. Still, consistency and continuity in superimposition are preserved through the proposed depth-ordered hybrid rendering strategy. Additionally, an implicit road representation comprised of a Signed Distance Function (SDF) is trained to supervise the road surface with subtle geometric attributes. Accompanied by the use of auxiliary transmittance loss and consistency loss, novel images with imperceptible boundary and elevated fidelity are ultimately obtained. Substantial experiments on the Waymo dataset prove that DHGS outperforms the state-of-the-art methods. The project page where more video evidences are given is: https://ironbrotherstyle.github.io/dhgs_web.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "neural rendering"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models",
    "authors": [
      "Marcel Rogge",
      "Didier Stricker"
    ],
    "abstract": "Current Gaussian Splatting approaches are effective for reconstructing entire scenes but lack the option to target specific objects, making them computationally expensive and unsuitable for object-specific applications. We propose a novel approach that leverages object masks to enable targeted reconstruction, resulting in object-centric models. Additionally, we introduce an occlusion-aware pruning strategy to minimize the number of Gaussians without compromising quality. Our method reconstructs compact object models, yielding object-centric Gaussian and mesh representations that are up to 96% smaller and up to 71% faster to train compared to the baseline while retaining competitive quality. These representations are immediately usable for downstream applications such as appearance editing and physics simulation without additional processing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "compact"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting",
    "authors": [
      "Tony Yu",
      "Yanlin Jin",
      "Ashok Veeraraghavan",
      "Akshat Dave",
      "Guha Balakrishnan"
    ],
    "abstract": "We present TranSplat, a 3D scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the Gaussian Splatting framework. Our approach addresses two critical challenges: (1) precise 3D object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. TranSplat fits a splatting model to the source scene, using 2D object masks to drive fine-grained 3D segmentation. Following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, TranSplat derives per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. This relighting strategy does not require explicitly estimating physical scene properties such as BRDFs. Evaluated on several synthetic and real-world scenes and objects, TranSplat yields excellent 3D object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. We conclude by discussing the limitations of the approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting",
    "authors": [
      "Feng Yang",
      "Wenliang Qian",
      "Wangmeng Zuo",
      "Hui Li"
    ],
    "abstract": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reinforcement Learning with Generalizable Gaussian Splatting",
    "authors": [
      "Jiaxu Wang",
      "Qiang Zhang",
      "Jingkai Sun",
      "Jiahang Cao",
      "Gang Han",
      "Wen Zhao",
      "Weining Zhang",
      "Yecheng Shao",
      "Yijie Guo",
      "Renjing Xu"
    ],
    "abstract": "An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box\", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Adaptive Light Aggregation and Diffusion-Guided Material Priors",
    "authors": [
      "Kang Du",
      "Zhihao Liang",
      "Yulin Shen",
      "Zeyu Wang"
    ],
    "abstract": "Gaussian Splatting (GS) has emerged as an effective representation for photorealistic rendering, but the underlying geometry, material, and lighting remain entangled, hindering scene editing. Existing GS-based methods struggle to disentangle these components under non-Lambertian conditions, especially in the presence of specularities and shadows. We propose \\textbf{GS-ID}, an end-to-end framework for illumination decomposition that integrates adaptive light aggregation with diffusion-based material priors. In addition to a learnable environment map for ambient illumination, we model spatially-varying local lighting using anisotropic spherical Gaussian mixtures (SGMs) that are jointly optimized with scene content. To better capture cast shadows, we associate each splat with a learnable unit vector that encodes shadow directions from multiple light sources, further improving material and lighting estimation. By combining SGMs with intrinsic priors from diffusion models, GS-ID significantly reduces ambiguity in light-material-geometry interactions and achieves state-of-the-art performance on inverse rendering and relighting benchmarks. Experiments also demonstrate the effectiveness of GS-ID for downstream applications such as relighting and scene composition.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "illumination",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Grid4D: 4D Decomposed Hash Encoding for High-Fidelity Dynamic Gaussian Splatting",
    "authors": [
      "Jiawei Xu",
      "Zexin Fan",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "Recently, Gaussian splatting has received more and more attention in the field of static scene rendering. Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models. However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality. To tackle these problems, we propose Grid4D, a dynamic scene rendering model based on Gaussian splatting and employing a novel explicit encoding method for the 4D input through the hash encoding. Different from plane-based explicit representations, we decompose the 4D encoding into one spatial and three temporal 3D hash encodings without the low-rank assumption. Additionally, we design a novel attention module that generates the attention scores in a directional range to aggregate the spatial and temporal features. The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features. Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction. Our experiments demonstrate that Grid4D significantly outperforms the state-of-the-art models in visual quality and rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term Service Robots",
    "authors": [
      "Bin Fu",
      "Jialin Li",
      "Bin Zhang",
      "Ruiping Wang",
      "Xilin Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has garnered significant attention in robotics for its explicit, high fidelity dense scene representation, demonstrating strong potential for robotic applications. However, 3DGS-based methods in robotics primarily focus on static scenes, with limited attention to the dynamic scene changes essential for long-term service robots. These robots demand sustained task execution and efficient scene updates-challenges current approaches fail to meet. To address these limitations, we propose GS-LTS (Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor robots to manage diverse tasks in dynamic environments over time. GS-LTS detects scene changes (e.g., object addition or removal) via single-image change detection, employs a rule-based policy to autonomously collect multi-view observations, and efficiently updates the scene representation through Gaussian editing. Additionally, we propose a simulation-based benchmark that automatically generates scene change data as compact configuration scripts, providing a standardized, user-friendly evaluation benchmark. Experimental results demonstrate GS-LTS's advantages in reconstruction, navigation, and superior scene updates-faster and higher quality than the image training baseline-advancing 3DGS for long-term robotic operations. Code and benchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compact",
      "robotics",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats",
    "authors": [
      "Kai Deng",
      "Yigong Zhang",
      "Jian Yang",
      "Jin Xie"
    ],
    "abstract": "Tracking and mapping in large-scale, unbounded outdoor environments using only monocular RGB input presents substantial challenges for existing SLAM systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) SLAM methods are typically limited to small, bounded indoor settings. To overcome these challenges, we introduce GigaSLAM, the first RGB NeRF / 3DGS-based SLAM framework for kilometer-scale outdoor environments, as demonstrated on the KITTI, KITTI 360, 4 Seasons and A2D2 datasets. Our approach employs a hierarchical sparse voxel map representation, where Gaussians are decoded by neural networks at multiple levels of detail. This design enables efficient, scalable mapping and high-fidelity viewpoint rendering across expansive, unbounded scenes. For front-end tracking, GigaSLAM utilizes a metric depth model combined with epipolar geometry and PnP algorithms to accurately estimate poses, while incorporating a Bag-of-Words-based loop closure mechanism to maintain robust alignment over long trajectories. Consequently, GigaSLAM delivers high-precision tracking and visually faithful rendering on urban outdoor benchmarks, establishing a robust SLAM solution for large-scale, long-term scenarios, and significantly extending the applicability of Gaussian Splatting SLAM systems to unbounded outdoor environments. GitHub: https://github.com/DengKaiCQ/GigaSLAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "outdoor",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum",
    "authors": [
      "Yaokun Li",
      "Lihe Ding",
      "Xiao Chen",
      "Guang Tan",
      "Tianfan Xue"
    ],
    "abstract": "Generating dynamic and interactive 3D objects, such as trees, has wide applications in virtual reality, games, and world simulation. Nevertheless, existing methods still face various challenges in generating realistic 4D motion for complex real trees. In this paper, we propose DynamicTree, the first framework that can generate long-term, interactive animation of 3D Gaussian Splatting trees. Unlike prior optimization-based methods, our approach generates dynamics in a fast feed-forward manner. The key success of our approach is the use of a compact sparse voxel spectrum to represent the tree movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline first generates mesh motion using the sparse voxel spectrum and then binds Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum can also serve as a basis for fast modal analysis under external forces, allowing real-time interactive responses. To train our model, we also introduce 4DTree, the first large-scale synthetic 4D tree dataset containing 8,786 animated tree meshes with semantic labels and 100-frame motion sequences. Extensive experiments demonstrate that our method achieves realistic and responsive tree animations, significantly outperforming existing approaches in both visual quality and computational efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "animation",
      "compact",
      "semantic",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting",
    "authors": [
      "Zhiyin Qian",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Andreas Geiger",
      "Siyu Tang"
    ],
    "abstract": "We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "human",
      "ar",
      "gaussian splatting",
      "avatar",
      "nerf",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering",
    "authors": [
      "Yilan Dong",
      "Haohe Liu",
      "Qing Wang",
      "Jiahao Yang",
      "Wenqing Wang",
      "Gregory Slabaugh",
      "Shanxin Yuan"
    ],
    "abstract": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on rigid skeletal motion with an oversimplified non-rigid motion model, which fails to capture fine geometric and appearance details. Additionally, they perform densification based solely on per-point gradients and process poses independently, ignoring spatial and temporal correlations. These limitations lead to geometric detail loss, temporal instability, and inefficient point distribution. To address these issues, we propose HandSplat, a novel Gaussian Splatting-based framework that enhances both fidelity and stability for hand rendering. To improve fidelity, we extend standard 3DGS attributes with implicit geometry and appearance embeddings for finer non-rigid motion modeling while preserving the static hand characteristic modeled by original 3DGS attributes. Additionally, we introduce a local gradient-aware densification strategy that dynamically refines Gaussian density in high-variation regions. To improve stability, we incorporate pose-conditioned attribute regularization to encourage attribute consistency across similar poses, mitigating temporal artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat surpasses existing methods in fidelity and stability while achieving real-time performance. We will release the code and pre-trained models upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeMS: Efficient Gaussian Splatting for Extreme Motion Blur",
    "authors": [
      "Gopi Raju Matta",
      "Trisha Reddypalli",
      "Vemunuri Divya Madhuri",
      "Kaushik Mitra"
    ],
    "abstract": "We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to handle severely motion-blurred images. State-of-the-art deblurring methods for extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches like Deblur-GS, typically assume access to sharp images for camera pose estimation and point cloud generation, an unrealistic assumption. Methods relying on COLMAP initialization, such as BAD-Gaussians, also fail due to unreliable feature correspondences under severe blur. To address these challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep learning-based Structure-from-Motion pipeline that estimates poses and generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which enables robust scene initialization by treating Gaussians as samples from a probability distribution, eliminating heuristic densification and pruning; and (3) joint optimization of camera trajectories and Gaussian parameters for stable reconstruction. While this pipeline produces strong results, inaccuracies may remain when all inputs are severely blurred. To mitigate this, we propose GeMS-E, which integrates a progressive refinement step using events: (4) Event-based Double Integral (EDI) deblurring restores sharper images that are then fed into GeMS, improving pose estimation, point cloud generation, and overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets. To our knowledge, this is the first framework to address extreme motion blur within 3DGS directly from severely blurred inputs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OMG: Opacity Matters in Material Modeling with Gaussian Splatting",
    "authors": [
      "Silong Yong",
      "Venkata Nagarjun Pudureddiyur Manivannan",
      "Bernhard Kerbl",
      "Zifu Wan",
      "Simon Stepputtis",
      "Katia Sycara",
      "Yaqi Xie"
    ],
    "abstract": "Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting has boosted it to the next level by showing real-time rendering potentials. An intuitive finding is that the models used for inverse rendering do not take into account the dependency of opacity w.r.t. material properties, namely cross section, as suggested by optics. Therefore, we develop a novel approach that adds this dependency to the modeling itself. Inspired by radiative transfer, we augment the opacity term by introducing a neural network that takes as input material properties to provide modeling of cross section and a physically correct activation function. The gradients for material properties are therefore not only from color but also from opacity, facilitating a constraint for their optimization. Therefore, the proposed method incorporates more accurate physical properties compared to previous works. We implement our method into 3 different baselines that use Gaussian Splatting for inverse rendering and achieve significant improvements universally in terms of novel view synthesis and material modeling.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting",
    "authors": [
      "Shuxin Liang",
      "Yihan Xiao",
      "Wenlu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: https://github.com/Shuxin-Liang/InnerGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-18",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "face",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians",
    "authors": [
      "Weihang Liu",
      "Yuhui Zhong",
      "Yuke Li",
      "Xi Chen",
      "Jiadi Cui",
      "Honglong Zhang",
      "Lan Xu",
      "Xin Lou",
      "Yujiao Shi",
      "Jingyi Yu",
      "Yingliang Zhang"
    ],
    "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for applications such as AR navigation, UAV based inspection, and smart city digital twins. While aerial imagery offers broad coverage and complements limitations of ground-based data, reconstructing city-scale environments from such views remains challenging due to occlusions, incomplete geometry, and high memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve scalability and visual quality but remain limited by dense primitive usage, long training times, and poor suit ability for edge devices. We propose CityGo, a hybrid framework that combines textured proxy geometry with residual and surrounding 3D Gaussians for lightweight, photorealistic rendering of urban scenes from aerial perspectives. Our approach first extracts compact building proxy meshes from MVS point clouds, then uses zero order SH Gaussians to generate occlusion-free textures via image-based rendering and back-projection. To capture high-frequency details, we introduce residual Gaussians placed based on proxy-photo discrepancies and guided by depth priors. Broader urban context is represented by surrounding Gaussians, with importance-aware downsampling applied to non-critical regions to reduce redundancy. A tailored optimization strategy jointly refines proxy textures and Gaussian parameters, enabling real-time rendering of complex urban scenes on mobile GPUs with significantly reduced training and memory requirements. Extensive experiments on real-world aerial datasets demonstrate that our hybrid representation significantly reduces training time, achieving on average 1.4x speedup, while delivering comparable visual fidelity to pure 3D Gaussian Splatting approaches. Furthermore, CityGo enables real-time rendering of large-scale urban scenes on mobile consumer GPUs, with substantially reduced memory usage and energy consumption.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-27",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "geometry",
      "compact",
      "urban scene",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling",
    "authors": [
      "Yuru Xiao",
      "Zihan Lin",
      "Chao Lu",
      "Deming Zhai",
      "Kui Jiang",
      "Wenbo Zhao",
      "Wei Zhang",
      "Junjun Jiang",
      "Huanran Wang",
      "Xianming Liu"
    ],
    "abstract": "Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "urban scene",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation",
    "authors": [
      "Lukas Meyer",
      "Josef Grün",
      "Maximilian Weiherer",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "abstract": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS) framework that is able to generate multi-view consistent novel views from images of multiple, independent cameras with different spectral domains. In contrast to previous approaches, our method does not require cross-modal camera calibration and is versatile enough to model a variety of different spectra, including thermal and near-infra red, without any algorithmic changes.   Unlike existing 3DGS-based frameworks that treat each modality separately (by optimizing per-channel spherical harmonics) and therefore fail to exploit the underlying spectral and spatial correlations, our method leverages a novel neural color representation that encodes multi-spectral information into a learned, compact, per-splat feature embedding. A shallow multi-layer perceptron (MLP) then decodes this embedding to obtain spectral color values, enabling joint learning of all bands within a unified representation.   Our experiments show that this simple yet effective strategy is able to improve multi-spectral rendering quality, while also leading to improved per-spectra rendering quality over state-of-the-art methods. We demonstrate the effectiveness of this new technique in agricultural applications to render vegetation indices, such as normalized difference vegetation index (NDVI).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "compact",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping",
    "authors": [
      "Tianhao Wu",
      "Jing Yang",
      "Zhilin Guo",
      "Jingyi Wan",
      "Fangcheng Zhong",
      "Cengiz Oztireli"
    ],
    "abstract": "By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "geometry",
      "body",
      "head",
      "gaussian splatting",
      "mapping",
      "avatar",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with Gaussian Splatting for Enhanced Human Avatars",
    "authors": [
      "Xiaonuo Dongye",
      "Hanzhi Guo",
      "Le Luo",
      "Haiyan Jiang",
      "Yihua Bao",
      "Zeyu Tian",
      "Dongdong Weng"
    ],
    "abstract": "With the advancement of virtual reality, the demand for 3D human avatars is increasing. The emergence of Gaussian Splatting technology has enabled the rendering of Gaussian avatars with superior visual quality and reduced computational costs. Despite numerous methods researchers propose for implementing drivable Gaussian avatars, limited attention has been given to balancing visual quality and computational costs. In this paper, we introduce LoDAvatar, a method that introduces levels of detail into Gaussian avatars through hierarchical embedding and selective detail enhancement methods. The key steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian optimization, and selective detail enhancement. We conducted experiments involving Gaussian avatars at various levels of detail, employing both objective assessments and subjective evaluations. The outcomes indicate that incorporating levels of detail into Gaussian avatars can decrease computational costs during rendering while upholding commendable visual quality, thereby enhancing runtime frame rates. We advocate adopting LoDAvatar to render multiple dynamic Gaussian avatars or extensive Gaussian scenes to balance visual quality and computational costs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-28",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "dynamic",
      "ar",
      "gaussian splatting",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lagrangian Hashing for Compressed Neural Field Representations",
    "authors": [
      "Shrisudhan Govindarajan",
      "Zeno Sambugaro",
      "Akhmedkhan",
      "Shabanov",
      "Towaki Takikawa",
      "Daniel Rebain",
      "Weiwei Sun",
      "Nicola Conci",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "abstract": "We present Lagrangian Hashing, a representation for neural fields combining the characteristics of fast training NeRF methods that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We achieve this by incorporating a point-based representation into the high-resolution layers of the hierarchical hash tables of an InstantNGP representation. As our points are equipped with a field of influence, our representation can be interpreted as a mixture of Gaussians stored within the hash table. We propose a loss that encourages the movement of our Gaussians towards regions that require more representation budget to be sufficiently well represented. Our main finding is that our representation allows the reconstruction of signals using a more compact representation without compromising quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks",
    "authors": [
      "Ruihan Xu",
      "Anthony Opipari",
      "Joshua Mah",
      "Stanley Lewis",
      "Haoran Zhang",
      "Hanzhe Guo",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as an approach for SO(2)-Equivariant 3D object reconstruction from single-view image observations.   GSNs take a single observation as input to generate a Gaussian splat representation describing the observed object's geometry and texture. By using a shared feature extractor before decoding Gaussian colors, covariances, positions, and opacities, GSNs achieve extremely high throughput (>150FPS). Experiments demonstrate that GSNs can be trained efficiently using a multi-view rendering loss and are competitive, in quality, with expensive diffusion-based reconstruction algorithms. The GSN model is validated on multiple benchmark experiments. Moreover, we demonstrate the potential for GSNs to be used within a robotic manipulation pipeline for object-centric grasping.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "geometry",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors",
    "authors": [
      "Yijia Guo",
      "Liwen Hu",
      "Yuanxi Bai",
      "Jiawei Yao",
      "Lei Ma",
      "Tiejun Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images. Fulfilling this requirement can be challenging in real-world scenarios especially when the camera moves fast, which severely limits the application of 3DGS. To address these challenges, we proposed Spike Gausian Splatting (SpikeGS), the first framework that integrates the spike streams into 3DGS pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With accumulation rasterization, interval supervision, and a specially designed pipeline, SpikeGS extracts detailed geometry and texture from high temporal resolution but texture lacking spike stream, reconstructs 3D scenes captured in 1 second. Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of SpikeGS compared with existing spike-based and deblur 3D scene reconstruction methods. Codes and data will be released soon.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering",
    "authors": [
      "Yuanhao Gong",
      "Lantao Yu",
      "Guanghui Yue"
    ],
    "abstract": "The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\\bf 100X} faster without losing the geometry representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Gaussian Splatting for Unbounded 3D City Generation",
    "authors": [
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ],
    "abstract": "3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation. However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2. In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass. Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation. 2) Spatial-aware Gaussian Attribute Decoder: We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points. Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "nerf",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FHGS: Feature-Homogenized Gaussian Splatting",
    "authors": [
      "Q. G. Duan",
      "Benyun Zhao",
      "Mingqiao Han Yijun Huang",
      "Ben M. Chen"
    ],
    "abstract": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently achieved notable advances. Although 3DGS related methods have efficient rendering capabilities, they fail to address the inherent contradiction between the anisotropic color representation of gaussian primitives and the isotropic requirements of semantic features, leading to insufficient cross-view feature consistency. To overcome the limitation, we proposes $\\textit{FHGS}$ (Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework inspired by physical models, which can achieve high-precision mapping of arbitrary 2D features from pre-trained models to 3D scenes while preserving the real-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$ introduces the following innovations: Firstly, a universal feature fusion architecture is proposed, enabling robust embedding of large-scale pre-trained models' semantic features (e.g., SAM, CLIP) into sparse 3D structures. Secondly, a non-differentiable feature fusion mechanism is introduced, which enables semantic features to exhibit viewpoint independent isotropic distributions. This fundamentally balances the anisotropic rendering of gaussian primitives and the isotropic expression of features; Thirdly, a dual-driven optimization strategy inspired by electric potential fields is proposed, which combines external supervision from semantic feature fields with internal primitive clustering guidance. This mechanism enables synergistic optimization of global semantic alignment and local structural consistency. More interactive results can be accessed on: https://fhgs.cuastro.org/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "semantic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization",
    "authors": [
      "Phuong Mai Dinh",
      "Van-Nam Huynh"
    ],
    "abstract": "Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-22",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FreeGaussian: Annotation-free Controllable 3D Gaussian Splats with Flow Derivatives",
    "authors": [
      "Qizhi Chen",
      "Delin Qu",
      "Junli Liu",
      "Yiwen Tang",
      "Haoming Song",
      "Dong Wang",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Reconstructing controllable Gaussian splats from monocular video is a challenging task due to its inherently insufficient constraints. Widely adopted approaches supervise complex interactions with additional masks and control signal annotations, limiting their real-world applications. In this paper, we propose an annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian motion from optical flow and camera motion using novel dynamic Gaussian constraints. By establishing a connection between 2D flows and 3D Gaussian dynamic control, our method enables self-supervised optimization and continuity of dynamic Gaussian motions from flow priors. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state with a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Quantitative and qualitative evaluations on extensive experiments demonstrate the state-of-the-art visual performance and control capability of our method. Project page: https://freegaussian.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "motion",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CF3: Compact and Fast 3D Feature Fields",
    "authors": [
      "Hyunjoon Lee",
      "Joonkyu Min",
      "Jaesik Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes",
    "authors": [
      "Xiaoyu Zhou",
      "Zhiwei Lin",
      "Xiaojun Shan",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "abstract": "We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes. For complex scenes with moving objects, we first sequentially and progressively model the static background of the entire scene with incremental static 3D Gaussians. We then leverage a composite dynamic Gaussian graph to handle multiple moving objects, individually reconstructing each object and restoring their accurate positions and occlusion relationships within the scene. We further use a LiDAR prior for Gaussian Splatting to reconstruct scenes with greater details and maintain panoramic consistency. DrivingGaussian outperforms existing methods in dynamic driving scene reconstruction and enables photorealistic surround-view synthesis with high-fidelity and multi-camera consistency. Our project page is at: https://github.com/VDIGPKU/DrivingGaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Blendshapes for Head Avatar Animation",
    "authors": [
      "Shengjie Ma",
      "Yanlin Weng",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "animation",
      "high-fidelity",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
    "authors": [
      "Zhengqing Gao",
      "Dongting Hu",
      "Jia-Wang Bian",
      "Huan Fu",
      "Yan Li",
      "Tongliang Liu",
      "Mingming Gong",
      "Kun Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "lightweight",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AI-Driven Stylization of 3D Environments",
    "authors": [
      "Yuanbo Chen",
      "Yixiao Kang",
      "Yukun Song",
      "Cyrus Vachha",
      "Sining Huang"
    ],
    "abstract": "In this system, we discuss methods to stylize a scene of 3D primitive objects into a higher fidelity 3D scene using novel 3D representations like NeRFs and 3D Gaussian Splatting. Our approach leverages existing image stylization systems and image-to-3D generative models to create a pipeline that iteratively stylizes and composites 3D objects into scenes. We show our results on adding generated objects into a scene and discuss limitations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images",
    "authors": [
      "Emanuel Garbin",
      "Guy Adam",
      "Oded Krams",
      "Zohar Barzelay",
      "Eran Guendelman",
      "Michael Schwarz",
      "Matteo Presutto",
      "Moran Vatelmacher",
      "Yigal Shenkman",
      "Eli Peker",
      "Itai Druker",
      "Uri Patish",
      "Yoav Blum",
      "Max Bluvstein",
      "Junxuan Li",
      "Rawal Khirodkar",
      "Shunsuke Saito"
    ],
    "abstract": "We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This \"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpecGaussian with Latent Features: A High-quality Modeling of the View-dependent Appearance for 3D Gaussian Splatting",
    "authors": [
      "Zhiru Wang",
      "Shiyun Xie",
      "Chengwei Pan",
      "Guoping Wang"
    ],
    "abstract": "Recently, the 3D Gaussian Splatting (3D-GS) method has achieved great success in novel view synthesis, providing real-time rendering while ensuring high-quality rendering results. However, this method faces challenges in modeling specular reflections and handling anisotropic appearance components, especially in dealing with view-dependent color under complex lighting conditions. Additionally, 3D-GS uses spherical harmonic to learn the color representation, which has limited ability to represent complex scenes. To overcome these challenges, we introduce Lantent-SpecGS, an approach that utilizes a universal latent neural descriptor within each 3D Gaussian. This enables a more effective representation of 3D feature fields, including appearance and geometry. Moreover, two parallel CNNs are designed to decoder the splatting feature maps into diffuse color and specular color separately. A mask that depends on the viewpoint is learned to merge these two colors, resulting in the final rendered image. Experimental results demonstrate that our method obtains competitive performance in novel view synthesis and extends the ability of 3D-GS to handle intricate scenarios with specular reflections.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians",
    "authors": [
      "Xiyu Zhang",
      "Chong Bao",
      "Yipeng Chen",
      "Hongjia Zhai",
      "Yitong Dong",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ],
    "abstract": "3D reconstruction of indoor and urban environments is a prominent research topic with various downstream applications. However, existing geometric priors for addressing low-texture regions in indoor and urban settings often lack global consistency. Moreover, Gaussian Splatting and implicit SDF fields often suffer from discontinuities or exhibit computational inefficiencies, resulting in a loss of detail. To address these issues, we propose an Atlanta-world guided implicit-structured Gaussian Splatting that achieves smooth indoor and urban scene reconstruction while preserving high-frequency details and rendering efficiency. By leveraging the Atlanta-world model, we ensure the accurate surface reconstruction for low-texture regions, while the proposed novel implicit-structured GS representations provide smoothness without sacrificing efficiency and high-frequency details. Specifically, we propose a semantic GS representation to predict the probability of all semantic regions and deploy a structure plane regularization with learnable plane indicators for global accurate surface reconstruction. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "urban scene",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSpeech: Audio-Driven Gaussian Avatars",
    "authors": [
      "Shivangi Aneja",
      "Artem Sevastopolsky",
      "Tobias Kirschstein",
      "Justus Thies",
      "Angela Dai",
      "Matthias Nießner"
    ],
    "abstract": "We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity animation sequences of photo-realistic, personalized 3D human head avatars from spoken audio. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple speech signal with 3D Gaussian splatting to create realistic, temporally coherent motion sequences. We propose a compact and efficient 3DGS-based avatar representation that generates expression-dependent color and leverages wrinkle- and perceptually-based losses to synthesize facial details, including wrinkles that occur with different expressions. To enable sequence modeling of 3D Gaussian splats with audio, we devise an audio-conditioned transformer model capable of extracting lip and expression features directly from audio input. Due to the absence of high-quality datasets of talking humans in correspondence with audio, we captured a new large-scale multi-view dataset of audio-visual sequences of talking humans with native English accents and diverse facial geometry. GaussianSpeech consistently achieves state-of-the-art performance with visually natural motion at real time rendering rates, while encompassing diverse facial expressions and styles.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "animation",
      "high-fidelity",
      "geometry",
      "compact",
      "motion",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving",
    "authors": [
      "Haiming Zhang",
      "Yiyao Zhu",
      "Wending Zhou",
      "Xu Yan",
      "Yingjie Cai",
      "Bingbing Liu",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes explicit dense BEV or volumetric construction, enabling highly efficient computation and accelerated inference. In this paper, we introduce SQS, a novel query-based splatting pre-training specifically designed to advance SPMs in autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian representations from sparse queries during pre-training, leveraging self-supervised splatting to learn fine-grained contextual features through the reconstruction of multi-view images and depth maps. During fine-tuning, the pre-trained Gaussian queries are seamlessly integrated into downstream networks via query interaction mechanisms that explicitly connect pre-trained queries with task-specific queries, effectively accommodating the diverse requirements of occupancy prediction and 3D object detection. Extensive experiments on autonomous driving benchmarks demonstrate that SQS delivers considerable performance gains across multiple query-based 3D perception tasks, notably in occupancy prediction and 3D object detection, outperforming prior state-of-the-art pre-training approaches by a significant margin (i.e., +1.3 mIoU on occupancy prediction and +1.0 NDS on 3D detection).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generating 360° Video is What You Need For a 3D Scene",
    "authors": [
      "Zhaoyang Zhang",
      "Yannick Hold-Geoffroy",
      "Miloš Hašan",
      "Ziwen Chen",
      "Fujun Luan",
      "Julie Dorsey",
      "Yiwei Hu"
    ],
    "abstract": "Generating 3D scenes is still a challenging task due to the lack of readily available scene data. Most existing methods only produce partial scenes and provide limited navigational freedom. We introduce a practical and scalable solution that uses 360° video as an intermediate scene representation, capturing the full-scene context and ensuring consistent visual content throughout the generation. We propose WorldPrompter, a generative pipeline that synthesizes traversable 3D scenes from text prompts. WorldPrompter incorporates a conditional 360° panoramic video generator, capable of producing a 128-frame video that simulates a person walking through and capturing a virtual environment. The resulting video is then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor, enabling a true walkable experience within the 3D scene. Experiments demonstrate that our panoramic video generation model, trained with a mix of image and video data, achieves convincing spatial and temporal consistency for static scenes. This is validated by an average COLMAP matching rate of 94.6\\%, allowing for high-quality panoramic Gaussian splat reconstruction and improved navigation throughout the scene. Qualitative and quantitative results also show it outperforms the state-of-the-art 360° video generators and 3D scene generation models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-02",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction",
    "authors": [
      "Wenfeng Huang",
      "Xiangyun Liao",
      "Yinling Qian",
      "Hao Liu",
      "Yongming Yang",
      "Wenjing Jia",
      "Qiong Wang"
    ],
    "abstract": "Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery. The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences. However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has improved reconstruction efficiency by enabling a faster rendering pipeline. Nevertheless, existing 3DGS methods often prioritize rendering speed while neglecting these critical issues. To address these challenges, we propose SAGS, a self-adaptive alias-free Gaussian splatting framework. We introduce an attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue reconstruction and better capture the fine details of tissue movement. Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate that our method achieves superior performance in all metrics of PSNR, SSIM, and LPIPS compared to the state of the art while also delivering better visualization quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation",
    "authors": [
      "Zhiqi Li",
      "Yiming Chen",
      "Peidong Liu"
    ],
    "abstract": "Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm, which is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the deformation network are learned via reference view photometric loss, score distillation loss as well as other regularizers in a two-stage manner. Extensive experiments demonstrate superior performance of our method. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "nerf",
      "face",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction",
    "authors": [
      "Jinlong Fan",
      "Bingyu Hu",
      "Xingguang Li",
      "Yuxiang Yang",
      "Jing Zhang"
    ],
    "abstract": "Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \\textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "avatar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis",
    "authors": [
      "Emmanouil Nikolakakis",
      "Utkarsh Gupta",
      "Jonathan Vengosh",
      "Justin Bui",
      "Razvan Marinescu"
    ],
    "abstract": "We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-04",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "sparse-view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones",
    "authors": [
      "Xuqian Ren",
      "Matias Turkulainen",
      "Jiepeng Wang",
      "Otto Seiskari",
      "Iaroslav Melekhov",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "abstract": "Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools. Our code is released in https://xuqianren.github.io/ags_mesh_website/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction",
    "authors": [
      "Chenfeng Hou",
      "Qi Xun Yeo",
      "Mengqi Guo",
      "Yongxin Su",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its high-quality rendering capabilities, ultra-fast training, and inference speeds. However, when we apply 3DGS to surface reconstruction tasks, especially in environments with dynamic objects and distractors, the method suffers from floating artifacts and color errors due to inconsistency from different viewpoints. To address this challenge, we propose Multi-View Consistency Gaussian Splatting for the domain of Robust Surface Reconstruction (\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a {heuristics-guided distractor masking} strategy for robust surface reconstruction in non-static environments. Compared to existing methods that rely on MLPs for distractor segmentation strategies, our approach separates distractors from static scene elements by comparing multi-view feature consistency, allowing us to obtain precise distractor masks early in training. Furthermore, we introduce a pruning measure based on multi-view contributions to reset transmittance, effectively reducing floating artifacts. Finally, a multi-view consistency loss is applied to achieve high-quality performance in surface reconstruction tasks. Experimental results demonstrate that MVGSR achieves competitive geometric accuracy and rendering fidelity compared to the state-of-the-art surface reconstruction algorithms. More information is available on our project page (https://mvgsr.github.io).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "lightweight",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Sicheng Yu",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction, which effectively balances rendering quality, efficiency, and speed. However, existing 3DGS approaches usually generate plausible outputs and face significant challenges in complex scene reconstruction, manifesting as incomplete holistic structural outlines and unclear local lighting effects. To address these issues simultaneously, we propose a novel decoupled optimization framework, which integrates wavelet decomposition into 3D Gaussian Splatting and 2D sampling. Technically, through 3D wavelet decomposition, our approach divides point clouds into high-frequency and low-frequency components, enabling targeted optimization for each. The low-frequency component captures global structural outlines and manages the distribution of Gaussians through voxelization. In contrast, the high-frequency component restores intricate geometric and textural details while incorporating a relight module to mitigate lighting artifacts and enhance photorealistic rendering. Additionally, a 2D wavelet decomposition is applied to the training images, simulating radiance variations. This provides critical guidance for high-frequency detail reconstruction, ensuring seamless integration of details with the global structure. Extensive experiments on challenging datasets demonstrate our method achieves state-of-the-art performance across various metrics, surpassing existing approaches and advancing the field of 3D scene reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-16",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction",
    "authors": [
      "Sheng-Hsiang Hung",
      "Ting-Yu Yen",
      "Wei-Fang Sun",
      "Simon See",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "high-fidelity",
      "outdoor",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization",
    "authors": [
      "Mengjiao Han",
      "Andres Sewell",
      "Joseph Insley",
      "Janet Knowles",
      "Victor A. Mateevitsi",
      "Michael E. Papka",
      "Steve Petruzza",
      "Silvio Rizzi"
    ],
    "abstract": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS) pipeline for scientific visualization. Building on previous work that demonstrated high-fidelity isosurface reconstruction using Gaussian primitives, we incorporate a multi-GPU training backend adapted from Grendel-GS to enable scalable processing of large datasets. By distributing optimization across GPUs, our method improves training throughput and supports high-resolution reconstructions that exceed single-GPU capacity. In our experiments, the system achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs compared to a single-GPU baseline, and successfully trains the Miranda dataset (18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays the groundwork for integrating 3D-GS into HPC-based scientific workflows, enabling real-time post hoc and in situ visualization of complex simulations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-05",
    "categories": [
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiao Tang",
      "Jiajun Tang",
      "Jiaqi Lin",
      "Yuxin Cheng",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching",
    "authors": [
      "Arjun P S",
      "Andrew Melnik",
      "Gora Chand Nandi"
    ],
    "abstract": "Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high quality",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments",
    "authors": [
      "Mingrui Li",
      "Yiming Zhou",
      "Hongxing Zhou",
      "Xinggang Hu",
      "Florian Roemer",
      "Hongyu Wang",
      "Ahmad Osman"
    ],
    "abstract": "Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "tracking",
      "localization",
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "mapping",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity",
    "authors": [
      "Tianqi Shen",
      "Shaohua Liu",
      "Jiaqi Feng",
      "Ziye Ma",
      "Ning An"
    ],
    "abstract": "Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-21",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "math.AT",
      "math.GT"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting",
    "authors": [
      "Zijing Guo",
      "Yunyang Zhao",
      "Lin Wang"
    ],
    "abstract": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "nerf",
      "face",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with Cycle-Aggregative Gaussian Splatting",
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Dan Xu"
    ],
    "abstract": "This paper tackles the problem of generalizable 3D-aware generation from monocular datasets, e.g., ImageNet. The key challenge of this task is learning a robust 3D-aware representation without multi-view or dynamic data, while ensuring consistent texture and geometry across different viewpoints. Although some baseline methods are capable of 3D-aware generation, the quality of the generated images still lags behind state-of-the-art 2D generation approaches, which excel in producing high-quality, detailed images. To address this severe limitation, we propose a novel feed-forward pipeline based on pixel-aligned Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and reliable 3D renderings from monocular inputs. In addition, we introduce a self-supervised cycle-aggregative constraint to enforce cross-view consistency in the learned 3D representation. This training strategy naturally allows aggregation of multiple aligned Gaussian primitives and significantly alleviates the interpolation limitations inherent in single-view pixel-aligned Gaussian Splatting. Furthermore, we incorporate video model priors to perform geometry-aware refinement, enhancing the generation of fine details in wide-viewpoint scenarios and improving the model's capability to capture intricate 3D textures. Extensive experiments demonstrate that our approach not only achieves high-quality, multi-view consistent 3D-aware generation from monocular datasets, but also significantly improves training and inference efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "H3D-DGS: Exploring Heterogeneous 3D Motion Representation for Deformable 3D Gaussian Splatting",
    "authors": [
      "Bing He",
      "Yunuo Chen",
      "Guo Lu",
      "Qi Wang",
      "Qunshan Gu",
      "Rong Xie",
      "Li Song",
      "Wenjun Zhang"
    ],
    "abstract": "Dynamic scene reconstruction poses a persistent challenge in 3D vision. Deformable 3D Gaussian Splatting has emerged as an effective method for this task, offering real-time rendering and high visual fidelity. This approach decomposes a dynamic scene into a static representation in a canonical space and time-varying scene motion. Scene motion is defined as the collective movement of all Gaussian points, and for compactness, existing approaches commonly adopt implicit neural fields or sparse control points. However, these methods predominantly rely on gradient-based optimization for all motion information. Due to the high degree of freedom, they struggle to converge on real-world datasets exhibiting complex motion. To preserve the compactness of motion representation and address convergence challenges, this paper proposes heterogeneous 3D control points, termed \\textbf{H3D control points}, whose attributes are obtained using a hybrid strategy combining optical flow back-projection and gradient-based methods. This design decouples directly observable motion components from those that are geometrically occluded. Specifically, components of 3D motion that project onto the image plane are directly acquired via optical flow back projection, while unobservable portions are refined through gradient-based optimization. Experiments on the Neu3DV and CMU-Panoptic datasets demonstrate that our method achieves superior performance over state-of-the-art deformable 3D Gaussian splatting techniques. Remarkably, our method converges within just 100 iterations and achieves a per-frame processing speed of 2 seconds on a single NVIDIA RTX 4070 GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations",
    "authors": [
      "Zhenyu Tang",
      "Chaoran Feng",
      "Xinhua Cheng",
      "Wangbo Yu",
      "Junwu Zhang",
      "Yuan Liu",
      "Xiaoxiao Long",
      "Wenping Wang",
      "Li Yuan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves impressive quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. In this paper, we aim to develop a simple yet effective method called NeuralGS that compresses the original 3DGS into a compact representation. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians within each cluster using different tiny MLPs, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 91-times average model size reduction without harming the visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "End-to-End Rate-Distortion Optimized 3D Gaussian Representation",
    "authors": [
      "Henan Wang",
      "Hanxin Zhu",
      "Tianyu He",
      "Runsen Feng",
      "Jiajun Deng",
      "Jiang Bian",
      "Zhibo Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40x, and surpasses existing methods in rate-distortion performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting",
    "authors": [
      "Javier Yu",
      "Timothy Chen",
      "Mac Schwager"
    ],
    "abstract": "3D Gaussian Splatting offers expressive scene reconstruction, modeling a broad range of visual, geometric, and semantic information. However, efficient real-time map reconstruction with data streamed from multiple robots and devices remains a challenge. To that end, we propose HAMMER, a server-based collaborative Gaussian Splatting method that leverages widely available ROS communication infrastructure to generate 3D, metric-semantic maps from asynchronous robot data-streams with no prior knowledge of initial robot positions and varying on-device pose estimators. HAMMER consists of (i) a frame alignment module that transforms local SLAM poses and image data into a global frame and requires no prior relative pose knowledge, and (ii) an online module for training semantic 3DGS maps from streaming data. HAMMER handles mixed perception modes, adjusts automatically for variations in image pre-processing among different devices, and distills CLIP semantic codes into the 3D scene for open-vocabulary language queries. In our real-world experiments, HAMMER creates higher-fidelity maps (2x) compared to competing baselines and is useful for downstream tasks, such as semantic goal-conditioned navigation (e.g., \"go to the couch\"). Accompanying content available at hammer-project.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "slam",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Diffusion Denoised Hyperspectral Gaussian Splatting",
    "authors": [
      "Sunil Kumar Narayanan",
      "Lingjun Zhao",
      "Lu Gan",
      "Yongsheng Chen"
    ],
    "abstract": "Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise determination of nutritional elements of samples. Recently, 3D reconstruction methods have been used to create implicit neural representations of HSI scenes, which can help localize the target object's nutrient composition spatially and spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit representation that can be used to render hyperspectral channel compositions of each spatial location from any viewing direction. However, it faces limitations in training time and rendering speed. In this paper, we propose Diffusion-Denoised Hyperspectral Gaussian Splatting (DD-HGS), which enhances the state-of-the-art 3D Gaussian Splatting (3DGS) method with wavelength-aware spherical harmonics, a Kullback-Leibler divergence-based spectral loss, and a diffusion-based denoiser to enable 3D explicit reconstruction of hyperspectral scenes across the full spectral range. We present extensive evaluations on diverse real-world hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of DD-HGS. The results demonstrate that DD-HGS achieves new state-of-the-art performance among previously published methods. Project page: https://dragonpg2000.github.io/DDHGS-website/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting",
    "authors": [
      "Chenghao Qian",
      "Yuhu Guo",
      "Wenjing Li",
      "Gustav Markkula"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods. See project page:https://jumponthemoon.github.io/weather-gs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Locality-aware Gaussian Compression for Fast and High-quality Rendering",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework that exploits the spatial coherence of 3D Gaussians for compact modeling of volumetric scenes. To this end, we first analyze the local coherence of 3D Gaussian attributes, and propose a novel locality-aware 3D Gaussian representation that effectively encodes locally-coherent Gaussian attributes using a neural field representation with a minimal storage requirement. On top of the novel representation, LocoGS is carefully designed with additional components such as dense initialization, an adaptive spherical harmonics bandwidth scheme and different encoding schemes for different Gaussian attributes to maximize compression performance. Experimental results demonstrate that our approach outperforms the rendering quality of existing compact Gaussian representations for representative real-world 3D datasets while achieving from 54.6$\\times$ to 96.6$\\times$ compressed storage size and from 2.1$\\times$ to 2.4$\\times$ rendering speed than 3DGS. Even our approach also demonstrates an averaged 2.4$\\times$ higher rendering speed than the state-of-the-art compression method with comparable compression performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing",
    "authors": [
      "Zhaonan Wang",
      "Manyi Li",
      "Changhe Tu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "compact",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering",
    "authors": [
      "Xiaobin Deng",
      "Changyu Diao",
      "Min Li",
      "Ruohan Yu",
      "Duanqing Xu"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splat Vulnerabilities",
    "authors": [
      "Matthew Hull",
      "Haoyang Yang",
      "Pratham Mehta",
      "Mansi Phute",
      "Aeree Cho",
      "Haoran Wang",
      "Matthew Lau",
      "Wenke Lee",
      "Willian T. Lunardi",
      "Martin Andreoni",
      "Polo Chau"
    ],
    "abstract": "With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting",
    "authors": [
      "Zeren Jiang",
      "Shaofei Wang",
      "Siyu Tang"
    ],
    "abstract": "Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "shadow",
      "human",
      "relightable",
      "ray tracing",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "avatar",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting",
    "authors": [
      "Gyusam Chang",
      "Tuan-Anh Vu",
      "Vivek Alumootil",
      "Harris Song",
      "Deanna Pham",
      "Sangpil Kim",
      "M. Khalid Jawed"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in agriculture remains underexplored. Agricultural scenes present unique challenges for 3D reconstruction methods, particularly due to uneven illumination, occlusions, and a limited field of view. To address these limitations, we introduce \\textbf{NIRPlant}, a novel multimodal dataset encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth, and LiDAR data collected under varied indoor and outdoor lighting conditions. By integrating NIR data, our approach enhances robustness and provides crucial botanical insights that extend beyond the visible spectrum. Additionally, we leverage text-based metadata derived from vegetation indices, such as NDVI, NDWI, and the chlorophyll index, which significantly enriches the contextual understanding of complex agricultural environments. To fully exploit these modalities, we propose \\textbf{NIRSplat}, an effective multimodal Gaussian splatting architecture employing a cross-attention mechanism combined with 3D point-based positional encoding, providing robust geometric priors. Comprehensive experiments demonstrate that \\textbf{NIRSplat} outperforms existing landmark methods, including 3DGS, CoR-GS, and InstantSplat, highlighting its effectiveness in challenging agricultural scenarios. The code and dataset are publicly available at: https://github.com/StructuresComp/3D-Reconstruction-NIR",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "outdoor",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided Framework",
    "authors": [
      "Shanzhi Yin",
      "Bolin Chen",
      "Xinju Wu",
      "Ru-Ling Liao",
      "Jie Chen",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "abstract": "This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-12",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "compression",
      "compact",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting",
    "authors": [
      "Jiahao Lu",
      "Yifan Zhang",
      "Qiuhong Shen",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "abstract": "3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems. Our code is available at https://github.com/jiahaolu97/poison-splat .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-10",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Survey on Fundamental Deep Learning 3D Reconstruction Techniques",
    "authors": [
      "Yonge Bai",
      "LikHang Wong",
      "TszYin Twan"
    ],
    "abstract": "This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction techniques that produce photo-realistic 3D models and scenes, highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their strengths and tradeoffs, and project future research trajectories in this rapidly evolving field. We provide a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction, offering insights into their potential applications and limitations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting",
    "authors": [
      "Yuanhao Cai",
      "Zihao Xiao",
      "Yixun Liang",
      "Minghan Qin",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Yaoyao Liu",
      "Alan Yuille"
    ],
    "abstract": "High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and only requiring 6.3% training time. Code and recalibrated data will be publicly available at https://github.com/caiyuanhao1998/HDR-GS . A brief video introduction of our work is available at https://youtu.be/wtU7Kcwe7ck",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting",
    "authors": [
      "Shubhendu Jena",
      "Shishir Reddy Vutukur",
      "Adnane Boukhayma"
    ],
    "abstract": "Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "shape reconstruction",
      "3d reconstruction",
      "face",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling",
    "authors": [
      "Fengyi Zhang",
      "Yadan Luo",
      "Tianjun Zhang",
      "Lin Zhang",
      "Zi Huang"
    ],
    "abstract": "The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes will be available at https://github.com/Xian-Bei/GaussianForest.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-13",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D Gaussian Spatting",
    "authors": [
      "Zhiying Yan",
      "Yiyuan Liang",
      "Shilv Cai",
      "Tao Zhang",
      "Sheng Zhong",
      "Luxin Yan",
      "Xu Zou"
    ],
    "abstract": "Semantic 4D Gaussians can be used for reconstructing and understanding dynamic scenes, with temporal variations than static scenes. Directly applying static methods to understand dynamic scenes will fail to capture the temporal features. Few works focus on dynamic scene understanding based on Gaussian Splatting, since once the same update strategy is employed for both dynamic and static parts, regardless of the distinction and interaction between Gaussians, significant artifacts and noise appear. We propose Dual-Hierarchical Optimization (DHO), which consists of Hierarchical Gaussian Flow and Hierarchical Gaussian Guidance in a divide-and-conquer manner. The former implements effective division of static and dynamic rendering and features. The latter helps to mitigate the issue of dynamic foreground rendering distortion in textured complex scenes. Extensive experiments show that our method consistently outperforms the baselines on both synthetic and real-world datasets, and supports various downstream tasks. Project Page: https://sweety-yan.github.io/DHO.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "understanding",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion",
    "authors": [
      "Yansong Xu",
      "Junlin Li",
      "Wei Zhang",
      "Siyu Chen",
      "Shengyong Zhang",
      "Yuquan Leng",
      "Weijia Zhou"
    ],
    "abstract": "3D gaussian splatting has advanced simultaneous localization and mapping (SLAM) technology by enabling real-time positioning and the construction of high-fidelity maps. However, the uncertainty in gaussian position and initialization parameters introduces challenges, often requiring extensive iterative convergence and resulting in redundant or insufficient gaussian representations. To address this, we introduce a novel adaptive densification method based on Fourier frequency domain analysis to establish gaussian priors for rapid convergence. Additionally, we propose constructing independent and unified sparse and dense maps, where a sparse map supports efficient tracking via Generalized Iterative Closest Point (GICP) and a dense map creates high-fidelity visual representations. This is the first SLAM system leveraging frequency domain analysis to achieve high-quality gaussian mapping in real-time. Experimental results demonstrate an average frame rate of 36 FPS on Replica and TUM RGB-D datasets, achieving competitive accuracy in both localization and mapping.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
    "authors": [
      "SeungJun Moon",
      "Hah Min Lew",
      "Seungeun Lee",
      "Ji-Su Kang",
      "Gyeong-Moon Park"
    ],
    "abstract": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-24",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "avatar",
      "ar",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration",
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Valerio Cambareri",
      "Martin R. Oswald",
      "Stefano Mattoccia"
    ],
    "abstract": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras",
    "authors": [
      "Zimu Liao",
      "Siyan Chen",
      "Rong Fu",
      "Yi Wang",
      "Zhongling Su",
      "Hao Luo",
      "Li Ma",
      "Linning Xu",
      "Bo Dai",
      "Hengjie Li",
      "Zhilin Pei",
      "Xingcheng Zhang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation for 3D Gaussian Splatting",
    "authors": [
      "Yansong Qu",
      "Dian Chen",
      "Xinyang Li",
      "Xiaofan Li",
      "Shengchuan Zhang",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "abstract": "Recent advancements in 3D scene editing have been propelled by the rapid development of generative models. Existing methods typically utilize generative models to perform text-guided editing on 3D representations, such as 3D Gaussian Splatting (3DGS). However, these methods are often limited to texture modifications and fail when addressing geometric changes, such as editing a character's head to turn around. Moreover, such methods lack accurate control over the spatial position of editing results, as language struggles to precisely describe the extent of edits. To overcome these limitations, we introduce DYG, an effective 3D drag-based editing method for 3D Gaussian Splatting. It enables users to conveniently specify the desired editing region and the desired dragging direction through the input of 3D masks and pairs of control points, thereby enabling precise control over the extent of editing. DYG integrates the strengths of the implicit triplane representation to establish the geometric scaffold of the editing results, effectively overcoming suboptimal editing outcomes caused by the sparsity of 3DGS in the desired editing regions. Additionally, we incorporate a drag-based Latent Diffusion Model into our method through the proposed Drag-SDS loss function, enabling flexible, multi-view consistent, and fine-grained editing. Extensive experiments demonstrate that DYG conducts effective drag-based editing guided by control point prompts, surpassing other baselines in terms of editing effect and quality, both qualitatively and quantitatively. Visit our project page at https://quyans.github.io/Drag-Your-Gaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation",
    "authors": [
      "Xiangrui Liu",
      "Xinju Wu",
      "Shiqi Wang",
      "Zhu Li",
      "Sam Kwong"
    ],
    "abstract": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers from substantial data volume due to inherent primitive redundancy. To enable future photorealistic 3D immersive visual communication applications, significant compression is essential for transmission over the existing Internet infrastructure. Hence, we propose Compressed Gaussian Splatting (CompGS++), a novel framework that leverages compact Gaussian primitives to achieve accurate 3D modeling with substantial size reduction for both static and dynamic scenes. Our design is based on the principle of eliminating redundancy both between and within primitives. Specifically, we develop a comprehensive prediction paradigm to address inter-primitive redundancy through spatial and temporal primitive prediction modules. The spatial primitive prediction module establishes predictive relationships for scene primitives and enables most primitives to be encoded as compact residuals, substantially reducing the spatial redundancy. We further devise a temporal primitive prediction module to handle dynamic scenes, which exploits primitive correlations across timestamps to effectively reduce temporal redundancy. Moreover, we devise a rate-constrained optimization module that jointly minimizes reconstruction error and rate consumption. This module effectively eliminates parameter redundancy within primitives and enhances the overall compactness of scene representations. Comprehensive evaluations across multiple benchmark datasets demonstrate that CompGS++ significantly outperforms existing methods, achieving superior compression performance while preserving accurate scene modeling. Our implementation will be made publicly available on GitHub to facilitate further research.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
    "authors": [
      "Yiming Huang",
      "Long Bai",
      "Beilei Cui",
      "Yanheng Li",
      "Tong Chen",
      "Jie Wang",
      "Jinlin Wu",
      "Zhen Lei",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ],
    "abstract": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lightweight",
      "localization",
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
    "authors": [
      "Chieh Hubert Lin",
      "Zhaoyang Lv",
      "Songyin Wu",
      "Zhen Xu",
      "Thu Nguyen-Phuoc",
      "Hung-Yu Tseng",
      "Julian Straub",
      "Numair Khan",
      "Lei Xiao",
      "Ming-Hsuan Yang",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong",
      "Zhengqin Li"
    ],
    "abstract": "We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-11",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "motion",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Image-Conditioned 3D Gaussian Splat Quantization",
    "authors": [
      "Xinshuang Liu",
      "Runfa Blark Li",
      "Keito Suzuki",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting",
    "authors": [
      "Jeongmin Bae",
      "Seoha Kim",
      "Youngsik Yun",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh"
    ],
    "abstract": "As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions. Project page: https://jeongminb.github.io/e-d3dgs/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting",
    "authors": [
      "Qipeng Yan",
      "Mingyang Sun",
      "Lihua Zhang"
    ],
    "abstract": "Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-04",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "high-fidelity",
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "avatar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey",
    "authors": [
      "Calvin Galagain",
      "Martyna Poreba",
      "François Goulette"
    ],
    "abstract": "In embedded systems, robots must perceive and interpret their environment efficiently to operate reliably in real-world conditions. Visual Semantic SLAM (Simultaneous Localization and Mapping) enhances standard SLAM by incorporating semantic information into the map, enabling more informed decision-making. However, implementing such systems on resource-limited hardware involves trade-offs between accuracy, computing efficiency, and power usage.   This paper provides a comparative review of recent Semantic Visual SLAM methods with a focus on their applicability to embedded platforms. We analyze three main types of architectures - Geometric SLAM, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting - and evaluate their performance on constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their accuracy, segmentation quality, memory usage, and energy consumption.   Our results show that methods based on NeRF and Gaussian Splatting achieve high semantic detail but demand substantial computing resources, limiting their use on embedded devices. In contrast, Semantic Geometric SLAM offers a more practical balance between computational cost and accuracy. The review highlights a need for SLAM algorithms that are better adapted to embedded environments, and it discusses key directions for improving their efficiency through algorithm-hardware co-design.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "survey",
      "localization",
      "semantic",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects",
    "authors": [
      "Justin Yu",
      "Kush Hari",
      "Karim El-Refai",
      "Arnav Dalal",
      "Justin Kerr",
      "Chung Min Kim",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "abstract": "Tracking and manipulating irregularly-shaped, previously unseen objects in dynamic environments is important for robotic applications in manufacturing, assembly, and logistics. Recently introduced Gaussian Splats efficiently model object geometry, but lack persistent state estimation for task-oriented manipulation. We present Persistent Object Gaussian Splat (POGS), a system that embeds semantics, self-supervised visual features, and object grouping features into a compact representation that can be continuously updated to estimate the pose of scanned objects. POGS updates object states without requiring expensive rescanning or prior CAD models of objects. After an initial multi-view scene capture and training phase, POGS uses a single stereo camera to integrate depth estimates along with self-supervised vision encoder features for object pose estimation. POGS supports grasping, reorientation, and natural language-driven manipulation by refining object pose estimates, facilitating sequential object reset operations with human-induced object perturbations and tool servoing, where robots recover tool pose despite tool perturbations of up to 30°. POGS achieves up to 12 consecutive successful object resets and recovers from 80% of in-grasp tool perturbations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-07",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "geometry",
      "compact",
      "semantic",
      "tracking",
      "dynamic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Yonghan Lee",
      "Sungmin Eum",
      "Dinesh Manocha",
      "Heesung Kwon"
    ],
    "abstract": "We present UAVTwin, a method for creating digital twins from real-world environments and facilitating data augmentation for training downstream models embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses on synthesizing foreground components, such as various human instances in motion within complex scene backgrounds, from UAV perspectives. This is achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing backgrounds along with controllable synthetic human models that display diverse appearances and actions in multiple poses. To the best of our knowledge, UAVTwin is the first approach for UAV-based perception that is capable of generating high-fidelity digital twins based on 3DGS. The proposed work significantly enhances downstream models through data augmentation for real-world environments with multiple dynamic objects and significant appearance variations-both of which typically introduce artifacts in 3DGS-based modeling. To tackle these challenges, we propose a novel appearance modeling strategy and a mask refinement module to enhance the training of 3D Gaussian Splatting. We demonstrate the high quality of neural rendering by achieving a 1.23 dB improvement in PSNR compared to recent methods. Furthermore, we validate the effectiveness of data augmentation by showing a 2.5% to 13.7% improvement in mAP for the human detection task.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "high quality",
      "neural rendering",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure",
    "authors": [
      "Xiufeng Huang",
      "Ziyuan Luo",
      "Qi Song",
      "Ruofei Wang",
      "Renjie Wan"
    ],
    "abstract": "The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
    "authors": [
      "Hui En Pang",
      "Shuai Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Tianwei Zhang",
      "Ziwei Liu"
    ],
    "abstract": "We present \\textbf{Disco4D}, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. \\textbf{1)} Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. \\textbf{2)} It adopts diffusion models to enhance the 3D generation process, \\textit{e.g.}, modeling occluded parts not visible in the input image. \\textbf{3)} It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in \\url{https://disco-4d.github.io/}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "animation",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars",
    "authors": [
      "Yahui Li",
      "Zhi Zeng",
      "Liming Pang",
      "Guixuan Zhang",
      "Shuwu Zhang"
    ],
    "abstract": "Modeling animatable human avatars from monocular or multi-view videos has been widely studied, with recent approaches leveraging neural radiance fields (NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in novel-view and novel-pose synthesis. However, existing methods often struggle to accurately capture the dynamics of loose clothing, as they primarily rely on global pose conditioning or static per-frame representations, leading to oversmoothing and temporal inconsistencies in non-rigid regions. To address this, We propose RealityAvatar, an efficient framework for high-fidelity digital human modeling, specifically targeting loosely dressed avatars. Our method leverages 3D Gaussian Splatting to capture complex clothing deformations and motion dynamics while ensuring geometric consistency. By incorporating a motion trend module and a latentbone encoder, we explicitly model pose-dependent deformations and temporal variations in clothing behavior. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach in capturing fine-grained clothing deformations and motion-driven shape variations. Our method significantly enhances structural fidelity and perceptual quality in dynamic human reconstruction, particularly in non-rigid regions, while achieving better consistency across temporal frames.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "avatar",
      "nerf",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging",
    "authors": [
      "Tianshuang Qiu",
      "Zehan Ma",
      "Karim El-Refai",
      "Hiya Shah",
      "Chung Min Kim",
      "Justin Kerr",
      "Ken Goldberg"
    ],
    "abstract": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view images. Such \"digital twins\" are useful for simulations, virtual reality, marketing, robot policy fine-tuning, and part inspection. 3D object scanning usually requires multi-camera arrays, precise laser scanners, or robot wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan, a pipeline for producing high-quality 3D Gaussian Splat models using a bi-manual robot that grasps an object with one gripper and rotates the object with respect to a stationary camera. The object is then re-grasped by a second gripper to expose surfaces that were occluded by the first gripper. We present the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as RAFT optical flow models to identify and isolate objects held by a robot gripper while removing the gripper and the background. We then modify the 3DGS training pipeline to support concatenated datasets with gripper occlusion, producing an omni-directional (360 degree view) model of the object. We apply Omni-Scan to part defect inspection, finding that it can identify visual or geometric defects in 12 different industrial and household objects with an average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be found at https://berkeleyautomation.github.io/omni-scan/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-01",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Compact 3D Gaussian Splatting For Dense Visual SLAM",
    "authors": [
      "Tianchen Deng",
      "Yaohui Chen",
      "Leyan Zhang",
      "Jianfei Yang",
      "Shenghai Yuan",
      "Jiuming Liu",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes. However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed. To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids. A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids. Then we observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian geometric attributes, i.e., the parameters. Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss. Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "compact",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Frequency-Aware Density Control via Reparameterization for High-Quality Rendering of 3D Gaussian Splatting",
    "authors": [
      "Zhaojie Zeng",
      "Yuesong Wang",
      "Lili Ju",
      "Tao Guan"
    ],
    "abstract": "By adaptively controlling the density and generating more Gaussians in regions with high-frequency information, 3D Gaussian Splatting (3DGS) can better represent scene details. From the signal processing perspective, representing details usually needs more Gaussians with relatively smaller scales. However, 3DGS currently lacks an explicit constraint linking the density and scale of 3D Gaussians across the domain, leading to 3DGS using improper-scale Gaussians to express frequency information, resulting in the loss of accuracy. In this paper, we propose to establish a direct relation between density and scale through the reparameterization of the scaling parameters and ensure the consistency between them via explicit constraints (i.e., density responds well to changes in frequency). Furthermore, we develop a frequency-aware density control strategy, consisting of densification and deletion, to improve representation quality with fewer Gaussians. A dynamic threshold encourages densification in high-frequency regions, while a scale-based filter deletes Gaussians with improper scale. Experimental results on various datasets demonstrate that our method outperforms existing state-of-the-art methods quantitatively and qualitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images",
    "authors": [
      "Shengjun Zhang",
      "Xin Fei",
      "Fangfu Liu",
      "Haixu Song",
      "Yueqi Duan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with Multi-modal Inputs",
    "authors": [
      "Qi Song",
      "Chenghong Li",
      "Haotong Lin",
      "Sida Peng",
      "Rui Huang"
    ],
    "abstract": "We present a novel approach, termed ADGaussian, for generalizable street scene reconstruction. The proposed method enables high-quality rendering from single-view input. Unlike prior Gaussian Splatting methods that primarily focus on geometry refinement, we emphasize the importance of joint optimization of image and depth features for accurate Gaussian prediction. To this end, we first incorporate sparse LiDAR depth as an additional input modality, formulating the Gaussian prediction process as a joint learning framework of visual information and geometric clue. Furthermore, we propose a multi-modal feature matching strategy coupled with a multi-scale Gaussian decoding model to enhance the joint refinement of multi-modal features, thereby enabling efficient multi-modal Gaussian learning. Extensive experiments on two large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our ADGaussian achieves state-of-the-art performance and exhibits superior zero-shot generalization capabilities in novel-view shifting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands relatively significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the effectiveness of our approach. Code is available at https://flexgs.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization",
    "authors": [
      "Christian Schmidt",
      "Jens Piekenbrinck",
      "Bastian Leibe"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via Universal Watermarking",
    "authors": [
      "Yuqi Tan",
      "Xiang Liu",
      "Shuzhao Xie",
      "Bin Chen",
      "Shu-Tao Xia",
      "Zhi Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene representation, providing rapid rendering speeds and high fidelity. As 3DGS gains prominence, safeguarding its intellectual property becomes increasingly crucial since 3DGS could be used to imitate unauthorized scene creations and raise copyright issues. Existing watermarking methods for implicit NeRFs cannot be directly applied to 3DGS due to its explicit representation and real-time rendering process, leaving watermarking for 3DGS largely unexplored. In response, we propose WATER-GS, a novel method designed to protect 3DGS copyrights through a universal watermarking strategy. First, we introduce a pre-trained watermark decoder, treating raw 3DGS generative modules as potential watermark encoders to ensure imperceptibility. Additionally, we implement novel 3D distortion layers to enhance the robustness of the embedded watermark against common real-world distortions of point cloud data. Comprehensive experiments and ablation studies demonstrate that WATER-GS effectively embeds imperceptible and robust watermarks into 3DGS without compromising rendering efficiency and quality. Our experiments indicate that the 3D distortion layers can yield up to a 20% improvement in accuracy rate. Notably, our method is adaptable to different 3DGS variants, including 3DGS compression frameworks and 2D Gaussian splatting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-07",
    "categories": [
      "cs.CR"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting",
    "authors": [
      "Junlin Hao",
      "Peiheng Wang",
      "Haoyang Wang",
      "Xinggong Zhang",
      "Zongming Guo"
    ],
    "abstract": "Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
    "authors": [
      "Yuxuan Wu",
      "Lei Pan",
      "Wenhua Wu",
      "Guangming Wang",
      "Yanzi Miao",
      "Fan Xu",
      "Hesheng Wang"
    ],
    "abstract": "Sim-to-Real refers to the process of transferring policies learned in simulation to the real world, which is crucial for achieving practical robotics applications. However, recent Sim2real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks. In recent years, with the emergence of radiance field reconstruction methods, especially 3D Gaussian splatting, it has become possible to construct realistic real-world scenes. To this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim-to-real transfer for vision-based deep reinforcement learning. We introduce a mesh-based 3D GS method with soft binding constraints, enhancing the rendering quality of mesh models. Then utilizing a GS editing approach to synchronize the rendering with the physics simulator, RL-GSBridge could reflect the visual interactions of the physical robot accurately. Through a series of sim-to-real experiments, including grasping and pick-and-place tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in real-world task completion during sim-to-real transfer. Furthermore, a series of rendering metrics and visualization results indicate that our proposed mesh-based 3D GS reduces artifacts in unstructured objects, demonstrating more realistic rendering performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) represents a significant advancement in the field of efficient and high-fidelity novel view synthesis. Despite recent progress, achieving accurate geometric reconstruction under sparse-view conditions remains a fundamental challenge. Existing methods often rely on non-local depth regularization, which fails to capture fine-grained structures and is highly sensitive to depth estimation noise. Furthermore, traditional smoothing methods neglect semantic boundaries and indiscriminately degrade essential edges and textures, consequently limiting the overall quality of reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware regularization framework for 3D Gaussian Splatting. DET-GS introduces a hierarchical geometric depth supervision framework that adaptively enforces multi-level geometric consistency, significantly enhancing structural fidelity and robustness against depth estimation noise. To preserve scene boundaries, we design an edge-aware depth regularization guided by semantic masks derived from Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving Total Variation loss that selectively smooths homogeneous regions while rigorously retaining high-frequency details and textures. Extensive experiments demonstrate that DET-GS achieves substantial improvements in both geometric accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on sparse-view novel view synthesis benchmarks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded Scenes",
    "authors": [
      "Zehao Yu",
      "Torsten Sattler",
      "Andreas Geiger"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and adaptive surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing Marching Tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to or even outperforms, neural implicit methods in both quality and speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trim 3D Gaussian Splatting for Accurate Geometry Representation",
    "authors": [
      "Lue Fan",
      "Yuxue Yang",
      "Minxing Li",
      "Hongsheng Li",
      "Zhaoxiang Zhang"
    ],
    "abstract": "In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality. Our project page is https://trimgs.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-11",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
    "authors": [
      "Hongyu Shen",
      "Junfeng Ni",
      "Yixin Chen",
      "Weishuo Li",
      "Mingtao Pei",
      "Siyuan Huang"
    ],
    "abstract": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EigenGS Representation: From Eigenspace to Gaussian Image Space",
    "authors": [
      "Lo-Wei Tai",
      "Ching-En Li",
      "Cheng-Lin Chen",
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ],
    "abstract": "Principal Component Analysis (PCA), a classical dimensionality reduction technique, and 2D Gaussian representation, an adaptation of 3D Gaussian Splatting for image representation, offer distinct approaches to modeling visual data. We present EigenGS, a novel method that bridges these paradigms through an efficient transformation pipeline connecting eigenspace and image-space Gaussian representations. Our approach enables instant initialization of Gaussian parameters for new images without requiring per-image optimization from scratch, dramatically accelerating convergence. EigenGS introduces a frequency-aware learning mechanism that encourages Gaussians to adapt to different scales, effectively modeling varied spatial frequencies and preventing artifacts in high-resolution reconstruction. Extensive experiments demonstrate that EigenGS not only achieves superior reconstruction quality compared to direct 2D Gaussian fitting but also reduces necessary parameter count and training time. The results highlight EigenGS's effectiveness and generalization ability across images with varying resolutions and diverse categories, making Gaussian-based image representation both high-quality and viable for real-time applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos",
    "authors": [
      "Rohit Jena",
      "Ganesh Subramanian Iyer",
      "Siddharth Choudhary",
      "Brandon Smith",
      "Pratik Chaudhari",
      "James Gee"
    ],
    "abstract": "We propose SplatArmor, a novel approach for recovering detailed and animatable human models by `armoring' a parameterized body model with 3D Gaussians. Our approach represents the human as a set of 3D Gaussians within a canonical space, whose articulation is defined by extending the skinning of the underlying SMPL geometry to arbitrary locations in the canonical space. To account for pose-dependent effects, we introduce a SE(3) field, which allows us to capture both the location and anisotropy of the Gaussians. Furthermore, we propose the use of a neural color field to provide color regularization and 3D supervision for the precise positioning of these Gaussians. We show that Gaussian splatting provides an interesting alternative to neural rendering based methods by leverging a rasterization primitive without facing any of the non-differentiability and optimization challenges typically faced in such approaches. The rasterization paradigms allows us to leverage forward skinning, and does not suffer from the ambiguities associated with inverse skinning and warping. We show compelling results on the ZJU MoCap and People Snapshot datasets, which underscore the effectiveness of our method for controllable human synthesis.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-17",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "neural rendering",
      "geometry",
      "body",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We propose DrivingForward, a feed-forward Gaussian Splatting model that reconstructs driving scenes from flexible surround-view input. Driving scene images from vehicle-mounted cameras are typically sparse, with limited overlap, and the movement of the vehicle further complicates the acquisition of camera extrinsics. To tackle these challenges and achieve real-time reconstruction, we jointly train a pose network, a depth network, and a Gaussian network to predict the Gaussian primitives that represent the driving scenes. The pose network and depth network determine the position of the Gaussian primitives in a self-supervised manner, without using depth ground truth and camera extrinsics during training. The Gaussian network independently predicts primitive parameters from each input image, including covariance, opacity, and spherical harmonics coefficients. At the inference stage, our model can achieve feed-forward reconstruction from flexible multi-frame surround-view input. Experiments on the nuScenes dataset show that our model outperforms existing state-of-the-art feed-forward and scene-optimized reconstruction methods in terms of reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting",
    "authors": [
      "Xiaoyang Yan",
      "Muleilan Pei",
      "Shaojie Shen"
    ],
    "abstract": "3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "head",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "authors": [
      "Hyunjin Kim",
      "Haebeom Jung",
      "Jaesik Park"
    ],
    "abstract": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending, show that our approach reduces the number of Gaussians needed, achieving faster convergence while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Complex-Valued Holographic Radiance Fields",
    "authors": [
      "Yicheng Zhan",
      "Dong-Ha Shin",
      "Seung-Hwan Baek",
      "Kaan Akşit"
    ],
    "abstract": "Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-10",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.ET"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DaRePlane: Direction-aware Representations for Dynamic Scene Reconstruction",
    "authors": [
      "Ange Lou",
      "Benjamin Planche",
      "Zhongpai Gao",
      "Yamin Li",
      "Tianyu Luan",
      "Hao Ding",
      "Meng Zheng",
      "Terrence Chen",
      "Ziyan Wu",
      "Jack Noble"
    ],
    "abstract": "Numerous recent approaches to modeling and re-rendering dynamic scenes leverage plane-based explicit representations, addressing slow training times associated with models like neural radiance fields (NeRF) and Gaussian splatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D plane-based representations is insufficient for high-fidelity re-rendering of scenes with complex motions. In response, we present DaRePlane, a novel direction-aware representation approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. Within NeRF pipelines, DaRePlane computes features for each space-time point by fusing vectors from these recovered planes, then passed to a tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane computes the features of Gaussian points, followed by a tiny multi-head MLP for spatial-time deformation prediction. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. To demonstrate the generality and efficiency of DaRePlane, we test it on both regular and surgical dynamic scenes, for both NeRF and GS systems. Extensive experiments show that DaRePlane yields state-of-the-art performance in novel view synthesis for various complex dynamic scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "ar",
      "nerf",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Turbo3D: Ultra-fast Text-to-3D Generation",
    "authors": [
      "Hanzhe Hu",
      "Tianwei Yin",
      "Fujun Luan",
      "Yiwei Hu",
      "Hao Tan",
      "Zexiang Xu",
      "Sai Bi",
      "Shubham Tulsiani",
      "Kai Zhang"
    ],
    "abstract": "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation",
    "authors": [
      "Tianheng Zhu",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-03",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "animation",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with SAM Mask Guidance",
    "authors": [
      "Jiyeop Kim",
      "Jongwoo Lim"
    ],
    "abstract": "We present a novel approach for 3D indoor scene reconstruction that combines 3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the room layout of the indoor scene, such as walls, ceilings, and floors, while employing 3D Gaussians for other objects. This hybrid approach leverages the strengths of both representations, offering enhanced flexibility and ease of editing. However, joint training of meshes and 3D Gaussians is challenging because it is not clear which primitive should affect which part of the rendered image. Objects close to the room layout often struggle during training, particularly when the room layout is textureless, which can lead to incorrect optimizations and unnecessary 3D Gaussians. To overcome these challenges, we employ Segment Anything Model (SAM) to guide the selection of primitives. The SAM mask loss enforces each instance to be represented by either Gaussians or meshes, ensuring clear separation and stable training. Furthermore, we introduce an additional densification stage without resetting the opacity after the standard densification. This stage mitigates the degradation of image quality caused by a limited number of 3D Gaussians after the standard densification.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGD: Boundary-Enhanced Segment Anything in 3D Gaussian via Gaussian Decomposition",
    "authors": [
      "Xu Hu",
      "Yuxi Wang",
      "Lue Fan",
      "Chuanchen Luo",
      "Junsong Fan",
      "Zhen Lei",
      "Qing Li",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "3D Gaussian Splatting has emerged as an alternative 3D representation for novel view synthesis, benefiting from its high-quality rendering results and real-time rendering speed. However, the 3D Gaussians learned by 3D-GS have ambiguous structures without any geometry constraints. This inherent issue in 3D-GS leads to a rough boundary when segmenting individual objects. To remedy these problems, we propose SAGD, a conceptually simple yet effective boundary-enhanced segmentation pipeline for 3D-GS to improve segmentation accuracy while preserving segmentation speed. Specifically, we introduce a Gaussian Decomposition scheme, which ingeniously utilizes the special structure of 3D Gaussian, finds out, and then decomposes the boundary Gaussians. Moreover, to achieve fast interactive 3D segmentation, we introduce a novel training-free pipeline by lifting a 2D foundation model to 3D-GS. Extensive experiments demonstrate that our approach achieves high-quality 3D segmentation without rough boundary issues, which can be easily applied to other scene editing tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting",
    "authors": [
      "Christian Homeyer",
      "Leon Begiristain",
      "Christoph Schnörr"
    ],
    "abstract": "Recent progress in scene synthesis makes standalone SLAM systems purely based on optimizing hyperprimitives with a Rendering objective possible. However, the tracking performance still lacks behind traditional and end-to-end SLAM systems. An optimal trade-off between robustness, speed and accuracy has not yet been reached, especially for monocular video. In this paper, we introduce a SLAM system based on an end-to-end Tracker and extend it with a Renderer based on recent 3D Gaussian Splatting techniques. Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering results on common SLAM benchmarks. We implemented multiple building blocks of modern SLAM systems to run in parallel, allowing for fast inference on common consumer GPU's. Recent progress in monocular depth prediction and camera calibration allows our system to achieve strong results even on in-the-wild data without known camera intrinsics. Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models",
    "authors": [
      "Paul Henderson",
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Titas Anciukevičius"
    ],
    "abstract": "We present a latent diffusion model over 3D scenes, that can be trained using only 2D image data. To achieve this, we first design an autoencoder that maps multi-view images to 3D Gaussian splats, and simultaneously builds a compressed latent representation of these splats. Then, we train a multi-view diffusion model over the latent space to learn an efficient generative model. This pipeline does not require object masks nor depths, and is suitable for complex scenes with arbitrary camera positions. We conduct careful experiments on two large-scale datasets of complex real-world scenes -- MVImgNet and RealEstate10K. We show that our approach enables generating 3D scenes in as little as 0.2 seconds, either from scratch, from a single input view, or from sparse input views. It produces diverse and high-quality results while running an order of magnitude faster than non-latent diffusion models and earlier NeRF-based generative models",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-18",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus",
    "authors": [
      "Jinchang Zhang",
      "Ningning Xu",
      "Hao Zhang",
      "Guoyu Lu"
    ],
    "abstract": "Depth estimation is a fundamental task in 3D geometry. While stereo depth estimation can be achieved through triangulation methods, it is not as straightforward for monocular methods, which require the integration of global and local information. The Depth from Defocus (DFD) method utilizes camera lens models and parameters to recover depth information from blurred images and has been proven to perform well. However, these methods rely on All-In-Focus (AIF) images for depth estimation, which is nearly impossible to obtain in real-world applications. To address this issue, we propose a self-supervised framework based on 3D Gaussian splatting and Siamese networks. By learning the blur levels at different focal distances of the same scene in the focal stack, the framework predicts the defocus map and Circle of Confusion (CoC) from a single defocused image, using the defocus map as input to DepthNet for monocular depth estimation. The 3D Gaussian splatting model renders defocused images using the predicted CoC, and the differences between these and the real defocused images provide additional supervision signals for the Siamese Defocus self-supervised network. This framework has been validated on both artificially synthesized and real blurred datasets. Subsequent quantitative and visualization experiments demonstrate that our proposed framework is highly effective as a DFD method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussNav: Gaussian Splatting for Visual Navigation",
    "authors": [
      "Xiaohan Lei",
      "Min Wang",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary challenge of IIN arises from the need to recognize the target object across varying viewpoints while ignoring potential distractors. Existing map-based navigation methods typically use Bird's Eye View (BEV) maps, which lack detailed texture representation of a scene. Consequently, while BEV maps are effective for semantic-level visual navigation, they are struggling for instance-level tasks. To this end, we propose a new framework for IIN, Gaussian Splatting for Visual Navigation (GaussNav), which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The GaussNav framework enables the agent to memorize both the geometry and semantic information of the scene, as well as retain the textural features of objects. By matching renderings of similar objects with the target, the agent can accurately identify, ground, and navigate to the specified object. Our GaussNav framework demonstrates a significant performance improvement, with Success weighted by Path Length (SPL) increasing from 0.347 to 0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset. The source code is publicly available at the link: https://github.com/XiaohanLei/GaussNav.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM",
    "authors": [
      "Renwu Li",
      "Wenjing Ke",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "We present MonoGS++, a novel fast and accurate Simultaneous Localization and Mapping (SLAM) method that leverages 3D Gaussian representations and operates solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods largely depended on depth sensors, our approach reduces the hardware dependency and only requires RGB input, leveraging online visual odometry (VO) to generate sparse point clouds in real-time. To reduce redundancy and enhance the quality of 3D scene reconstruction, we implemented a series of methodological enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian insertion to avoid adding redundant Gaussians in previously well-reconstructed areas. Secondly, we introduced clarity-enhancing Gaussian densification module and planar regularization to handle texture-less areas and flat surfaces better. We achieved precise camera tracking results both on the synthetic Replica and real-world TUM-RGBD datasets, comparable to those of the state-of-the-art. Additionally, our method realized a significant 5.57x improvement in frames per second (fps) over the previous state-of-the-art, MonoGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "localization",
      "tracking",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation",
    "authors": [
      "Haozhe Lou",
      "Yurong Liu",
      "Yike Pan",
      "Yiran Geng",
      "Jianteng Chen",
      "Wenlong Ma",
      "Chenglong Li",
      "Lin Wang",
      "Hengzhen Feng",
      "Lu Shi",
      "Liyi Luo",
      "Yongliang Shi"
    ],
    "abstract": "Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes.   We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms.   This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods.   The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-27",
    "categories": [
      "cs.RO",
      "math.NA",
      "math.OC"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting",
    "authors": [
      "Hung Nguyen",
      "An Le",
      "Runfa Li",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "View-Dependent Deformation Fields for 2D Editing of 3D Models",
    "authors": [
      "Martin El Mqirmi",
      "Noam Aigerman"
    ],
    "abstract": "We propose a method for authoring non-realistic 3D objects (represented as either 3D Gaussian Splats or meshes), that comply with 2D edits from specific viewpoints. Namely, given a 3D object, a user chooses different viewpoints and interactively deforms the object in the 2D image plane of each view. The method then produces a \"deformation field\" - an interpolation between those 2D deformations in a smooth manner as the viewpoint changes. Our core observation is that the 2D deformations do not need to be tied to an underlying object, nor share the same deformation space. We use this observation to devise a method for authoring view-dependent deformations, holding several technical contributions: first, a novel way to compositionality-blend between the 2D deformations after lifting them to 3D - this enables the user to \"stack\" the deformations similarly to layers in an editing software, each deformation operating on the results of the previous; second, a novel method to apply the 3D deformation to 3D Gaussian Splats; third, an approach to author the 2D deformations, by deforming a 2D mesh encapsulating a rendered image of the object. We show the versatility and efficacy of our method by adding cartoonish effects to objects, providing means to modify human characters, fitting 3D models to given 2D sketches and caricatures, resolving occlusions, and recreating classic non-realistic paintings as 3D models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping",
    "authors": [
      "Yiming Huang",
      "Beilei Cui",
      "Long Bai",
      "Zhen Chen",
      "Jinlin Wu",
      "Zhen Li",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is essential for precise surgical interventions and robotic tasks in minimally invasive procedures. While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM with high-quality novel view synthesis and fast rendering, these systems struggle with accurate depth and surface reconstruction due to multi-view inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between the reconstructed frames. In this work, we present Endo-2DTAM, a real-time endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which consists of tracking, mapping, and bundle adjustment modules for geometrically accurate reconstruction. Our robust tracking module combines point-to-point and point-to-plane distance metrics, while the mapping module utilizes normal consistency and depth distortion to enhance surface reconstruction quality. We also introduce a pose-consistent strategy for efficient and geometrically coherent keyframe sampling. Extensive experiments on public endoscopic datasets demonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth reconstruction of surgical scenes while maintaining computationally efficient tracking, high-quality visual appearance, and real-time rendering. Our code will be released at github.com/lastbasket/Endo-2DTAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "localization",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes",
    "authors": [
      "Sicheng Yu",
      "Chong Cheng",
      "Yifan Zhou",
      "Xiaojun Yang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can produce high-fidelity novel views. However, previous GS-based methods primarily target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation models, hence underperforming in outdoor scenarios. To address this issue, we propose a RGB-only gaussian splatting SLAM method for unbounded outdoor scenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network to generate consistent pointmaps between frames for pose estimation. Compared to commonly used depth maps, pointmaps include spatial relationships and scene geometry across multiple views, enabling robust camera pose estimation. Then, we propose integrating the estimated camera poses with 3DGS rendering as an end-to-end differentiable pipeline. Our method achieves simultaneous optimization of camera poses and 3DGS scene parameters, significantly enhancing system tracking accuracy. Specifically, we also design an adaptive scale mapper for the pointmap regression network, which provides more accurate pointmap mapping to the 3DGS map representation. Our experiments on the Waymo dataset demonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS methods, and achieves state-of-the-art results in novel view synthesis. Project Page: https://3dagentworld.github.io/opengs-slam/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "outdoor",
      "geometry",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation",
    "authors": [
      "Xiaolu Hou",
      "Mingcheng Li",
      "Dingkang Yang",
      "Jiawei Chen",
      "Ziyun Qian",
      "Xiao Zhao",
      "Yue Jiang",
      "Jinjie Wei",
      "Qingyao Xu",
      "Lihua Zhang"
    ],
    "abstract": "With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-15",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "lightweight",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
    "authors": [
      "Atakan Topaloglu",
      "Kunyi Li",
      "Michael Niemeyer",
      "Nassir Navab",
      "A. Murat Tekalp",
      "Federico Tombari"
    ],
    "abstract": "Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training",
    "authors": [
      "Yiming Wang",
      "Lucy Chai",
      "Xuan Luo",
      "Michael Niemeyer",
      "Manuel Lagunas",
      "Stephen Lombardi",
      "Siyu Tang",
      "Tiancheng Sun"
    ],
    "abstract": "We study the problem of novel view streaming from sparse-view videos, which aims to generate a continuous sequence of high-quality, temporally consistent novel views as new input frames arrive. However, existing novel view synthesis methods struggle with temporal coherence and visual fidelity, leading to flickering and inconsistency. To address these challenges, we introduce history-awareness, leveraging previous frames to reconstruct the scene and improve quality and stability. We propose a hybrid splat-voxel feed-forward scene reconstruction approach that combines Gaussian Splatting to propagate information over time, with a hierarchical voxel grid for temporal fusion. Gaussian primitives are efficiently warped over time using a motion graph that extends 2D tracking models to 3D motion, while a sparse voxel transformer integrates new temporal observations in an error-aware manner. Crucially, our method does not require training on multi-view video datasets, which are currently limited in size and diversity, and can be directly applied to sparse-view video streams in a history-aware manner at inference time. Our approach achieves state-of-the-art performance in both static and streaming scene reconstruction, effectively reducing temporal artifacts and visual artifacts while running at interactive rates (15 fps with 350ms delay) on a single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "tracking",
      "motion",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression",
    "authors": [
      "Lei Liu",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Dong Xu"
    ],
    "abstract": "In this work, we propose a novel compression framework for 3D Gaussian Splatting (3DGS) data. Building on anchor-based 3DGS methodologies, our approach compresses all attributes within each anchor by introducing a novel Hybrid Entropy Model for 3D Gaussian Splatting (HEMGS) to achieve hybrid lossy-lossless compression. It consists of three main components: a variable-rate predictor, a hyperprior network, and an autoregressive network. First, unlike previous methods that adopt multiple models to achieve multi-rate lossy compression, thereby increasing training overhead, our variable-rate predictor enables variable-rate compression with a single model and a hyperparameter $λ$ by producing a learned Quantization Step feature for versatile lossy compression. Second, to improve lossless compression, the hyperprior network captures both scene-agnostic and scene-specific features to generate a prior feature, while the autoregressive network employs an adaptive context selection algorithm with flexible receptive fields to produce a contextual feature. By integrating these two features, HEMGS can accurately estimate the distribution of the current coding element within each attribute, enabling improved entropy coding and reduced storage. We integrate HEMGS into a compression framework, and experimental results on four benchmarks indicate that HEMGS achieves about a 40% average reduction in size while maintaining rendering quality over baseline methods and achieving state-of-the-art compression results.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks",
    "authors": [
      "Alex Quach",
      "Makram Chahine",
      "Alexander Amini",
      "Ramin Hasani",
      "Daniela Rus"
    ],
    "abstract": "Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target manoeuvres curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-21",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "dynamic",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
    "authors": [
      "Chenlu Zhan",
      "Yufei Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "abstract": "Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "localization",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes",
    "authors": [
      "Yuekun Wu",
      "Yik Lung Pang",
      "Andrea Cavallaro",
      "Changjae Oh"
    ],
    "abstract": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-13",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "gaussian splatting",
      "sparse-view",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM",
    "authors": [
      "Samuel Cerezo",
      "Gaetano Meli",
      "Tomás Berriel Martins",
      "Kirill Safronov",
      "Javier Civera"
    ],
    "abstract": "Models and methods originally developed for Novel View Synthesis and Scene Rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as sequential operations and, in many settings, multi-modality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. Additionally, the data are often collected using sensors which are handheld or mounted on drones or mobile robots, which complicates the accurate reproduction of sensor motions. To bridge these gaps, we introduce SLAM&Render, a novel dataset designed to benchmark methods in the intersection between SLAM, Novel View Rendering and Gaussian Splatting. Recorded with a robot manipulator, it uniquely includes 40 sequences with time-synchronized RGB-D images, IMU readings, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of recent integrations of SLAM paradigms within robotic applications. The dataset features five setups with consumer and industrial objects under four controlled lighting conditions, each with separate training and test trajectories. All sequences are static with different levels of object rearrangements and occlusions. Our experimental results, obtained with several baselines from the literature, validate SLAM&Render as a relevant benchmark for this emerging research area.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "illumination",
      "localization",
      "lighting",
      "slam",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment Any 4D Gaussians",
    "authors": [
      "Shengxiang Ji",
      "Guanjun Wu",
      "Jiemin Fang",
      "Jiazhong Cen",
      "Taoran Yi",
      "Wenyu Liu",
      "Qi Tian",
      "Xinggang Wang"
    ],
    "abstract": "Modeling, understanding, and reconstructing the real world are crucial in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable success in modeling and understanding 3D scenes. Similarly, various 4D representations have demonstrated the ability to capture the dynamics of the 4D world. However, there is a dearth of research focusing on segmentation within 4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an efficient temporal identity feature field is introduced to handle Gaussian drifting, with the potential to learn precise identity features from noisy and sparse input. Additionally, a 4D segmentation refinement process is proposed to remove artifacts. Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians and shows the ability to remove, recolor, compose, and render high-quality anything masks. More demos are available at: https://jsxzs.github.io/sa4d/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "dynamic",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds",
    "authors": [
      "Xiaohan Sun",
      "Yinghan Xu",
      "John Dingliana",
      "Carol O'Sullivan"
    ],
    "abstract": "Efficient and realistic crowd rendering is an important element of many real-time graphics applications such as Virtual Reality (VR) and games. To this end, Levels of Detail (LOD) avatar representations such as polygonal meshes, image-based impostors, and point clouds have been proposed and evaluated. More recently, 3D Gaussian Splatting has been explored as a potential method for real-time crowd rendering. In this paper, we present a two-alternative forced choice (2AFC) experiment that aims to determine the perceived quality of 3D Gaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians), and the avatar height in Pixels (corresponding to the viewing distance). Participants viewed pairs of animated 3D Gaussian avatars and were tasked with choosing the most detailed one. Our findings can inform the optimization of LOD strategies in Gaussian-based crowd rendering, thereby helping to achieve efficient rendering while maintaining visual quality in real-time applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "avatar",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSpa: An \"Optimizing-Sparsifying\" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting",
    "authors": [
      "Yangming Zhang",
      "Wenqi Jia",
      "Wei Niu",
      "Miao Yin"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient \"optimizing-sparsifying\" solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10$\\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://noodle-lab.github.io/gaussianspa/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World Hypothesis",
    "authors": [
      "Shuhong Liu",
      "Tianchen Deng",
      "Heng Zhou",
      "Liuzhuozheng Li",
      "Hongyu Wang",
      "Danwei Wang",
      "Mingrui Li"
    ],
    "abstract": "Gaussian Splatting SLAMs have made significant advancements in improving the efficiency and fidelity of real-time reconstructions. However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles. To address this challenge, we present Manhattan Gaussian SLAM, an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness. By seamlessly integrating fused line segments derived from structured scenes, our method ensures robust tracking in textureless indoor areas. Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion. Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting",
    "authors": [
      "Bo Chen",
      "Shoukang Hu",
      "Qi Chen",
      "Chenpeng Du",
      "Ran Yi",
      "Yanmin Qian",
      "Xie Chen"
    ],
    "abstract": "We present GStalker, a 3D audio-driven talking face generation model with Gaussian Splatting for both fast training (40 minutes) and real-time rendering (125 FPS) with a 3$\\sim$5 minute video for training material, in comparison with previous 2D and 3D NeRF-based modeling frameworks which require hours of training and seconds of rendering per frame. Specifically, GSTalker learns an audio-driven Gaussian deformation field to translate and transform 3D Gaussians to synchronize with audio information, in which multi-resolution hashing grid-based tri-plane and temporal smooth module are incorporated to learn accurate deformation for fine-grained facial details. In addition, a pose-conditioned deformation field is designed to model the stabilized torso. To enable efficient optimization of the condition Gaussian deformation field, we initialize 3D Gaussians by learning a coarse static Gaussian representation. Extensive experiments in person-specific videos with audio tracks validate that GSTalker can generate high-fidelity and audio-lips synchronized results with fast training and real-time rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "face",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human Hair Reconstruction with Strand-Aligned 3D Gaussians",
    "authors": [
      "Egor Zakharov",
      "Vanessa Sklyarova",
      "Michael Black",
      "Giljoo Nam",
      "Justus Thies",
      "Otmar Hilliges"
    ],
    "abstract": "We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization",
    "authors": [
      "Zhentao Huang",
      "Di Wu",
      "Zhenbang He",
      "Minglun Gong"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support",
    "authors": [
      "Chenqi Zhang",
      "Yu Feng",
      "Jieru Zhao",
      "Guangda Liu",
      "Wenchao Ding",
      "Chentao Wu",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient Density Control for 3D Gaussian Splatting",
    "authors": [
      "Xiaobin Deng",
      "Changyu Diao",
      "Min Li",
      "Ruohan Yu",
      "Duanqing Xu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated outstanding performance in novel view synthesis, achieving a balance between rendering quality and real-time performance. 3DGS employs Adaptive Density Control (ADC) to increase the number of Gaussians. However, the clone and split operations within ADC are not sufficiently efficient, impacting optimization speed and detail recovery. Additionally, overfitted Gaussians that affect rendering quality may exist, and the original ADC is unable to remove them. To address these issues, we propose two key innovations: (1) Long-Axis Split, which precisely controls the position, shape, and opacity of child Gaussians to minimize the difference before and after splitting. (2) Recovery-Aware Pruning, which leverages differences in recovery speed after resetting opacity to prune overfitted Gaussians, thereby improving generalization performance. Experimental results show that our method significantly enhances rendering quality. Due to resubmission reasons, this version has been abandoned. The improved version is available at https://xiaobin2001.github.io/improved-gs-web .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital-twin imaging based on descattering Gaussian splatting",
    "authors": [
      "Suguru Shimomura",
      "Kazuki Yamanouchi",
      "Jun Tanida"
    ],
    "abstract": "Three-dimensional imaging through scattering media is important in medical science and astronomy. We propose a digital-twin imaging method based on Gaussian splatting to observe an object behind a scattering medium. A digital twin model built through data assimilation, emulates the behavior of objects and environmental changes in a virtual space. By constructing a digital twin using point clouds composed of Gaussians and simulating the scattering process through the convolution of a point spread function, three-dimensional objects behind a scattering medium can be reproduced as a digital twin. In this study, a high-contrast digital twin reproducing a three-dimensional object was successfully constructed from degraded images, assuming that data were acquired from wavefronts disturbed by a scattering medium. This technique reproduces objects by integrating data processing with image measurements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-03",
    "categories": [
      "physics.optics"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering",
    "authors": [
      "Xiaohan Sun",
      "Yinghan Xu",
      "John Dingliana",
      "Carol O'Sullivan"
    ],
    "abstract": "We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "dynamic",
      "ar",
      "gaussian splatting",
      "avatar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VDG: Vision-Only Dynamic Gaussian for Driving Simulation",
    "authors": [
      "Hao Li",
      "Jingfeng Li",
      "Dingwen Zhang",
      "Chenming Wu",
      "Jieqi Shi",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Junwei Han"
    ],
    "abstract": "Dynamic Gaussian splatting has led to impressive scene reconstruction and image synthesis advances in novel views. Existing methods, however, heavily rely on pre-computed poses and Gaussian initialization by Structure from Motion (SfM) algorithms or expensive sensors. For the first time, this paper addresses this issue by integrating self-supervised VO into our pose-free dynamic Gaussian method (VDG) to boost pose and depth initialization and static-dynamic decomposition. Moreover, VDG can work with only RGB image input and construct dynamic scenes at a faster speed and larger scenes compared with the pose-free dynamic view-synthesis method. We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods. Additional video and source code will be posted on our project page at https://3d-aigc.github.io/VDG.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation",
    "authors": [
      "Yongsung Kim",
      "Minjun Park",
      "Jooyoung Choi",
      "Sungroh Yoon"
    ],
    "abstract": "Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction",
    "authors": [
      "Yixin Yang",
      "Yang Zhou",
      "Hui Huang"
    ],
    "abstract": "Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We find that the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectify the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS. Code is available at https://github.com/XiaoXinyyx/Unbiased_Surfel.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Three-Dimensional MRI Reconstruction with Gaussian Representations: Tackling the Undersampling Problem",
    "authors": [
      "Tengya Peng",
      "Ruyi Zha",
      "Zhen Li",
      "Xiaofeng Liu",
      "Qing Zou"
    ],
    "abstract": "Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in the field of computer vision, but remains unexplored in the field of magnetic resonance imaging (MRI). This study explores its potential for the reconstruction of isotropic resolution 3D MRI from undersampled k-space data. We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs 3D Gaussian distributions as an explicit representation for MR volumes. Experimental evaluations indicate that this method can effectively reconstruct voxelized MR images, achieving a quality on par with that of well-established 3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR scheme operates under a self-supervised framework, obviating the need for extensive training datasets or prior model training. This approach introduces significant innovations to the domain, notably the adaptation of 3DGS to MRI reconstruction and the novel application of the existing 3DGS methodology to decompose MR signals, which are presented in a complex-valued format.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-10",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "authors": [
      "Seungtae Nam",
      "Xiangyu Sun",
      "Gyeongjin Kang",
      "Younggeun Lee",
      "Seungjun Oh",
      "Eunbyung Park"
    ],
    "abstract": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study",
    "authors": [
      "Shi Qiu",
      "Binzhu Xie",
      "Qixuan Liu",
      "Pheng-Ann Heng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as an innovative and efficient 3D representation technique. While its potential for extended reality (XR) applications is frequently highlighted, its practical effectiveness remains underexplored. In this work, we examine three distinct 3DGS-based approaches for virtual environment (VE) creation, leveraging their unique strengths for efficient and visually compelling scene representation. By conducting a comparable study, we evaluate the feasibility of 3DGS in creating immersive VEs, identify its limitations in XR applications, and discuss future research and development opportunities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-16",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction",
    "authors": [
      "Kunyi Li",
      "Michael Niemeyer",
      "Zeyu Chen",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Planar Gaussian Splatting",
    "authors": [
      "Farhad G. Zanjani",
      "Hong Cai",
      "Hanno Ackermann",
      "Leila Mirvakhabova",
      "Fatih Porikli"
    ],
    "abstract": "This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering approach to learn the 3D geometry and parse the 3D planes of a scene, directly from multiple RGB images. The PGS leverages Gaussian primitives to model the scene and employ a hierarchical Gaussian mixture approach to group them. Similar Gaussians are progressively merged probabilistically in the tree-structured Gaussian mixtures to identify distinct 3D plane instances and form the overall 3D scene geometry. In order to enable the grouping, the Gaussian primitives contain additional parameters, such as plane descriptors derived by lifting 2D masks from a general 2D segmentation model and surface normals. Experiments show that the proposed PGS achieves state-of-the-art performance in 3D planar reconstruction without requiring either 3D plane labels or depth supervision. In contrast to existing supervised methods that have limited generalizability and struggle under domain shift, PGS maintains its performance across datasets thanks to its neural rendering and scene-specific optimization mechanism, while also being significantly faster than existing optimization-based approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "neural rendering",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MixedGaussianAvatar: Realistically and Geometrically Accurate Head Avatar via Mixed 2D-3D Gaussians",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Qingpo Wuwu",
      "Xinyi Wang",
      "Xingyu Xiao",
      "Ming Lu"
    ],
    "abstract": "Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We use a unified mixed Gaussian representation to integrate the two modalities of 2D image and 3D mesh. Furthermore, the comprehensive experiments demonstrate the superiority of MixedGaussianAvatar. The code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs",
    "authors": [
      "Sadra Safadoust",
      "Fabio Tosi",
      "Fatma Güney",
      "Matteo Poggi"
    ],
    "abstract": "3D Gaussian Splatting (GS) significantly struggles to accurately represent the underlying 3D scene geometry, resulting in inaccuracies and floating artifacts when rendering depth maps. In this paper, we address this limitation, undertaking a comprehensive analysis of the integration of depth priors throughout the optimization process of Gaussian primitives, and present a novel strategy for this purpose. This latter dynamically exploits depth cues from a readily available stereo network, processing virtual stereo pairs rendered by the GS model itself during training and achieving consistent self-improvement of the scene representation. Experimental results on three popular datasets, breaking ground as the first to assess depth accuracy for these models, validate our findings.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes",
    "authors": [
      "Yuang Shi",
      "Simone Gasparini",
      "Géraldine Morin",
      "Chenggang Yang",
      "Wei Tsang Ooi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising representation for photorealistic rendering of 3D scenes. However, its high storage requirements pose significant challenges for practical applications. We observe that Gaussians exhibit distinct roles and characteristics that are analogous to traditional artistic techniques -- Like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features like edges and contours; While other Gaussians represent broader, smoother regions, that are analogous to broader brush strokes that add volume and depth to a painting. Based on this observation, we propose a novel hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians, which cover smooth regions. Sketch Gaussians are efficiently encoded using parametric models, leveraging their geometric coherence, while Patch Gaussians undergo optimized pruning, retraining, and vector quantization to maintain volumetric consistency and storage efficiency. Our comprehensive evaluation across diverse indoor and outdoor scenes demonstrates that this structure-aware approach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41% in LPIPS at equivalent model sizes, and correspondingly, for an indoor scene, our model maintains the visual quality with 2.3% of the original model size.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-22",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis",
    "authors": [
      "Kaijun Deng",
      "Dezhi Zheng",
      "Jindong Xie",
      "Jinbao Wang",
      "Weicheng Xie",
      "Linlin Shen",
      "Siyang Song"
    ],
    "abstract": "Accurately synthesizing talking face videos and capturing fine facial features for individuals with long hair presents a significant challenge. To tackle these challenges in existing methods, we propose a decomposed per-embedding Gaussian fields (DEGSTalk), a 3D Gaussian Splatting (3DGS)-based talking face synthesis method for generating realistic talking faces with long hairs. Our DEGSTalk employs Deformable Pre-Embedding Gaussian Fields, which dynamically adjust pre-embedding Gaussian primitives using implicit expression coefficients. This enables precise capture of dynamic facial regions and subtle expressions. Additionally, we propose a Dynamic Hair-Preserving Portrait Rendering technique to enhance the realism of long hair motions in the synthesized videos. Results show that DEGSTalk achieves improved realism and synthesis quality compared to existing approaches, particularly in handling complex facial dynamics and hair preservation. Our code will be publicly available at https://github.com/CVI-SZU/DEGSTalk.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-28",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PGC: Physics-Based Gaussian Cloth from a Single Pose",
    "authors": [
      "Michelle Guo",
      "Matt Jen-Yuan Chiang",
      "Igor Santesteban",
      "Nikolaos Sarafianos",
      "Hsiao-yu Chen",
      "Oshri Halimi",
      "Aljaž Božič",
      "Shunsuke Saito",
      "Jiajun Wu",
      "C. Karen Liu",
      "Tuur Stuyck",
      "Egor Larionov"
    ],
    "abstract": "We introduce a novel approach to reconstruct simulation-ready garments with intricate appearance. Despite recent advancements, existing methods often struggle to balance the need for accurate garment reconstruction with the ability to generalize to new poses and body shapes or require large amounts of data to achieve this. In contrast, our method only requires a multi-view capture of a single static frame. We represent garments as hybrid mesh-embedded 3D Gaussian splats, where the Gaussians capture near-field shading and high-frequency details, while the mesh encodes far-field albedo and optimized reflectance parameters. We achieve novel pose generalization by exploiting the mesh from our hybrid approach, enabling physics-based simulation and surface rendering techniques, while also capturing fine details with Gaussians that accurately reconstruct garment details. Our optimized garments can be used for simulating garments on novel poses, and garment relighting. Project page: https://phys-gaussian-cloth.github.io .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-26",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "body",
      "ar",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery",
    "authors": [
      "Yanzhe Lyu",
      "Kai Cheng",
      "Xin Kang",
      "Xuejin Chen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis reveals that the 3D-GS densification operation lacks adaptiveness and faces a dilemma between geometry coverage and detail recovery. To address this, we introduce a novel densification operation, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoGS3D: Single-view 3D Reconstruction via Geometric-aware Diffusion Model and Gaussian Splatting",
    "authors": [
      "Qijun Feng",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "We introduce GeoGS3D, a novel two-stage framework for reconstructing detailed 3D objects from single-view images. Inspired by the success of pre-trained 2D diffusion models, our method incorporates an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, facilitating the generation of multi-view consistent images. During the following Gaussian Splatting, these images are fused with epipolar attention, fully utilizing the geometric correlations across views. Moreover, we propose a novel metric, Gaussian Divergence Significance (GDS), to prune unnecessary operations during optimization, significantly accelerating the reconstruction process. Extensive experiments demonstrate that GeoGS3D generates images with high consistency across views and reconstructs high-quality 3D objects, both qualitatively and quantitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Inpainting with Depth-Guided Cross-View Consistency",
    "authors": [
      "Sheng-Yu Huang",
      "Zi-Ting Chou",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting",
    "authors": [
      "Atticus J. Zeller",
      "Haijuan Wu"
    ],
    "abstract": "We present GSplatLoc, a camera localization method that leverages the differentiable rendering capabilities of 3D Gaussian splatting for ultra-precise pose estimation. By formulating pose estimation as a gradient-based optimization problem that minimizes discrepancies between rendered depth maps from a pre-existing 3D Gaussian scene and observed depth images, GSplatLoc achieves translational errors within 0.01 cm and near-zero rotational errors on the Replica dataset - significantly outperforming existing methods. Evaluations on the Replica and TUM RGB-D datasets demonstrate the method's robustness in challenging indoor environments with complex camera motions. GSplatLoc sets a new benchmark for localization in dense mapping, with important implications for applications requiring accurate real-time localization, such as robotics and augmented reality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "robotics",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting",
    "authors": [
      "Yuning Huang",
      "Jiahao Pang",
      "Fengqing Zhu",
      "Dong Tian"
    ],
    "abstract": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
    "authors": [
      "Jonas Kulhanek",
      "Marie-Julie Rakotosaona",
      "Fabian Manhardt",
      "Christina Tsalicoglou",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Songyou Peng",
      "Federico Tombari"
    ],
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "outdoor",
      "dynamic",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MotionGS : Compact Gaussian Splatting SLAM by Motion Filter",
    "authors": [
      "Xinli Guo",
      "Weidong Zhang",
      "Ruonan Liu",
      "Peng Han",
      "Hongtian Chen"
    ],
    "abstract": "With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed tracking is achieved by feature extraction and motion filter on each frame. The joint optimization of poses and 3D Gaussians runs through the entire mapping process. Additionally, the coarse-to-fine pose estimation and compact Gaussian scene representation are implemented by dual keyframe selection and novel loss functions. Experimental results demonstrate that the proposed algorithm not only outperforms the existing methods in tracking and mapping, but also has less memory usage.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compact",
      "tracking",
      "slam",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images",
    "authors": [
      "Guichen Huang",
      "Ruoyu Wang",
      "Xiangjun Gao",
      "Che Sun",
      "Yuwei Wu",
      "Shenghua Gao",
      "Yunde Jia"
    ],
    "abstract": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\\% compared to existing per-pixel Gaussian prediction methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "high-fidelity",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation",
    "authors": [
      "Wenkai Liu",
      "Tao Guan",
      "Bin Zhu",
      "Lili Ju",
      "Zikai Song",
      "Dan Li",
      "Yuesong Wang",
      "Wei Yang"
    ],
    "abstract": "In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology. However, its application to large-scale, high-resolution scenes (exceeding 4k$\\times$4k pixels) is hindered by the excessive computational requirements for managing a large number of Gaussians. Addressing this, we introduce 'EfficientGS', an advanced approach that optimizes 3DGS for high-resolution, large-scale scenes. We analyze the densification process in 3DGS and identify areas of Gaussian over-proliferation. We propose a selective strategy, limiting Gaussian increase to key primitives, thereby enhancing the representational efficiency. Additionally, we develop a pruning mechanism to remove redundant Gaussians, those that are merely auxiliary to adjacent ones. For further enhancement, we integrate a sparse order increment for Spherical Harmonics (SH), designed to alleviate storage constraints and reduce training overhead. Our empirical evaluations, conducted on a range of datasets including extensive 4K+ aerial images, demonstrate that 'EfficientGS' not only expedites training and rendering times but also achieves this with a model size approximately tenfold smaller than conventional 3DGS while maintaining high rendering fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
    "authors": [
      "Yifei Liu",
      "Zhihang Zhong",
      "Yifan Zhan",
      "Sheng Xu",
      "Xiao Sun"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing",
    "authors": [
      "Minnan Pei",
      "Gang Li",
      "Junwen Si",
      "Zeyu Zhu",
      "Zitao Mo",
      "Peisong Wang",
      "Zhuoran Song",
      "Xiaoyao Liang",
      "Jian Cheng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering technique for high-fidelity view synthesis, prompting the development of dedicated 3DGS accelerators for resource-constrained platforms. The conventional decoupled preprocessing-rendering dataflow in existing accelerators has two major limitations: 1) a significant portion of preprocessed Gaussians are not used in rendering, and 2) the same Gaussian gets repeatedly loaded across different tile renderings, resulting in substantial computational and data movement overhead. To address these issues, we propose GCC, a novel accelerator designed for fast and energy-efficient 3DGS inference. GCC introduces a novel dataflow featuring: 1) \\textit{cross-stage conditional processing}, which interleaves preprocessing and rendering to dynamically skip unnecessary Gaussian preprocessing; and 2) \\textit{Gaussian-wise rendering}, ensuring that all rendering operations for a given Gaussian are completed before moving to the next, thereby eliminating duplicated Gaussian loading. We also propose an alpha-based boundary identification method to derive compact and accurate Gaussian regions, thereby reducing rendering costs. We implement our GCC accelerator in 28nm technology. Extensive experiments demonstrate that GCC significantly outperforms the state-of-the-art 3DGS inference accelerator, GSCore, in both performance and energy efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-21",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "neural rendering",
      "high-fidelity",
      "compact",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "authors": [
      "Yuheng Yuan",
      "Qiuhong Shen",
      "Xingyi Yang",
      "Xinchao Wang"
    ],
    "abstract": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing",
    "authors": [
      "Tian-Xing Xu",
      "Wenbo Hu",
      "Yu-Kun Lai",
      "Ying Shan",
      "Song-Hai Zhang"
    ],
    "abstract": "3D Gaussian splatting, emerging as a groundbreaking approach, has drawn increasing attention for its capabilities of high-fidelity reconstruction and real-time rendering. However, it couples the appearance and geometry of the scene within the Gaussian attributes, which hinders the flexibility of editing operations, such as texture swapping. To address this issue, we propose a novel approach, namely Texture-GS, to disentangle the appearance from the geometry by representing it as a 2D texture mapped onto the 3D surface, thereby facilitating appearance editing. Technically, the disentanglement is achieved by our proposed texture mapping module, which consists of a UV mapping MLP to learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian intersections, and a learnable texture to capture the fine-grained appearance. Extensive experiments on the DTU dataset demonstrate that our method not only facilitates high-fidelity appearance editing but also achieves real-time rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Takeo Igarashi",
      "Yufeng Wang",
      "ZeSheng Wang",
      "Yi Yang",
      "Wenrui Ding",
      "Shuchang Zhou"
    ],
    "abstract": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting",
    "authors": [
      "Daniel Wang",
      "Patrick Rim",
      "Tian Tian",
      "Dong Lao",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "We introduce ODE-GS, a novel approach that integrates 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to enable future extrapolation of dynamic 3D scenes. Unlike existing dynamic scene reconstruction methods, which rely on time-conditioned deformation networks and are limited to interpolation within a fixed time window, ODE-GS eliminates timestamp dependency by modeling Gaussian parameter trajectories as continuous-time latent dynamics. Our approach first learns an interpolation model to generate accurate Gaussian trajectories within the observed window, then trains a Transformer encoder to aggregate past trajectories into a latent state evolved via a neural ODE. Finally, numerical integration produces smooth, physically plausible future Gaussian trajectories, enabling rendering at arbitrary future timestamps. On the D-NeRF, NVFi, and HyperNeRF benchmarks, ODE-GS achieves state-of-the-art extrapolation performance, improving metrics by 19.8% compared to leading baselines, demonstrating its ability to accurately represent and predict 3D scene dynamics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering",
    "authors": [
      "Jiameng Li",
      "Yue Shi",
      "Jiezhang Cao",
      "Bingbing Ni",
      "Wenjun Zhang",
      "Kai Zhang",
      "Luc Van Gool"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has attracted great attention in novel view synthesis because of its superior rendering efficiency and high fidelity. However, the trained Gaussians suffer from severe zooming degradation due to non-adjustable representation derived from single-scale training. Though some methods attempt to tackle this problem via post-processing techniques such as selective rendering or filtering techniques towards primitives, the scale-specific information is not involved in Gaussians. In this paper, we propose a unified optimization method to make Gaussians adaptive for arbitrary scales by self-adjusting the primitive properties (e.g., color, shape and size) and distribution (e.g., position). Inspired by the mipmap technique, we design pseudo ground-truth for the target scale and propose a scale-consistency guidance loss to inject scale information into 3D Gaussians. Our method is a plug-in module, applicable for any 3DGS models to solve the zoom-in and zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes",
    "authors": [
      "Jaeyoung Chung",
      "Suyoung Lee",
      "Hyeongjin Nam",
      "Jaerin Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene. Project page: https://luciddreamer-cvlab.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
    "authors": [
      "Xingxing Zuo",
      "Pouya Samangouei",
      "Yunwen Zhou",
      "Yan Di",
      "Mingyang Li"
    ],
    "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of the same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the project page.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images",
    "authors": [
      "Wangbo Yu",
      "Chaoran Feng",
      "Jiye Tang",
      "Jiashu Yang",
      "Zhenyu Tang",
      "Xu Jia",
      "Yuchao Yang",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has demonstrated exceptional capabilities in 3D scene reconstruction and novel view synthesis. However, its training heavily depends on high-quality, sharp images and accurate camera poses. Fulfilling these requirements can be challenging in non-ideal real-world scenarios, where motion-blurred images are commonly encountered in high-speed moving cameras or low-light environments that require long exposure times. To address these challenges, we introduce Event Stream Assisted Gaussian Splatting (EvaGaussians), a novel approach that integrates event streams captured by an event camera to assist in reconstructing high-quality 3D-GS from blurry images. Capitalizing on the high temporal resolution and dynamic range offered by the event camera, we leverage the event streams to explicitly model the formation process of motion-blurred images and guide the deblurring reconstruction of 3D-GS. By jointly optimizing the 3D-GS parameters and recovering camera motion trajectories during the exposure time, our method can robustly facilitate the acquisition of high-fidelity novel views with intricate texture details. We comprehensively evaluated our method and compared it with previous state-of-the-art deblurring rendering methods. Both qualitative and quantitative comparisons demonstrate that our method surpasses existing techniques in restoring fine details from blurry images and producing high-fidelity novel views.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Jiho Jang",
      "Jin-Young Kim",
      "Soonwoo Kwon",
      "Changick Kim"
    ],
    "abstract": "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting",
    "authors": [
      "Mengjiao Ma",
      "Qi Ma",
      "Yue Li",
      "Jiahuan Cheng",
      "Runyi Yang",
      "Bin Ren",
      "Nikola Popovic",
      "Mingqiang Wei",
      "Nicu Sebe",
      "Luc Van Gool",
      "Theo Gevers",
      "Martin R. Oswald",
      "Danda Pani Paudel"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "segmentation",
      "geometry",
      "outdoor",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images",
    "authors": [
      "Jungho Lee",
      "Donghyeong Kim",
      "Dogyoon Lee",
      "Suhwan Cho",
      "Minhyeok Lee",
      "Wonjoon Lee",
      "Taeoh Kim",
      "Dongyoon Wee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention due to its high-quality novel view rendering, motivating research to address real-world challenges. A critical issue is the camera motion blur caused by movement during exposure, which hinders accurate 3D scene reconstruction. In this study, we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that reconstructs precise 3D scenes from motion-blurred images while maintaining real-time rendering speed. Considering the complex motion patterns inherent in real-world camera movements, we predict continuous camera trajectories using neural ordinary differential equations (ODEs). To ensure accurate modeling, we employ rigid body transformations, preserving the shape and size of the object but rely on the discrete integration of sampled frames. To better approximate the continuous nature of motion blur, we introduce a continuous motion refinement (CMR) transformation that refines rigid transformations by incorporating additional learnable parameters. By revisiting fundamental camera theory and leveraging advanced neural ODE techniques, we achieve precise modeling of continuous camera trajectories, leading to improved reconstruction accuracy. Extensive experiments demonstrate state-of-the-art performance both quantitatively and qualitatively on benchmark datasets, which include a wide range of motion blur scenarios, from moderate to extreme blur.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "motion",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians",
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Han Gao",
      "Xiaoke Jiang",
      "Yao Yuan",
      "Lei Zhang"
    ],
    "abstract": "Recently, Gaussian splatting has demonstrated significant success in novel view synthesis. Current methods often regress Gaussians with pixel or point cloud correspondence, linking each Gaussian with a pixel or a 3D point. This leads to the redundancy of Gaussians being used to overfit the correspondence rather than the objects represented by the 3D Gaussians themselves, consequently wasting resources and lacking accurate geometries or textures. In this paper, we introduce LeanGaussian, a novel approach that treats each query in deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or point cloud correspondence constraints. We leverage deformable decoder to iteratively refine the Gaussians layer-by-layer with the image features as keys and values. Notably, the center of each 3D Gaussian is defined as 3D reference points, which are then projected onto the image for deformable attention in 2D space. On both the ShapeNet SRN dataset (category level) and the Google Scanned Objects dataset (open-category level, trained with the Objaverse dataset), our approach, outperforms prior methods by approximately 6.1%, achieving a PSNR of 25.44 and 22.36, respectively. Additionally, our method achieves a 3D reconstruction speed of 7.2 FPS and rendering speed 500 FPS. Codes are available at https://github.com/jwubz123/LeanGaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GAP: Gaussianize Any Point Clouds with Text Guidance",
    "authors": [
      "Weiqi Zhang",
      "Junsheng Zhou",
      "Haotian Geng",
      "Wenyuan Zhang",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh",
    "authors": [
      "Jing Wen",
      "Xiaoming Zhao",
      "Zhongzheng Ren",
      "Alexander G. Schwing",
      "Shenlong Wang"
    ],
    "abstract": "We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians",
    "authors": [
      "Bingling Li",
      "Shengyi Chen",
      "Luchao Wang",
      "Kaimin Liao",
      "Sijie Yan",
      "Yuanjun Xiong"
    ],
    "abstract": "In this work, we explore the possibility of training high-parameter 3D Gaussian splatting (3DGS) models on large-scale, high-resolution datasets. We design a general model parallel training method for 3DGS, named RetinaGS, which uses a proper rendering equation and can be applied to any scene and arbitrary distribution of Gaussian primitives. It enables us to explore the scaling behavior of 3DGS in terms of primitive numbers and training resolutions that were difficult to explore before and surpass previous state-of-the-art reconstruction quality. We observe a clear positive trend of increasing visual quality when increasing primitive numbers with our method. We also demonstrate the first attempt at training a 3DGS model with more than one billion primitives on the full MatrixCity dataset that attains a promising visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis",
    "authors": [
      "Jipeng Lyu",
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ],
    "abstract": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "tracking",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Refining Gaussian Splatting: A Volumetric Densification Approach",
    "authors": [
      "Mohamed Abdul Gafoor",
      "Marius Preda",
      "Titus Zaharia"
    ],
    "abstract": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SAGS: Structure-Aware 3D Gaussian Splatting",
    "authors": [
      "Evangelos Ververas",
      "Rolandos Alexandros Potamias",
      "Jifei Song",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ],
    "abstract": "Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene's geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page https://eververas.github.io/SAGS/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "neural rendering",
      "lightweight",
      "high-fidelity",
      "geometry",
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Photoreal Scene Reconstruction from an Egocentric Device",
    "authors": [
      "Zhaoyang Lv",
      "Maurizio Monge",
      "Ka Chen",
      "Yufeng Zhu",
      "Michael Goesele",
      "Jakob Engel",
      "Zhao Dong",
      "Richard Newcombe"
    ],
    "abstract": "In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device's visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at http://www.projectaria.com/photoreal-reconstruction/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior",
    "authors": [
      "Minghao Wen",
      "Shengjie Wu",
      "Kangkan Wang",
      "Dong Liang"
    ],
    "abstract": "3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a \"one-shot deal\", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HEIR: Learning Graph-Based Motion Hierarchies",
    "authors": [
      "Cheng Zheng",
      "William Koch",
      "Baiang Li",
      "Felix Heide"
    ],
    "abstract": "Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality",
    "authors": [
      "Anastasiya Pechko",
      "Piotr Borycki",
      "Joanna Waczyńska",
      "Daniel Barczyk",
      "Agata Szymańska",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce GS-Verse (Gaussian Splatting for Virtual Environment Rendering and Scene Editing), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, GS-Verse facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "face",
      "vr",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes",
    "authors": [
      "Mark C. Eid",
      "Ana I. L. Namburete",
      "João F. Henriques"
    ],
    "abstract": "Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-08",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "survey",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "authors": [
      "Chuandong Liu",
      "Huijiao Wang",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Plug-and-Play PDE Optimization for 3D Gaussian Splatting: Toward High-Quality Rendering and Reconstruction",
    "authors": [
      "Yifan Mo",
      "Youcheng Cai",
      "Ligang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction by achieving high-quality novel view synthesis with fast rendering speed, introducing 3D Gaussian primitives to represent the scene. However, 3DGS encounters blurring and floaters when applied to complex scenes, caused by the reconstruction of redundant and ambiguous geometric structures. We attribute this issue to the unstable optimization of the Gaussians. To address this limitation, we present a plug-and-play PDE-based optimization method that overcomes the optimization constraints of 3DGS-based approaches in various tasks, such as novel view synthesis and surface reconstruction. Firstly, we theoretically derive that the 3DGS optimization procedure can be modeled as a PDE, and introduce a viscous term to ensure stable optimization. Secondly, we use the Material Point Method (MPM) to obtain a stable numerical solution of the PDE, which enhances both global and local constraints. Additionally, an effective Gaussian densification strategy and particle constraints are introduced to ensure fine-grained details. Extensive qualitative and quantitative experiments confirm that our method achieves state-of-the-art rendering and reconstruction quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-17",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Discretized Gaussian Representation for Tomographic Reconstruction",
    "authors": [
      "Shaokai Wu",
      "Yuxiang Lu",
      "Yapan Guo",
      "Wei Ji",
      "Suizhi Huang",
      "Fengyu Yang",
      "Shalayiding Sirejiding",
      "Qichen He",
      "Jing Tong",
      "Yanbiao Ji",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "abstract": "Computed Tomography (CT) enables detailed cross-sectional imaging but continues to face challenges in balancing reconstruction quality and computational efficiency. While deep learning-based methods have significantly improved image quality and noise reduction, they typically require large-scale training data and intensive computation. Recent advances in scene reconstruction, such as Neural Radiance Fields and 3D Gaussian Splatting, offer alternative perspectives but are not well-suited for direct volumetric CT reconstruction. In this work, we propose Discretized Gaussian Representation (DGR), a novel framework that reconstructs the 3D volume directly using a set of discretized Gaussian functions in an end-to-end manner. To further enhance efficiency, we introduce Fast Volume Reconstruction, a highly parallelized technique that aggregates Gaussian contributions into the voxel grid with minimal overhead. Extensive experiments on both real-world and synthetic datasets demonstrate that DGR achieves superior reconstruction quality and runtime performance across various CT reconstruction scenarios. Our code is publicly available at https://github.com/wskingdom/DGR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-07",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FSFSplatter: Build Surface and Novel Views with Sparse-Views within 2min",
    "authors": [
      "Yibin Zhao",
      "Yihan Pan",
      "Jun Nan",
      "Liwei Chen",
      "Jianjun Yi"
    ],
    "abstract": "Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU, Replica, and BlendedMVS datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatTalk: 3D VQA with Gaussian Splatting",
    "authors": [
      "Anh Thai",
      "Songyou Peng",
      "Kyle Genova",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ],
    "abstract": "Language-guided 3D scene understanding is important for advancing applications in robotics, AR/VR, and human-computer interaction, enabling models to comprehend and interact with 3D environments through natural language. While 2D vision-language models (VLMs) have achieved remarkable success in 2D VQA tasks, progress in the 3D domain has been significantly slower due to the complexity of 3D data and the high cost of manual annotations. In this work, we introduce SplatTalk, a novel method that uses a generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens suitable for direct input into a pretrained LLM, enabling effective zero-shot 3D visual question answering (3D VQA) for scenes with only posed images. During experiments on multiple benchmarks, our approach outperforms both 3D models trained specifically for the task and previous 2D-LMM-based models utilizing only images (our setting), while achieving competitive performance with state-of-the-art 3D LMMs that additionally utilize 3D inputs. Project website: https://splat-talk.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting",
    "authors": [
      "Zhexi Peng",
      "Tianjia Shao",
      "Yong Liu",
      "Jingke Zhou",
      "Yin Yang",
      "Jingdong Wang",
      "Kun Zhou"
    ],
    "abstract": "We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "large scene",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSGAN: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats",
    "authors": [
      "Sangeek Hyun",
      "Jae-Pil Heo"
    ],
    "abstract": "Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a naïve generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability. Project page: https://hse1032.github.io/gsgan.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ray casting",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting",
    "authors": [
      "Liheng Zhang",
      "Weihao Yu",
      "Zubo Lu",
      "Haozhi Gu",
      "Jin Huang"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
    "authors": [
      "Tao Wang",
      "Mengyu Li",
      "Geduo Zeng",
      "Cheng Meng",
      "Qiong Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering. The code is publicly available at https://github.com/DrunkenPoet/GHAP",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "lightweight",
      "geometry",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianStyle: Gaussian Head Avatar via StyleGAN",
    "authors": [
      "Pinxin Liu",
      "Luchuan Song",
      "Daoan Zhang",
      "Hang Hua",
      "Yunlong Tang",
      "Huaijin Tu",
      "Jiebo Luo",
      "Chenliang Xu"
    ],
    "abstract": "Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes",
    "authors": [
      "Gaochao Song",
      "Chong Cheng",
      "Hao Wang"
    ],
    "abstract": "In this paper we present a novel method for efficient and effective 3D surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF) based works typically require extensive training and rendering time due to the adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS) uses an explicit and discrete representation, hence the reconstructed surface is built by the huge number of Gaussian primitives, which leads to excessive memory consumption and rough surface details in sparse Gaussian areas. To address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which establish a continuous scene representation based on discrete 3DGS through kernel regression. The GVKF integrates fast 3DGS rasterization and highly effective scene implicit representations, achieving high-fidelity open scene surface reconstruction. Experiments on challenging scene datasets demonstrate the efficiency and effectiveness of our proposed GVKF, featuring with high reconstruction quality, real-time rendering speed, significant savings in storage and training memory consumption.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians",
    "authors": [
      "Pengchong Hu",
      "Zhizhong Han"
    ],
    "abstract": "Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). State-of-the-art methods employ 3D Gaussians to represent a scene, and render these Gaussians through splatting for higher efficiency and better rendering. However, these methods cannot scale up to extremely large scenes, due to the inefficient tracking and mapping strategies that need to optimize all 3D Gaussians in the limited GPU memories throughout the training to maintain the geometry and color consistency to previous RGBD observations. To resolve this issue, we propose novel tracking and mapping strategies to work with a novel 3D representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied 3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels, without needing to learn locations, rotations, and multi-dimensional variances. Tying Gaussians to views not only significantly saves storage but also allows us to employ many more Gaussians to represent local details in the limited GPU memory. Moreover, our strategies remove the need of maintaining all Gaussians learnable throughout the training, while improving rendering quality, and tracking accuracy. We justify the effectiveness of these designs, and report better performance over the latest methods on the widely used benchmarks in terms of rendering and tracking accuracy and scalability. Please see our project page for code and videos at https://machineperceptionlab.github.io/VTGaussian-SLAM-Project .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "large scene",
      "tracking",
      "localization",
      "slam",
      "ar",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "dynamic",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "avatar",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting",
    "authors": [
      "Kyusun Cho",
      "Joungbin Lee",
      "Heeji Yoon",
      "Yeobin Hong",
      "Jaehoon Ko",
      "Sangjun Ahn",
      "Seungryong Kim"
    ],
    "abstract": "We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker's superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at https://github.com/KU-CVLAB/GaussianTalker/ .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-24",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly Enhanced Quality",
    "authors": [
      "Taoran Yi",
      "Jiemin Fang",
      "Zanwei Zhou",
      "Junjie Wang",
      "Guanjun Wu",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wenyu Liu",
      "Xinggang Wang",
      "Qi Tian"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3D-GS) has achieved great success in reconstructing and rendering real-world scenes. To transfer the high rendering quality to generation tasks, a series of research works attempt to generate 3D-Gaussian assets from text. However, the generated assets have not achieved the same quality as those in reconstruction tasks. We observe that Gaussians tend to grow without control as the generation process may cause indeterminacy. Aiming at highly enhancing the generation quality, we propose a novel framework named GaussianDreamerPro. The main idea is to bind Gaussians to reasonable geometry, which evolves over the whole generation process. Along different stages of our framework, both the geometry and appearance can be enriched progressively. The final output asset is constructed with 3D Gaussians bound to mesh, which shows significantly enhanced details and quality compared with previous methods. Notably, the generated asset can also be seamlessly integrated into downstream manipulation pipelines, e.g. animation, composition, and simulation etc., greatly promoting its potential in wide applications. Demos are available at https://taoranyi.com/gaussiandreamerpro/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "compact",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatFormer: Point Transformer for Robust 3D Gaussian Splatting",
    "authors": [
      "Yutong Chen",
      "Marko Mihajlovic",
      "Xiyi Chen",
      "Yiming Wang",
      "Sergey Prokudin",
      "Siyu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MBA-SLAM: Motion Blur Aware Gaussian Splatting SLAM",
    "authors": [
      "Peng Wang",
      "Lingzhe Zhao",
      "Yin Zhang",
      "Shiyu Zhao",
      "Peidong Liu"
    ],
    "abstract": "Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual deblur SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs and enhance image deblurring. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras' local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach. Code is available at https://github.com/WU-CVGL/MBA-SLAM.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-13",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "localization",
      "lighting",
      "slam",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting",
    "authors": [
      "Yasmine Omri",
      "Connor Ding",
      "Tsachy Weissman",
      "Thierry Tambe"
    ],
    "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "compact",
      "semantic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
    "authors": [
      "Adam Yang",
      "Nadula Kadawedduwa",
      "Tianfu Wang",
      "Sunny Sharma",
      "Emily F. Wisinski",
      "Jhayron S. Pérez-Carrasquilla",
      "Kyle J. C. Hall",
      "Dean Calhoun",
      "Jonathan Starfeldt",
      "Timothy P. Canty",
      "Maria Molina",
      "Christopher Metzler"
    ],
    "abstract": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting with State-Space Modeling",
    "authors": [
      "Junli Deng",
      "Yihao Luo"
    ],
    "abstract": "Dynamic scene rendering has taken a leap forward with the rise of 4D Gaussian Splatting, but there's still one elusive challenge: how to make 3D Gaussians move through time as naturally as they would in the real world, all while keeping the motion smooth and consistent. In this paper, we unveil a fresh approach that blends state-space modeling with Wasserstein geometry, paving the way for a more fluid and coherent representation of dynamic scenes. We introduce a State Consistency Filter that merges prior predictions with the current observations, enabling Gaussians to stay true to their way over time. We also employ Wasserstein distance regularization to ensure smooth, consistent updates of Gaussian parameters, reducing motion artifacts. Lastly, we leverage Wasserstein geometry to capture both translational motion and shape deformations, creating a more physically plausible model for dynamic scenes. Our approach guides Gaussians along their natural way in the Wasserstein space, achieving smoother, more realistic motion and stronger temporal coherence. Experimental results show significant improvements in rendering quality and efficiency, outperforming current state-of-the-art techniques.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities",
    "authors": [
      "Peizhi Yan",
      "Rabab Ward",
      "Qiang Tang",
      "Shan Du"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the \"Gaussian Deja-vu\" framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NEDS-SLAM: A Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting",
    "authors": [
      "Yiming Ji",
      "Yang Liu",
      "Guanghu Xie",
      "Boyu Ma",
      "Zongwu Xie"
    ],
    "abstract": "We propose NEDS-SLAM, a dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time. In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption. Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View Pruning method to eliminate outlier gaussians, thereby effectively enhancing the quality of scene representations. Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "lightweight",
      "compact",
      "semantic",
      "tracking",
      "slam",
      "head",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions",
    "authors": [
      "Junjie Wang",
      "Jiemin Fang",
      "Xiaopeng Zhang",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "abstract": "Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes -- 2 hours).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces",
    "authors": [
      "Yingwenqi Jiang",
      "Jiadong Tu",
      "Yuan Liu",
      "Xifeng Gao",
      "Xiaoxiao Long",
      "Wenping Wang",
      "Yuexin Ma"
    ],
    "abstract": "The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds. However, the explicit and discrete representation encounters challenges when applied to scenes featuring reflective surfaces. In this paper, we present GaussianShader, a novel method that applies a simplified shading function on 3D Gaussians to enhance the neural rendering in scenes with reflective surfaces while preserving the training and rendering efficiency. The main challenge in applying the shading function lies in the accurate normal estimation on discrete 3D Gaussians. Specifically, we proposed a novel normal estimation framework based on the shortest axis directions of 3D Gaussians with a delicately designed loss to make the consistency between the normals and the geometries of Gaussian spheres. Experiments show that GaussianShader strikes a commendable balance between efficiency and visual quality. Our method surpasses Gaussian Splatting in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When compared to prior works handling reflective surfaces, such as Ref-NeRF, our optimization time is significantly accelerated (23h vs. 0.58h). Please click on our project website to see more results.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GauSTAR: Gaussian Surface Tracking and Reconstruction",
    "authors": [
      "Chengwei Zheng",
      "Lixin Xue",
      "Juan Zarate",
      "Jie Song"
    ],
    "abstract": "3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GauSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GauSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GauSTAR maintains the mesh topology and tracks the meshes using Gaussians. For regions where topology changes, GauSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://eth-ait.github.io/GauSTAR/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "tracking",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS",
    "authors": [
      "Zhihao Guo",
      "Peng Wang",
      "Zidong Chen",
      "Xiangyu Kong",
      "Yan Lyu",
      "Guanyu Gao",
      "Liangxiu Han"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\\% PSNR improvements on the MipNeRF 360 dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization",
    "authors": [
      "KL Navaneet",
      "Kossar Pourahmadi Meibodi",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a new method for modeling and rendering 3D radiance fields that achieves much faster learning and rendering time compared to SOTA NeRF methods. However, it comes with a drawback in the much larger storage demand compared to NeRF methods since it needs to store the parameters for several 3D Gaussians. We notice that many Gaussians may share similar parameters, so we introduce a simple vector quantization method based on K-means to quantize the Gaussian parameters while optimizing them. Then, we store the small codebook along with the index of the code for each Gaussian. We compress the indices further by sorting them and using a method similar to run-length encoding. Moreover, we use a simple regularizer to encourage zero opacity (invisible Gaussians) to reduce the storage and rendering time by a large factor through reducing the number of Gaussians. We do extensive experiments on standard benchmarks as well as an existing 3D dataset that is an order of magnitude larger than the standard benchmarks used in this field. We show that our simple yet effective method can reduce the storage cost for 3DGS by 40 to 50x and rendering time by 2 to 3x with a very small drop in the quality of rendered images.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Guanjun Wu",
      "Taoran Yi",
      "Jiemin Fang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Wei",
      "Wenyu Liu",
      "Qi Tian",
      "Xinggang Wang"
    ],
    "abstract": "Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-10-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM",
    "authors": [
      "Mingrui Li",
      "Weijian Chen",
      "Na Cheng",
      "Jingyuan Xu",
      "Dong Li",
      "Hongyu Wang"
    ],
    "abstract": "The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread attention due to its excellent performance in real-time high-fidelity rendering. However, in real-world environments with dynamic objects, existing 3DGS-based SLAM systems often face mapping errors and tracking drift issues. To address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM system tailored for dynamic scenes. In terms of tracking, unlike traditional methods, we directly perform dynamic segmentation on Gaussians and map them back to the front-end to obtain dynamic point labels through a Gaussian pyramid network, achieving precise dynamic removal and robust tracking. For mapping, we impose rendering penalties on dynamically labeled Gaussians, which are updated through the network, to avoid irreversible erroneous removal caused by simple pruning. Our results on real-world datasets demonstrate that our method is competitive in tracking compared to baseline methods, generating fewer artifacts and higher-quality reconstructions in rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-05",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "high-fidelity",
      "tracking",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning",
    "authors": [
      "Wenzhi Guo",
      "Bing Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for photorealistic view synthesis, representing scenes with spatially distributed Gaussian primitives. While highly effective for rendering, achieving accurate and complete surface reconstruction remains challenging due to the unstructured nature of the representation and the absence of explicit geometric supervision. In this work, we propose DiGS, a unified framework that embeds Signed Distance Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong and interpretable surface priors. By associating each Gaussian with a learnable SDF value, DiGS explicitly aligns primitives with underlying geometry and improves cross-view consistency. To further ensure dense and coherent coverage, we design a geometry-guided grid growth strategy that adaptively distributes Gaussians along geometry-consistent regions under a multi-scale hierarchy. Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and Tanks& Temples, demonstrate that DiGS consistently improves reconstruction accuracy and completeness while retaining high rendering fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-09",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGBD GS-ICP SLAM",
    "authors": [
      "Seongbo Ha",
      "Jiung Yeon",
      "Hyeonwoo Yu"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting",
    "authors": [
      "Huibin Zhao",
      "Weipeng Guan",
      "Peng Lu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the complementary characteristics of LiDAR and image sensors to capture both geometric structures and visual details of 3D scenes. To this end, the 3D Gaussians are initialized from colourized LiDAR points and optimized using differentiable rendering. In order to achieve high-fidelity mapping, we introduce a pyramid-based training approach to effectively learn multi-level features and incorporate depth loss derived from LiDAR measurements to improve geometric feature perception. Through well-designed strategies for Gaussian-Map expansion, keyframe selection, thread management, and custom CUDA acceleration, our framework achieves real-time photo-realistic mapping. Numerical experiments are performed to evaluate the superior performance of our method compared to state-of-the-art 3D reconstruction systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-05",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "acceleration",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
    "authors": [
      "Yuxi Hu",
      "Jun Zhang",
      "Kuangyi Chen",
      "Zhe Zhang",
      "Friedrich Fraundorfer"
    ],
    "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "geometry",
      "ar",
      "gaussian splatting",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding",
    "authors": [
      "Haoyi Jiang",
      "Liu Liu",
      "Tianheng Cheng",
      "Xinjie Wang",
      "Tianwei Lin",
      "Zhizhong Su",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "3D Semantic Occupancy Prediction is fundamental for spatial understanding, yet existing approaches face challenges in scalability and generalization due to their reliance on extensive labeled data and computationally intensive voxel-wise representations. In this paper, we introduce GaussTR, a novel Gaussian-based Transformer framework that unifies sparse 3D modeling with foundation model alignment through Gaussian representations to advance 3D spatial understanding. GaussTR predicts sparse sets of Gaussians in a feed-forward manner to represent 3D scenes. By splatting the Gaussians into 2D views and aligning the rendered features with foundation models, GaussTR facilitates self-supervised 3D representation learning and enables open-vocabulary semantic occupancy prediction without requiring explicit annotations. Empirical experiments on the Occ3D-nuScenes dataset demonstrate GaussTR's state-of-the-art zero-shot performance of 12.27 mIoU, along with a 40% reduction in training time. These results highlight the efficacy of GaussTR for scalable and holistic 3D spatial understanding, with promising implications in autonomous driving and embodied agents. The code is available at https://github.com/hustvl/GaussTR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "understanding",
      "ar",
      "autonomous driving",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores",
    "authors": [
      "Zimu Liao",
      "Jifeng Ding",
      "Siwei Cui",
      "Ruixuan Gong",
      "Boni Hu",
      "Yi Wang",
      "Hengjie Li",
      "XIngcheng Zhang",
      "Hui Wang",
      "Rong Fu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where conditional alpha-blending dominates the computational cost in the rendering pipeline. This paper proposes TC-GS, an algorithm-independent universal module that expands the applicability of Tensor Core (TCU) for 3DGS, leading to substantial speedups and seamless integration into existing 3DGS optimization frameworks. The key innovation lies in mapping alpha computation to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS implementations. TC-GS provides plug-and-play acceleration for existing top-tier acceleration algorithms and integrates seamlessly with rendering pipeline designs, such as Gaussian compression and redundancy elimination algorithms. Additionally, we introduce a global-to-local coordinate transformation to mitigate rounding errors from quadratic terms of pixel coordinates caused by Tensor Core half-precision computation. Extensive experiments demonstrate that our method maintains rendering quality while providing an additional 2.18x speedup over existing Gaussian acceleration algorithms, thereby achieving a total acceleration of up to 5.6x.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-30",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "acceleration",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos",
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Mi Zhou",
      "Jinwen Chen",
      "Dan Xu"
    ],
    "abstract": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes",
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Luyang Tang",
      "Ronggang Wang"
    ],
    "abstract": "Reconstructing dynamic scenes from video sequences is a highly promising task in the multimedia domain. While previous methods have made progress, they often struggle with slow rendering and managing temporal complexities such as significant motion and object appearance/disappearance. In this paper, we propose SaRO-GS as a novel dynamic scene representation capable of achieving real-time rendering while effectively handling temporal complexities in dynamic scenes. To address the issue of slow rendering speed, we adopt a Gaussian primitive-based representation and optimize the Gaussians in 4D space, which facilitates real-time rendering with the assistance of 3D Gaussian Splatting. Additionally, to handle temporally complex dynamic scenes, we introduce a Scale-aware Residual Field. This field considers the size information of each Gaussian primitive while encoding its residual feature and aligns with the self-splitting behavior of Gaussian primitives. Furthermore, we propose an Adaptive Optimization Schedule, which assigns different optimization strategies to Gaussian primitives based on their distinct temporal properties, thereby expediting the reconstruction of dynamic regions. Through evaluations on monocular and multi-view datasets, our method has demonstrated state-of-the-art performance. Please see our project page at https://yjb6.github.io/SaRO-GS.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-09",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding",
    "authors": [
      "Junyan Su",
      "Baozhu Zhao",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point features with anchor embeddings has significantly enhanced the performance of newer 3DGS variants. While significant advances have been made, it is still challenging to boost rendering performance. Feature embeddings have difficulty accurately representing colors from different perspectives under varying lighting conditions, which leads to a washed-out appearance. Another reason is the lack of a proper densification strategy that prevents Gaussian point growth in thinly initialized areas, resulting in blurriness and needle-shaped artifacts. To address them, we propose Metamon-GS, from innovative viewpoints of variance-guided densification strategy and multi-level hash grid. The densification strategy guided by variance specifically targets Gaussians with high gradient variance in pixels and compensates for the importance of regions with extra Gaussians to improve reconstruction. The latter studies implicit global lighting conditions and accurately interprets color from different perspectives and feature embeddings. Our thorough experiments on publicly available datasets show that Metamon-GS surpasses its baseline model and previous versions, delivering superior quality in rendering novel views.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes",
    "authors": [
      "Isabella Liu",
      "Hao Su",
      "Xiaolong Wang"
    ],
    "abstract": "Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of detailed structure and time consistency from dynamic observations. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh from dynamic input. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from dynamic observations. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: https://www.liuisabella.com/DG-Mesh",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "high-fidelity",
      "geometry",
      "dynamic",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
    "authors": [
      "Sharath Girish",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ],
    "abstract": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce per-point memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach develops a pruning stage which results in scene representations with fewer Gaussians, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce storage memory by more than an order of magnitude all while preserving the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x lesser memory and faster training/inference speed. Project page and code is available https://efficientgaussian.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "lightweight",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction",
    "authors": [
      "Meixi Song",
      "Xin Lin",
      "Dizhe Zhang",
      "Haodong Li",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery",
    "authors": [
      "Mengya Xu",
      "Ziqi Guo",
      "An Wang",
      "Long Bai",
      "Hongliang Ren"
    ],
    "abstract": "As a crucial and intricate task in robotic minimally invasive surgery, reconstructing surgical scenes using stereo or monocular endoscopic video holds immense potential for clinical applications. NeRF-based techniques have recently garnered attention for the ability to reconstruct scenes implicitly. On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly using 3D Gaussians and projects them onto a 2D plane as a replacement for the complex volume rendering in NeRF. However, these methods face challenges regarding surgical scene reconstruction, such as slow inference, dynamic scenes, and surgical tool occlusion. This work explores and reviews state-of-the-art (SOTA) approaches, discussing their innovations and implementation principles. Furthermore, we replicate the models and conduct testing and evaluation on two datasets. The test results demonstrate that with advancements in these techniques, achieving real-time, high-quality reconstructions becomes feasible.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-08",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities",
    "authors": [
      "Yanqi Bao",
      "Tianyu Ding",
      "Jing Huo",
      "Yaoli Liu",
      "Yuxin Li",
      "Wenbin Li",
      "Yang Gao",
      "Jiebo Luo"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the potential to become a mainstream method for 3D representations. It can effectively transform multi-view images into explicit 3D Gaussian through efficient training, and achieve real-time rendering of novel views. This survey aims to analyze existing 3DGS-related works from multiple intersecting perspectives, including related tasks, technologies, challenges, and opportunities. The primary objective is to provide newcomers with a rapid understanding of the field and to assist researchers in methodically organizing existing technologies and challenges. Specifically, we delve into the optimization, application, and extension of 3DGS, categorizing them based on their focuses or motivations. Additionally, we summarize and classify nine types of technical modules and corresponding improvements identified in existing works. Based on these analyses, we further examine the common challenges and technologies across various tasks, proposing potential research opportunities.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
    "authors": [
      "Jiahui Lei",
      "Yijia Weng",
      "Adam Harley",
      "Leonidas Guibas",
      "Kostas Daniilidis"
    ],
    "abstract": "We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping",
    "authors": [
      "Emanuele Giacomini",
      "Luca Di Giammarino",
      "Lorenzo De Rebotti",
      "Giorgio Grisetti",
      "Martin R. Oswald"
    ],
    "abstract": "LiDARs provide accurate geometric measurements, making them valuable for ego-motion estimation and reconstruction tasks. Although its success, managing an accurate and lightweight representation of the environment still poses challenges. Both classic and NeRF-based solutions have to trade off accuracy over memory and processing times. In this work, we build on recent advancements in Gaussian Splatting methods to develop a novel LiDAR odometry and mapping pipeline that exclusively relies on Gaussian primitives for its scene representation. Leveraging spherical projection, we drive the refinement of the primitives uniquely from LiDAR measurements. Experiments show that our approach matches the current registration performance, while achieving SOTA results for mapping tasks with minimal GPU requirements. This efficiency makes it a strong candidate for further exploration and potential adoption in real-time robotics estimation tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-21",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "robotics",
      "motion",
      "ar",
      "mapping",
      "gaussian splatting",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RGS-DR: Deferred Reflections and Residual Shading in 2D Gaussian Splatting",
    "authors": [
      "Georgios Kouros",
      "Minye Wu",
      "Tinne Tuytelaars"
    ],
    "abstract": "In this work, we address specular appearance in inverse rendering using 2D Gaussian splatting with deferred shading and argue for a refinement stage to improve specular detail, thereby bridging the gap with reconstruction-only methods. Our pipeline estimates editable material properties and environment illumination while employing a directional residual pass that captures leftover view-dependent effects for further refining novel view synthesis. In contrast to per-Gaussian shading with shortest-axis normals and normal residuals, which tends to result in more noisy geometry and specular appearance, a pixel-deferred surfel formulation with specular residuals yields sharper highlights, cleaner materials, and improved editability. We evaluate our approach on rendering and reconstruction quality on three popular datasets featuring glossy objects, and also demonstrate high-quality relighting and material editing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "reflection",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression",
    "authors": [
      "Francesco Di Sario",
      "Riccardo Renzulli",
      "Marco Grangetto",
      "Akihiro Sugimoto",
      "Enzo Tartaglione"
    ],
    "abstract": "3D Gaussian Splatting enhances real-time performance in novel view synthesis by representing scenes with mixtures of Gaussians and utilizing differentiable rasterization. However, it typically requires large storage capacity and high VRAM, demanding the design of effective pruning and compression techniques. Existing methods, while effective in some scenarios, struggle with scalability and fail to adapt models based on critical factors such as computing capabilities or bandwidth, requiring to re-train the model under different configurations. In this work, we propose a novel, model-agnostic technique that organizes Gaussians into several hierarchical layers, enabling progressive Level of Detail (LoD) strategy. This method, combined with recent approach of compression of 3DGS, allows a single model to instantly scale across several compression ratios, with minimal to none impact to quality compared to a single non-scalable model and without requiring re-training. We validate our approach on typical datasets and benchmarks, showcasing low distortion and substantial gains in terms of scalability and adaptability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "ar",
      "gaussian splatting",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing",
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Zixuan Zeng",
      "Yuxuan Yao",
      "Li Zhang"
    ],
    "abstract": "In inverse rendering, accurately modeling visibility and indirect radiance for incident light is essential for capturing secondary effects. Due to the absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have either adopted a simplified rendering equation or used learnable parameters to approximate incident light, resulting in inaccurate material and lighting estimations. To this end, we introduce inter-reflective Gaussian splatting (IRGS) for inverse rendering. To capture inter-reflection, we apply the full rendering equation without simplification and compute incident radiance on the fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we present an efficient optimization scheme to handle the computational demands of Monte Carlo sampling for rendering equation evaluation. Furthermore, we introduce a novel strategy for querying the indirect radiance of incident light when relighting the optimized scenes. Extensive experiments on multiple standard benchmarks validate the effectiveness of IRGS, demonstrating its capability to accurately model complex inter-reflection effects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "lighting",
      "ar",
      "gaussian splatting",
      "reflection",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM",
    "authors": [
      "Yinlong Bai",
      "Hongxin Zhang",
      "Sheng Zhong",
      "Junkai Niu",
      "Hai Li",
      "Yijia He",
      "Yi Zhou"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques. Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs). These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality. In this paper, we improve existing methods in terms of GPU memory usage while enhancing rendering quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we propose merging them in voxel space based on geometric similarity. This reduces GPU memory usage without impacting system runtime performance. Furthermore, rendering quality is improved by initializing 3D Gaussian primitives via Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire scene. Quantitative and qualitative evaluations on publicly available datasets demonstrate the effectiveness of our improvements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "slam",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GASP: Gaussian Splatting for Physic-Based Simulations",
    "authors": [
      "Piotr Borycki",
      "Weronika Smolak",
      "Joanna Waczyńska",
      "Marcin Mazur",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "Physics simulation is paramount for modeling and utilizing 3D scenes in various real-world applications. However, integrating with state-of-the-art 3D scene rendering techniques such as Gaussian Splatting (GS) remains challenging. Existing models use additional meshing mechanisms, including triangle or tetrahedron meshing, marching cubes, or cage meshes. Alternatively, we can modify the physics-grounded Newtonian dynamics to align with 3D Gaussian components. Current models take the first-order approximation of a deformation map, which locally approximates the dynamics by linear transformations. In contrast, our GS for Physics-Based Simulations (GASP) pipeline uses parametrized flat Gaussian distributions. Consequently, the problem of modeling Gaussian components using the physics engine is reduced to working with 3D points. In our work, we present additional rules for manipulating Gaussians, demonstrating how to adapt the pipeline to incorporate meshes, control Gaussian sizes during simulations, and enhance simulation efficiency. This is achieved through the Gaussian grouping strategy, which implements hierarchical structuring and enables simulations to be performed exclusively on selected Gaussians. The resulting solution can be integrated into any physics engine that can be treated as a black box. As demonstrated in our studies, the proposed pipeline exhibits superior performance on a diverse range of benchmark datasets designed for 3D object rendering. The project webpage, which includes additional visualizations, can be found at https://waczjoan.github.io/GASP.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Coca-Splat: Collaborative Optimization for Camera Parameters and 3D Gaussians",
    "authors": [
      "Jiamin Wu",
      "Hongyang Li",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ],
    "abstract": "In this work, we introduce Coca-Splat, a novel approach to addressing the challenges of sparse view pose-free scene reconstruction and novel view synthesis (NVS) by jointly optimizing camera parameters with 3D Gaussians. Inspired by deformable DEtection TRansformer, we design separate queries for 3D Gaussians and camera parameters and update them layer by layer through deformable Transformer layers, enabling joint optimization in a single network. This design demonstrates better performance because to accurately render views that closely approximate ground-truth images relies on precise estimation of both 3D Gaussians and camera parameters. In such a design, the centers of 3D Gaussians are projected onto each view by camera parameters to get projected points, which are regarded as 2D reference points in deformable cross-attention. With camera-aware multi-view deformable cross-attention (CaMDFA), 3D Gaussians and camera parameters are intrinsically connected by sharing the 2D reference points. Additionally, 2D reference point determined rays (RayRef) defined from camera centers to the reference points assist in modeling relationship between 3D Gaussians and camera parameters through RQ-decomposition on an overdetermined system of equations derived from the rays, enhancing the relationship between 3D Gaussians and camera parameters. Extensive evaluation shows that our approach outperforms previous methods, both pose-required and pose-free, on RealEstate10K and ACID within the same pose-free setting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing",
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ],
    "abstract": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-23",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction",
    "authors": [
      "David Charatan",
      "Sizhe Li",
      "Andrea Tagliasacchi",
      "Vincent Sitzmann"
    ],
    "abstract": "We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-19",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video",
    "authors": [
      "Qiang Hu",
      "Zihan Zheng",
      "Houqiang Zhong",
      "Sihua Fu",
      "Li Song",
      "XiaoyunZhang",
      "Guangtao Zhai",
      "Yanfeng Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has substantial potential for enabling photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number of Gaussians and their associated attributes poses significant challenges for storage and transmission. Existing methods typically handle dynamic 3DGS representation and compression separately, neglecting motion information and the rate-distortion (RD) trade-off during training, leading to performance degradation and increased model redundancy. To address this gap, we propose 4DGC, a novel rate-aware 4D Gaussian compression framework that significantly reduces storage size while maintaining superior RD performance for FVV. Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation that utilizes a compact motion grid combined with sparse compensated Gaussians to exploit inter-frame similarities. This representation effectively handles large motions, preserving quality and reducing temporal redundancy. Furthermore, we present an end-to-end compression scheme that employs differentiable quantization and a tiny implicit entropy model to compress the motion grid and compensated Gaussians efficiently. The entire framework is jointly optimized using a rate-distortion trade-off. Extensive experiments demonstrate that 4DGC supports variable bitrates and consistently outperforms existing methods in RD performance across multiple datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections",
    "authors": [
      "Yongtang Bao",
      "Chengjie Tang",
      "Yuze Wang",
      "Haojie Li"
    ],
    "abstract": "Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancing Single Image to 3D Generation using Gaussian Splatting and Hybrid Diffusion Priors",
    "authors": [
      "Hritam Basak",
      "Hadi Tabatabaee",
      "Shreekant Gayaka",
      "Ming-Feng Li",
      "Xin Yang",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Min Sun",
      "Zhaozheng Yin"
    ],
    "abstract": "3D object generation from a single image involves estimating the full 3D geometry and texture of unseen views from an unposed RGB image captured in the wild. Accurately reconstructing an object's complete 3D structure and texture has numerous applications in real-world scenarios, including robotic manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements in 3D object generation have introduced techniques that reconstruct an object's 3D shape and texture by optimizing the efficient representation of Gaussian Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable disparity exists between the training datasets of these models, leading to distinct differences in their outputs. While 2D models generate highly detailed visuals, they lack cross-view consistency in geometry and texture. In contrast, 3D models ensure consistency across different views but often result in overly smooth textures. We propose bridging the gap between 2D and 3D diffusion models to address this limitation by integrating a two-stage frequency-based distillation loss with Gaussian Splatting. Specifically, we leverage geometric priors in the low-frequency spectrum from a 3D diffusion model to maintain consistent geometry and use a 2D diffusion model to refine the fidelity and texture in the high-frequency spectrum of the generated 3D structure, resulting in more detailed and fine-grained outcomes. Our approach enhances geometric consistency and visual quality, outperforming the current SOTA. Additionally, we demonstrate the easy adaptability of our method for efficient object pose estimation and tracking.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation",
    "authors": [
      "Jinwei Lin"
    ],
    "abstract": "One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression",
    "authors": [
      "Zicong Chen",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Lei Liu",
      "Dong Xu"
    ],
    "abstract": "Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints. Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively. Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding. Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook. By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-26",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting",
    "authors": [
      "Boying Li",
      "Zhixi Cai",
      "Yuan-Fang Li",
      "Ian Reid",
      "Hamid Rezatofighi"
    ],
    "abstract": "We propose Hier-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a novel hierarchical categorical representation, which enables accurate global 3D semantic mapping, scaling-up capability, and explicit semantic label prediction in the 3D world. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making it particularly challenging and costly for scene understanding. To address this problem, we introduce a novel hierarchical representation that encodes semantic information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs). We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization. Furthermore, we enhance the whole SLAM system, resulting in improved tracking and mapping performance. Our \\MethodName{} outperforms existing dense SLAM methods in both mapping and tracking accuracy, while achieving a 2x operation speed-up. Additionally, it achieves on-par semantic rendering performance compared to existing methods while significantly reducing storage and training time requirements. Rendering FPS impressively reaches 2,000 with semantic information and 3,000 without it. Most notably, it showcases the capability of handling the complex real-world scene with more than 500 semantic classes, highlighting its valuable scaling-up capability. The open-source code is available at https://github.com/LeeBY68/Hier-SLAM",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "compact",
      "lighting",
      "slam",
      "tracking",
      "ar",
      "gaussian splatting",
      "mapping",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene",
    "authors": [
      "Jiahao Wu",
      "Rui Peng",
      "Zhiyan Wang",
      "Lu Xiao",
      "Luyang Tang",
      "Jinbo Yan",
      "Kaiqiang Xiong",
      "Ronggang Wang"
    ],
    "abstract": "Novel view synthesis has long been a practical but challenging task, although the introduction of numerous methods to solve this problem, even combining advanced representations like 3D Gaussian Splatting, they still struggle to recover high-quality results and often consume too much storage memory and training time. In this paper we propose Swift4D, a divide-and-conquer 3D Gaussian Splatting method that can handle static and dynamic primitives separately, achieving a good trade-off between rendering quality and efficiency, motivated by the fact that most of the scene is the static primitive and does not require additional dynamic properties. Concretely, we focus on modeling dynamic transformations only for the dynamic primitives which benefits both efficiency and quality. We first employ a learnable decomposition strategy to separate the primitives, which relies on an additional parameter to classify primitives as static or dynamic. For the dynamic primitives, we employ a compact multi-resolution 4D Hash mapper to transform these primitives from canonical space into deformation space at each timestamp, and then mix the static and dynamic primitives to produce the final output. This divide-and-conquer method facilitates efficient training and reduces storage redundancy. Our method not only achieves state-of-the-art rendering quality while being 20X faster in training than previous SOTA methods with a minimum storage requirement of only 30MB on real-world datasets. Code is available at https://github.com/WuJH2001/swift4d.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views",
    "authors": [
      "Zhenya Yang"
    ],
    "abstract": "Surgical simulation is essential for medical training, enabling practitioners to develop crucial skills in a risk-free environment while improving patient safety and surgical outcomes. However, conventional methods for building simulation environments are cumbersome, time-consuming, and difficult to scale, often resulting in poor details and unrealistic simulations. In this paper, we propose a Gaussian Splatting-based framework to directly reconstruct interactive surgical scenes from endoscopic data while ensuring efficiency, rendering quality, and realism. A key challenge in this data-driven simulation paradigm is the restricted movement of endoscopic cameras, which limits viewpoint diversity. As a result, the Gaussian Splatting representation overfits specific perspectives, leading to reduced geometric accuracy. To address this issue, we introduce a novel virtual camera-based regularization method that adaptively samples virtual viewpoints around the scene and incorporates them into the optimization process to mitigate overfitting. An effective depth-based regularization is applied to both real and virtual views to further refine the scene geometry. To enable fast deformation simulation, we propose a sparse control node-based Material Point Method, which integrates physical properties into the reconstructed scene while significantly reducing computational costs. Experimental results on representative surgical data demonstrate that our method can efficiently reconstruct and simulate surgical scenes from sparse endoscopic views. Notably, our method takes only a few minutes to reconstruct the surgical scene and is able to produce physically plausible deformations in real-time with user-defined interactions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "medical",
      "geometry",
      "ar",
      "gaussian splatting",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
    "authors": [
      "Lei Yao",
      "Yi Wang",
      "Yi Zhang",
      "Moyun Liu",
      "Lap-Pui Chau"
    ],
    "abstract": "The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HART: Human Aligned Reconstruction Transformer",
    "authors": [
      "Xiyi Chen",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Taewon Kang",
      "Sergey Prokudin",
      "Ming Lin"
    ],
    "abstract": "We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "sparse-view",
      "geometry",
      "body",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
    "authors": [
      "Mengqi Guo",
      "Bo Xu",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "neural rendering",
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mip-Splatting: Alias-free 3D Gaussian Splatting",
    "authors": [
      "Zehao Yu",
      "Anpei Chen",
      "Binbin Huang",
      "Torsten Sattler",
      "Andreas Geiger"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency. However, strong artifacts can be observed when changing the sampling rate, \\eg, by changing focal length or camera distance. We find that the source for this phenomenon can be attributed to the lack of 3D frequency constraints and the usage of a 2D dilation filter. To address this problem, we introduce a 3D smoothing filter which constrains the size of the 3D Gaussian primitives based on the maximal sampling frequency induced by the input views, eliminating high-frequency artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip filter, which simulates a 2D box filter, effectively mitigates aliasing and dilation issues. Our evaluation, including scenarios such a training on single-scale images and testing on multiple scales, validates the effectiveness of our approach.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Bootstrap-GS: Self-Supervised Augmentation for High-Fidelity Gaussian Splatting",
    "authors": [
      "Yifei Gao",
      "Kerui Ren",
      "Jie Ou",
      "Lei Wang",
      "Jiaji Wu",
      "Jun Cheng"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) have established new benchmarks for rendering quality and efficiency in 3D reconstruction. However, 3D-GS faces critical limitations when generating novel views that significantly deviate from those encountered during training. Moreover, issues such as dilation and aliasing arise during zoom operations. These challenges stem from a fundamental issue: training sampling deficiency. In this paper, we introduce a bootstrapping framework to address this problem. Our approach synthesizes pseudo-ground truth from novel views that align with the limited training set and reintegrates these synthesized views into the training pipeline. Experimental results demonstrate that our bootstrapping technique not only reduces artifacts but also improves quantitative metrics. Furthermore, our technique is highly adaptable, allowing various Gaussian-based method to benefit from its integration.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-29",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting",
    "authors": [
      "Xiaoyang Lyu",
      "Yang-Tian Sun",
      "Yi-Hua Huang",
      "Xiuzhe Wu",
      "Ziyi Yang",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ],
    "abstract": "In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal",
    "authors": [
      "Yuxin Wang",
      "Qianyi Wu",
      "Guofeng Zhang",
      "Dan Xu"
    ],
    "abstract": "This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM",
    "authors": [
      "Gyuhyeon Pak",
      "Hae Min Cho",
      "Euntai Kim"
    ],
    "abstract": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D reconstruction and robust camera pose tracking in real-time by representing each scene element using a Gaussian distribution constrained to the local tangent plane. This effectively models the local surface as a 2D Gaussian disk aligned with the underlying geometry, leading to more consistent depth interpretation across multiple viewpoints compared to conventional 3D ellipsoid-based representations with isotropic uncertainty. To integrate this representation into the SLAM pipeline, we embed the surface-aligned Gaussian disks into a Generalized ICP framework by introducing anisotropic covariance prior without altering the underlying registration formulation. Furthermore we propose a geometry-aware loss that supervises photometric, depth, and normal consistency. Our system achieves real-time operation while preserving both visual and geometric fidelity. Extensive experiments on the Replica and TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems in terms of localization accuracy, reconstruction completeness, while maintaining the rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-24",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering",
    "authors": [
      "Zehao Li",
      "Hao Jiang",
      "Yujun Cai",
      "Jianing Chen",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Yiwei Wang",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ],
    "abstract": "Although dynamic scene reconstruction has long been a fundamental challenge in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a promising direction by enabling high-quality, real-time rendering through explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic reconstruction often suffer from \\textit{spatio-temporal incoherence} during initialization, where canonical Gaussians are constructed by aggregating observations from multiple frames without temporal distinction. This results in spatio-temporally entangled representations, making it difficult to model dynamic motion accurately. To overcome this limitation, we propose \\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a plug-and-play module that learns spatio-temporal probability distributions for each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation field, and a consistency regularization to jointly disentangle spatial and temporal patterns. Extensive experiments demonstrate that incorporating our module into existing 3DGS-based dynamic scene reconstruction frameworks leads to notable improvements in both reconstruction quality and spatio-temporal consistency across synthetic and real-world benchmarks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes",
    "authors": [
      "Zhuopeng Li",
      "Yilin Zhang",
      "Chenming Wu",
      "Jianke Zhu",
      "Liangjun Zhang"
    ],
    "abstract": "The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural rendering, enabling real-time production of high-quality renderings. However, the previous 3DGS-based methods have limitations in urban scenes due to reliance on initial Structure-from-Motion(SfM) points and difficulties in rendering distant, sky and low-texture areas. To overcome these challenges, we propose a hybrid optimization method named HO-Gaussian, which combines a grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency on SfM point initialization, allowing for rendering of urban scenes, and incorporates the Point Densitification to enhance rendering quality in problematic regions during training. Furthermore, we introduce Gaussian Direction Encoding as an alternative for spherical harmonics in the rendering pipeline, which enables view-dependent color representation. To account for multi-camera systems, we introduce neural warping to enhance object consistency across different cameras. Experimental results on widely used autonomous driving datasets demonstrate that HO-Gaussian achieves photo-realistic rendering in real-time on multi-camera urban datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "urban scene",
      "motion",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering",
    "authors": [
      "Tao Lu",
      "Mulin Yu",
      "Linning Xu",
      "Yuanbo Xiangli",
      "Limin Wang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved the state-of-the-art rendering quality and speed combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less area and lighting effects. We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrates an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing the rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian Splatting",
    "authors": [
      "Weiwei Cai",
      "Weicai Ye",
      "Peng Ye",
      "Tong He",
      "Tao Chen"
    ],
    "abstract": "Dynamic scene reconstruction has garnered significant attention in recent years due to its capabilities in high-quality and real-time rendering. Among various methodologies, constructing a 4D spatial-temporal representation, such as 4D-GS, has gained popularity for its high-quality rendered images. However, these methods often produce suboptimal surfaces, as the discrete 3D Gaussian point clouds fail to align with the object's surface precisely. To address this problem, we propose DynaSurfGS to achieve both photorealistic rendering and high-fidelity surface reconstruction of dynamic scenarios. Specifically, the DynaSurfGS framework first incorporates Gaussian features from 4D neural voxels with the planar-based Gaussian Splatting to facilitate precise surface reconstruction. It leverages normal regularization to enforce the smoothness of the surface of dynamic objects. It also incorporates the as-rigid-as-possible (ARAP) constraint to maintain the approximate rigidity of local neighborhoods of 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain closely aligned throughout. Extensive experiments demonstrate that DynaSurfGS surpasses state-of-the-art methods in both high-fidelity surface reconstruction and photorealistic rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving. Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources. To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features. We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics. We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position. We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption. Code is available at: https://github.com/huang-yh/GaussianFormer.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "semantic",
      "ar",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianCut: Interactive segmentation via graph cut for 3D Gaussian Splatting",
    "authors": [
      "Umangi Jain",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ],
    "abstract": "We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts intuitive user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians. Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and further refine these coarse estimates using our graph construction. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any additional segmentation-aware training.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction",
    "authors": [
      "Xiufeng Huang",
      "Ka Chun Cheung",
      "Runmin Cong",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced Image-to-3D content creation but requires substantial computational resources and large datasets, posing challenges to training models from scratch. Current methods usually entangle the prediction of 3D Gaussian geometry and appearance, which rely heavily on data-driven priors and result in slow regression speeds. To address this, we propose \\method, a disentangled framework for efficient 3D Gaussian prediction. Our method extracts features from local image pairs using a stereo vision backbone and fuses them via global attention blocks. Dedicated point and Gaussian prediction heads generate multi-view point-maps for geometry and Gaussian features for appearance, combined as GS-maps to represent the 3DGS object. A refinement network enhances these GS-maps for high-quality reconstruction. Unlike existing methods that depend on camera parameters, our approach achieves pose-free 3D reconstruction, improving robustness and practicality. By reducing resource demands while maintaining high-quality outputs, \\method provides an efficient, scalable solution for real-world 3D content generation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On the Skinning of Gaussian Avatars",
    "authors": [
      "Nikolaos Zioulis",
      "Nikolaos Kotarelas",
      "Georgios Albanis",
      "Spyridon Thermos",
      "Anargyros Chatzitofis"
    ],
    "abstract": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "human",
      "ar",
      "gaussian splatting",
      "mapping",
      "avatar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction",
    "authors": [
      "Binxiao Huang",
      "Zhengwu Liu",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (GS) has emerged as a powerful representation for high-quality scene reconstruction, offering compelling rendering quality. However, the training process of GS often suffers from slow convergence due to inefficient densification and suboptimal spatial distribution of Gaussian primitives. In this work, we present a comprehensive analysis of the split and clone operations during the densification phase, revealing their distinct roles in balancing detail preservation and computational efficiency. Building upon this analysis, we propose a global-to-local densification strategy, which facilitates more efficient growth of Gaussians across the scene space, promoting both global coverage and local refinement. To cooperate with the proposed densification strategy and promote sufficient diffusion of Gaussian primitives in space, we introduce an energy-guided coarse-to-fine multi-resolution training framework, which gradually increases resolution based on energy density in 2D images. Additionally, we dynamically prune unnecessary Gaussian primitives to speed up the training. Extensive experiments on MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our approach significantly accelerates training,achieving over 2x speedup with fewer Gaussian primitives and superior reconstruction performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D Gaussian Splatting Approach",
    "authors": [
      "Chaozheng Wen",
      "Jingwen Tong",
      "Yingdong Hu",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a long-standing challenge. This issue has been escalated due to denser network deployment, larger antenna arrays, and broader bandwidth in next-generation networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting (3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. While WRF-GS demonstrates remarkable effectiveness, it faces limitations in capturing high-frequency signal variations caused by complex multipath effects. To overcome these limitations, we propose WRF-GS+, an enhanced framework that integrates electromagnetic wave physics into the neural network design. WRF-GS+ leverages deformable 3D Gaussians to model both static and dynamic components of the WRF, significantly improving its ability to characterize signal variations. In addition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS modeling process and improving computational efficiency. Experimental results demonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial spectrum synthesis, including ray tracing and other deep-learning approaches. Notably, WRF-GS+ achieves state-of-the-art performance in the received signal strength indication (RSSI) and channel state information (CSI) prediction tasks, surpassing existing methods by more than 0.7 dB and 3.36 dB, respectively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-06",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ray tracing",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
    "authors": [
      "Hannah Schieber",
      "Dominik Frischmann",
      "Victor Schaack",
      "Simon Boche",
      "Angela Schoellig",
      "Stefan Leutenegger",
      "Daniel Roth"
    ],
    "abstract": "Mobile reconstruction has the potential to support time-critical tasks such as tele-guidance and disaster response, where operators must quickly gain an accurate understanding of the environment. Full high-fidelity scene reconstruction is computationally expensive and often unnecessary when only specific points of interest (POIs) matter for timely decision making. We address this challenge with CoRe-GS, a semantic POI-focused extension of Gaussian Splatting (GS). Instead of optimizing every scene element uniformly, CoRe-GS first produces a fast segmentation-ready GS representation and then selectively refines splats belonging to semantically relevant POIs detected during data acquisition. This targeted refinement reduces training time to 25\\% compared to full semantic GS while improving novel view synthesis quality in the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM) and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs enables faster and higher-quality mobile reconstruction tailored to operational needs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Digital Twin Generation from Visual Data: A Survey",
    "authors": [
      "Andrew Melnik",
      "Benjamin Alt",
      "Giang Nguyen",
      "Artur Wilkowski",
      "Maciej Stefańczyk",
      "Qirui Wu",
      "Sinan Harms",
      "Helge Rhodin",
      "Manolis Savva",
      "Michael Beetz"
    ],
    "abstract": "This survey explores recent developments in generating digital twins from videos. Such digital twins can be used for robotics application, media content creation, or design and construction works. We analyze various approaches, including 3D Gaussian Splatting, generative in-painting, semantic segmentation, and foundation models highlighting their advantages and limitations. Additionally, we discuss challenges such as occlusions, lighting variations, and scalability, as well as potential future research directions. This survey aims to provide a comprehensive overview of state-of-the-art methodologies and their implications for real-world applications. Awesome list: https://github.com/ndrwmlnk/awesome-digital-twins",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "survey",
      "semantic",
      "lighting",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering",
    "authors": [
      "Yanyan Li",
      "Chenyu Lyu",
      "Yan Di",
      "Guangyao Zhai",
      "Gim Hee Lee",
      "Federico Tombari"
    ],
    "abstract": "During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces. This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data. To mitigate this issue, we propose a novel approach called GeoGaussian. Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy. Finally, the pipeline ensures that the scene's geometry and texture are maintained through constrained optimization processes with explicit geometry constraints. Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions. Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
    "authors": [
      "Shuting He",
      "Guangquan Jie",
      "Changshuo Wang",
      "Yun Zhou",
      "Shuming Hu",
      "Guanbin Li",
      "Henghui Ding"
    ],
    "abstract": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting",
    "authors": [
      "Jaeseok Park",
      "Chanoh Park",
      "Minsu Kim",
      "Soohwan Kim"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ControlGS: Consistent Structural Compression Control for Deployment-Aware Gaussian Splatting",
    "authors": [
      "Fengdi Zhang",
      "Yibao Sun",
      "Hongkun Cao",
      "Ruqi Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a highly deployable real-time method for novel view synthesis. In practice, it requires a universal, consistent control mechanism that adjusts the trade-off between rendering quality and model compression without scene-specific tuning, enabling automated deployment across different device performances and communication bandwidths. In this work, we present ControlGS, a control-oriented optimization framework that maps the trade-off between Gaussian count and rendering quality to a continuous, scene-agnostic, and highly responsive control axis. Extensive experiments across a wide range of scene scales and types (from small objects to large outdoor scenes) demonstrate that, by adjusting a globally unified control hyperparameter, ControlGS can flexibly generate models biased toward either structural compactness or high fidelity, regardless of the specific scene scale or complexity, while achieving markedly higher rendering quality with the same or fewer Gaussians compared to potential competing methods. Project page: https://zhang-fengdi.github.io/ControlGS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "outdoor",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting",
    "authors": [
      "Chi Yan",
      "Delin Qu",
      "Dan Xu",
      "Bin Zhao",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li"
    ],
    "abstract": "In this paper, we introduce \\textbf{GS-SLAM} that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussians in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. Project page: https://gs-slam.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Segment Any 3D Gaussians",
    "authors": [
      "Jiazhong Cen",
      "Jiemin Fang",
      "Chen Yang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D promptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D visual prompts as input, SAGA can segment the corresponding 3D target represented by 3D Gaussians within 4 ms. This is achieved by attaching an scale-gated affinity feature to each 3D Gaussian to endow it a new property towards multi-granularity segmentation. Specifically, a scale-aware contrastive training strategy is proposed for the scale-gated affinity feature learning. It 1) distills the segmentation capability of the Segment Anything Model (SAM) from 2D masks into the affinity features and 2) employs a soft scale gate mechanism to deal with multi-granularity ambiguity in 3D segmentation through adjusting the magnitude of each feature channel according to a specified 3D physical scale. Evaluations demonstrate that SAGA achieves real-time multi-granularity segmentation with quality comparable to state-of-the-art methods. As one of the first methods addressing promptable segmentation in 3D-GS, the simplicity and effectiveness of SAGA pave the way for future advancements in this field. Our code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
    "authors": [
      "Changyue Shi",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Minghao Chen",
      "Wenwen Pan",
      "Yan Yang",
      "Jiajun Ding",
      "Zhou Yu",
      "Jun Yu"
    ],
    "abstract": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation",
    "authors": [
      "Shuzhao Xie",
      "Weixiang Zhang",
      "Chen Tang",
      "Yunpeng Bai",
      "Rongwei Lu",
      "Shijia Ge",
      "Zhi Wang"
    ],
    "abstract": "3D Gaussian Splatting demonstrates excellent quality and speed in novel view synthesis. Nevertheless, the huge file size of the 3D Gaussians presents challenges for transmission and storage. Current works design compact models to replace the substantial volume and attributes of 3D Gaussians, along with intensive training to distill information. These endeavors demand considerable training time, presenting formidable hurdles for practical deployment. To this end, we propose MesonGS, a codec for post-training compression of 3D Gaussians. Initially, we introduce a measurement criterion that considers both view-dependent and view-independent factors to assess the impact of each Gaussian point on the rendering output, enabling the removal of insignificant points. Subsequently, we decrease the entropy of attributes through two transformations that complement subsequent entropy coding techniques to enhance the file compression rate. More specifically, we first replace rotation quaternions with Euler angles; then, we apply region adaptive hierarchical transform to key attributes to reduce entropy. Lastly, we adopt finer-grained quantization to avoid excessive information loss. Moreover, a well-crafted finetune scheme is devised to restore quality. Extensive experiments demonstrate that MesonGS significantly reduces the size of 3D Gaussians while preserving competitive quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting",
    "authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framework EG-Gaussian, which utilizes epipolar geometry and graph networks for 3D scene reconstruction. Initially, we integrate epipolar geometry into the 3DGS initialization phase to enhance initial 3DGS point construction. Then, we specifically design a graph learning module to refine 3DGS spatial features, in which we incorporate both spatial coordinates and angular relationships among neighboring points. Experiments on indoor and outdoor benchmark datasets demonstrate that our approach significantly improves reconstruction accuracy compared to 3DGS-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "geometry",
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction",
    "authors": [
      "Rui Wang",
      "Quentin Lohmeyer",
      "Mirko Meboldt",
      "Siyu Tang"
    ],
    "abstract": "Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstructionin highly dynamic, interaction-rich environments. Project page: https://batfacewayne.github.io/DeGauss.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying",
    "authors": [
      "Hairong Yin",
      "Huangying Zhan",
      "Yi Xu",
      "Raymond A. Yeh"
    ],
    "abstract": "Open-vocabulary 3D scene understanding is crucial for robotics applications, such as natural language-driven manipulation, human-robot interaction, and autonomous navigation. Existing methods for querying 3D Gaussian Splatting often struggle with inconsistent 2D mask supervision and lack a robust 3D point-level retrieval mechanism. In this work, (i) we present a novel point-level querying framework that performs tracking on segmentation masks to establish a semantically consistent ground-truth for distilling the language Gaussians; (ii) we introduce a GT-anchored querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians. Extensive experiments on three benchmark datasets demonstrate that the proposed method outperforms state-of-the-art performance. Our method achieves an mIoU improvement of +4.14, +20.42, and +1.7 on the LERF, 3D-OVS, and Replica datasets. These results validate our framework as a promising step toward open-vocabulary understanding in real-world robotic systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "human",
      "semantic",
      "tracking",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting",
    "authors": [
      "Michael Büttner",
      "Jonathan Francis",
      "Helge Rhodin",
      "Andrew Melnik"
    ],
    "abstract": "This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-05",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
    "authors": [
      "Joongho Jo",
      "Hyeongwon Kim",
      "Jongsun Park"
    ],
    "abstract": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-21",
    "categories": [
      "cs.CV",
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration",
    "authors": [
      "Chong Cheng",
      "Yu Hu",
      "Sicheng Yu",
      "Beizhen Zhao",
      "Zijian Wang",
      "Hao Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving",
    "authors": [
      "Nan Huang",
      "Xiaobao Wei",
      "Wenzhao Zheng",
      "Pengju An",
      "Ming Lu",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ],
    "abstract": "Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving. Despite the efficacy of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting (3DGS) emerges as a promising direction due to its faster speed and more explicit representation. However, most existing street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios. To facilitate efficient 3D scene reconstruction without costly annotations, we propose a self-supervised street Gaussian ($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from 4D consistency. We represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics. We conduct extensive experiments on the challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our $\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves the best performance without using 3D annotations. Code is available at: https://github.com/nnanhuang/S3Gaussian/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "4d",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving",
    "authors": [
      "Rui Song",
      "Chenwei Liang",
      "Yan Xia",
      "Walter Zimmer",
      "Hu Cao",
      "Holger Caesar",
      "Andreas Festag",
      "Alois Knoll"
    ],
    "abstract": "Dynamic scene rendering opens new avenues in autonomous driving by enabling closed-loop simulations with photorealistic data, which is crucial for validating end-to-end algorithms. However, the complex and highly dynamic nature of traffic environments presents significant challenges in accurately rendering these scenes. In this paper, we introduce a novel 4D Gaussian Splatting (4DGS) approach, which incorporates context and temporal deformation awareness to improve dynamic scene rendering. Specifically, we employ a 2D semantic segmentation foundation model to self-supervise the 4D semantic features of Gaussians, ensuring meaningful contextual embedding. Simultaneously, we track the temporal deformation of each Gaussian across adjacent frames. By aggregating and encoding both semantic and temporal deformation features, each Gaussian is equipped with cues for potential deformation compensation within 3D space, facilitating a more precise representation of dynamic scenes. Experimental results show that our method improves 4DGS's ability to capture fine details in dynamic scene rendering for autonomous driving and outperforms other self-supervised methods in 4D reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms semantic features with each Gaussian, enabling broader applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ARGS: Advanced Regularization on Aligning Gaussians over the Surface",
    "authors": [
      "Jeong Uk Lee",
      "Sung Hee Choi"
    ],
    "abstract": "Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, \"needle-like\" shapes-by favoring more balanced, \"disk-like\" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ],
    "abstract": "Neural rendering techniques have made substantial progress in generating photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has achieved high quality novel view synthesis as well as fast rendering speed. However, 3D Gaussians lack proficiency in defining accurate 3D geometric structures despite their explicit primitive representations. This is due to the fact that Gaussian's attributes are primarily tailored and fine-tuned for rendering diverse 2D images by their anisotropic nature. To pave the way for efficient 3D reconstruction, we present Spherical Gaussians, a simple and effective representation for 3D geometric boundaries, from which we can directly reconstruct 3D feature curves from a set of calibrated multi-view images. Spherical Gaussians is optimized from grid initialization with a view-based rendering loss, where a 2D edge map is rendered at a specific view and then compared to the ground-truth edge map extracted from the corresponding image, without the need for any 3D guidance or supervision. Given Spherical Gaussians serve as intermedia for the robust edge representation, we further introduce a novel optimization-based algorithm called SGCR to directly extract accurate parametric curves from aligned Spherical Gaussians. We demonstrate that SGCR outperforms existing state-of-the-art methods in 3D edge reconstruction while enjoying great efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "neural rendering",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting",
    "authors": [
      "Shujuan Li",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "abstract": "Reconstructing open surfaces from multi-view images is vital in digitalizing complex objects in daily life. A widely used strategy is to learn unsigned distance functions (UDFs) by checking if their appearance conforms to the image observations through neural rendering. However, it is still hard to learn continuous and implicit UDF representations through 3D Gaussians splatting (3DGS) due to the discrete and explicit scene representation, i.e., 3D Gaussians. To resolve this issue, we propose a novel approach to bridge the gap between 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D Gaussian planes on surfaces, and then, leverage the self-supervision and gradient-based inference to supervise unsigned distances in both near and far area to surfaces. To this end, we introduce novel constraints and strategies to constrain the learning of 2D Gaussians to pursue more stable optimization and more reliable self-supervision, addressing the challenges brought by complicated gradient field on or near the zero level set of UDFs. We report numerical and visual comparisons with the state-of-the-art on widely used benchmarks and real data to show our advantages in terms of accuracy, efficiency, completeness, and sharpness of reconstructed open surfaces with boundaries.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "authors": [
      "Yunxiao Shi",
      "Yinhao Zhu",
      "Shizhong Han",
      "Jisoo Jeong",
      "Amin Ansari",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "abstract": "Occupancy prediction infers fine-grained 3D geometry and semantics from camera images of the surrounding environment, making it a critical perception task for autonomous driving. Existing methods either adopt dense grids as scene representation, which is difficult to scale to high resolution, or learn the entire scene using a single set of sparse queries, which is insufficient to handle the various object characteristics. In this paper, we present ODG, a hierarchical dual sparse Gaussian representation to effectively capture complex scene dynamics. Building upon the observation that driving scenes can be universally decomposed into static and dynamic counterparts, we define dual Gaussian queries to better model the diverse scene objects. We utilize a hierarchical Gaussian transformer to predict the occupied voxel centers and semantic classes along with the Gaussian parameters. Leveraging the real-time rendering capability of 3D Gaussian Splatting, we also impose rendering supervision with available depth and semantic map annotations injecting pixel-level alignment to boost occupancy learning. Extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets new state-of-the-art results while maintaining low inference cost.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field",
    "authors": [
      "Jiarui Hu",
      "Xianhao Chen",
      "Boyin Feng",
      "Guanglin Li",
      "Liangjing Yang",
      "Hujun Bao",
      "Guofeng Zhang",
      "Zhaopeng Cui"
    ],
    "abstract": "Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-24",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "localization",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RadSplatter: Extending 3D Gaussian Splatting to Radio Frequencies for Wireless Radiomap Extrapolation",
    "authors": [
      "Yiheng Wang",
      "Ye Xue",
      "Shutao Zhang",
      "Tsung-Hui Chang"
    ],
    "abstract": "A radiomap represents the spatial distribution of wireless signal strength, critical for applications like network optimization and autonomous driving. However, constructing radiomap relies on measuring radio signal power across the entire system, which is costly in outdoor environments due to large network scales. We present RadSplatter, a framework that extends 3D Gaussian Splatting (3DGS) to radio frequencies for efficient and accurate radiomap extrapolation from sparse measurements. RadSplatter models environmental scatterers and radio paths using 3D Gaussians, capturing key factors of radio wave propagation. It employs a relaxed-mean (RM) scheme to reparameterize the positions of 3D Gaussians from noisy and dense 3D point clouds. A camera-free 3DGS-based projection is proposed to map 3D Gaussians onto 2D radio beam patterns. Furthermore, a regularized loss function and recursive fine-tuning using highly structured sparse measurements in real-world settings are applied to ensure robust generalization. Experiments on synthetic and real-world data show state-of-the-art extrapolation accuracy and execution speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-18",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "outdoor",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
    "authors": [
      "Yufan Chen",
      "Lizhen Wang",
      "Qijing Li",
      "Hongjiang Xiao",
      "Shengping Zhang",
      "Hongxun Yao",
      "Yebin Liu"
    ],
    "abstract": "The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "head",
      "gaussian splatting",
      "ar",
      "avatar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images",
    "authors": [
      "Simon Niedermayr",
      "Christoph Neuhauser Rüdiger Westermann"
    ],
    "abstract": "We introduce an image upscaling technique tailored for 3D Gaussian Splatting (3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher rendering speeds and reduces artifacts commonly observed in 3DGS reconstructions. Our technique upscales low-resolution 3DGS renderings with a marginal increase in cost by directly leveraging the analytical image gradients of Gaussians for gradient-based bicubic spline interpolation. The technique is agnostic to the specific 3DGS implementation, achieving novel view synthesis at rates 3x-4x higher than the baseline implementation. Through extensive experiments on multiple datasets, we showcase the performance improvements and high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS images. We further demonstrate the integration of gradient-aware upscaling into the gradient-based optimization of a 3DGS model and analyze its effects on reconstruction quality and performance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-18",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression",
    "authors": [
      "Kangli Wang",
      "Shihao Li",
      "Qianxi Yi",
      "Wei Gao"
    ],
    "abstract": "Recently, immersive media and autonomous driving applications have significantly advanced through 3D Gaussian Splatting (3DGS), which offers high-fidelity rendering and computational efficiency. Despite these advantages, 3DGS as a display-oriented representation requires substantial storage due to its numerous Gaussian attributes. Current compression methods have shown promising results but typically neglect the compression of Gaussian spatial positions, creating unnecessary bitstream overhead. We conceptualize Gaussian primitives as point clouds and propose leveraging point cloud compression techniques for more effective storage. AI-based point cloud compression demonstrates superior performance and faster inference compared to MPEG Geometry-based Point Cloud Compression (G-PCC). However, direct application of existing models to Gaussian compression may yield suboptimal results, as Gaussian point clouds tend to exhibit globally sparse yet locally dense geometric distributions that differ from conventional point cloud characteristics. To address these challenges, we introduce GausPcgc for Gaussian point cloud geometry compression along with a specialized training dataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud compression into Gaussian compression pipelines, achieving superior compression ratios. The framework complements existing Gaussian compression methods while delivering significant performance improvements. All code, data, and pre-trained models will be publicly released to facilitate further research advances in this field.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "high-fidelity",
      "geometry",
      "head",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS4: Generalizable Sparse Splatting Semantic SLAM",
    "authors": [
      "Mingqi Jiang",
      "Chanho Kim",
      "Chen Ziwen",
      "Li Fuxin"
    ],
    "abstract": "Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, we propose to optimize GS for only 1-5 iterations that corrects drift and floaters when significant pose changes are detected. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "tracking",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TrackGS: Optimizing COLMAP-Free 3D Gaussian Splatting with Global Track Constraints",
    "authors": [
      "Dongbo Shi",
      "Shen Cao",
      "Lubin Fan",
      "Bojian Wu",
      "Jinhui Guo",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) has advanced ability on novel view synthesis, it still depends on accurate pre-computaed camera parameters, which are hard to obtain and prone to noise. Previous COLMAP-Free methods optimize camera poses using local constraints, but they often struggle in complex scenarios. To address this, we introduce TrackGS, which incorporates feature tracks to globally constrain multi-view geometry. We select the Gaussians associated with each track, which will be trained and rescaled to an infinitesimally small size to guarantee the spatial accuracy. We also propose minimizing both reprojection and backprojection errors for better geometric consistency. Moreover, by deriving the gradient of intrinsics, we unify camera parameter estimation with 3DGS training into a joint optimization framework, achieving SOTA performance on challenging datasets with severe camera movements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "geometry",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "Marcus A. Brubaker",
      "David J. Fleet"
    ],
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-06",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling",
    "authors": [
      "Zhe Li",
      "Yipengjing Sun",
      "Zerong Zheng",
      "Lizhen Wang",
      "Shengping Zhang",
      "Yebin Liu"
    ],
    "abstract": "Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front & back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. To tackle the realistic relighting of animatable avatars, we introduce physically-based rendering into the avatar representation for decomposing avatar materials and environment illumination. Overall, our method can create lifelike avatars with dynamic, realistic, generalized and relightable appearances. Experiments show that our method outperforms other state-of-the-art approaches.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "relightable",
      "illumination",
      "high-fidelity",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "avatar",
      "nerf",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation",
    "authors": [
      "Yuwen Tao",
      "Kanglei Zhou",
      "Xin Tan",
      "Yuan Xie"
    ],
    "abstract": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "ar",
      "gaussian splatting",
      "understanding",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction",
    "authors": [
      "Ziyi Yang",
      "Xinyu Gao",
      "Wen Zhou",
      "Shaohui Jiao",
      "Yuqing Zhang",
      "Xiaogang Jin"
    ],
    "abstract": "Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world datasets. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-09-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "high-fidelity",
      "dynamic",
      "head",
      "ar",
      "real-time rendering",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering",
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng",
      "Ruijie Zhu",
      "Yanzhe Liang",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Xu Zhou"
    ],
    "abstract": "Dynamic scenes rendering is an intriguing yet challenging problem. Although current methods based on NeRF have achieved satisfactory performance, they still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS) has garnered researchers attention due to their outstanding rendering quality and real-time speed. Therefore, a new paradigm has been proposed: defining a canonical 3D gaussians and deforming it to individual frames in deformable fields. However, since the coordinates of canonical 3D gaussians are filled with noise, which can transfer noise into the deformable fields, and there is currently no method that adequately considers the aggregation of 4D information. Therefore, we propose Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering (DN-4DGS). Specifically, a Noise Suppression Strategy is introduced to change the distribution of the coordinates of the canonical 3D gaussians and suppress noise. Additionally, a Decoupled Temporal-Spatial Aggregation Module is designed to aggregate information from adjacent points and frames. Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars",
    "authors": [
      "Shota Sasaki",
      "Jane Wu",
      "Ko Nishino"
    ],
    "abstract": "This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes ``movement-dependent'' cloth deformation via physical simulation, rather than merely relying on ``pose-dependent'' rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes' Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "body",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "avatar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow",
    "authors": [
      "Simon Boeder",
      "Fabian Gigengack",
      "Benjamin Risse"
    ],
    "abstract": "Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "autonomous driving",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quadratic Gaussian Splatting: High Quality Surface Reconstruction with Second-order Geometric Primitives",
    "authors": [
      "Ziyu Zhang",
      "Binbin Huang",
      "Hanqing Jiang",
      "Liyang Zhou",
      "Xiaojun Xiang",
      "Shunhan Shen"
    ],
    "abstract": "We propose Quadratic Gaussian Splatting (QGS), a novel representation that replaces static primitives with deformable quadric surfaces (e.g., ellipse, paraboloids) to capture intricate geometry. Unlike prior works that rely on Euclidean distance for primitive density modeling--a metric misaligned with surface geometry under deformation--QGS introduces geodesic distance-based density distributions. This innovation ensures that density weights adapt intrinsically to the primitive curvature, preserving consistency during shape changes (e.g., from planar disks to curved paraboloids). By solving geodesic distances in closed form on quadric surfaces, QGS enables surface-aware splatting, where a single primitive can represent complex curvature that previously required dozens of planar surfels, potentially reducing memory usage while maintaining efficient rendering via fast ray-quadric intersection. Experiments on DTU, Tanks and Temples, and MipNeRF360 datasets demonstrate state-of-the-art surface reconstruction, with QGS reducing geometric error (chamfer distance) by 33% over 2DGS and 27% over GOF on the DTU dataset. Crucially, QGS retains competitive appearance quality, bridging the gap between geometric precision and visual fidelity for applications like robotics and immersive reality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "efficient rendering",
      "high quality",
      "geometry",
      "robotics",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatOverflow: Asynchronous Hardware Troubleshooting",
    "authors": [
      "Amritansh Kwatra",
      "Tobias Weinberg",
      "Ilan Mandel",
      "Ritik Batra",
      "Peter He",
      "Francois Guimbretiere",
      "Thijs Roumen"
    ],
    "abstract": "As tools for designing and manufacturing hardware become more accessible, smaller producers can develop and distribute novel hardware. However, processes for supporting end-user hardware troubleshooting or routine maintenance aren't well defined. As a result, providing technical support for hardware remains ad-hoc and challenging to scale. Inspired by patterns that helped scale software troubleshooting, we propose a workflow for asynchronous hardware troubleshooting: SplatOverflow.   SplatOverflow creates a novel boundary object, the SplatOverflow scene, that users reference to communicate about hardware. A scene comprises a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD model. The splat captures the current state of the hardware, and the registered CAD model acts as a referential anchor for troubleshooting instructions. With SplatOverflow, remote maintainers can directly address issues and author instructions in the user's workspace. Workflows containing multiple instructions can easily be shared between users and recontextualized in new environments.   In this paper, we describe the design of SplatOverflow, the workflows it enables, and its utility to different kinds of users. We also validate that non-experts can use SplatOverflow to troubleshoot common problems with a 3D printer in a usability study.   Project Page: https://amritkwatra.com/research/splatoverflow.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-04",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting",
    "authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To achieve a compact size, we propose HAC++, which leverages the relationships between unorganized anchors and a structured hash grid, utilizing their mutual information for context modeling. Additionally, HAC++ captures intra-anchor contextual relationships to further enhance compression performance. To facilitate entropy coding, we utilize Gaussian distributions to precisely estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Moreover, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Overall, HAC++ achieves a remarkable size reduction of over 100X compared to vanilla 3DGS when averaged on all datasets, while simultaneously improving fidelity. It also delivers more than 20X size reduction compared to Scaffold-GS. Our code is available at https://github.com/YihangChen-ee/HAC-plus.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs",
    "authors": [
      "Guibiao Liao",
      "Qing Li",
      "Zhenyu Bao",
      "Guoping Qiu",
      "Kanglin Liu"
    ],
    "abstract": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches have shown significant performance with dense input images. However, they exhibit poor performance when confronted with sparse inputs, primarily due to the sparse distribution of Gaussian points and insufficient view supervision. To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC) Regularization for open-world free view synthesis with sparse inputs. Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by utilizing view-changed images generated from the video generation model and view-constraint Gaussian points densification. Additionally, SPC mitigates limited view supervision by employing semantic-prompt-based consistency constraints developed by SAM2. This approach leverages available semantics from training views, serving as instructive prompts, to optimize visually overlapping regions in novel views with 2D and 3D consistency constraints. Extensive experiments demonstrate the superior performance of SPC-GS across Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world semantic segmentation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Stochastic Ray Tracing of Transparent 3D Gaussians",
    "authors": [
      "Xin Sun",
      "Iliyan Georgiev",
      "Yun Fei",
      "Miloš Hašan"
    ],
    "abstract": "3D Gaussian splatting has been widely adopted as a 3D representation for novel-view synthesis, relighting, and 3D generation tasks. It delivers realistic and detailed results through a collection of explicit 3D Gaussian primitives, each carrying opacity and view-dependent color. However, efficient rendering of many transparent primitives remains a significant challenge. Existing approaches either rasterize the Gaussians with approximate per-view sorting or rely on high-end RTX GPUs. This paper proposes a stochastic ray-tracing method to render 3D clouds of transparent primitives. Instead of processing all ray-Gaussian intersections in sequential order, each ray traverses the acceleration structure only once, randomly accepting and shading a single intersection (or $N$ intersections, using a simple extension). This approach minimizes shading time and avoids primitive sorting along the ray, thereby minimizing register usage and maximizing parallelism even on low-end GPUs. The cost of rays through the Gaussian asset is comparable to that of standard mesh-intersection rays. The shading is unbiased and has low variance, as our stochastic acceptance achieves importance sampling based on accumulated weight. The alignment with Monte Carlo philosophy simplifies implementation and integration into a conventional path-tracing framework.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-09",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "ray tracing",
      "lighting",
      "acceleration",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis",
    "authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable success in synthesizing novel views given multiple views of a static scene. Yet, 3DGS faces challenges when applied to dynamic scenes because 3D Gaussian parameters need to be updated per timestep, requiring a large amount of memory and at least a dozen observations per timestep. To address these limitations, we present a compact dynamic 3D Gaussian representation that models positions and rotations as functions of time with a few parameter approximations while keeping other properties of 3DGS including scale, color and opacity invariant. Our method can dramatically reduce memory usage and relax a strict multi-view assumption. In our experiments on monocular and multi-view scenarios, we show that our method not only matches state-of-the-art methods, often linked with slower rendering speeds, in terms of high rendering quality but also significantly surpasses them by achieving a rendering speed of $118$ frames per second (FPS) at a resolution of 1,352$\\times$1,014 on a single GPU.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting",
    "authors": [
      "Yijia Guo",
      "Wenkai Huang",
      "Yang Li",
      "Gaolei Li",
      "Hang Zhang",
      "Liwen Hu",
      "Jianhua Li",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience. Codes and data will be released at https://water-gs.github.io.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-04",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction",
    "authors": [
      "Zhening Liu",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Rui Song",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "The recent development of 3D Gaussian Splatting (3DGS) has led to great interest in 4D dynamic spatial reconstruction. Existing approaches mainly rely on full-length multi-view videos, while there has been limited exploration of online reconstruction methods that enable on-the-fly training and per-timestep streaming. Current 3DGS-based streaming methods treat the Gaussian primitives uniformly and constantly renew the densified Gaussians, thereby overlooking the difference between dynamic and static features as well as neglecting the temporal continuity in the scene. To address these limitations, we propose a novel three-stage pipeline for iterative streamable 4D dynamic spatial reconstruction. Our pipeline comprises a selective inheritance stage to preserve temporal continuity, a dynamics-aware shift stage to distinguish dynamic and static primitives and optimize their movements, and an error-guided densification stage to accommodate emerging objects. Our method achieves state-of-the-art performance in online 4D reconstruction, demonstrating the fastest on-the-fly training, superior representation quality, and real-time rendering capability. Project page: https://www.liuzhening.top/DASS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction",
    "authors": [
      "Yifan Zhou",
      "Beizhen Zhao",
      "Pengcheng Wu",
      "Hao Wang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Voyager: Real-Time Splatting City-Scale Gaussians on Resource-Constrained Devices",
    "authors": [
      "Zheng Liu",
      "He Zhu",
      "Xinyang Li",
      "Yirun Wang",
      "Yujiao Shi",
      "Yiming Gan",
      "Wei Li",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is an emerging technique for photorealistic 3D scene rendering. However, rendering city-scale 3DGS scenes on resource-constrained mobile devices in real-time remains a significant challenge due to two compute-intensive stages: level-of-detail (LoD) search and rasterization.   In this paper, we propose Voyager, an effective solution to accelerate city-scale 3DGS rendering on mobile devices. Our key insight is that, under normal user motion, the number of newly visible Gaussians within the view frustum remains roughly constant. Leveraging this temporal correlation, we propose a temporal-aware LoD search to identify the necessary Gaussians for the remaining rendering stages. For the remaining rendering process, we accelerate the bottleneck stage, rasterization, via preemptive $α$-filtering. With all optimizations above, our system can deliver low-latency, city-scale 3DGS rendering on mobile devices. Compared to existing solutions, Voyager achieves up to 6.6$\\times$ speedup and 85\\% energy savings with superior rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-03",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDRGS: High Dynamic Range Gaussian Splatting",
    "authors": [
      "Jiahao Wu",
      "Lu Xiao",
      "Rui Peng",
      "Kaiqiang Xiong",
      "Ronggang Wang"
    ],
    "abstract": "Recent years have witnessed substantial advancements in the field of 3D reconstruction from 2D images, particularly following the introduction of the neural radiance field (NeRF) technique. However, reconstructing a 3D high dynamic range (HDR) radiance field, which aligns more closely with real-world conditions, from 2D multi-exposure low dynamic range (LDR) images continues to pose significant challenges. Approaches to this issue fall into two categories: grid-based and implicit-based. Implicit methods, using multi-layer perceptrons (MLP), face inefficiencies, limited solvability, and overfitting risks. Conversely, grid-based methods require significant memory and struggle with image quality and long training times. In this paper, we introduce Gaussian Splatting-a recent, high-quality, real-time 3D reconstruction technique-into this domain. We further develop the High Dynamic Range Gaussian Splatting (HDR-GS) method, designed to address the aforementioned challenges. This method enhances color dimensionality by including luminance and uses an asymmetric grid for tone-mapping, swiftly and precisely converting pixel irradiance to color. Our approach improves HDR scene recovery accuracy and integrates a novel coarse-to-fine strategy to speed up model convergence, enhancing robustness against sparse viewpoints and exposure extremes, and preventing local optima. Extensive testing confirms that our method surpasses current state-of-the-art techniques in both synthetic and real-world scenarios.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "nerf",
      "face",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors",
    "authors": [
      "Mingwei Li",
      "Pu Pang",
      "Hehe Fan",
      "Hua Huang",
      "Yi Yang"
    ],
    "abstract": "Reconstructing transparent surfaces is essential for tasks such as robotic manipulation in labs, yet it poses a significant challenge for 3D reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods often encounter a transparency-depth dilemma, where the pursuit of photorealistic rendering through standard $α$-blending undermines geometric precision, resulting in considerable depth estimation errors for transparent materials. To address this issue, we introduce Transparent Surface Gaussian Splatting (TSGS), a new framework that separates geometry learning from appearance refinement. In the geometry learning stage, TSGS focuses on geometry by using specular-suppressed inputs to accurately represent surfaces. In the second stage, TSGS improves visual fidelity through anisotropic specular modeling, crucially maintaining the established opacity to ensure geometric accuracy. To enhance depth inference, TSGS employs a first-surface depth extraction method. This technique uses a sliding window over $α$-blending weights to pinpoint the most likely surface location and calculates a robust weighted average depth. To evaluate the transparent surface reconstruction task under realistic conditions, we collect a TransLab dataset that includes complex transparent laboratory glassware. Extensive experiments on TransLab show that TSGS achieves accurate geometric reconstruction and realistic rendering of transparent objects simultaneously within the efficient 3DGS framework. Specifically, TSGS significantly surpasses current leading methods, achieving a 37.3% reduction in chamfer distance and an 8.0% improvement in F1 score compared to the top baseline. The code and dataset are available at https://longxiang-ai.github.io/TSGS/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting",
    "authors": [
      "Zexu Huang",
      "Min Xu",
      "Stuart Perry"
    ],
    "abstract": "Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering",
    "authors": [
      "Junyu Zhou",
      "Yuyang Huang",
      "Wenrui Dai",
      "Junni Zou",
      "Ziyang Zheng",
      "Nuowen Kan",
      "Chenglin Li",
      "Hongkai Xiong"
    ],
    "abstract": "Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time rendering while maintaining high-fidelity novel view synthesis. However, 3DGS resorts to the Gaussian function that is low-pass by nature and is restricted in representing high-frequency details in 3D scenes. Moreover, it causes redundant primitives with degraded training and rendering efficiency and excessive memory overhead. To overcome these limitations, we propose 3D Gabor Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with multiple directional 3D frequency responses for radiance field representation supervised by multi-view images. The proposed 3D Gabor-based primitive forms a filter bank incorporating multiple 3D Gabor kernels at different frequencies to enhance flexibility and efficiency in capturing fine 3D details. Furthermore, to achieve novel view rendering, an efficient CUDA-based rasterizer is developed to project the multiple directional 3D frequency components characterized by 3D Gabor-based primitives onto the 2D image plane, and a frequency-adaptive mechanism is presented for adaptive joint optimization of primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless integration into existing 3DGS paradigms to enhance both efficiency and quality of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat outperforms 3DGS and its variants using alternative primitives, and achieves state-of-the-art rendering quality across both real-world and synthetic scenes. Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously reduced number of primitives and memory consumption.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
    "authors": [
      "Gaia Di Lorenzo",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "Learning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "semantic",
      "geometry",
      "localization",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes",
    "authors": [
      "Xinjie Zhang",
      "Zhening Liu",
      "Yifan Zhang",
      "Xingtong Ge",
      "Dailan He",
      "Tongda Xu",
      "Yan Wang",
      "Zehong Lin",
      "Shuicheng Yan",
      "Jun Zhang"
    ],
    "abstract": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190$\\times$ and 125$\\times$ on the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field. Code is available at https://github.com/Xinjie-Q/MEGA.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "lightweight",
      "dynamic",
      "ar",
      "gaussian splatting",
      "face",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ],
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at https://github.com/CUHK-AIM-Group/MonoSplat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
    "authors": [
      "Chuanyu Fu",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Guanying Chen",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
    "authors": [
      "Shaocong Dong",
      "Lihe Ding",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue",
      "Dan Xu"
    ],
    "abstract": "3D object generation has undergone significant advancements, yielding high-quality results. However, fall short of achieving precise user control, often yielding results that do not align with user expectations, thus limiting their applicability. User-envisioning 3D object generation faces significant challenges in realizing its concepts using current generative models due to limited interaction capabilities. Existing methods mainly offer two approaches: (i) interpreting textual instructions with constrained controllability, or (ii) reconstructing 3D objects from 2D images. Both of them limit customization to the confines of the 2D reference and potentially introduce undesirable artifacts during the 3D lifting process, restricting the scope for direct and versatile 3D modifications. In this work, we introduce Interactive3D, an innovative framework for interactive 3D generation that grants users precise control over the generative process through extensive 3D interaction capabilities. Interactive3D is constructed in two cascading stages, utilizing distinct 3D representations. The first stage employs Gaussian Splatting for direct user interaction, allowing modifications and guidance of the generative direction at any intermediate step through (i) Adding and Removing components, (ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv) Semantic Editing. Subsequently, the Gaussian splats are transformed into InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to further add details and extract the geometry in the second stage. Our experiments demonstrate that Interactive3D markedly improves the controllability and quality of 3D generation. Our project webpage is available at \\url{https://interactive-3d.github.io/}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-25",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos",
    "authors": [
      "Colton Stearns",
      "Adam Harley",
      "Mikaela Uy",
      "Florian Dubost",
      "Federico Tombari",
      "Gordon Wetzstein",
      "Leonidas Guibas"
    ],
    "abstract": "Gaussian splatting has become a popular representation for novel-view synthesis, exhibiting clear strengths in efficiency, photometric quality, and compositional edibility. Following its success, many works have extended Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while also tracking scene geometry far better than alternative representations. Yet, these methods assume dense multi-view videos as supervision. In this work, we are interested in extending the capability of Gaussian scene representations to casually captured monocular videos. We show that existing 4D Gaussian methods dramatically fail in this setup because the monocular setting is underconstrained. Building off this finding, we propose a method we call Dynamic Gaussian Marbles, which consist of three core modifications that target the difficulties of the monocular setting. First, we use isotropic Gaussian \"marbles'', reducing the degrees of freedom of each Gaussian. Second, we employ a hierarchical divide and-conquer learning strategy to efficiently guide the optimization towards solutions with globally coherent motion. Finally, we add image-level and geometry-level priors into the optimization, including a tracking loss that takes advantage of recent progress in point tracking. By constraining the optimization, Dynamic Gaussian Marbles learns Gaussian trajectories that enable novel-view rendering and accurately capture the 3D motion of the scene elements. We evaluate on the Nvidia Dynamic Scenes dataset and the DyCheck iPhone dataset, and show that Gaussian Marbles significantly outperforms other Gaussian baselines in quality, and is on-par with non-Gaussian representations, all while maintaining the efficiency, compositionality, editability, and tracking benefits of Gaussians. Our project page can be found here https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "tracking",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AugGS: Self-augmented Gaussians with Structural Masks for Sparse-view 3D Reconstruction",
    "authors": [
      "Bi'an Du",
      "Lingbei Meng",
      "Wei Hu"
    ],
    "abstract": "Sparse-view 3D reconstruction is a major challenge in computer vision, aiming to create complete three-dimensional models from limited viewing angles. Key obstacles include: 1) a small number of input images with inconsistent information; 2) dependence on input image quality; and 3) large model parameter sizes. To tackle these issues, we propose a self-augmented two-stage Gaussian splatting framework enhanced with structural masks for sparse-view 3D reconstruction. Initially, our method generates a basic 3D Gaussian representation from sparse inputs and renders multi-view images. We then fine-tune a pre-trained 2D diffusion model to enhance these images, using them as augmented data to further optimize the 3D Gaussians. Additionally, a structural masking strategy during training enhances the model's robustness to sparse inputs and noise. Experiments on benchmarks like MipNeRF360, OmniObject3D, and OpenIllumination demonstrate that our approach achieves state-of-the-art performance in perceptual quality and multi-view consistency with sparse inputs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "illumination",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy",
    "authors": [
      "Ao Gao",
      "Luosong Guo",
      "Tao Chen",
      "Zhao Wang",
      "Ying Tai",
      "Jian Yang",
      "Zhenyu Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting",
    "authors": [
      "Anil Armagan",
      "Albert Saà-Garriga",
      "Bruno Manganelli",
      "Mateusz Nowak",
      "Mehmet Kerim Yucel"
    ],
    "abstract": "Gaussian splatting (GS) for 3D reconstruction has become quite popular due to their fast training, inference speeds and high quality reconstruction. However, GS-based reconstructions generally consist of millions of Gaussians, which makes them hard to use on computationally constrained devices such as smartphones. In this paper, we first propose a principled analysis of advances in efficient GS methods. Then, we propose Trick-GS, which is a careful combination of several strategies including (1) progressive training with resolution, noise and Gaussian scales, (2) learning to prune and mask primitives and SH bands by their significance, and (3) accelerated GS training framework. Trick-GS takes a large step towards resource-constrained GS, where faster run-time, smaller and faster-convergence of models is of paramount concern. Our results on three datasets show that Trick-GS achieves up to 2x faster training, 40x smaller disk size and 2x faster rendering speed compared to vanilla GS, while having comparable accuracy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "high quality",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting",
    "authors": [
      "Xiaotong Huang",
      "He Zhu",
      "Zihan Liu",
      "Weikai Lin",
      "Xiaohong Liu",
      "Zhezhi He",
      "Jingwen Leng",
      "Minyi Guo",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a crucial rendering technique for many real-time applications. However, the limited hardware resources on today's mobile platforms hinder these applications, as they struggle to achieve real-time performance. In this paper, we propose SeeLe, a general framework designed to accelerate the 3DGS pipeline for resource-constrained mobile devices.   Specifically, we propose two GPU-oriented techniques: hybrid preprocessing and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU compute and memory pressure by reducing the number of irrelevant Gaussians during rendering. The key is to combine our view-dependent scene representation with online filtering. Meanwhile, contribution-aware rasterization improves the GPU utilization at the rasterization stage by prioritizing Gaussians with high contributions while reducing computations for those with low contributions. Both techniques can be seamlessly integrated into existing 3DGS pipelines with minimal fine-tuning. Collectively, our framework achieves 2.6$\\times$ speedup and 32.3\\% model reduction while achieving superior rendering quality compared to existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "acceleration"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM",
    "authors": [
      "Zhicong Sun",
      "Jacqueline Lo",
      "Jinxing Hu"
    ],
    "abstract": "Simultaneous localization and mapping (SLAM) technology has recently achieved photorealistic mapping capabilities thanks to the real-time, high-fidelity rendering enabled by 3D Gaussian Splatting (3DGS). However, due to the static representation of scenes, current 3DGS-based SLAM encounters issues with pose drift and failure to reconstruct accurate maps in dynamic environments. To address this problem, we present D4DGS-SLAM, the first SLAM method based on 4DGS map representation for dynamic environments. By incorporating the temporal dimension into scene representation, D4DGS-SLAM enables high-quality reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we can obtain the dynamics, visibility, and reliability of scene points, and filter out unstable dynamic points for tracking accordingly. When optimizing Gaussian points, we apply different isotropic regularization terms to Gaussians with varying dynamic characteristics. Experimental results on real-world dynamic scene datasets demonstrate that our method outperforms state-of-the-art approaches in both camera pose tracking and map quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
    "authors": [
      "Yusen Xie",
      "Zhenmin Huang",
      "Jianhao Jiao",
      "Dimitrios Kanoulas",
      "Jun Ma"
    ],
    "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-14",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting",
    "authors": [
      "Hexu Zhao",
      "Xiwen Min",
      "Xiaoteng Liu",
      "Moonjun Gong",
      "Yiming Li",
      "Ang Li",
      "Saining Xie",
      "Jinyang Li",
      "Aurojit Panda"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "large scene",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians",
    "authors": [
      "Zixun Huang",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the quality and efficiency of differentiable rendering. However, its high efficiency stems from an approximation of projecting 3D Gaussians onto the image plane as 2D Gaussians, which inherently limits rendering quality--particularly under large Field-of-View (FoV) camera inputs. While several recent works have extended 3DGS to mitigate these approximation errors, none have successfully achieved both exactness and high efficiency simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient Volumetric Gaussian Rendering method. Starting from first principles, we derive a closed-form expression for the density integral along a ray traversing a 3D Gaussian distribution. This formulation enables precise forward rendering with arbitrary camera models and supports gradient-based optimization of 3D Gaussian parameters. To ensure both exactness and real-time performance, we propose an efficient method for computing a tight Particle Bounding Frustum (PBF) for each 3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also introduce a novel Bipolar Equiangular Projection (BEAP) representation to accelerate ray association under generic camera models. BEAP further provides a more uniform ray sampling strategy to apply supervision, which empirically improves reconstruction quality. Experiments on multiple pinhole and fisheye datasets show that our method consistently outperforms prior methods, establishing a new state-of-the-art in real-time neural rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection",
    "authors": [
      "Xiaokai Bai",
      "Chenxu Zhou",
      "Lianqing Zheng",
      "Si-Yuan Cao",
      "Jianan Liu",
      "Xiaohan Zhang",
      "Yiming Li",
      "Zhengzhuang Zhang",
      "Hui-liang Shen"
    ],
    "abstract": "4D millimeter-wave radar is a promising sensing modality for autonomous driving, yet effective 3D object detection from 4D radar and monocular images remains challenging. Existing fusion approaches either rely on instance proposals lacking global context or dense BEV grids constrained by rigid structures, lacking a flexible and adaptive representation for diverse scenes. To address this, we propose RaGS, the first framework that leverages 3D Gaussian Splatting (GS) to fuse 4D radar and monocular cues for 3D object detection. 3D GS models the scene as a continuous field of Gaussians, enabling dynamic resource allocation to foreground objects while maintaining flexibility and efficiency. Moreover, the velocity dimension of 4D radar provides motion cues that help anchor and refine the spatial distribution of Gaussians. Specifically, RaGS adopts a cascaded pipeline to construct and progressively refine the Gaussian field. It begins with Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse Gaussian centers. Then, Iterative Multimodal Aggregation (IMA) explicitly exploits image semantics and implicitly integrates 4D radar velocity geometry to refine the Gaussians within regions of interest. Finally, Multi-level Gaussian Fusion (MGF) renders the Gaussian field into hierarchical BEV features for 3D object detection. By dynamically focusing on sparse and informative regions, RaGS achieves object-centric precision and comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes demonstrate its robustness and SOTA performance. Code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "localization",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "autonomous driving",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TimeWalker: Personalized Neural Space for Lifelong Head Avatars",
    "authors": [
      "Dongwei Pan",
      "Yang Li",
      "Hongsheng Li",
      "Kwan-Yee Lin"
    ],
    "abstract": "We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "animation",
      "compact",
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "avatar",
      "ar",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing",
    "authors": [
      "Teng Xu",
      "Jiamin Chen",
      "Peng Chen",
      "Youjia Zhang",
      "Junqing Yu",
      "Wei Yang"
    ],
    "abstract": "Editing objects within a scene is a critical functionality required across a broad spectrum of applications in computer vision and graphics. As 3D Gaussian Splatting (3DGS) emerges as a frontier in scene representation, the effective modification of 3D Gaussian scenes has become increasingly vital. This process entails accurately retrieve the target objects and subsequently performing modifications based on instructions. Though available in pieces, existing techniques mainly embed sparse semantics into Gaussians for retrieval, and rely on an iterative dataset update paradigm for editing, leading to over-smoothing or inconsistency issues. To this end, this paper proposes a systematic approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and editing. In contrast to the top-down language grounding approach for 3D Gaussians, we adopt a bottom-up language aggregation strategy to generate a denser language embedded 3D Gaussians that supports open-vocabulary retrieval. To overcome the over-smoothing and inconsistency issues in editing, we propose a Coherent Score Distillation (CSD) that aggregates a 2D image editing diffusion model and a multi-view diffusion model for score distillation, producing multi-view consistent editing with much finer details. In various experiments, we demonstrate that our TIGER is able to accomplish more consistent and realistic edits than prior work.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting",
    "authors": [
      "Ziyi Yang",
      "Xinyu Gao",
      "Yangtian Sun",
      "Yihua Huang",
      "Xiaoyang Lyu",
      "Wen Zhou",
      "Shaohui Jiao",
      "Xiaojuan Qi",
      "Xiaogang Jin"
    ],
    "abstract": "The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. Project page is https://ingra14m.github.io/Spec-Gaussian-website/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization",
    "authors": [
      "Rafał Tobiasz",
      "Grzegorz Wilczyński",
      "Marcin Mazur",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer graphics. GS-based algorithms almost always bypass classical methods such as ray tracing, which offers numerous inherent advantages for rendering. For example, ray tracing is able to handle incoherent rays for advanced lighting effects, including shadows and reflections. To address this limitation, we introduce MeshSplats, a method which converts GS to a mesh-like format. Following the completion of training, MeshSplats transforms Gaussian elements into mesh faces, enabling rendering using ray tracing methods with all their associated benefits. Our model can be utilized immediately following transformation, yielding a mesh of slightly reduced quality without additional training. Furthermore, we can enhance the reconstruction quality through the application of a dedicated optimization algorithm that operates on mesh faces rather than Gaussian components. The efficacy of our method is substantiated by experimental results, underscoring its extensive applications in computer graphics and image processing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "ray tracing",
      "lighting",
      "gaussian splatting",
      "face",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D Reconstruction",
    "authors": [
      "Jianghao Shen",
      "Nan Xue",
      "Tianfu Wu"
    ],
    "abstract": "Learning 3D scene representation from a single-view image is a long-standing fundamental problem in computer vision, with the inherent ambiguity in predicting contents unseen from the input view. Built on the recently proposed 3D Gaussian Splatting (3DGS), the Splatter Image method has made promising progress on fast single-image novel view synthesis via learning a single 3D Gaussian for each pixel based on the U-Net feature map of an input image. However, it has limited expressive power to represent occluded components that are not observable in the input view. To address this problem, this paper presents a Hierarchical Splatter Image method in which a pixel is worth more than one 3D Gaussians. Specifically, each pixel is represented by a parent 3D Gaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are learned as done in the vanilla Splatter Image. Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron (MLP) which takes as input the projected image features of a parent 3D Gaussian and the embedding of a target camera view. Both parent and child 3D Gaussians are learned end-to-end in a stage-wise way. The joint condition of input image features from eyes of the parent Gaussians and the target camera position facilitates learning to allocate child Gaussians to ``see the unseen'', recovering the occluded details that are often missed by parent Gaussians.   In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D datasets with state-of-the-art performance obtained, especially showing promising capabilities of reconstructing occluded contents in the input view.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "lightweight",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Reducing the Memory Footprint of 3D Gaussian Splatting",
    "authors": [
      "Panagiotis Papantonakis",
      "Georgios Kopanas",
      "Bernhard Kerbl",
      "Alexandre Lanvin",
      "George Drettakis"
    ],
    "abstract": "3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and real-time rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolution-aware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a 27 reduction in overall size on disk on the standard datasets we tested, along with a 1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction",
    "authors": [
      "Hanlin Chen",
      "Fangyin Wei",
      "Chen Li",
      "Tianxin Huang",
      "Yunsong Wang",
      "Gim Hee Lee"
    ],
    "abstract": "Although 3D Gaussian Splatting has been widely studied because of its realistic and efficient novel-view synthesis, it is still challenging to extract a high-quality surface from the point-based representation. Previous works improve the surface by incorporating geometric priors from the off-the-shelf normal estimator. However, there are two main limitations: 1) Supervising normals rendered from 3D Gaussians effectively updates the rotation parameter but is less effective for other geometric parameters; 2) The inconsistency of predicted normal maps across multiple views may lead to severe reconstruction artifacts. In this paper, we propose a Depth-Normal regularizer that directly couples normal with other geometric parameters, leading to full updates of the geometric parameters from normal regularization. We further propose a confidence term to mitigate inconsistencies of normal predictions across multiple views. Moreover, we also introduce a densification and splitting strategy to regularize the size and distribution of 3D Gaussians for more accurate surface modeling. Compared with Gaussian-based baselines, experiments show that our approach obtains better reconstruction quality and maintains competitive appearance quality at faster training speed and 100+ FPS rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views",
    "authors": [
      "Liang Zhao",
      "Zehan Bao",
      "Yi Xie",
      "Hong Chen",
      "Yaohui Chen",
      "Weifu Li"
    ],
    "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field, achieving both panoptic and interactive segmentation of 3D scenes. However, existing methodologies often overlook the critical need for reconstructing specified targets with complex structures from sparse views. To address this issue, we introduce TSGaussian, a novel framework that combines semantic constraints with depth priors to avoid geometry degradation in challenging novel view synthesis tasks. Our approach prioritizes computational resources on designated targets while minimizing background allocation. Bounding boxes from YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask predictions, ensuring semantic accuracy and cost efficiency. TSGaussian effectively clusters 3D gaussians by introducing a compact identity encoding for each Gaussian ellipsoid and incorporating 3D spatial consistency regularization. Leveraging these modules, we propose a pruning strategy to effectively reduce redundancy in 3D gaussians. Extensive experiments demonstrate that TSGaussian outperforms state-of-the-art methods on three standard datasets and a new challenging dataset we collected, achieving superior results in novel view synthesis of specific objects. Code is available at: https://github.com/leon2000-ai/TSGaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "compact",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "REdiSplats: Ray Tracing for Editable Gaussian Splatting",
    "authors": [
      "Krzysztof Byrski",
      "Grzegorz Wilczyński",
      "Weronika Smolak-Dyżewska",
      "Piotr Borycki",
      "Dawid Baran",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "abstract": "Gaussian Splatting (GS) has become one of the most important neural rendering algorithms. GS represents 3D scenes using Gaussian components with trainable color and opacity. This representation achieves high-quality renderings with fast inference. Regrettably, it is challenging to integrate such a solution with varying light conditions, including shadows and light reflections, manual adjustments, and a physical engine. Recently, a few approaches have appeared that incorporate ray-tracing or mesh primitives into GS to address some of these caveats. However, no such solution can simultaneously solve all the existing limitations of the classical GS. Consequently, we introduce REdiSplats, which employs ray tracing and a mesh-based representation of flat 3D Gaussians. In practice, we model the scene using flat Gaussian distributions parameterized by the mesh. We can leverage fast ray tracing and control Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats allows modeling of light conditions, manual adjustments, and physical simulation. Furthermore, we can render our models using 3D tools such as Blender or Nvdiffrast, which opens the possibility of integrating them with all existing 3D graphics techniques dedicated to mesh representations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "shadow",
      "neural rendering",
      "ray tracing",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians",
    "authors": [
      "Zeyu Xiao",
      "Zhenyi Wu",
      "Mingyang Sun",
      "Qipeng Yan",
      "Yufan Guo",
      "Zhuoer Liang",
      "Lihua Zhang"
    ],
    "abstract": "3D Gaussian Splatting has achieved remarkable success in reconstructing both static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian primitives, interactions between objects suffer from inaccurate 3D segmentation, imprecise deformation among different materials, and severe rendering artifacts. To address these challenges, we introduce PIG: Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach that combines 3D object segmentation with the simulation of interacting objects in high precision. Firstly, our method facilitates fast and accurate mapping from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation. Secondly, we assign unique physical properties to correspondingly segmented objects within the scene for multi-material coupled interactions. Finally, we have successfully embedded constraint scales into deformation gradients, specifically clamping the scaling and rotation properties of the Gaussian primitives to eliminate artifacts and achieve geometric fidelity and visual consistency. Experimental results demonstrate that our method not only outperforms the state-of-the-art (SOTA) in terms of visual quality, but also opens up new directions and pipelines for the field of physically realistic scene generation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives",
    "authors": [
      "Bartłomiej Baranowski",
      "Stefano Esposito",
      "Patricia Gschoßmann",
      "Anpei Chen",
      "Andreas Geiger"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation",
    "authors": [
      "Jinsheng Quan",
      "Chunshi Wang",
      "Yawei Luo"
    ],
    "abstract": "This paper aims to model the dynamics of 3D Gaussians from visual observations to support temporal extrapolation. Existing dynamic 3D reconstruction methods often struggle to effectively learn underlying dynamics or rely heavily on manually defined physical priors, which limits their extrapolation capabilities. To address this issue, we propose a novel dynamic 3D Gaussian Splatting prior-free motion extrapolation framework based on particle dynamics systems. The core advantage of our method lies in its ability to learn differential equations that describe the dynamics of 3D Gaussians, and follow them during future frame extrapolation. Instead of simply fitting to the observed visual frame sequence, we aim to more effectively model the gaussian particle dynamics system. To this end, we introduce a dynamics latent state vector into the standard Gaussian kernel and design a dynamics latent space encoder to extract initial state. Subsequently, we introduce a Neural ODEs-based dynamics module that models the temporal evolution of Gaussian in dynamics latent space. Finally, a Gaussian kernel space decoder is used to decode latent state at the specific time step into the deformation. Experimental results demonstrate that the proposed method achieves comparable rendering quality with existing approaches in reconstruction tasks, and significantly outperforms them in future frame extrapolation. Our code is available at https://github.com/QuanJinSheng/ParticleGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MVGamba: Unify 3D Content Generation as State Space Sequence Modeling",
    "authors": [
      "Xuanyu Yi",
      "Zike Wu",
      "Qiuhong Shen",
      "Qingshan Xu",
      "Pan Zhou",
      "Joo-Hwee Lim",
      "Shuicheng Yan",
      "Xinchao Wang",
      "Hanwang Zhang"
    ],
    "abstract": "Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (e.g., Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\times$ of the model size.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting",
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Yuanshuo Cheng",
      "Wangmeng Zuo"
    ],
    "abstract": "In this paper, we aim ambitiously for a realistic yet challenging problem, namely, how to reconstruct high-quality 3D scenes from sparse low-resolution views that simultaneously suffer from deficient perspectives and clarity. Whereas existing methods only deal with either sparse views or low-resolution observations, they fail to handle such hybrid and complicated scenarios. To this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting framework, dubbed S2Gaussian, that can reconstruct structure-accurate and detail-faithful 3D scenes with only sparse and low-resolution views. The S2Gaussian operates in a two-stage fashion. In the first stage, we initially optimize a low-resolution Gaussian representation with depth regularization and densify it to initialize the high-resolution Gaussians through a tailored Gaussian Shuffle Split operation. In the second stage, we refine the high-resolution Gaussians with the super-resolved images generated from both original sparse views and pseudo-views rendered by the low-resolution Gaussians. In which a customized blur-free inconsistency modeling scheme and a 3D robust optimization strategy are elaborately designed to mitigate multi-view inconsistency and eliminate erroneous updates caused by imperfect supervision. Extensive experiments demonstrate superior results and in particular establishing new state-of-the-art performances with more consistent geometry and finer details.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors",
    "authors": [
      "Tianxin Huang",
      "Zhiwen Yan",
      "Yuyang Zhao",
      "Gim Hee Lee"
    ],
    "abstract": "3D point clouds directly collected from objects through sensors are often incomplete due to self-occlusion. Conventional methods for completing these partial point clouds rely on manually organized training sets and are usually limited to object categories seen during training. In this work, we propose a test-time framework for completing partial point clouds across unseen categories without any requirement for training. Leveraging point rendering via Gaussian Splatting, we develop techniques of Partial Gaussian Initialization, Zero-shot Fractal Completion, and Point Cloud Extraction that utilize priors from pre-trained 2D diffusion models to infer missing regions and extract uniform completed point clouds. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects. Our project page is at \\url{https://tianxinhuang.github.io/projects/ComPC/}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting",
    "authors": [
      "Ruijie Zhu",
      "Mulin Yu",
      "Linning Xu",
      "Lihan Jiang",
      "Yixuan Li",
      "Tianzhu Zhang",
      "Jiangmiao Pang",
      "Bo Dai"
    ],
    "abstract": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-21",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "high-fidelity",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "From NeRFs to Gaussian Splats, and Back",
    "authors": [
      "Siming He",
      "Zach Osman",
      "Pratik Chaudhari"
    ],
    "abstract": "For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compact",
      "robotics",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality",
    "authors": [
      "Shuo Xin",
      "Haiyu Wang",
      "Sai Qian Zhang"
    ],
    "abstract": "Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\\times$ while maintaining visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-05",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "efficient rendering",
      "neural rendering",
      "tracking",
      "dynamic",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels",
    "authors": [
      "Yi Hu",
      "Huiyang Zhou"
    ],
    "abstract": "3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-30",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes",
    "authors": [
      "Ke Wu",
      "Kaizhao Zhang",
      "Zhiwei Zhang",
      "Shanshuai Yuan",
      "Muer Tie",
      "Julong Wei",
      "Zijun Xu",
      "Jieru Zhao",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "urban scene",
      "understanding",
      "ar",
      "gaussian splatting",
      "mapping",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting",
    "authors": [
      "Jiaxin Zhang",
      "Junjun Jiang",
      "Youyu Chen",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "abstract": "Accurate object segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D segmentation based on 3D Gaussian Splatting (3DGS) struggles with accurately delineating object boundaries, as Gaussian primitives often span across object edges due to their inherent volume and the lack of semantic guidance during training. In order to tackle these challenges, we introduce Clear Object Boundaries for 3DGS Segmentation (COB-GS), which aims to improve segmentation accuracy by clearly delineating blurry boundaries of interwoven Gaussian primitives within the scene. Unlike existing approaches that remove ambiguous Gaussians and sacrifice visual quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and visual information, allowing the two different levels to cooperate with each other effectively. Specifically, for the semantic guidance, we introduce a boundary-adaptive Gaussian splitting technique that leverages semantic gradient statistics to identify and split ambiguous Gaussians, aligning them closely with object boundaries. For the visual optimization, we rectify the degraded suboptimal texture of the 3DGS scene, particularly along the refined boundary structures. Experimental results show that COB-GS substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained model, yielding clear boundaries while preserving high visual quality. Code is available at https://github.com/ZestfulJX/COB-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting",
    "authors": [
      "Shaojie Ma",
      "Yawei Luo",
      "Wei Yang",
      "Yi Yang"
    ],
    "abstract": "3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
    "authors": [
      "Xianglong He",
      "Junyi Chen",
      "Sida Peng",
      "Di Huang",
      "Yangguang Li",
      "Xiaoshui Huang",
      "Chun Yuan",
      "Wanli Ouyang",
      "Tong He"
    ],
    "abstract": "In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency. Our project page is: https://gvgen.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering",
    "authors": [
      "Keyang Ye",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes -- surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve anti-aliasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "compact",
      "geometry",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting",
    "authors": [
      "Ashkan Taghipour",
      "Vahid Naghshin",
      "Benjamin Southwell",
      "Farid Boussaid",
      "Hamid Laga",
      "Mohammed Bennamoun"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "robotics",
      "ar",
      "gaussian splatting",
      "nerf",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation",
    "authors": [
      "Rohan Chacko",
      "Nicolai Haeni",
      "Eldar Khaliullin",
      "Lin Sun",
      "Douglas Lee"
    ],
    "abstract": "We introduce Lifting By Gaussians (LBG), a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently, 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM, etc.), together with features from CLIP and DINOv2, directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches, LBG requires no per-scene training, allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular, enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore, our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "segmentation",
      "semantic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians",
    "authors": [
      "Letian Huang",
      "Dongwei Ye",
      "Jialin Dan",
      "Chengzhi Tao",
      "Huiwen Liu",
      "Kun Zhou",
      "Bo Ren",
      "Yuanqi Li",
      "Yanwen Guo",
      "Jie Guo"
    ],
    "abstract": "The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "reflection",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "efficient rendering",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation",
    "authors": [
      "Umar Farooq",
      "Jean-Yves Guillemaut",
      "Adrian Hilton",
      "Marco Volino"
    ],
    "abstract": "The field of Novel View Synthesis has been revolutionized by 3D Gaussian Splatting (3DGS), which enables high-quality scene reconstruction that can be rendered in real-time. 3DGS-based techniques typically suffer from high GPU memory and disk storage requirements which limits their practical application on consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated coarse-to-fine optimization framework that aims to minimize the number of Gaussian primitives used to represent a scene, thus reducing memory and storage demands. Opti3DGS leverages image frequency modulation, initially enforcing a coarse scene representation and progressively refining it by modulating frequency details in the training images. On the baseline 3DGS, we demonstrate an average reduction of 62% in Gaussians, a 40% reduction in the training GPU memory requirements and a 20% reduction in optimization time without sacrificing the visual quality. Furthermore, we show that our method integrates seamlessly with many 3DGS-based techniques, consistently reducing the number of Gaussian primitives while maintaining, and often improving, visual quality. Additionally, Opti3DGS inherently produces a level-of-detail scene representation at no extra cost, a natural byproduct of the optimization pipeline. Results and code will be made publicly available.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-18",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
    "authors": [
      "Lifeng Lin",
      "Rongfeng Lu",
      "Quan Chen",
      "Haofan Ren",
      "Ming Lu",
      "Yaoqi Sun",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in practical 3D reconstruction applications. Recently, many methods based on the 3D Gaussian Splatting (3DGS) framework have been proposed to address sparse-view 3D reconstruction. Although these methods have made considerable advancements, they still show significant issues with overfitting. To reduce the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number Control (VGNC) approach based on generative novel view synthesis (NVS) models. To the best of our knowledge, this is the first attempt to alleviate the overfitting issue of sparse-view 3DGS with generative validation images. Specifically, we first introduce a validation image generation method based on a generative NVS model. We then propose a Gaussian number control strategy that utilizes generated validation images to determine the optimal Gaussian numbers, thereby reducing the issue of overfitting. We conducted detailed experiments on various sparse-view 3DGS baselines and datasets to evaluate the effectiveness of VGNC. Extensive experiments show that our approach not only reduces overfitting but also improves rendering quality on the test set while decreasing the number of Gaussian points. This reduction lowers storage demands and accelerates both training and rendering. The code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive Voxelization for Transform coding of 3D Gaussian splatting data",
    "authors": [
      "Chenjunjie Wang",
      "Shashank N. Sridhara",
      "Eduardo Pavez",
      "Antonio Ortega",
      "Cheng Chang"
    ],
    "abstract": "We present a novel compression framework for 3D Gaussian splatting (3DGS) data that leverages transform coding tools originally developed for point clouds. Contrary to existing 3DGS compression methods, our approach can produce compressed 3DGS models at multiple bitrates in a computationally efficient way. Point cloud voxelization is a discretization technique that point cloud codecs use to improve coding efficiency while enabling the use of fast transform coding algorithms. We propose an adaptive voxelization algorithm tailored to 3DGS data, to avoid the inefficiencies introduced by uniform voxelization used in point cloud codecs. We ensure the positions of larger volume Gaussians are represented at high resolution, as these significantly impact rendering quality. Meanwhile, a low-resolution representation is used for dense regions with smaller Gaussians, which have a relatively lower impact on rendering quality. This adaptive voxelization approach significantly reduces the number of Gaussians and the bitrate required to encode the 3DGS data. After voxelization, many Gaussians are moved or eliminated. Thus, we propose to fine-tune/recolor the remaining 3DGS attributes with an initialization that can reduce the amount of retraining required. Experimental results on pre-trained datasets show that our proposed compression framework outperforms existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-30",
    "categories": [
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Effective Gaussian Management for High-fidelity Object Reconstruction",
    "authors": [
      "Jiateng Liu",
      "Hao Gao",
      "Jiu-Cheng Xie",
      "Chi-Man Pun",
      "Jian Xiong",
      "Haolun Li",
      "Junxin Chen",
      "Feng Xu"
    ],
    "abstract": "This paper presents an effective Gaussian management framework for high-fidelity scene reconstruction of appearance and geometry. Departing from recent Gaussian Splatting (GS) methods that rely on indiscriminate attribute assignment, our approach introduces a novel densification strategy called \\emph{GauSep} that selectively activates Gaussian color or normal attributes. Together with a tailored rendering pipeline, termed \\emph{Separate Rendering}, this strategy alleviates gradient conflicts arising from dual supervision and yields improved reconstruction quality. In addition, we develop \\emph{GauRep}, an adaptive and integrated Gaussian representation that reduces redundancy both at the individual and global levels, effectively balancing model capacity and number of parameters. To provide reliable geometric supervision essential for effective management, we also introduce \\emph{CoRe}, a novel surface reconstruction module that distills normal fields from the SDF branch to the Gaussian branch through a confidence mechanism. Notably, our management framework is model-agnostic and can be seamlessly incorporated into other architectures, simultaneously improving performance and reducing model size. Extensive experiments demonstrate that our approach achieves superior performance in reconstructing both appearance and geometry compared with state-of-the-art methods, while using significantly fewer parameters.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations: Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; and Wavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss",
    "authors": [
      "Wenjun Lu",
      "Haodong Chen",
      "Anqi Yi",
      "Yuk Ying Chung",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Novel view synthesis is a fundamental task in 3D computer vision that aims to reconstruct realistic images from a set of posed input views. However, reconstruction quality degrades significantly under sparse-view conditions due to limited geometric cues. Existing methods, such as Neural Radiance Fields (NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from blurred details and structural artifacts when trained with insufficient views. Recent works have identified the quality of rendered depth as a key factor in mitigating these artifacts, as it directly affects geometric accuracy and view consistency. In this paper, we address these challenges by introducing Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that progressively refines geometry from coarse to fine levels. Central to HDGS is a novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and estimated monocular depths across multiple spatial scales. By enforcing multi-scale depth consistency, our method substantially improves structural fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU benchmarks demonstrate that HDGS achieves state-of-the-art performance under sparse-view settings while maintaining efficient and high-quality rendering",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting",
    "authors": [
      "Sara Sabour",
      "Lily Goli",
      "George Kopanas",
      "Mark Matthews",
      "Dmitry Lagun",
      "Leonidas Guibas",
      "Alec Jacobson",
      "David J. Fleet",
      "Andrea Tagliasacchi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction, offering efficient training and rendering speeds, making it suitable for real-time applications.However, current methods require highly controlled environments (no moving people or wind-blown elements, and consistent lighting) to meet the inter-view consistency assumption of 3DGS. This makes reconstruction of real-world captures problematic. We present SpotLessSplats, an approach that leverages pre-trained and general-purpose features coupled with robust optimization to effectively ignore transient distractors. Our method achieves state-of-the-art reconstruction quality both visually and quantitatively, on casual captures. Additional results available at: https://spotlesssplats.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-28",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "abstract": "Novel view synthesis (NVS) has shown significant promise for applications in cinematographic production, particularly through the exploitation of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D scenes, enabling the creation of new shots that are challenging to capture in the real world due to set topology or expensive equipment requirement. This innovation also offers cinematographic advantages such as smooth camera movements, virtual re-shoots, slow-motion effects, etc. This paper explores dynamic NVS with the aim of facilitating the model selection process. We showcase its potential through a short montage filmed using various NVS models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "motion",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
    "authors": [
      "Dai Sun",
      "Huhao Guan",
      "Kun Zhang",
      "Xike Xie",
      "S. Kevin Zhou"
    ],
    "abstract": "Dynamic and static components in scenes often exhibit distinct properties, yet most 4D reconstruction methods treat them indiscriminately, leading to suboptimal performance in both cases. This work introduces SDD-4DGS, the first framework for static-dynamic decoupled 4D scene reconstruction based on Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic perception coefficient that is naturally integrated into the Gaussian reconstruction pipeline, enabling adaptive separation of static and dynamic components. With carefully designed implementation strategies to realize this theoretical framework, our method effectively facilitates explicit learning of motion patterns for dynamic elements while maintaining geometric stability for static structures. Extensive experiments on five benchmark datasets demonstrate that SDD-4DGS consistently outperforms state-of-the-art methods in reconstruction fidelity, with enhanced detail restoration for static structures and precise modeling of dynamic motions. The code will be released.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
    "authors": [
      "JongMin Lee",
      "Sungjoo Yoo"
    ],
    "abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods. Project page: https://icetea-cv.github.io/densesfm/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Adaptive 3D Gaussian Splatting Video Streaming",
    "authors": [
      "Han Gong",
      "Qiyue Li",
      "Zhi Liu",
      "Hao Zhou",
      "Peng Yuan Zhou",
      "Zhu Li",
      "Jie Li"
    ],
    "abstract": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compression",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images",
    "authors": [
      "Changha Shin",
      "Woong Oh Cho",
      "Seon Joo Kim"
    ],
    "abstract": "360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering",
    "authors": [
      "Yunzhou Song",
      "Heguang Lin",
      "Jiahui Lei",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "abstract": "Recent advancements in neural rendering, particularly 2D Gaussian Splatting (2DGS), have shown promising results for jointly reconstructing fine appearance and geometry by leveraging 2D Gaussian surfels. However, current methods face significant challenges when rendering at arbitrary viewpoints, such as anti-aliasing for down-sampled rendering, and texture detail preservation for high-resolution rendering. We proposed a novel method to align the 2D surfels with texture maps and augment it with per-ray depth sorting and fisher-based pruning for rendering consistency and efficiency. With correct order, per-surfel texture maps significantly improve the capabilities to capture fine details. Additionally, to render high-fidelity details in varying viewpoints, we designed a frustum-based sampling method to mitigate the aliasing artifacts. Experimental results on benchmarks and our custom texture-rich dataset demonstrate that our method surpasses existing techniques, particularly in detail preservation and anti-aliasing.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations",
    "authors": [
      "Fadi Khatib",
      "Dror Moran",
      "Guy Trostianetsky",
      "Yoni Kasten",
      "Meirav Galun",
      "Ronen Basri"
    ],
    "abstract": "We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting",
    "authors": [
      "Elena Alegret",
      "Kunyi Li",
      "Sen Wang",
      "Siyun Liang",
      "Michael Niemeyer",
      "Stefano Gasperini",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "abstract": "3D scene reconstruction and understanding have gained increasing popularity, yet existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images. In this paper, we present GALA, a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. To extend to generalized language feature fields, we introduce the core contribution of GALA, a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning. Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RT-GuIDE: Real-Time Gaussian Splatting for Information-Driven Exploration",
    "authors": [
      "Yuezhan Tao",
      "Dexter Ong",
      "Varun Murali",
      "Igor Spasojevic",
      "Pratik Chaudhari",
      "Vijay Kumar"
    ],
    "abstract": "We propose a framework for active mapping and exploration that leverages Gaussian splatting for constructing dense maps. Further, we develop a GPU-accelerated motion planning algorithm that can exploit the Gaussian map for real-time navigation. The Gaussian map constructed onboard the robot is optimized for both photometric and geometric quality while enabling real-time situational awareness for autonomy. We show through viewpoint selection experiments that our method yields comparable Peak Signal-to-Noise Ratio (PSNR) and similar reconstruction error to state-of-the-art approaches, while being orders of magnitude faster to compute. In closed-loop physics-based simulation and real-world experiments, our algorithm achieves better map quality (at least 0.8dB higher PSNR and more than 16% higher geometric reconstruction accuracy) than maps constructed by a state-of-the-art method, enabling semantic segmentation using off-the-shelf open-set models. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/RT GuIDE/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "semantic",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization",
    "authors": [
      "Yueming Xu",
      "Haochen Jiang",
      "Zhongyang Xiao",
      "Jianfeng Feng",
      "Li Zhang"
    ],
    "abstract": "Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-13",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "tracking",
      "localization",
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "mapping",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians",
    "authors": [
      "Kerui Ren",
      "Lihan Jiang",
      "Tao Lu",
      "Mulin Yu",
      "Linning Xu",
      "Zhangkai Ni",
      "Bo Dai"
    ],
    "abstract": "The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "large scene",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting",
    "authors": [
      "Xiufeng Huang",
      "Ruiqi Li",
      "Yiu-ming Cheung",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D assets. To protect the copyright of these assets, digital watermarking techniques can be applied to embed ownership information discreetly within 3DGS models. However, existing watermarking methods for meshes, point clouds, and implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS models use explicit 3D Gaussians with distinct structures and do not rely on neural networks. Naively embedding the watermark on a pre-trained 3DGS can cause obvious distortion in rendered images. In our work, we propose an uncertainty-based method that constrains the perturbation of model parameters to achieve invisible watermarking for 3DGS. At the message decoding stage, the copyright messages can be reliably extracted from both 3D Gaussians and 2D rendered images even under various forms of 3D and 2D distortions. We conduct extensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate the effectiveness of our proposed method, demonstrating state-of-the-art performance on both message decoding accuracy and view synthesis quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting",
    "authors": [
      "Qihang Zhang",
      "Yinghao Xu",
      "Chaoyang Wang",
      "Hsin-Ying Lee",
      "Gordon Wetzstein",
      "Bolei Zhou",
      "Ceyuan Yang"
    ],
    "abstract": "Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing. Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
    "authors": [
      "Lukas Höllein",
      "Aljaž Božič",
      "Michael Zollhöfer",
      "Matthias Nießner"
    ],
    "abstract": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting",
    "authors": [
      "Xingyue Peng",
      "Yuandong Lyu",
      "Lang Zhang",
      "Jian Zhu",
      "Songtao Wang",
      "Jiaxin Deng",
      "Songxin Lu",
      "Weiliang Ma",
      "Dangen She",
      "Peng Jia",
      "XianPeng Lang"
    ],
    "abstract": "Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "segmentation",
      "semantic",
      "lighting",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "autonomous driving",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis",
    "authors": [
      "Yuanhao Cai",
      "Yixun Liang",
      "Jiahao Wang",
      "Angtian Wang",
      "Yulun Zhang",
      "Xiaokang Yang",
      "Zongwei Zhou",
      "Alan Yuille"
    ],
    "abstract": "X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73x inference speed. The application on sparse-view CT reconstruction also reveals the practical values of our method. Code is publicly available at https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-07",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios",
    "authors": [
      "Zikang Yuan",
      "Yuechuan Pu",
      "Hongcheng Luo",
      "Fengtian Lang",
      "Cheng Chi",
      "Teng Li",
      "Yingying Shen",
      "Haiyang Sun",
      "Bing Wang",
      "Xin Yang"
    ],
    "abstract": "Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.RO",
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "neural rendering",
      "dynamic",
      "ar",
      "gaussian splatting",
      "nerf",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion",
    "authors": [
      "Rui Qian",
      "Haozhi Cao",
      "Tianchen Deng",
      "Shenghai Yuan",
      "Lihua Xie"
    ],
    "abstract": "Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory consumption by more than 9.3%. The code will be released upon acceptance.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "semantic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ],
    "abstract": "3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "illumination",
      "geometry",
      "lighting",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction",
    "authors": [
      "Ruyi Zha",
      "Tao Jun Lin",
      "Yuanhao Cai",
      "Jiwen Cao",
      "Yanhao Zhang",
      "Hongdong Li"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown integration bias in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12$\\times$ faster than NeRF-based methods and on par with traditional algorithms. Code and models are available on the project page https://github.com/Ruyi-Zha/r2_gaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-31",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction",
    "authors": [
      "Sierra Bonilla",
      "Shuai Zhang",
      "Dimitrios Psychogyios",
      "Danail Stoyanov",
      "Francisco Vasconcelos",
      "Sophia Bano"
    ],
    "abstract": "Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "localization",
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction",
    "authors": [
      "Haodong Xiang",
      "Xinghui Li",
      "Kai Cheng",
      "Xiansong Lai",
      "Wanting Zhang",
      "Zhichao Liao",
      "Long Zeng",
      "Xueping Liu"
    ],
    "abstract": "Embodied intelligence requires precise reconstruction and rendering to simulate large-scale real-world data. Although 3D Gaussian Splatting (3DGS) has recently demonstrated high-quality results with real-time performance, it still faces challenges in indoor scenes with large, textureless regions, resulting in incomplete and noisy reconstructions due to poor point cloud initialization and underconstrained optimization. Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, we propose a unified optimization framework that integrates neural signed distance fields (SDFs) with 3DGS for accurate geometry reconstruction and real-time rendering. This framework incorporates a neural SDF field to guide the densification and pruning of Gaussians, enabling Gaussians to model scenes accurately even with poor initialized point clouds. Simultaneously, the geometry represented by Gaussians improves the efficiency of the SDF field by piloting its point sampling. Additionally, we introduce two regularization terms based on normal and edge priors to resolve geometric ambiguities in textureless areas and enhance detail accuracy. Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction provides a comprehensive description of the surrounding scenes and has become an essential task for 3D perception. Most existing methods focus on offline perception from one or a few views and cannot be applied to embodied agents that demand to gradually perceive the scene through progressive embodied exploration. In this paper, we formulate an embodied 3D occupancy prediction task to target this practical scenario and propose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize the global scene with uniform 3D semantic Gaussians and progressively update local regions observed by the embodied agent. For each update, we extract semantic and structural features from the observed image and efficiently incorporate them via deformable cross-attention to refine the regional Gaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global 3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown (i.e., uniformly distributed) environment and maintains an explicit global memory of it with 3D Gaussians. It gradually gains knowledge through the local refinement of regional Gaussians, which is consistent with how humans understand new scenes through embodied exploration. We reorganize an EmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the evaluation of the embodied 3D occupancy prediction task. Our EmbodiedOcc outperforms existing methods by a large margin and accomplishes the embodied occupancy prediction with high accuracy and efficiency. Code: https://github.com/YkiWu/EmbodiedOcc.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "semantic",
      "ar",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Splatting SLAM",
    "authors": [
      "Hidenobu Matsuki",
      "Riku Murai",
      "Paul H. J. Kelly",
      "Andrew J. Davison"
    ],
    "abstract": "We present the first application of 3D Gaussian Splatting in monocular SLAM, the most fundamental but the hardest setup for Visual SLAM. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to RGB-D SLAM when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-11",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "tracking",
      "slam",
      "motion",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense Mapping",
    "authors": [
      "Meng Wang",
      "Junyi Wang",
      "Changqun Xia",
      "Chen Wang",
      "Yue Qi"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has recently demonstrated promising advancements in RGB-D online dense mapping. Nevertheless, existing methods excessively rely on per-pixel depth cues to perform map densification, which leads to significant redundancy and increased sensitivity to depth noise. Additionally, explicitly storing 3D Gaussian parameters of room-scale scene poses a significant storage challenge. In this paper, we introduce OG-Mapping, which leverages the robust scene structural representation capability of sparse octrees, combined with structured 3D Gaussian representations, to achieve efficient and robust online dense mapping. Moreover, OG-Mapping employs an anchor-based progressive map refinement strategy to recover the scene structures at multiple levels of detail. Instead of maintaining a small number of active keyframes with a fixed keyframe window as previous approaches do, a dynamic keyframe window is employed to allow OG-Mapping to better tackle false local minima and forgetting issues. Experimental results demonstrate that OG-Mapping delivers more robust and superior realism mapping results than existing Gaussian-based RGB-D online mapping methods with a compact model, and no additional post-processing is required.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "compact",
      "dynamic",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Improving Gaussian Splatting with Localized Points Management",
    "authors": [
      "Haosen Yang",
      "Chenhao Zhang",
      "Wenqing Wang",
      "Marco Volino",
      "Adrian Hilton",
      "Li Zhang",
      "Xiatian Zhu"
    ],
    "abstract": "Point management is critical for optimizing 3D Gaussian Splatting models, as point initiation (e.g., via structure from motion) is often distributionally inappropriate. Typically, Adaptive Density Control (ADC) algorithm is adopted, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset. We reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) due to inability of identifying all 3D zones requiring point densification, and lacking an appropriate mechanism to handle ill-conditioned points with negative impacts (e.g., occlusion due to false high opacity). To address these limitations, we propose a Localized Point Management (LPM) strategy, capable of identifying those error-contributing zones in greatest need for both point addition and geometry calibration. Zone identification is achieved by leveraging the underlying multiview geometry constraints, subject to image rendering errors. We apply point densification in the identified zones and then reset the opacity of the points in front of these regions, creating a new opportunity to correct poorly conditioned points. Serving as a versatile plugin, LPM can be seamlessly integrated into existing static 3D and dynamic 4D Gaussian Splatting models with minimal additional cost. Experimental evaluations validate the efficacy of our LPM in boosting a variety of existing 3D/4D models both quantitatively and qualitatively. Notably, LPM improves both static 3DGS and dynamic SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, excelling on challenging datasets such as Tanks & Temples and the Neural 3D Video dataset.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions",
    "authors": [
      "Qingyuan Zhou",
      "Yuehu Gong",
      "Weidong Yang",
      "Jiaze Li",
      "Yeqi Luo",
      "Baixin Xu",
      "Shuhao Li",
      "Ben Fei",
      "Ying He"
    ],
    "abstract": "Novel view synthesis (NVS) and surface reconstruction (SR) are essential tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks are often addressed independently, with GS-based rendering methods struggling under diverse light conditions and failing to produce accurate surfaces, while GS-based reconstruction methods frequently compromise rendering quality. This raises a central question: must rendering and reconstruction always involve a trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian splatting for Surface Reconstruction that enhances both rendering quality and 3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction, providing precise geometry information to the 3D-GS branch. Leveraging this geometry, the 3D-GS branch employs a geometry-guided illumination decomposition module that captures reflected and transmitted components, enabling realistic rendering under varied light conditions. Using the transmitted component as supervision, the 2D-GS branch also achieves high-fidelity surface reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS branches undergo alternating optimization, providing mutual supervision. Prior to this, each branch completes an independent warm-up phase, with an early stopping strategy implemented to reduce computational costs. We evaluate MGSR on a diverse set of synthetic and real-world datasets, at both object and scene levels, demonstrating strong performance in rendering and surface reconstruction. Code is available at https://github.com/TsingyuanChou/MGSR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "high-fidelity",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Mesh-based Gaussian Splatting for Real-time Large-scale Deformation",
    "authors": [
      "Lin Gao",
      "Jie Yang",
      "Bo-Tao Zhang",
      "Jia-Mu Sun",
      "Yu-Jie Yuan",
      "Hongbo Fu",
      "Yu-Kun Lai"
    ],
    "abstract": "Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-07",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for Geometrically Accurate and Artifact-Reduced Reconstruction",
    "authors": [
      "Miriam Jäger",
      "Markus Hillemann",
      "Boris Jutzi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene reconstruction using 3D Gaussians. However, neither the centers nor surfaces of the Gaussians are accurately aligned to the object surface, complicating their direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically produces floater artifacts, increasing the number of Gaussians and storage requirements. To address these issues, we present FeatureGS, which incorporates an additional geometric loss term based on an eigenvalue-derived 3D shape feature into the optimization process of 3DGS. The goal is to improve geometric accuracy and enhance properties of planar surfaces with reduced structural entropy in local 3D neighborhoods.We present four alternative formulations for the geometric loss term based on 'planarity' of Gaussians, as well as 'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We provide quantitative and qualitative evaluations on 15 scenes of the DTU benchmark dataset focusing on following key aspects: Geometric accuracy and artifact-reduction, measured by the Chamfer distance, and memory efficiency, evaluated by the total number of Gaussians. Additionally, rendering quality is monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses floater artifacts, while maintaining comparable photometric rendering quality. The geometric loss with 'planarity' from Gaussians provides the highest geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces floater artifacts and number of Gaussians the most. This makes FeatureGS a strong method for geometrically accurate, artifact-reduced and memory-efficient 3D scene reconstruction, enabling the direct use of Gaussian centers for geometric representation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors",
    "authors": [
      "Tingyang Zhang",
      "Qingzhe Gao",
      "Weiyu Li",
      "Libin Liu",
      "Baoquan Chen"
    ],
    "abstract": "Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation. Recently, some studies have successfully constructed animatable 3D models from monocular videos. However, these approaches require sufficient view coverage of the object within the input video and typically necessitate significant time and computational costs for training and rendering. This limitation restricts the practical applications. In this work, we propose a method to build animatable 3D Gaussian Splatting from monocular video with diffusion priors. The 3D Gaussian representations significantly accelerate the training and rendering process, and the diffusion priors allow the method to learn 3D models with limited viewpoints. We also present the rigid regularization to enhance the utilization of the priors. We perform an extensive evaluation across various real-world videos, demonstrating its superior performance compared to the current state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing",
    "authors": [
      "Jian Gao",
      "Chun Gu",
      "Youtian Lin",
      "Zhihao Li",
      "Hao Zhu",
      "Xun Cao",
      "Li Zhang",
      "Yao Yao"
    ],
    "abstract": "In this paper, we present a novel differentiable point-based rendering framework to achieve photo-realistic relighting. To make the reconstructed scene relightable, we enhance vanilla 3D Gaussians by associating extra properties, including normal vectors, BRDF parameters, and incident lighting from various directions. From a collection of multi-view images, the 3D scene is optimized through 3D Gaussian Splatting while BRDF and lighting are decomposed by physically based differentiable rendering. To produce plausible shadow effects in photo-realistic relighting, we introduce an innovative point-based ray tracing with the bounding volume hierarchies for efficient visibility pre-computation. Extensive experiments demonstrate our improved BRDF estimation, novel view synthesis and relighting results compared to state-of-the-art approaches. The proposed framework showcases the potential to revolutionize the mesh-based graphics pipeline with a point-based pipeline enabling editing, tracing, and relighting.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-11-27",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "shadow",
      "relightable",
      "ray tracing",
      "lighting",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds",
    "authors": [
      "Youyu Chen",
      "Junjun Jiang",
      "Kui Jiang",
      "Xiao Tang",
      "Zhihao Li",
      "Xianming Liu",
      "Yinyu Nie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where the rendering resolution and the primitive number, concluded as the optimization complexity, dominate the time cost in primitive optimization. In this paper, we propose DashGaussian, a scheduling scheme over the optimization complexity of 3DGS that strips redundant complexity to accelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as progressively fitting 3DGS to higher levels of frequency components in the training views, and propose a dynamic rendering resolution scheme that largely reduces the optimization complexity based on this formulation. Besides, we argue that a specific rendering resolution should cooperate with a proper primitive number for a better balance between computing redundancy and fitting quality, where we schedule the growth of the primitives to synchronize with the rendering resolution. Extensive experiments show that our method accelerates the optimization of various 3DGS backbones by 45.7% on average while preserving the rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering",
    "authors": [
      "Meenakshi Krishnan",
      "Liam Fowl",
      "Ramani Duraiswami"
    ],
    "abstract": "Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis. However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail. We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh. The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming. We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-14",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "animation",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "face",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting",
    "authors": [
      "Phuc Nguyen Xuan",
      "Thanh Nguyen Canh",
      "Huu-Hung Nguyen",
      "Nak Young Chong",
      "Xiem HoangVan"
    ],
    "abstract": "This survey comprehensively reviews the evolving field of multi-robot collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian Splatting (3DGS). As an explicit scene representation, 3DGS has enabled unprecedented real-time, high-fidelity rendering, ideal for robotics. However, its use in multi-robot systems introduces significant challenges in maintaining global consistency, managing communication, and fusing data from heterogeneous sources. We systematically categorize approaches by their architecture -- centralized, distributed -- and analyze core components like multi-agent consistency and alignment, communication-efficient, Gaussian representation, semantic distillation, fusion and pose optimization, and real-time scalability. In addition, a summary of critical datasets and evaluation metrics is provided to contextualize performance. Finally, we identify key open challenges and chart future research directions, including lifelong mapping, semantic association and mapping, multi-model for robustness, and bridging the Sim2Real gap.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey",
      "high-fidelity",
      "semantic",
      "localization",
      "slam",
      "robotics",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting",
    "authors": [
      "Haofan Ren",
      "Qingsong Yan",
      "Ming Lu",
      "Rongfeng Lu",
      "Zunjie Zhu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced neural fields, as it enables high-fidelity rendering with impressive visual quality. However, 3DGS has difficulty accurately representing surfaces. In contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian disks. Despite advancements in geometric fidelity, rendering quality remains compromised, highlighting the challenge of achieving both high-quality rendering and precise geometric structures. This indicates that optimizing both geometric and rendering quality in a single training stage is currently unfeasible. To overcome this limitation, we present 2DGS-R, a new method that uses a hierarchical training approach to improve rendering quality while maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians with the normal consistency regularization. Then 2DGS-R selects the 2D Gaussians with inadequate rendering quality and applies a novel in-place cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R model with opacity frozen. Experimental results show that compared to the original 2DGS, our method requires only 1\\% more storage and minimal additional training time. Despite this negligible overhead, it achieves high-quality rendering results while preserving fine geometric structures. These findings indicate that our approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lighting",
      "head",
      "gaussian splatting",
      "ar",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians",
    "authors": [
      "Avinash Paliwal",
      "Wei Ye",
      "Jinhui Xiong",
      "Dmytro Kotovenko",
      "Rakesh Ranjan",
      "Vikas Chandra",
      "Nima Khademi Kalantari"
    ],
    "abstract": "The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ],
    "abstract": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "compression",
      "motion",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhancement of 3D Gaussian Splatting using Raw Mesh for Photorealistic Recreation of Architectures",
    "authors": [
      "Ruizhe Wang",
      "Chunliang Hua",
      "Tomakayev Shingys",
      "Mengyuan Niu",
      "Qingxin Yang",
      "Lizhong Gao",
      "Yi Zheng",
      "Junyan Yang",
      "Qiao Wang"
    ],
    "abstract": "The photorealistic reconstruction and rendering of architectural scenes have extensive applications in industries such as film, games, and transportation. It also plays an important role in urban planning, architectural design, and the city's promotion, especially in protecting historical and cultural relics. The 3D Gaussian Splatting, due to better performance over NeRF, has become a mainstream technology in 3D reconstruction. Its only input is a set of images but it relies heavily on geometric parameters computed by the SfM process. At the same time, there is an existing abundance of raw 3D models, that could inform the structural perception of certain buildings but cannot be applied. In this paper, we propose a straightforward method to harness these raw 3D models to guide 3D Gaussians in capturing the basic shape of the building and improve the visual quality of textures and details when photos are captured non-systematically. This exploration opens up new possibilities for improving the effectiveness of 3D reconstruction techniques in the field of architectural design.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "abstract": "This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle",
    "authors": [
      "Youtian Lin",
      "Zuozhuo Dai",
      "Siyu Zhu",
      "Yao Yao"
    ],
    "abstract": "We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d reconstruction",
      "nerf",
      "3d gaussian",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting",
    "authors": [
      "Jun Guo",
      "Xiaojian Ma",
      "Yue Fan",
      "Huaping Liu",
      "Qing Li"
    ],
    "abstract": "Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, with wide-ranging applications in embodied agents and augmented reality systems. Existing methods adopt neurel rendering methods as 3D representations and jointly optimize color and semantic features to achieve rendering and scene understanding simultaneously. In this paper, we introduce Semantic Gaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our key idea is to distill knowledge from 2D pre-trained models to 3D Gaussians. Unlike existing methods, we design a versatile projection approach that maps various 2D semantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, which is based on spatial relationship and need no additional training. We further build a 3D semantic network that directly predicts the semantic component from raw 3D Gaussians for fast inference. The quantitative results on ScanNet segmentation and LERF object localization demonstates the superior performance of our method. Additionally, we explore several applications of Semantic Gaussians including object part segmentation, instance segmentation, scene editing, and spatiotemporal segmentation with better qualitative results over 2D and 3D baselines, highlighting its versatility and effectiveness on supporting diverse downstream tasks.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "localization",
      "semantic",
      "lighting",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis under Diverse Multi-view Camera Settings",
    "authors": [
      "Yingdong Hu",
      "Zhening Liu",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Feed-forward based 3D Gaussian Splatting methods have demonstrated exceptional capability in real-time novel view synthesis for human models. However, current approaches are confined to either dense viewpoint configurations or restricted image resolutions. These limitations hinder their flexibility in free-viewpoint rendering across a wide range of camera view angle discrepancies, and also restrict their ability to recover fine-grained human details in real time using commonly available GPUs. To address these challenges, we propose a novel pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse multi-view camera settings. Specifically, we first design an Efficient Cross-View Attention (EVA) module to effectively fuse cross-view information under high resolution inputs and sparse view settings, while minimizing temporal and computational overhead. Additionally, we introduce a feature refinement mechianism to predict the attributes of the 3D Gaussians and assign a feature value to each Gaussian, enabling the correction of artifacts caused by geometric inaccuracies in position estimation and enhancing overall visual fidelity. Experimental results on the THuman2.0 and THumansit datasets showcase the superiority of EVA-Gaussian in rendering quality across diverse camera settings. Project page: https://zhenliuzju.github.io/huyingdong/EVA-Gaussian.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation",
    "authors": [
      "Kai Cheng",
      "Xiaoxiao Long",
      "Kaizhi Yang",
      "Yao Yao",
      "Wei Yin",
      "Yuexin Ma",
      "Wenping Wang",
      "Xuejin Chen"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-02-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "neural rendering",
      "motion",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "4D Radar-Inertial Odometry based on Gaussian Modeling and Multi-Hypothesis Scan Matching",
    "authors": [
      "Fernando Amodeo",
      "Luis Merino",
      "Fernando Caballero"
    ],
    "abstract": "4D millimeter-wave (mmWave) radars are sensors that provide robustness against adverse weather conditions (rain, snow, fog, etc.), and as such they are increasingly used for odometry and SLAM (Simultaneous Location and Mapping). However, the noisy and sparse nature of the returned scan data proves to be a challenging obstacle for existing registration algorithms, especially those originally intended for more accurate sensors such as LiDAR. Following the success of 3D Gaussian Splatting for vision, in this paper we propose a summarized representation for radar scenes based on global simultaneous optimization of 3D Gaussians as opposed to voxel-based approaches, and leveraging its inherent probability distribution function for registration. Moreover, we propose tackling the problem of radar noise by optimizing multiple scan matching hypotheses in order to further increase the robustness of the system against local optima of the function. Finally, following existing practice we implement an Extended Kalman Filter-based Radar-Inertial Odometry pipeline in order to evaluate the effectiveness of our system. Experiments using publicly available 4D radar datasets show that our Gaussian approach is comparable to existing registration algorithms, outperforming them in several sequences.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-18",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "ar",
      "gaussian splatting",
      "mapping",
      "3d gaussian",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and Streaming",
    "authors": [
      "Jiaxuan Zhu",
      "Hao Tang"
    ],
    "abstract": "Representing and rendering dynamic scenes from 2D images is a fundamental yet challenging problem in computer vision and graphics. This survey provides a comprehensive review of the evolution and advancements in dynamic scene representation and rendering, with a particular emphasis on recent progress in Neural Radiance Fields based and 3D Gaussian Splatting based reconstruction methods. We systematically summarize existing approaches, categorize them according to their core principles, compile relevant datasets, compare the performance of various methods on these benchmarks, and explore the challenges and future research directions in this rapidly evolving field. In total, we review over 170 relevant papers, offering a broad perspective on the state of the art in this domain.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting",
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Vasu Singla",
      "Mayuka Jayawardhana",
      "Matthias Zwicker",
      "Tom Goldstein"
    ],
    "abstract": "Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-14",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution",
    "authors": [
      "Jintong Hu",
      "Bin Xia",
      "Bin Chen",
      "Wenming Yang",
      "Lei Zhang"
    ],
    "abstract": "Implicit neural representations (INRs) have significantly advanced the field of arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based ASSR networks first extract features from the given low-resolution image using an encoder, and then render the super-resolved result via a multi-layer perceptron decoder. Although these approaches have shown promising results, their performance is constrained by the limited representation ability of discrete latent codes in the encoded features. In this paper, we propose a novel ASSR method named GaussianSR that overcomes this limitation through 2D Gaussian Splatting (2DGS). Unlike traditional methods that treat pixels as discrete points, GaussianSR represents each pixel as a continuous Gaussian field. The encoded features are simultaneously refined and upsampled by rendering the mutually stacked Gaussian fields. As a result, long-range dependencies are established to enhance representation ability. In addition, a classifier is developed to dynamically assign Gaussian kernels to all pixels to further improve flexibility. All components of GaussianSR (i.e., encoder, classifier, Gaussian kernels, and decoder) are jointly learned end-to-end. Experiments demonstrate that GaussianSR achieves superior ASSR performance with fewer parameters than existing methods while enjoying interpretable and content-aware feature aggregations.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "NovelGS: Consistent Novel-view Denoising via Large Gaussian Reconstruction Model",
    "authors": [
      "Jinpeng Liu",
      "Jiale Xu",
      "Weihao Cheng",
      "Yiming Gao",
      "Xintao Wang",
      "Ying Shan",
      "Yansong Tang"
    ],
    "abstract": "We introduce NovelGS, a diffusion model for Gaussian Splatting (GS) given sparse-view images. Recent works leverage feed-forward networks to generate pixel-aligned Gaussians, which could be fast rendered. Unfortunately, the method was unable to produce satisfactory results for areas not covered by the input images due to the formulation of these methods. In contrast, we leverage the novel view denoising through a transformer-based network to generate 3D Gaussians. Specifically, by incorporating both conditional views and noisy target views, the network predicts pixel-aligned Gaussians for each view. During training, the rendered target and some additional views of the Gaussians are supervised. During inference, the target views are iteratively rendered and denoised from pure noise. Our approach demonstrates state-of-the-art performance in addressing the multi-view image reconstruction challenge. Due to generative modeling of unseen regions, NovelGS effectively reconstructs 3D objects with consistent and sharp textures. Experimental results on publicly available datasets indicate that NovelGS substantially surpasses existing image-to-3D frameworks, both qualitatively and quantitatively. We also demonstrate the potential of NovelGS in generative tasks, such as text-to-3D and image-to-3D, by integrating it with existing multiview diffusion models. We will make the code publicly accessible.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams",
    "authors": [
      "Yang LI",
      "Jinglu Wang",
      "Lei Chu",
      "Xiao Li",
      "Shiu-hong Kao",
      "Ying-Cong Chen",
      "Yan Lu"
    ],
    "abstract": "The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene reconstruction and novel view synthesis. With the growing interest of interactive applications that need immediate feedback, online 3DGS reconstruction in real-time is in high demand. However, none of existing methods yet meet the demand due to three main challenges: the absence of predetermined camera parameters, the need for generalizable 3DGS optimization, and the necessity of reducing redundancy. We propose StreamGS, an online generalizable 3DGS reconstruction method for unposed image streams, which progressively transform image streams to 3D Gaussian streams by predicting and aggregating per-frame Gaussians. Our method overcomes the limitation of the initial point reconstruction \\cite{dust3r} in tackling out-of-domain (OOD) issues by introducing a content adaptive refinement. The refinement enhances cross-frame consistency by establishing reliable pixel correspondences between adjacent frames. Such correspondences further aid in merging redundant Gaussians through cross-frame feature aggregation. The density of Gaussians is thereby reduced, empowering online reconstruction by significantly lowering computational and memory costs. Extensive experiments on diverse datasets have demonstrated that StreamGS achieves quality on par with optimization-based approaches but does so 150 times faster, and exhibits superior generalizability in handling OOD scenes.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars",
    "authors": [
      "Forrest Iandola",
      "Stanislav Pidhorskyi",
      "Igor Santesteban",
      "Divam Gupta",
      "Anuj Pahuja",
      "Nemanja Bartolovic",
      "Frank Yu",
      "Emanuel Garbin",
      "Tomas Simon",
      "Shunsuke Saito"
    ],
    "abstract": "Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset. Demo videos are available at https://forresti.github.io/squeezeme.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "animation",
      "lightweight",
      "high-fidelity",
      "body",
      "head",
      "ar",
      "avatar",
      "vr",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Kailun Yang",
      "Jiajun Zhai",
      "Shangwei Guo",
      "Lin Wang",
      "Kaiwei Wang"
    ],
    "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "animation",
      "semantic",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations",
    "authors": [
      "Ziren Gong",
      "Xiaohan Li",
      "Fabio Tosi",
      "Youmin Zhang",
      "Stefano Mattoccia",
      "Jun Wu",
      "Matteo Poggi"
    ],
    "abstract": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "slam",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "HGS-Planner: Hierarchical Planning Framework for Active Scene Reconstruction Using 3D Gaussian Splatting",
    "authors": [
      "Zijun Xu",
      "Rui Jin",
      "Ke Wu",
      "Yi Zhao",
      "Zhiwei Zhang",
      "Jieru Zhao",
      "Fei Gao",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "abstract": "In complex missions such as search and rescue,robots must make intelligent decisions in unknown environments, relying on their ability to perceive and understand their surroundings. High-quality and real-time reconstruction enhances situational awareness and is crucial for intelligent robotics. Traditional methods often struggle with poor scene representation or are too slow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting (3DGS), we propose a hierarchical planning framework for fast and high-fidelity active reconstruction. Our method evaluates completion and quality gain to adaptively guide reconstruction, integrating global and local planning for efficiency. Experiments in simulated and real-world environments show our approach outperforms existing real-time methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "robotics",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: a vision-based low overhead framework for 3D scene reconstruction",
    "authors": [
      "Yunji Feng",
      "Chengpu Yu",
      "Fengrui Ran",
      "Zhi Yang",
      "Yinni Liu"
    ],
    "abstract": "This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-14",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
    "authors": [
      "Anushka Agarwal",
      "Muhammad Yusuf Hassan",
      "Talha Chafekar"
    ],
    "abstract": "We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "MANUS: Markerless Grasp Capture using Articulated 3D Gaussians",
    "authors": [
      "Chandradeep Pokhariya",
      "Ishaan N Shah",
      "Angela Xing",
      "Zekun Li",
      "Kefan Chen",
      "Avinash Sharma",
      "Srinath Sridhar"
    ],
    "abstract": "Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that does not represent hand shape accurately resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "robotics",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model",
    "authors": [
      "Yufei Wang",
      "Zhihao Li",
      "Lanqing Guo",
      "Wenhan Yang",
      "Alex C. Kot",
      "Bihan Wen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity. However, the large number of Gaussians and their associated attributes require effective compression techniques. Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence. Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work. We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency. To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed. Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "compression",
      "compact",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair",
    "authors": [
      "Weihang Li",
      "Weirong Chen",
      "Shenhan Qian",
      "Jiajie Chen",
      "Daniel Cremers",
      "Haoang Li"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views",
    "authors": [
      "Yuki Fujimura",
      "Takahiro Kushida",
      "Kazuya Kitano",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa"
    ],
    "abstract": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Monocular and Generalizable Gaussian Talking Head Animation",
    "authors": [
      "Shengjie Gong",
      "Haojie Li",
      "Jiapeng Tang",
      "Dongming Hu",
      "Shuangping Huang",
      "Hao Chen",
      "Tianshui Chen",
      "Zhuoman Liu"
    ],
    "abstract": "In this work, we introduce Monocular and Generalizable Gaussian Talking Head Animation (MGGTalk), which requires monocular datasets and generalizes to unseen identities without personalized re-training. Compared with previous 3D Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or tedious personalized learning/inference, MGGtalk enables more practical and broader applications. However, in the absence of multi-view and personalized training data, the incompleteness of geometric and appearance information poses a significant challenge. To address these challenges, MGGTalk explores depth information to enhance geometric and facial symmetry characteristics to supplement both geometric and appearance features. Initially, based on the pixel-wise geometric information obtained from depth estimation, we incorporate symmetry operations and point cloud filtering techniques to ensure a complete and precise position parameter for 3DGS. Subsequently, we adopt a two-stage strategy with symmetric priors for predicting the remaining 3DGS parameters. We begin by predicting Gaussian parameters for the visible facial regions of the source image. These parameters are subsequently utilized to improve the prediction of Gaussian parameters for the non-visible regions. Extensive experiments demonstrate that MGGTalk surpasses previous state-of-the-art methods, achieving superior performance across various metrics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "head",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Shadow Casting for Neural Characters",
    "authors": [
      "Luis Bolanos",
      "Shih-Yang Su",
      "Helge Rhodin"
    ],
    "abstract": "Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-01-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "neural rendering",
      "geometry",
      "outdoor",
      "lighting",
      "dynamic",
      "motion",
      "gaussian splatting",
      "head",
      "ar",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering",
    "authors": [
      "Euntae Choi",
      "Sungjoo Yoo"
    ],
    "abstract": "We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering. We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods. In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh. Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-16",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian",
      "relighting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GSemSplat: Generalizable Semantic 3D Gaussian Splatting from Uncalibrated Image Pairs",
    "authors": [
      "Xingrui Wang",
      "Cuiling Lan",
      "Hanxin Zhu",
      "Zhibo Chen",
      "Yan Lu"
    ],
    "abstract": "Modeling and understanding the 3D world is crucial for various applications, from augmented reality to robotic navigation. Recent advancements based on 3D Gaussian Splatting have integrated semantic information from multi-view images into Gaussian primitives. However, these methods typically require costly per-scene optimization from dense calibrated images, limiting their practicality. In this paper, we consider the new task of generalizable 3D semantic field modeling from sparse, uncalibrated image pairs. Building upon the Splatt3R architecture, we introduce GSemSplat, a framework that learns open-vocabulary semantic representations linked to 3D Gaussians without the need for per-scene optimization, dense image collections or calibration. To ensure effective and reliable learning of semantic features in 3D space, we employ a dual-feature approach that leverages both region-specific and context-aware semantic features as supervision in the 2D space. This allows us to capitalize on their complementary strengths. Experimental results on the ScanNet++ dataset demonstrate the effectiveness and superiority of our approach compared to the traditional scene-specific method. We hope our work will inspire more research into generalizable 3D understanding.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting",
    "authors": [
      "Jiazhong Cen",
      "Xudong Zhou",
      "Jiemin Fang",
      "Changsong Wen",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wei Shen",
      "Qi Tian"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D scene reconstruction from RGB images. Many studies extend this paradigm for language-driven open-vocabulary scene understanding. However, most of them simply project 2D semantic features onto 3D Gaussians and overlook a fundamental gap between 2D and 3D understanding: a 3D object may exhibit various semantics from different viewpoints--a phenomenon we term view-dependent semantics. To address this challenge, we propose LaGa (Language Gaussians), which establishes cross-view semantic connections by decomposing the 3D scene into objects. Then, it constructs view-aggregated semantic representations by clustering semantic descriptors and reweighting them based on multi-view semantics. Extensive experiments demonstrate that LaGa effectively captures key information from view-dependent semantics, enabling a more comprehensive understanding of 3D scenes. Notably, under the same settings, LaGa achieves a significant improvement of +18.7% mIoU over the previous SOTA on the LERF-OVS dataset. Our code is available at: https://github.com/SJTU-DeepVisionLab/LaGa.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "semantic",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud",
    "authors": [
      "Natsuki Takama",
      "Shintaro Ito",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Gaussian Splatting (GS) has gained attention as a fast and effective method for novel view synthesis. It has also been applied to 3D reconstruction using multi-view images and can achieve fast and accurate 3D reconstruction. However, GS assumes that the input contains a large number of multi-view images, and therefore, the reconstruction accuracy significantly decreases when only a limited number of input images are available. One of the main reasons is the insufficient number of 3D points in the sparse point cloud obtained through Structure from Motion (SfM), which results in a poor initialization for optimizing the Gaussian primitives. We propose a new 3D reconstruction method, called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along with COLMAP MVS to generate highly accurate and dense 3D point clouds, which are then used to initialize 2D Gaussians. Through experiments on the DTU dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of objects using just three images. The project page is available at https://gsisaoki.github.io/SPARSE2DGS/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "sparse-view",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians",
    "authors": [
      "Shuyi Jiang",
      "Qihao Zhao",
      "Hossein Rahmani",
      "De Wen Soh",
      "Jun Liu",
      "Na Zhao"
    ],
    "abstract": "Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel part-aware compositional reconstruction method, called GaussianBlock, that enables semantically coherent and disentangled representations, allowing for precise and physical editing akin to building blocks, while simultaneously maintaining high fidelity. Our GaussianBlock introduces a hybrid representation that leverages the advantages of both primitives, known for their flexible actionability and editability, and 3D Gaussians, which excel in reconstruction quality. Specifically, we achieve semantically coherent primitives through a novel attention-guided centering loss derived from 2D semantic priors, complemented by a dynamic splitting and fusion strategy. Furthermore, we utilize 3D Gaussians that hybridize with primitives to refine structural details and enhance fidelity. Additionally, a binding inheritance strategy is employed to strengthen and maintain the connection between the two. Our reconstructed scenes are evidenced to be disentangled, compositional, and compact across diverse benchmarks, enabling seamless, direct and precise editing while maintaining high quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high quality",
      "compact",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction",
    "authors": [
      "Bin Zhang",
      "Bi Zeng",
      "Zexin Peng"
    ],
    "abstract": "In recent years, Neural Radiance Fields (NeRF) has revolutionized three-dimensional (3D) reconstruction with its implicit representation. Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the implicit representation of neural networks and instead directly represents scenes as point clouds with Gaussian-shaped distributions. While this shift has notably elevated the rendering quality and speed of radiance fields but inevitably led to a significant increase in memory usage. Additionally, effectively rendering dynamic scenes in 3D-GS has emerged as a pressing challenge. To address these concerns, this paper purposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction. Firstly, we use a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements. Subsequently, we introduce a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, thereby further compressing 3D Gaussian model. Finally, motion noise of points is mitigated through static constraints and motion consistency constraints. Experimental results demonstrate that our method surpasses existing approaches in rendering quality and speed, while significantly reducing the memory usage associated with 3D-GS, making it highly suitable for various tasks such as novel view synthesis, and dynamic mapping.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "mapping",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
    "authors": [
      "Seokhun Choi",
      "Hyeonseop Song",
      "Jaechul Kim",
      "Taehyeong Kim",
      "Hoseok Do"
    ],
    "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy. Our project page is available at https://seokhunchoi.github.io/Click-Gaussian",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "segmentation",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-GVINS: A Tightly-integrated GNSS-Visual-Inertial Navigation System Augmented by 3D Gaussian Splatting",
    "authors": [
      "Zelin Zhou",
      "Saurav Uprety",
      "Shichuang Nie",
      "Hongzhou Yang"
    ],
    "abstract": "Recently, the emergence of 3D Gaussian Splatting (3DGS) has drawn significant attention in the area of 3D map reconstruction and visual SLAM. While extensive research has explored 3DGS for indoor trajectory tracking using visual sensor alone or in combination with Light Detection and Ranging (LiDAR) and Inertial Measurement Unit (IMU), its integration with GNSS for large-scale outdoor navigation remains underexplored. To address these concerns, we proposed GS-GVINS: a tightly-integrated GNSS-Visual-Inertial Navigation System augmented by 3DGS. This system leverages 3D Gaussian as a continuous differentiable scene representation in largescale outdoor environments, enhancing navigation performance through the constructed 3D Gaussian map. Notably, GS-GVINS is the first GNSS-Visual-Inertial navigation application that directly utilizes the analytical jacobians of SE3 camera pose with respect to 3D Gaussians. To maintain the quality of 3DGS rendering in extreme dynamic states, we introduce a motionaware 3D Gaussian pruning mechanism, updating the map based on relative pose translation and the accumulated opacity along the camera ray. For validation, we test our system under different driving environments: open-sky, sub-urban, and urban. Both self-collected and public datasets are used for evaluation. The results demonstrate the effectiveness of GS-GVINS in enhancing navigation accuracy across diverse driving environments.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-16",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "tracking",
      "slam",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting",
    "authors": [
      "Hongyun Yu",
      "Zhan Qu",
      "Qihang Yu",
      "Jianchuan Chen",
      "Zhonghua Jiang",
      "Zhiwen Chen",
      "Shengyu Zhang",
      "Jimin Xu",
      "Fei Wu",
      "Chengfei Lv",
      "Gang Yu"
    ],
    "abstract": "Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-22",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "head",
      "ar",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting",
    "authors": [
      "Kangjie Chen",
      "Yingji Zhong",
      "Zhihao Li",
      "Jiaqi Lin",
      "Youyu Chen",
      "Minghan Qin",
      "Haoqian Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel view synthesis under dense-view settings. However, in sparse-view scenarios, despite the realistic renderings in training views, 3DGS occasionally manifests appearance artifacts in novel views. This paper investigates the appearance artifacts in sparse-view 3DGS and uncovers a core limitation of current approaches: the optimized Gaussians are overly-entangled with one another to aggressively fit the training views, which leads to a neglect of the real appearance distribution of the underlying scene and results in appearance artifacts in novel views. The analysis is based on a proposed metric, termed Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians, i.e., co-adaptation, by computing the pixel-wise variance across multiple renderings of the same viewpoint, with different random subsets of Gaussians. The analysis reveals that the degree of co-adaptation is naturally alleviated as the number of training views increases. Based on the analysis, we propose two lightweight strategies to explicitly mitigate the co-adaptation in sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise injection to the opacity. Both strategies are designed to be plug-and-play, and their effectiveness is validated across various methods and benchmarks. We hope that our insights into the co-adaptation effect will inspire the community to achieve a more comprehensive understanding of sparse-view 3DGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "lightweight",
      "ar",
      "gaussian splatting",
      "understanding",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "On Scaling Up 3D Gaussian Splatting Training",
    "authors": [
      "Hexu Zhao",
      "Haoyang Weng",
      "Daohan Lu",
      "Ang Li",
      "Jinyang Li",
      "Aurojit Panda",
      "Saining Xie"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing",
    "authors": [
      "Ri-Zhao Qiu",
      "Ge Yang",
      "Weijia Zeng",
      "Xiaolong Wang"
    ],
    "abstract": "Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "semantic",
      "dynamic",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting",
    "authors": [
      "Sahil Jain",
      "Avik Kuthiala",
      "Prabhdeep Singh Sethi",
      "Prakanshul Saxena"
    ],
    "abstract": "Recent advancements in radiance fields have opened new avenues for creating high-quality 3D assets and scenes. Style transfer can enhance these 3D assets with diverse artistic styles, transforming creative expression. However, existing techniques are often slow or unable to localize style transfer to specific objects. We introduce StyleSplat, a lightweight method for stylizing 3D objects in scenes represented by 3D Gaussians from reference style images. Our approach first learns a photorealistic representation of the scene using 3D Gaussian splatting while jointly segmenting individual 3D objects. We then use a nearest-neighbor feature matching loss to finetune the Gaussians of the selected objects, aligning their spherical harmonic coefficients with the style image to ensure consistency and visual appeal. StyleSplat allows for quick, customizable style transfer and localized stylization of multiple objects within a scene, each with a different style. We demonstrate its effectiveness across various 3D scenes and styles, showcasing enhanced control and customization in 3D creation.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-07-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "lightweight",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module",
    "authors": [
      "Yichen Zhang",
      "Zihan Wang",
      "Jiali Han",
      "Peilin Li",
      "Jiaxun Zhang",
      "Jianqiang Wang",
      "Lei He",
      "Keqiang Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based representations and volumetric rendering techniques, enabling real-time, high-quality rendering. However, 3DGS models typically overfit to single-scene training and are highly sensitive to the initialization of Gaussian ellipsoids, heuristically derived from Structure from Motion (SfM) point clouds, which limits both generalization and practicality. To address these limitations, we propose GS-Net, a generalizable, plug-and-play 3DGS module that densifies Gaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure representation. To the best of our knowledge, GS-Net is the first plug-and-play 3DGS module with cross-scene generalization capabilities. Additionally, we introduce the CARLA-NVS dataset, which incorporates additional camera viewpoints to thoroughly evaluate reconstruction and rendering quality. Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR improvement of 2.08 dB for conventional viewpoints and 1.86 dB for novel viewpoints, confirming the method's effectiveness and robustness.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering",
    "authors": [
      "Yihao Wang",
      "Marcus Klasson",
      "Matias Turkulainen",
      "Shuzhe Wang",
      "Juho Kannala",
      "Arno Solin"
    ],
    "abstract": "Gaussian splatting enables fast novel view synthesis in static 3D environments. However, reconstructing real-world environments remains challenging as distractors or occluders break the multi-view consistency assumption required for accurate 3D reconstruction. Most existing methods rely on external semantic information from pre-trained models, introducing additional computational overhead as pre-processing steps or during optimization. In this work, we propose a novel method, DeSplat, that directly separates distractors and static scene elements purely based on volume rendering of Gaussian primitives. We initialize Gaussians within each camera view for reconstructing the view-specific distractors to separately model the static 3D scene and distractors in the alpha compositing stages. DeSplat yields an explicit scene separation of static elements and distractors, achieving comparable results to prior distractor-free approaches without sacrificing rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data sets for distractor-free novel view synthesis. See the project website at https://aaltoml.github.io/desplat/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-29",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "semantic",
      "head",
      "gaussian splatting",
      "ar",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "abstract": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "geometry",
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "sparse view"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image",
    "authors": [
      "Stanislaw Szymanowicz",
      "Eldar Insafutdinov",
      "Chuanxia Zheng",
      "Dylan Campbell",
      "João F. Henriques",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "We propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a \"foundation\" model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input. Code, models, demo, and more results are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-06-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field",
    "authors": [
      "Jiyang Li",
      "Lechao Cheng",
      "Zhangye Wang",
      "Tingting Mu",
      "Jingxuan He"
    ],
    "abstract": "Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at https://pokerlishao.github.io/LoopGaussian/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "dynamic",
      "motion",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting",
    "authors": [
      "Lingting Zhu",
      "Guying Lin",
      "Jinnan Chen",
      "Xinjie Zhang",
      "Zhenchao Jin",
      "Zhao Wang",
      "Lequan Yu"
    ],
    "abstract": "While Implicit Neural Representations (INRs) have demonstrated significant success in image representation, they are often hindered by large training memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged as a promising solution in 3D reconstruction due to its high-quality novel view synthesis and rapid rendering capabilities, positioning it as a valuable tool for a broad spectrum of applications. In particular, a GS-based representation, 2DGS, has shown potential for image fitting. In our work, we present \\textbf{L}arge \\textbf{I}mages are \\textbf{G}aussians (\\textbf{LIG}), which delves deeper into the application of 2DGS for image representations, addressing the challenge of fitting large images with 2DGS in the situation of numerous Gaussian points, through two distinct modifications: 1) we adopt a variant of representation and optimization strategy, facilitating the fitting of a large number of Gaussian points; 2) we propose a Level-of-Gaussian approach for reconstructing both coarse low-frequency initialization and fine high-frequency details. Consequently, we successfully represent large images as Gaussian points and achieve high-quality large image representation, demonstrating its efficacy across various types of large images. Code is available at {\\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction",
    "authors": [
      "Zhenyang Li",
      "Xiaoyang Bai",
      "Tongchen Zhang",
      "Pengfei Shen",
      "Weiwei Xu",
      "Yifan Peng"
    ],
    "abstract": "High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-07-31",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "dynamic",
      "motion",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "vr",
      "3d gaussian",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field",
    "authors": [
      "Rong Liu",
      "Rui Xu",
      "Yue Hu",
      "Meida Chen",
      "Andrew Feng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (https://rongliu-leo.github.io/AtomGS/).",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh",
    "authors": [
      "Lewis A G Stuart",
      "Michael P Pound"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D reconstructions, but these scenes often require specialised renderers for effective visualisation. In contrast, point clouds are a widely used 3D representation and are compatible with most popular 3D processing software, yet converting 3DGS scenes into point clouds is a complex challenge. In this work we introduce 3DGS-to-PC, a flexible and highly customisable framework that is capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We sample points probabilistically from each Gaussian as a 3D density function. We additionally threshold new points using the Mahalanobis distance to the Gaussian centre, preventing extreme outliers. The result is a point cloud that closely represents the shape encoded into the 3D Gaussian scene. Individual Gaussians use spherical harmonics to adapt colours depending on view, and each point may contribute only subtle colour hints to the resulting rendered scene. To avoid spurious or incorrect colours that do not fit with the final point cloud, we recalculate Gaussian colours via a customised image rendering approach, assigning each Gaussian the colour of the pixel to which it contributes most across all views. 3DGS-to-PC also supports mesh generation through Poisson Surface Reconstruction, applied to points sampled from predicted surface Gaussians. This allows coloured meshes to be generated from 3DGS scenes without the need for re-training. This package is highly customisable and capability of simple integration into existing 3DGS pipelines. 3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud and surface-based formats.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-13",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d reconstruction",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting",
    "authors": [
      "Abhishek Saroha",
      "Florian Hofherr",
      "Mariia Gladkova",
      "Cecilia Curreli",
      "Or Litany",
      "Daniel Cremers"
    ],
    "abstract": "Stylizing a dynamic scene based on an exemplar image is critical for various real-world applications, including gaming, filmmaking, and augmented and virtual reality. However, achieving consistent stylization across both spatial and temporal dimensions remains a significant challenge. Most existing methods are designed for static scenes and often require an optimization process for each style image, limiting their adaptability. We introduce ZDySS, a zero-shot stylization framework for dynamic scenes, allowing our model to generalize to previously unseen style images at inference. Our approach employs Gaussian splatting for scene representation, linking each Gaussian to a learned feature vector that renders a feature map for any given view and timestamp. By applying style transfer on the learned feature vectors instead of the rendered feature map, we enhance spatio-temporal consistency across frames. Our method demonstrates superior performance and coherence over state-of-the-art baselines in tests on real-world dynamic scenes, making it a robust solution for practical applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis",
    "authors": [
      "Pedro Martin",
      "António Rodrigues",
      "João Ascenso",
      "Maria Paula Queluz"
    ],
    "abstract": "Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-18",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "human",
      "geometry",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
    "authors": [
      "Junfu Guo",
      "Yu Xin",
      "Gaoyi Liu",
      "Kai Xu",
      "Ligang Liu",
      "Ruizhen Hu"
    ],
    "abstract": "We tackle the challenge of concurrent reconstruction at the part level with the RGB appearance and estimation of motion parameters for building digital twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method. With two distinct sets of multi-view imagery, each depicting an object in separate static articulation configurations, we reconstruct the articulated object in 3D Gaussian representations with both appearance and geometry information at the same time. Our approach decoupled multiple highly interdependent parameters through a multi-step optimization process, thereby achieving a stable optimization procedure and high-quality outcomes. We introduce ArticulatedGS, a self-supervised, comprehensive framework that autonomously learns to model shapes and appearances at the part level and synchronizes the optimization of motion parameters, all without reliance on 3D supervision, motion cues, or semantic labels. Our experimental results demonstrate that, among comparable methodologies, our approach has achieved optimal outcomes in terms of part segmentation accuracy, motion estimation accuracy, and visual quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "geometry",
      "semantic",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting",
    "authors": [
      "Konstantinos D. Polyzos",
      "Athanasios Bacharis",
      "Saketh Madhuvarasu",
      "Nikos Papanikolopoulos",
      "Tara Javidi"
    ],
    "abstract": "Gaussian splatting (GS) along with its extensions and variants provides outstanding performance in real-time scene rendering while meeting reduced storage demands and computational efficiency. While the selection of 2D images capturing the scene of interest is crucial for the proper initialization and training of GS, hence markedly affecting the rendering performance, prior works rely on passively and typically densely selected 2D images. In contrast, this paper proposes `ActiveInitSplat', a novel framework for active selection of training images for proper initialization and training of GS. ActiveInitSplat relies on density and occupancy criteria of the resultant 3D scene representation from the selected 2D images, to ensure that the latter are captured from diverse viewpoints leading to better scene coverage and that the initialized Gaussian functions are well aligned with the actual 3D structure. Numerical tests on well-known simulated and real environments demonstrate the merits of ActiveInitSplat resulting in significant GS rendering performance improvement over passive GS baselines, in the widely adopted LPIPS, SSIM, and PSNR metrics.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
    "authors": [
      "Bo-Hsu Ke",
      "You-Zhe Xie",
      "Yu-Lun Liu",
      "Wei-Chen Chiu"
    ],
    "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
    "authors": [
      "Weijie Wang",
      "Yeqing Chen",
      "Zeyu Zhang",
      "Hengyu Liu",
      "Haoxiao Wang",
      "Zhiyuan Feng",
      "Wenkang Qin",
      "Zheng Zhu",
      "Donny Y. Chen",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-09-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering",
    "authors": [
      "Shuai Zhang",
      "Huangxuan Zhao",
      "Zhenghong Zhou",
      "Guanjun Wu",
      "Chuansheng Zheng",
      "Xinggang Wang",
      "Wenyu Liu"
    ],
    "abstract": "Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the opacity offsets of the Gaussian, using these opacity-varying Gaussians to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art render quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code is available at https://github.com/hustvl/TOGS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-03-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "sparse view",
      "head",
      "gaussian splatting",
      "real-time rendering",
      "ar",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression",
    "authors": [
      "Yu-Ting Zhan",
      "Cheng-Yuan Ho",
      "Hebi Yang",
      "Yi-Hsin Chen",
      "Jui Chiu Chiang",
      "Yu-Lun Liu",
      "Wen-Hsiao Peng"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D representation. Much research has been focused on reducing its storage requirements and memory footprint. However, the needs to compress and transmit the 3DGS representation to the remote side are overlooked. This new application calls for rate-distortion-optimized 3DGS compression. How to quantize and entropy encode sparse Gaussian primitives in the 3D space remains largely unexplored. Few early attempts resort to the hyperprior framework from learned image compression. But, they fail to utilize fully the inter and intra correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work, termed CAT-3DGS, introduces a context-adaptive triplane approach to their rate-distortion-optimized coding. It features multi-scale triplanes, oriented according to the principal axes of Gaussian primitives in the 3D space, to capture their inter correlation (i.e. spatial correlation) for spatial autoregressive coding in the projected 2D planes. With these triplanes serving as the hyperprior, we further perform channel-wise autoregressive coding to leverage the intra correlation within each individual Gaussian primitive. Our CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively skips from coding those Gaussian primitives that potentially have little impact on the rendering quality. When trained end-to-end to strike a good rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art compression performance on the commonly used real-world datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "gaussian splatting",
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Junuk Cha",
      "Soohyun Hwang",
      "Hyein Hwang",
      "Seungryul Baek"
    ],
    "abstract": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem that can find numerous applications. Despite recent advances, there is no comprehensive pipeline yet for bimanual class-agnostic interaction reconstruction from a monocular RGB video, where two hands and an unknown object are interacting with each other. Previous works tackled the limited hand-object interaction case, where object templates are pre-known or only one hand is involved in the interaction. The bimanual interaction reconstruction exhibits severe occlusions introduced by complex interactions between two hands and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an unknown object from a monocular video. To robustly obtain object Gaussians avoiding severe occlusions, we leverage prior knowledge of pre-trained diffusion model with score distillation sampling (SDS) loss, to reconstruct unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model (i.e., MANO) and share a single Gaussian for two hands to effectively accumulate hand 3D information, given limited views. To further consider the 3D alignment between hands and objects, we include the interacting-subjects optimization step during Gaussian optimization. Our method achieves the state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering quality (PSNR, SSIM, LPIPS), respectively.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-04-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction",
    "authors": [
      "Zhihao Tang",
      "Shenghao Yang",
      "Hongtao Zhang",
      "Mingbo Zhao"
    ],
    "abstract": "Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-05-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "fast",
      "high-fidelity",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis",
    "authors": [
      "Sheng Miao",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Xu Yan",
      "Hongyu Zhou",
      "Yue Wang",
      "Bingbing Liu",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "Novel view synthesis of urban scenes is essential for autonomous driving-related applications.Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixel-aligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feed-forward 3DGS- and NeRF-based methods.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "fast",
      "urban scene",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians",
    "authors": [
      "Dasong Gao",
      "Peter Zhi Xuan Li",
      "Vivienne Sze",
      "Sertac Karaman"
    ],
    "abstract": "Constructing a high-fidelity representation of the 3D scene using a monocular camera can enable a wide range of applications on mobile devices, such as micro-robots, smartphones, and AR/VR headsets. On these devices, memory is often limited in capacity and its access often dominates the consumption of compute energy. Although Gaussian Splatting (GS) allows for high-fidelity reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a large number of past images is stored to retrain Gaussians for reducing catastrophic forgetting. These images often require two-orders-of-magnitude higher memory than the map itself and thus dominate the total memory usage. In this work, we present GEVO, a GS-based monocular SLAM framework that achieves comparable fidelity as prior methods by rendering (instead of storing) them from the existing map. Novel Gaussian initialization and optimization techniques are proposed to remove artifacts from the map and delay the degradation of the rendered images over time. Across a variety of environments, GEVO achieves comparable map fidelity while reducing the memory overhead to around 58 MBs, which is up to 94x lower than prior works.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-14",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "high-fidelity",
      "slam",
      "head",
      "gaussian splatting",
      "ar",
      "vr"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction",
    "authors": [
      "Yihui Li",
      "Chengxin Lv",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "motion",
      "ar",
      "gaussian splatting",
      "3d reconstruction"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction",
    "authors": [
      "Deborah Pintani",
      "Ariel Caputo",
      "Noah Lewis",
      "Marc Stamminger",
      "Fabio Pellacini",
      "Andrea Giachetti"
    ],
    "abstract": "Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis. In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes. In stage two, foreground Gaussians are initialized from a Structure-from-Motion reconstruction, added and refined using the standard rendering loss, while the background set remains fixed but contributes to the final image formation. Experiments on diverse outdoor datasets show that our method reduces background artifacts and improves perceptual quality compared to state-of-the-art baselines. Moreover, the explicit background separation enables automatic, object-free environment map estimation, opening new possibilities for photorealistic outdoor rendering and mixed-reality applications.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-10",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "outdoor",
      "motion",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic World at 343 FPS",
    "authors": [
      "Hao Feng",
      "Hao Sun",
      "Wei Xie",
      "Zhi Zuo",
      "Zhengzhe Liu"
    ],
    "abstract": "While dynamic novel view synthesis from 2D videos has seen progress, achieving efficient reconstruction and rendering of dynamic scenes remains a challenging task. In this paper, we introduce Disentangled 4D Gaussian Splatting (Disentangled4DGS), a novel representation and rendering pipeline that achieves real-time performance without compromising visual fidelity. Disentangled4DGS decouples the temporal and spatial components of 4D Gaussians, avoiding the need for slicing first and four-dimensional matrix calculations in prior methods. By projecting temporal and spatial deformations into dynamic 2D Gaussians and deferring temporal processing, we minimize redundant computations of 4DGS. Our approach also features a gradient-guided flow loss and temporal splitting strategy to reduce artifacts. Experiments demonstrate a significant improvement in rendering speed and quality, achieving 343 FPS when render 1352*1014 resolution images on a single RTX3090 while reducing storage requirements by at least 4.5%. Our approach sets a new benchmark for dynamic novel view synthesis, outperforming existing methods on both multi-view and monocular dynamic scene datasets.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-03-28",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamic",
      "ar",
      "gaussian splatting",
      "4d",
      "deformation"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting",
    "authors": [
      "Zhenyu Bao",
      "Guibiao Liao",
      "Kaichen Zhou",
      "Kanglin Liu",
      "Qing Li",
      "Guoping Qiu"
    ],
    "abstract": "Despite the photorealistic novel view synthesis (NVS) performance achieved by the original 3D Gaussian splatting (3DGS), its rendering quality significantly degrades with sparse input views. This performance drop is mainly caused by the limited number of initial points generated from the sparse input, insufficient supervision during the training process, and inadequate regularization of the oversized Gaussian ellipsoids. To handle these issues, we propose the LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis task. In specific, we propose a loop-based Progressive Gaussian Initialization (PGI) strategy that could iteratively densify the initialized point cloud using the rendered pseudo images during the training process. Then, the sparse and reliable depth from the Structure from Motion, and the window-based dense monocular depth are leveraged to provide precise geometric supervision via the proposed Depth-alignment Regularization (DAR). Additionally, we introduce a novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian ellipsoids leading to large pixel errors. Comprehensive experiments on four datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art methods for sparse-input novel view synthesis, across indoor, outdoor, and object-level scenes with various image resolutions.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-08-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "outdoor",
      "motion",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation",
    "authors": [
      "Luzhou Ge",
      "Xiangyu Zhu",
      "Zhuo Yang",
      "Xuesong Li"
    ],
    "abstract": "In real-world scenarios, environment changes caused by human or agent activities make it extremely challenging for robots to perform various long-term tasks. Recent works typically struggle to effectively understand and adapt to dynamic environments due to the inability to update their environment representations in memory according to environment changes and lack of fine-grained reconstruction of the environments. To address these challenges, we propose DynamicGSG, a dynamic, high-fidelity, open-vocabulary scene graph construction system leveraging Gaussian splatting. DynamicGSG builds hierarchical scene graphs using advanced vision language models to represent the spatial and semantic relationships between objects in the environments, utilizes a joint feature loss we designed to supervise Gaussian instance grouping while optimizing the Gaussian maps, and locally updates the Gaussian scene graphs according to real environment changes for long-term environment adaptation. Experiments and ablation studies demonstrate the performance and efficacy of our proposed method in terms of semantic segmentation, language-guided object retrieval, and reconstruction quality. Furthermore, we validate the dynamic updating capabilities of our system in real laboratory environments. The source code and supplementary experimental materials will be released at:~\\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/Dynamic-GSG}.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-02-21",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "human",
      "high-fidelity",
      "semantic",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Human101: Training 100+FPS Human Gaussians in 100s from 1 View",
    "authors": [
      "Mingwei Li",
      "Jiachen Tao",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "abstract": "Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2023-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "human",
      "animation",
      "high-fidelity",
      "body",
      "dynamic",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D Boundary Precision",
    "authors": [
      "Zehao Li",
      "Wenwei Han",
      "Yujun Cai",
      "Hao Jiang",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Zhaoqi Wang"
    ],
    "abstract": "While 3D Gaussian Splatting enables high-quality real-time rendering, existing Gaussian-based frameworks for 3D semantic segmentation still face significant challenges in boundary recognition accuracy. To address this, we propose a novel 3DGS-based framework named GradiSeg, incorporating Identity Encoding to construct a deeper semantic understanding of scenes. Our approach introduces two key modules: Identity Gradient Guided Densification (IGD) and Local Adaptive K-Nearest Neighbors (LA-KNN). The IGD module supervises gradients of Identity Encoding to refine Gaussian distributions along object boundaries, aligning them closely with boundary contours. Meanwhile, the LA-KNN module employs position gradients to adaptively establish locality-aware propagation of Identity Encodings, preventing irregular Gaussian spreads near boundaries. We validate the effectiveness of our method through comprehensive experiments. Results show that GradiSeg effectively addresses boundary-related issues, significantly improving segmentation accuracy without compromising scene reconstruction quality. Furthermore, our method's robust segmentation capability and decoupled Identity Encoding representation make it highly suitable for various downstream scene editing tasks, including 3D object removal, swapping and so on.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "semantic",
      "recognition",
      "understanding",
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization",
    "authors": [
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Lei Huang",
      "Lin Gu",
      "Jin Zheng",
      "Xiao Bai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting rendering. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields, we observe that the two radiance fields exhibit point disagreement and rendering disagreement that can unsupervisedly predict reconstruction quality, stemming from the randomness of densification implementation. We further quantify the two disagreements and demonstrate the negative correlation between them and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurate and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "geometry",
      "compact",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery",
    "authors": [
      "Hongsheng Wang",
      "Weiyue Zhang",
      "Sihao Liu",
      "Xinrui Zhou",
      "Jing Li",
      "Zhanyun Tang",
      "Shengyu Zhang",
      "Fei Wu",
      "Feng Lin"
    ],
    "abstract": "Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-05-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human",
      "high-fidelity",
      "semantic",
      "body",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "RenderWorld: World Model with Self-Supervised 3D Label",
    "authors": [
      "Ziyang Yan",
      "Wenzhen Dong",
      "Yihua Shao",
      "Yuhang Lu",
      "Liu Haiyang",
      "Jingwen Liu",
      "Haozhe Wang",
      "Zhe Wang",
      "Yan Wang",
      "Fabio Remondino",
      "Yuexin Ma"
    ],
    "abstract": "End-to-end autonomous driving with vision-only is not only more cost-effective compared to LiDAR-vision fusion but also more reliable than traditional methods. To achieve a economical and robust purely visual autonomous driving system, we propose RenderWorld, a vision-only end-to-end autonomous driving framework, which generates 3D occupancy labels using a self-supervised gaussian-based Img2Occ Module, then encodes the labels by AM-VAE, and uses world model for forecasting and planning. RenderWorld employs Gaussian Splatting to represent 3D scenes and render 2D images greatly improves segmentation accuracy and reduces GPU memory consumption compared with NeRF-based methods. By applying AM-VAE to encode air and non-air separately, RenderWorld achieves more fine-grained scene element representation, leading to state-of-the-art performance in both 4D occupancy forecasting and motion planning from autoregressive world model.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-09-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "4d",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution",
    "authors": [
      "Shuja Khalid",
      "Mohamed Ibrahim",
      "Yang Liu"
    ],
    "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-06-09",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "high-fidelity",
      "dynamic",
      "ar",
      "gaussian splatting",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors",
    "authors": [
      "Minsu Kim",
      "Subin Jeon",
      "In Cho",
      "Mijin Yoo",
      "Seon Joo Kim"
    ],
    "abstract": "Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-08-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting",
      "real-time rendering",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Robust Gaussian Splatting",
    "authors": [
      "François Darmon",
      "Lorenzo Porzi",
      "Samuel Rota-Bulò",
      "Peter Kontschieder"
    ],
    "abstract": "In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2024-04-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "motion",
      "ar",
      "gaussian splatting",
      "nerf",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "Towards Better Robustness: Pose-Free 3D Gaussian Splatting for Arbitrarily Long Videos",
    "authors": [
      "Zhen-Hui Dong",
      "Sheng Ye",
      "Yu-Hui Wen",
      "Nannan Li",
      "Yong-Jin Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to its efficiency and high-fidelity rendering. 3DGS training requires a known camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax this restriction but still face difficulties when handling long sequences with complex camera trajectories. In this paper, we propose Rob-GS, a robust framework to progressively estimate camera poses and optimize 3DGS for arbitrarily long video inputs. In particular, by leveraging the inherent continuity of videos, we design an adjacent pose tracking method to ensure stable pose estimation between consecutive frames. To handle arbitrarily long inputs, we propose a Gaussian visibility retention check strategy to adaptively split the video sequence into several segments and optimize them separately. Extensive experiments on Tanks and Temples, ScanNet, and a self-captured dataset show that Rob-GS outperforms the state-of-the-arts.",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "motion",
      "ar",
      "gaussian splatting",
      "face",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": ""
  },
  {
    "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
    "authors": [
      "Sung-Yeon Park",
      "Adam Lee",
      "Juanwu Lu",
      "Can Cui",
      "Luyang Jiang",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ahmadreza Moradipari",
      "Ziran Wang"
    ],
    "abstract": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
    "arxiv_url": "",
    "pdf_url": "",
    "published_date": "2025-10-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "motion",
      "ar",
      "gaussian splatting",
      "4d"
    ],
    "citations": 0,
    "semantic_url": ""
  }
]